{
  "hash": "1edfb2507a0ae4ff2cf77e675d253ff9",
  "result": {
    "markdown": "---\nextype: string\nexsolution: NA\nexname: tengku01\nexpoints: 1\ncategories:\n- tidymodels\n- prediction\n- yacsda\n- statlearning\n- trees\n- speed\n- string\ndate: '2023-05-17'\nslug: Tengku-Hanis01\ntitle: Tengku-Hanis01\n\n---\n\n\n\n\n\n\n\n# Aufgabe\n\nBearbeiten Sie [diese Fallstudie von Tengku Hanis](https://tengkuhanis.netlify.app/post/hyperparameter-tuning-in-tidymodels/)!\n\n\n\n\n\n\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n\n# Lösung\n\nDie folgende Lösung basiert auf der oben angegebenen Fallstudie.\n\n\nPakete laden:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.0 ──\n✔ broom        1.0.4     ✔ rsample      1.1.1\n✔ dials        1.2.0     ✔ tune         1.1.1\n✔ infer        1.0.4     ✔ workflows    1.1.3\n✔ modeldata    1.1.0     ✔ workflowsets 1.0.1\n✔ parsnip      1.1.0     ✔ yardstick    1.2.0\n✔ recipes      1.0.6     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n```\n:::\n\n```{.r .cell-code}\nlibrary(finetune)\n```\n:::\n\n\nDaten importieren:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(income, package = \"kernlab\")\n```\n:::\n\n\n\nDatensatz vereinfachen:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2021)\nincome2 <- \n  income %>% \n  filter(INCOME == \"[75.000-\" | INCOME == \"[50.000-75.000)\") %>% \n  slice_sample(n = 600) %>% \n  mutate(INCOME = fct_drop(INCOME), \n         INCOME = fct_recode(INCOME, \n                             rich = \"[75.000-\",\n                             less_rich = \"[50.000-75.000)\"), \n         INCOME = factor(INCOME, ordered = F)) %>% \n  mutate(across(-INCOME, fct_drop))\n```\n:::\n\n\n\nCheck:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDataExplorer::plot_missing(income)\n```\n\n::: {.cell-output-display}\n![](Tengku-Hanis01_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n`{DataExplorer}` sieht nach einem nützlichen Paket aus. Check it out [hier](https://cran.r-project.org/web/packages/DataExplorer/vignettes/dataexplorer-intro.html)!\n\n\n\nDaten aufteilen (\"Spending our data budget\"):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2021)\ndat_index <- initial_split(income2, strata = INCOME)\ndat_train <- training(dat_index)\ndat_test <- testing(dat_index)\n```\n:::\n\n\nKreuzvalidierung:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2021)\ndat_cv <- vfold_cv(dat_train, v = 10, repeats = 1, strata = INCOME)\n```\n:::\n\n\n\n\n\nRezept:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_rec <- \n  recipe(INCOME ~ ., data = dat_train) %>% \n  step_impute_mode(all_predictors()) %>% \n  step_ordinalscore(AGE, EDUCATION, AREA, HOUSEHOLD.SIZE, UNDER18)\n```\n:::\n\n\n\n\nAls Modell (im engeren Sinne) nutzen wir ein Random-Forest-Modell:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_mod <- \n  rand_forest(mtry = tune(),\n              trees = tune(),\n              min_n = tune()) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"ranger\")\n```\n:::\n\n\n\nWie man sieht, geben wir 3 Tuningparameter an.\n\n\nModell und Rezept zum Workflow zusammenfassen:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_wf <- \n  workflow() %>% \n  add_recipe(dat_rec) %>% \n  add_model(rf_mod)\n```\n:::\n\n\n\n\n\nTuning Grids definieren:\n\nWichtig ist, dass wir genau die Parameter angeben im Grid, die wir auch zum Tunen getaggt haben.\nDas kann man händisch erledigen:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Regular grid:\nreg_grid <- grid_regular(mtry(c(1, 13)), \n                         trees(), \n                         min_n(), \n                         levels = 3)\n\n# Random grid mit 100 Kandidaten:\nrand_grid <- grid_random(mtry(c(1, 13)), \n                         trees(), \n                         min_n(), \n                         size = 100)\n```\n:::\n\n\n\n\n\nWir speichern die Vorhersagen aller Folds im Train-Sample,\num die Modellgüte im Train- bzw. Validierungssample anschauen zu können:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nctrl <- control_grid(save_pred = T,\n                     extract = extract_model)\nmeasure <- metric_set(roc_auc)\n```\n:::\n\n\nAußerdem haben wir als Gütemaß `roc_auc` definiert.\n\nIn der Fallstudie wurde noch `extract = extract_model` bei `control_grid()` ergänzt. \nDas lassen wir der Einfachheit halber mal weg.\n\n\nParallelisieren auf mehreren Kernen,\num Rechenzeit zu sparen:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(doParallel)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: foreach\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'foreach'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: iterators\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: parallel\n```\n:::\n\n```{.r .cell-code}\n# Create a cluster object and then register: \ncl <- makePSOCKcluster(4)\nregisterDoParallel(cl)\n```\n:::\n\n\n\nWie viele CPUs hat mein Computer? \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndetectCores(logical = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4\n```\n:::\n:::\n\n\n\n\nJetzt geht's ab: Tuning und Fitting!\n\nHier das \"reguläre Gitter\" an Tuningkandidaten:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2021)\ntune_regular <- \n  rf_wf %>% \n  tune_grid(\n    resamples = dat_cv, \n    grid = reg_grid,         \n    control = ctrl, \n    metrics = measure)\n\nstopCluster(cl)\n```\n:::\n\n\n\n\nDie Modellgüte im Vergleich zwischen den Tuning-Kandidaten kann man sich schön ausgeben lassen:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(tune_regular)\n```\n\n::: {.cell-output-display}\n![](Tengku-Hanis01_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nGeht aber nur, wenn man oben gesagt hat, dass man die Predictions speichern möchte.\n\nWelche Kandidatin war am besten:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(tune_regular)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     7  2000     2 roc_auc binary     0.690    10  0.0178 Preprocessor1_Model08\n2     7  2000    40 roc_auc binary     0.689    10  0.0184 Preprocessor1_Model26\n3     7  1000     2 roc_auc binary     0.688    10  0.0162 Preprocessor1_Model05\n4    13  1000    21 roc_auc binary     0.687    10  0.0155 Preprocessor1_Model15\n5     7  2000    21 roc_auc binary     0.687    10  0.0161 Preprocessor1_Model17\n```\n:::\n:::\n\n\n\nSo kann man sich die beste Kandidatin anschauen:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(tune_regular) %>% \n  arrange(-mean) %>% \n  slice(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1     7  2000     2 roc_auc binary     0.690    10  0.0178 Preprocessor1_Model08\n```\n:::\n:::\n\n\n\nAber man kann sich auch von Tidymodels einfach die beste Kandidatin sagen lassen:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_rf <-\n  select_best(tune_regular, \"roc_auc\")\n```\n:::\n\n\n\nAuf dieser Basis können wir jetzt den\nWorkflow finalisieren, also die Tuningparameter einfüllen:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_wf <- \n  rf_wf %>% \n  finalize_workflow(best_rf)\nfinal_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_impute_mode()\n• step_ordinalscore()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = 7\n  trees = 2000\n  min_n = 2\n\nComputational engine: ranger \n```\n:::\n:::\n\n\n\nUnd mit diesen Werten den ganzen Train-Datensatz fitten:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_fit <- \n  final_wf %>%\n  last_fit(dat_index) \n```\n:::\n\n\n\nWie gut ist das jetzt?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_fit %>%\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.576 Preprocessor1_Model1\n2 roc_auc  binary         0.599 Preprocessor1_Model1\n```\n:::\n:::\n\n\n\n\n\n\n\n\n---\n\nCategories: \n\n- tidymodels\n- prediction\n- yacsda\n- statlearning\n- trees\n- speed\n- string\n\n",
    "supporting": [
      "Tengku-Hanis01_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}