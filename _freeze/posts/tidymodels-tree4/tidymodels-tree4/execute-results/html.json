{
  "hash": "eaf09234d16889d6b1d71b1658d4998d",
  "result": {
    "engine": "knitr",
    "markdown": "---\nexname: tidymodels-tree4\nexpoints: 1\nextype: string\nexsolution: NA\ncategories:\n- statlearning\n- trees\n- tidymodels\n- speed\n- string\ndate: '2023-11-08'\nslug: tidymodels-tree4\ntitle: tidymodels-tree4\n\n---\n\n\n\n\n\n\n\n\n# Aufgabe\n\n\nBerechnen Sie folgendes einfache Modell:\n\n1. Entscheidungsbaum\n\n\nModellformel: `body_mass_g ~ .` (Datensatz `palmerpenguins::penguins`)\n\nHier geht es darum, die Geschwindigkeit (und den Ressourcenverbrauch) beim Fitten zu verringern.\nBenutzen Sie dazu folgende Methoden\n\n- Auslassen gering performanter Tuningparameterwerte\n- Verwenden Sie ein Anova-Grid-Search!\n- Parallelisieren Sie auf mehrere Kerne (wenn möglich).\n\n\nHinweise:\n\n- Tunen Sie alle Parameter (die der Engine anbietet). \n- Verwenden Sie Defaults, wo nicht anders angegeben.\n- Beachten Sie die [üblichen Hinweise](https://datenwerk.netlify.app/hinweise).\n\n\n\n\n\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n\n# Lösung\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n✔ broom        1.0.5     ✔ recipes      1.0.8\n✔ dials        1.2.0     ✔ rsample      1.2.0\n✔ dplyr        1.1.3     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n```\n\n\n:::\n\n```{.r .cell-code}\ndata(\"penguins\", package = \"palmerpenguins\")\nlibrary(tictoc)  # Zeitmessung\nlibrary(finetune)  # tune_race_anova\nlibrary(doParallel)  # mehrere CPUs nutzen \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: foreach\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'foreach'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: iterators\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: parallel\n```\n\n\n:::\n\n```{.r .cell-code}\nset.seed(42)\n```\n:::\n\n\n\n\nEntfernen wir Fälle ohne y-Wert:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <-\n  penguins %>% \n  drop_na(body_mass_g)\n```\n:::\n\n\n\n\n## Daten teilen\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nd_split <- initial_split(d)\nd_train <- training(d_split)\nd_test <- testing(d_split)\n```\n:::\n\n\n\n## Modell(e)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_tree <-\n  decision_tree(mode = \"regression\",\n                cost_complexity = tune(),\n                tree_depth = tune(),\n                min_n = tune())\n```\n:::\n\n\n\n\n\n## Rezept(e)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_plain <- \n  recipe(body_mass_g ~ ., data = d_train)\n```\n:::\n\n\n\n\n## Resampling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nrsmpl <- vfold_cv(d_train)\n```\n:::\n\n\n\n## Workflows\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_tree <-\n  workflow() %>%  \n  add_recipe(rec_plain) %>% \n  add_model(mod_tree)\n```\n:::\n\n\n\n\n\n\n## Tuning-Grid\n\nTuninggrid:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntune_grid <- grid_regular(extract_parameter_set_dials(mod_tree), levels = 5)\n```\n:::\n\n\n\n*Hinweis*: Andere Arten von Tuning-Grids sind sinnvoller,\nhier ist nur zum Vergleich mit anderen Aufgaben diese Form des Tuning-Grids gewählt.\n\nDie Zeilen im Tuninggrid zeigen uns, für wie viele Modellparameter ein Modell berechnet wird.\nNatürlich üblicherweise jedes Modell mit Resampling.\nDa kommt in Summe ein mitunter sehr große Menge an Modellberechnungen zusammen.\n\n\n## Ohne Speed-up\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nfit_tree <-\n  tune_grid(object = wf_tree,\n            grid = tune_grid,\n            metrics = metric_set(rmse),\n            resamples = rsmpl)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n78.464 sec elapsed\n```\n\n\n:::\n:::\n\n\nDie angegebene Rechenzeit bezieht sich auf einen 4-Kerne-MacBook Pro (2020).\n\n\n## Mit Speeed-up 1\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nfit_tree2 <-\n  tune_race_anova(object = wf_tree,\n            grid = tune_grid,\n            metrics = metric_set(rmse),\n            control = control_race(verbose = FALSE,\n                                   pkgs = c(\"tidymodels\"),\n                                   save_pred = TRUE),\n            resamples = rsmpl)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n72.066 sec elapsed\n```\n\n\n:::\n:::\n\n\n\n## Mit Speeed-up 2\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoParallel::registerDoParallel()\n\ntic()\nfit_tree2 <-\n  tune_race_anova(\n    object = wf_tree,\n    grid = tune_grid,\n    metrics = metric_set(rmse),\n    control = control_race(verbose = FALSE,\n                           pkgs = c(\"tidymodels\"),\n                           save_pred = TRUE),\n            resamples = rsmpl)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n27.524 sec elapsed\n```\n\n\n:::\n:::\n\n\n\n\n\n## Mit Speeed-up 3\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoParallel::registerDoParallel()\n\ntic()\nfit_tree2 <-\n  tune_grid(object = wf_tree,\n            grid = tune_grid,\n            metrics = metric_set(rmse),\n            control = control_grid(verbose = FALSE,\n                                   save_pred = TRUE),\n            resamples = rsmpl)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n27.312 sec elapsed\n```\n\n\n:::\n:::\n\n\n\n## Fazit\n\nMit Speed-up ist schneller also ohne.\nHier haben wir einen Entscheidungsbaum berechnet, der ist nicht so sehr parallelisierbar. \nBei einem \"Wald-Modell\", wie Random Forests, sollte der Vorteil der Parallisierung viel deutlich sein.\n\n\n\n\n\n\n\n---\n\nCategories: \n\n- statlearning\n- trees\n- tidymodels\n- speed\n- string\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}