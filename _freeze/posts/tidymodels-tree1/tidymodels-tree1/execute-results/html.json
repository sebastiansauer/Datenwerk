{
  "hash": "f04787954431e8c23248f72d14bff445",
  "result": {
    "markdown": "---\nexname: tidymodels-tree1\nexpoints: 1\nextype: string\nexsolution: NA\ncategories:\n- stat-learning\n- trees\n- tidymodels\n- string\ndate: '2023-05-09'\nslug: tidymodels-tree1\ntitle: tidymodels-tree1\n\n---\n\n\n\n\n\n\n\n# Aufgabe\n\n\nBerechnen Sie folgende prädiktiven Modelle und vergleichen Sie die Modellgüte:\n\n1. Entscheidungsbaum\n2. Bagging (Bootstrap-Bäume)\n\n\nModellformel: `am ~ .` (Datensatz `mtcars`)\n\nBerichten Sie die Modellgüte (ROC-AUC).\n\nHinweise:\n\n- Tunen Sie alle Parameter (die der Engine anbietet). \n- Verwenden Sie Defaults, wo nicht anders angegeben.\n- Führen Sie eine $v=2$-fache Kreuzvalidierung durch (weil die Stichprobe so klein ist).\n- Beachten Sie die [üblichen Hinweise](https://datenwerk.netlify.app/hinweise).\n\n\n\n\n\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n\n# Lösung\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ broom        1.0.4     ✔ recipes      1.0.5\n✔ dials        1.2.0     ✔ rsample      1.1.1\n✔ dplyr        1.1.1     ✔ tibble       3.2.1\n✔ ggplot2      3.4.2     ✔ tidyr        1.3.0\n✔ infer        1.0.4     ✔ tune         1.1.0\n✔ modeldata    1.1.0     ✔ workflows    1.1.3\n✔ parsnip      1.0.4     ✔ workflowsets 1.0.0\n✔ purrr        1.0.1     ✔ yardstick    1.1.0\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n```\n:::\n\n```{.r .cell-code}\ndata(mtcars)\nlibrary(tictoc)  # Zeitmessung\nlibrary(baguette)\n```\n:::\n\n\n\nFür Klassifikation verlangt Tidymodels eine nominale AV, keine numerische:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmtcars <-\n  mtcars %>% \n  mutate(am = factor(am))\n```\n:::\n\n\n\n\n## Daten teilen\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_split <- initial_split(mtcars)\nd_train <- training(d_split)\nd_test <- testing(d_split)\n```\n:::\n\n\n\n## Modell(e)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_tree <-\n  decision_tree(mode = \"classification\",\n                cost_complexity = tune(),\n                tree_depth = tune(),\n                min_n = tune())\n\nmod_bag <-\n  bag_tree(mode = \"classification\",\n           cost_complexity = tune(),\n           tree_depth = tune(),\n           min_n = tune())\n```\n:::\n\n\n\n\n\n## Rezept(e)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_plain <- \n  recipe(am ~ ., data = d_train)\n```\n:::\n\n\n\n\n## Resampling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrsmpl <- vfold_cv(d_train, v = 2)\n```\n:::\n\n\n\n## Workflows\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_tree <-\n  workflow() %>%  \n  add_recipe(rec_plain) %>% \n  add_model(mod_tree)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_bag <-\n  workflow() %>%  \n  add_recipe(rec_plain) %>% \n  add_model(mod_bag)\n```\n:::\n\n\n\n\n\n\n## Tuning/Fitting\n\nTuninggrid:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntune_grid <- grid_regular(extract_parameter_set_dials(mod_tree), levels = 5)\ntune_grid\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 125 × 3\n   cost_complexity tree_depth min_n\n             <dbl>      <int> <int>\n 1    0.0000000001          1     2\n 2    0.0000000178          1     2\n 3    0.00000316            1     2\n 4    0.000562              1     2\n 5    0.1                   1     2\n 6    0.0000000001          4     2\n 7    0.0000000178          4     2\n 8    0.00000316            4     2\n 9    0.000562              4     2\n10    0.1                   4     2\n# ℹ 115 more rows\n```\n:::\n:::\n\n\nDa beide Modelle die gleichen Tuningparameter aufweisen,\nbrauchen wir nur ein Grid zu erstellen.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nfit_tree <-\n  tune_grid(object = wf_tree,\n            grid = tune_grid,\n            metrics = metric_set(roc_auc),\n            resamples = rsmpl)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n→ A | warning: 21 samples were requested but there were 12 rows in the data. 12 will be used.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThere were issues with some computations   A: x1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThere were issues with some computations   A: x13\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n→ B | warning: 30 samples were requested but there were 12 rows in the data. 12 will be used.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThere were issues with some computations   A: x13\nThere were issues with some computations   A: x25   B: x9\n→ C | warning: 40 samples were requested but there were 12 rows in the data. 12 will be used.\nThere were issues with some computations   A: x25   B: x9\nThere were issues with some computations   A: x25   B: x25   C: x5\nThere were issues with some computations   A: x25   B: x25   C: x25\nThere were issues with some computations   A: x26   B: x25   C: x25\nThere were issues with some computations   A: x30   B: x25   C: x25\nThere were issues with some computations   A: x50   B: x27   C: x25\nThere were issues with some computations   A: x50   B: x48   C: x25\nThere were issues with some computations   A: x50   B: x50   C: x46\nThere were issues with some computations   A: x50   B: x50   C: x50\n```\n:::\n\n```{.r .cell-code}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n32.392 sec elapsed\n```\n:::\n\n```{.r .cell-code}\nfit_tree\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Tuning results\n# 2-fold cross-validation \n# A tibble: 2 × 4\n  splits          id    .metrics           .notes           \n  <list>          <chr> <list>             <list>           \n1 <split [12/12]> Fold1 <tibble [125 × 7]> <tibble [75 × 3]>\n2 <split [12/12]> Fold2 <tibble [125 × 7]> <tibble [75 × 3]>\n\nThere were issues with some computations:\n\n  - Warning(s) x50: 21 samples were requested but there were 12 rows in the data. 12 ...\n  - Warning(s) x50: 30 samples were requested but there were 12 rows in the data. 12 ...\n  - Warning(s) x50: 40 samples were requested but there were 12 rows in the data. 12 ...\n\nRun `show_notes(.Last.tune.result)` for more information.\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nfit_bag <-\n  tune_grid(object = wf_bag,\n            grid = tune_grid,\n            metrics = metric_set(roc_auc),\n            resamples = rsmpl)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n→ A | warning: There were 11 warnings in `dplyr::mutate()`.\n               The first warning was:\n               ℹ In argument: `model = iter(...)`.\n               Caused by warning:\n               ! 21 samples were requested but there were 12 rows in the data. 12 will be used.\n               ℹ Run `dplyr::last_dplyr_warnings()` to see the 10 remaining warnings.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThere were issues with some computations   A: x1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThere were issues with some computations   A: x4\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThere were issues with some computations   A: x8\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThere were issues with some computations   A: x11\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThere were issues with some computations   A: x14\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThere were issues with some computations   A: x17\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThere were issues with some computations   A: x20\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThere were issues with some computations   A: x24\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n→ B | warning: There were 11 warnings in `dplyr::mutate()`.\n               The first warning was:\n               ℹ In argument: `model = iter(...)`.\n               Caused by warning:\n               ! 30 samples were requested but there were 12 rows in the data. 12 will be used.\n               ℹ Run `dplyr::last_dplyr_warnings()` to see the 10 remaining warnings.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThere were issues with some computations   A: x24\nThere were issues with some computations   A: x25   B: x2\nThere were issues with some computations   A: x25   B: x5\nThere were issues with some computations   A: x25   B: x9\nThere were issues with some computations   A: x25   B: x12\nThere were issues with some computations   A: x25   B: x15\nThere were issues with some computations   A: x25   B: x19\nThere were issues with some computations   A: x25   B: x22\nThere were issues with some computations   A: x25   B: x25\n→ C | warning: There were 11 warnings in `dplyr::mutate()`.\n               The first warning was:\n               ℹ In argument: `model = iter(...)`.\n               Caused by warning:\n               ! 40 samples were requested but there were 12 rows in the data. 12 will be used.\n               ℹ Run `dplyr::last_dplyr_warnings()` to see the 10 remaining warnings.\nThere were issues with some computations   A: x25   B: x25\nThere were issues with some computations   A: x25   B: x25   C: x4\nThere were issues with some computations   A: x25   B: x25   C: x7\nThere were issues with some computations   A: x25   B: x25   C: x10\nThere were issues with some computations   A: x25   B: x25   C: x13\nThere were issues with some computations   A: x25   B: x25   C: x16\nThere were issues with some computations   A: x25   B: x25   C: x20\nThere were issues with some computations   A: x25   B: x25   C: x23\nThere were issues with some computations   A: x26   B: x25   C: x25\nThere were issues with some computations   A: x27   B: x25   C: x25\nThere were issues with some computations   A: x31   B: x25   C: x25\nThere were issues with some computations   A: x34   B: x25   C: x25\nThere were issues with some computations   A: x37   B: x25   C: x25\nThere were issues with some computations   A: x40   B: x25   C: x25\nThere were issues with some computations   A: x43   B: x25   C: x25\nThere were issues with some computations   A: x47   B: x25   C: x25\nThere were issues with some computations   A: x50   B: x25   C: x25\nThere were issues with some computations   A: x50   B: x29   C: x25\nThere were issues with some computations   A: x50   B: x32   C: x25\nThere were issues with some computations   A: x50   B: x35   C: x25\nThere were issues with some computations   A: x50   B: x38   C: x25\nThere were issues with some computations   A: x50   B: x42   C: x25\nThere were issues with some computations   A: x50   B: x45   C: x25\nThere were issues with some computations   A: x50   B: x48   C: x25\nThere were issues with some computations   A: x50   B: x50   C: x26\nThere were issues with some computations   A: x50   B: x50   C: x29\nThere were issues with some computations   A: x50   B: x50   C: x33\nThere were issues with some computations   A: x50   B: x50   C: x36\nThere were issues with some computations   A: x50   B: x50   C: x40\nThere were issues with some computations   A: x50   B: x50   C: x43\nThere were issues with some computations   A: x50   B: x50   C: x46\nThere were issues with some computations   A: x50   B: x50   C: x50\nThere were issues with some computations   A: x50   B: x50   C: x50\n```\n:::\n\n```{.r .cell-code}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n213.226 sec elapsed\n```\n:::\n:::\n\n\n## Bester Kandidat\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(fit_tree)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 9\n  cost_complexity tree_depth min_n .metric .estimator  mean     n std_err\n            <dbl>      <int> <int> <chr>   <chr>      <dbl> <int>   <dbl>\n1    0.0000000001          1    11 roc_auc binary     0.806     2  0.0278\n2    0.0000000178          1    11 roc_auc binary     0.806     2  0.0278\n3    0.00000316            1    11 roc_auc binary     0.806     2  0.0278\n4    0.000562              1    11 roc_auc binary     0.806     2  0.0278\n5    0.1                   1    11 roc_auc binary     0.806     2  0.0278\n# ℹ 1 more variable: .config <chr>\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(fit_bag)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 9\n  cost_complexity tree_depth min_n .metric .estimator  mean     n std_err\n            <dbl>      <int> <int> <chr>   <chr>      <dbl> <int>   <dbl>\n1    0.0000000178         11     2 roc_auc binary     0.972     2  0.0278\n2    0.000562             11     2 roc_auc binary     0.972     2  0.0278\n3    0.0000000001          4     2 roc_auc binary     0.963     2  0.0370\n4    0.0000000178          4     2 roc_auc binary     0.963     2  0.0370\n5    0.000562              1    21 roc_auc binary     0.942     2  0.0162\n# ℹ 1 more variable: .config <chr>\n```\n:::\n:::\n\n\n\nBagging erzielte eine klar bessere Modellgüte (in den Validierungssamples) als das Entscheidungsbaum-Modell.\n\n\n## Finalisieren\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_best_finalized <-\n  wf_bag %>% \n  finalize_workflow(select_best(fit_bag))\n```\n:::\n\n\n\n## Last Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_fit <- \n  last_fit(object = wf_best_finalized, d_split)\n\ncollect_metrics(final_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary             1 Preprocessor1_Model1\n2 roc_auc  binary             1 Preprocessor1_Model1\n```\n:::\n:::\n\n\nWie man sieht, ist die Modellgüte im Test-Sample schlechter als in den Train- bzw. Validierungssamples; ein typischer Befund.\n\n\n\n\n\n---\n\nCategories: \n\n- stat-learning\n- trees\n- tidymodels\n- string\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}