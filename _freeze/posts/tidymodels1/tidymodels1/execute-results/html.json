{
  "hash": "78b819e524f48fb0b9e5fdb90417dc36",
  "result": {
    "markdown": "---\nexname: tidymodels1\nextype: schoice\nexsolution: 100000\nexshuffle: 5\nexoints: 1\ncategories:\n- ds1\n- tidymodels\n- prediction\n- yacsda\n- statlearning\n- dyn\n- schoice\ndate: '2023-05-17'\nslug: tidymodels1\ntitle: tidymodels1\n\n---\n\n\n\n\n\n\n\n\n\n\n\n# Aufgabe\n\nProf. Süß übt sich im statistischen Lernen. Dazu will er das Überleben im Titanic-Unglück Vorhersagen; es handelt sich um eine klassische Aufgabe im statistischen Lernen. Betrachten Sie dazu den folgenden R-Code sowie die Kommentare dazu. Wählen Sie die am besten passende Aussage.\n\n\nZuerst lädt er die nötigen R-Pakete:\n\n\n::: {.cell hash='tidymodels1_cache/html/unnamed-chunk-1_98bfa215a56e602954f9752adbe3cd78'}\n\n```{.r .cell-code}\nlibrary(tidyverse)  # data wrangling\nlibrary(tidymodels)  # modelling\nlibrary(broom)  # tidy model output\nlibrary(parallel)  # multiple cores -- *nix only, d.h. Mac und Linux\nlibrary(finetune)  # tune race anova\n```\n:::\n\n\n\nDann initialisiert er die Anzahl der Prozessoren auf seinem Computer:\n\n\n::: {.cell hash='tidymodels1_cache/html/unnamed-chunk-2_9e790703bb73681c1e3ca5167c43b6df'}\n\n```{.r .cell-code}\ncores <- parallel::detectCores(logical = FALSE)\ncores\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4\n```\n:::\n:::\n\n\n\nDaten importieren:\n\n\n\n::: {.cell hash='tidymodels1_cache/html/unnamed-chunk-3_f43b9efbbe36582f800fc879ddb32ac2'}\n\n```{.r .cell-code}\ndata_path <- \"https://raw.githubusercontent.com/sebastiansauer/Lehre\"\ntraindata_path_url  <- \"/main/data/titanic/titanic_train.csv\"\ntestdata_path_url <- \"/main/data/titanic/titanic_test.csv\"\n\ntraindata_url <- paste0(data_path, traindata_path_url)\ntestdata_url <- paste0(data_path, testdata_path_url)\n\n\n# import the data:\ntrain_raw <- read_csv(traindata_url)\ntest <- read_csv(testdata_url)\n```\n:::\n\n\n\nUnd aufbereiten:\n\n\n::: {.cell hash='tidymodels1_cache/html/unnamed-chunk-4_c44327ab1ae410e5daec975dc6b8d756'}\n\n```{.r .cell-code}\n# drop unused variables:\ntrain <-\n  train_raw %>% \n  select(-c(Name, Cabin, Ticket))\n\n# convert string to factors:\ntrain2 <- \n  train %>% \n  mutate(across(where(is.character), as.factor))\n  \n# convert numeric outcome to nominal, to indicate classification:\ntrain2 <- \n  train2 %>% \n  mutate(Survived = as.factor(Survived))\n```\n:::\n\n\n\nGibt es fehlende Werte in der AV?\n\n\n::: {.cell hash='tidymodels1_cache/html/unnamed-chunk-5_2768ea94780dd00bcc89680ca737695c'}\n\n```{.r .cell-code}\nsum(is.na(train2$Survived))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\n\n\n\nVorverarbeitung des Datensatzes macht er via ein `recipe` aus `tidymodels`: \n\n\n\n::: {.cell hash='tidymodels1_cache/html/unnamed-chunk-6_d7ff9fd78790e66aad82e14ac7414ec7'}\n\n```{.r .cell-code}\ntitanic_recipe <- \n  \n  # define model formula:\n  recipe(Survived ~ ., data = train2) %>%\n  \n  # Use \"ID\" etc as ID, not as predictor:\n  update_role(PassengerId, new_role = \"ID\") %>% \n  \n   # impute missing values:\n  step_impute_bag(all_predictors()) %>% \n  \n  # convert to dummy variables:\n  step_dummy(all_nominal_predictors())\n```\n:::\n\n\n\nCheck no missings:\n\n\n::: {.cell hash='tidymodels1_cache/html/unnamed-chunk-7_5d224a444a3d13443cfde7c055fb25ef'}\n\n```{.r .cell-code}\ntitanic_train_baked <- titanic_recipe %>% prep() %>% bake(new_data = NULL)\n\nsum(is.na(titanic_train_baked))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\n\n\nDann definiert ein ein Modell:\n\n\n\n::: {.cell hash='tidymodels1_cache/html/unnamed-chunk-8_50a8023d4da80564237c41f4c3126a8e'}\n\n```{.r .cell-code}\nrf_mod2 <- \n  rand_forest(mtry = tune(), # tune mtry\n              min_n = tune(), # tune minimal n per node\n              trees = 1000) %>%  # set number of trees to 1000\n  set_engine(\"ranger\", \n             num.threads = cores) %>% \n  set_mode(\"classification\")\n```\n:::\n\n\n\n... und ein Kreuzvalidierungsschema:\n\n\n\n::: {.cell hash='tidymodels1_cache/html/unnamed-chunk-9_91a38c50d9c836b1eee126a537d6bcd9'}\n\n```{.r .cell-code}\ntrain_cv <- vfold_cv(train2, \n                     v = 10,\n                     repeats = 1, \n                     strata = \"Survived\")\n```\n:::\n\n\n\n\nAus der Hilfe zu `vfold_cv`: \n\n\n---\n\n**V-Fold Cross-Validation**\n\n*Description*\n\nV-fold cross-validation randomly splits the data into V groups of roughly equal size (called \"folds\"). A resample of the analysis data consisted of V-1 of the folds while the assessment set contains the final fold. In basic V-fold cross-validation (i.e. no repeats), the number of resamples is equal to V.\n\n*Usage*\n\n`vfold_cv(data, v = 10, repeats = 1, strata = NULL, breaks = 4, ...)`\n\n*Arguments*\n\n`data` A data frame.\n\n`v` The number of partitions of the data set.\n\n`repeats` The number of times to repeat the V-fold partitioning.\n\n`strata` A variable that is used to conduct stratified sampling to create the folds. This could be a single character value or a variable name that corresponds to a variable that exists in the data frame.\n\n`breaks` A single number giving the number of bins desired to stratify a numeric stratification variable.\n\n`...` Not currently used.\n\n*Details*\n\nThe strata argument causes the random sampling to be conducted within the stratification variable. This can help ensure that the number of data points in the analysis data is equivalent to the proportions in the original data set. (Strata below 10% of the total are pooled together.) When more than one repeat is requested, the basic V-fold cross-validation is conducted each time. For example, if three repeats are used with v = 10, there are a total of 30 splits which as three groups of 10 that are generated separately.\n\n\n---\n\n\nSo entsteht dieser Workflow:\n\n\n\n::: {.cell hash='tidymodels1_cache/html/unnamed-chunk-10_b8c137e1977ccffd1ee3e401ad33574e'}\n\n```{.r .cell-code}\ntitanic_rf_wf2 <-\n  workflow() %>% \n  add_model(rf_mod2) %>% \n  add_recipe(titanic_recipe)\n```\n:::\n\n\n\n\nJetzt: Fit the grid!\n\n\n\n\n::: {.cell hash='tidymodels1_cache/html/unnamed-chunk-11_80556534e86b740f7d08919690363113'}\n\n```{.r .cell-code}\nset.seed(42)\n\nn_candidates <- 2\n\nrf_res2 <- \n  titanic_rf_wf2 %>% \n  tune_race_anova(\n    resamples = train_cv,\n    grid = n_candidates,  # test 25 different tuning parameter values\n    #control = control_grid(save_pred = TRUE),\n    metrics = metric_set(roc_auc))\n```\n:::\n\n\nMit dem Parameter `grid` kann man die Anzahl der zu berechnenden Kandidaten-Modelle festlegen.\n\nFür gute Vorhersagen bieten sich hohe Werte an; das \nkostet aber Rechenzeit.\n\n\nAus den Resampling-Kandidaten wählt er nun das beste aus:\n\n\n::: {.cell hash='tidymodels1_cache/html/unnamed-chunk-12_443a5e9973b43b480bcd1390184b15f9'}\n\n```{.r .cell-code}\nrf_best2 <- \n  rf_res2 %>% \n  select_best(metric = \"roc_auc\")\nrf_best2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n   mtry min_n .config             \n  <int> <int> <chr>               \n1     4     9 Preprocessor1_Model2\n```\n:::\n:::\n\n\n\nDas beste Kandidatenmodell nutzt er nun, um den ganzen Train-Datensatz zu \"fitten\":\n\n\n\n::: {.cell hash='tidymodels1_cache/html/unnamed-chunk-13_1334c3da29ac1db7ace6d55fd69cac86'}\n\n```{.r .cell-code}\n# write best parameter values to the workflow:\nrf_final_wf2 <- \n  titanic_rf_wf2 %>% \n  finalize_workflow(rf_best2)\n\n# fit the model:\nrf_final_model2 <- \nrf_final_wf2 %>% \n  fit(train2)\n```\n:::\n\n\n\nZum Abschluss speichert er die Vorhersagen, die er dann bei Kaggle einreichen will:\n\n\n::: {.cell hash='tidymodels1_cache/html/unnamed-chunk-14_d70ba79d519fcd70943951dd5c4b58c4'}\n\n```{.r .cell-code}\nrf2_preds <- \n  predict(rf_final_model2, new_data = test)  # compute prediction on test set\n```\n:::\n\n\nEin letzter Blick auf die Verteilung der vorhergesagten Werte:\n\n\n::: {.cell hash='tidymodels1_cache/html/unnamed-chunk-15_2a102619ea6b2c08e65ebcddb6b2ab82'}\n\n```{.r .cell-code}\ncount(rf2_preds, .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n  .pred_class     n\n  <fct>       <int>\n1 0             285\n2 1             133\n```\n:::\n:::\n\n\n\nAuf Basis dieser Analyse: Wählen Sie am besten passende Aussage!\n\nAnswerlist\n----------\n* Es wurden 2 Kandidaten von Tuningparameterwerten in die Analyse einbezogen.\n* Es wurde kein Parameter-Tuning durchgeführt.\n* Die Metrik $AUC$ sollte *nicht* für Klassifikationsmodelle verwendet werden.\n* Es wurde eine 10-fache Kreuzvalidierung (ohne Wiederholungen) verwendet.\n* Die Anzahl der Bäume im Random Forest wurde hier *nicht* ins Parametertuning einbezogen; allerdings wäre es sinnvoll (und üblich), dies zu tun.\n* der Parameter `mtry` wurde hier *nicht* ins Parametertuning einbezogen.\n\n\n\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n\n# Lösung\n\n\nAnswerlist\n----------\n\n\n* Wahr\n* Falsch\n* Falsch\n* Falsch\n* Falsch\n* Falsch\n\n\n\n\n\n---\n\nCategories: \n\n- ds1\n- tidymodels\n- prediction\n- yacsda\n- statlearning\n- dyn\n- schoice\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}