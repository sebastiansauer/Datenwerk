{
  "hash": "281104551e4c4377c79feb88128a8275",
  "result": {
    "engine": "knitr",
    "markdown": "---\nexname: germeval07-de-wordvec-no-resamples\nexpoints: 1\nextype: string\nexsolution: NA\ncategories:\n- 2023\n- textmining\n- datawrangling\n- germeval\n- prediction\n- tidymodels\n- string\ndate: '2023-11-15'\nslug: germeval07\ntitle: germeval07\n\n---\n\n\n\n\n\n\n# Aufgabe\n\nErstellen Sie ein prädiktives Modell für Textdaten. \nNutzen Sie *dDeutsche Word-Vektoren* für das Feature-Engineering.\n\nNutzen Sie die [GermEval-2018-Daten](https://heidata.uni-heidelberg.de/dataset.xhtml?persistentId=doi:10.11588/data/0B5VML).\n\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\n\nDie Daten sind auch über das R-Paket [PradaData](https://github.com/sebastiansauer/pradadata/tree/master/data-raw/GermEval-2018-Data-master) zu beziehen.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ndata(\"germeval_train\", package = \"pradadata\")\ndata(\"germeval_test\", package = \"pradadata\")\n```\n:::\n\n\nDie AV lautet `c1`. Die (einzige) UV lautet: `text`.\n\n\nHinweise:\n\n- Orientieren Sie sich im Übrigen an den [allgemeinen Hinweisen des Datenwerks](https://datenwerk.netlify.app/hinweise).\n- Nutzen Sie Tidymodels.\n- Nutzen Sie [Wikipedia2Vec](https://wikipedia2vec.github.io/wikipedia2vec/) als Grundlage für die Wordembeddings in deutscher Sprache. Laden Sie die Daten herunter (Achtung: ca. 2.8 GB).\n\n\n\n\n\n\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n\n# Lösung\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train <-\n  germeval_train |> \n  select(id, c1, text)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tictoc)\nlibrary(tidymodels)\nlibrary(syuzhet)\nlibrary(beepr)\nlibrary(textrecipes)\n```\n:::\n\n\n\nEine [Vorlage für ein Tidymodels-Pipeline findet sich hier](https://datenwerk.netlify.app/posts/tidymodels-vorlage2/tidymodels-vorlage2.html).\n\n\n## Deutsche Textvektoren importieren\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwiki_de_embeds_path <- \"/Users/sebastiansaueruser/datasets/word-embeddings/wikipedia2vec/dewiki_20180420_100d.txt\"\n\n\ntic()\nwiki_de_embeds <- arrow::read_feather(file = \"/Users/sebastiansaueruser/datasets/word-embeddings/wikipedia2vec/part-0.arrow\")\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.53 sec elapsed\n```\n\n\n:::\n\n```{.r .cell-code}\nnames(wiki_de_embeds)[1] <- \"word\"\n\nwiki <- as_tibble(wiki_de_embeds)\n```\n:::\n\n\nDie Arrow-Datei ist *viel* schneller zu importieren als die Text-Datei.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nwiki_de_embeds <-\n  data.table::fread(file = wiki_de_embeds_path,\n                    sep = \" \",\n                    header = FALSE,\n                    showProgress = FALSE)  # progressbar\ntoc()\n```\n:::\n\n\n\n\nAls Parquet-Datei speichern (effizienter):\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\narrow::write_dataset(wiki_de_embeds, path = \"/Users/sebastiansaueruser/datasets/word-embeddings/wikipedia2vec\",\n                     format = \"arrow\")\ntoc()\n```\n:::\n\n\n\n## Workflow\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# model:\nmod1 <-\n  logistic_reg()\n\n\n# recipe:\nrec1 <-\n  recipe(c1 ~ ., data = d_train) |> \n  update_role(id, new_role = \"id\")  |> \n  #update_role(c2, new_role = \"ignore\") |> \n  step_tokenize(text) %>%\n  step_stopwords(text, keep = FALSE) %>%\n  step_word_embeddings(text,\n                       embeddings = wiki,\n                       aggregation = \"mean\") |> \n  step_normalize(all_numeric_predictors()) \n\n\n# workflow:\nwf1 <-\n  workflow() %>% \n  add_model(mod1) %>% \n  add_recipe(rec1)\n```\n:::\n\n\n## Preppen/Baken\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nrec1_prepped <- prep(rec1)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n48.189 sec elapsed\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(rec1_prepped)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$var_info\n# A tibble: 3 × 4\n  variable type      role      source  \n  <chr>    <list>    <chr>     <chr>   \n1 id       <chr [2]> id        original\n2 text     <chr [3]> predictor original\n3 c1       <chr [3]> outcome   original\n\n$term_info\n# A tibble: 102 × 4\n   variable          type      role      source  \n   <chr>             <list>    <chr>     <chr>   \n 1 id                <chr [2]> id        original\n 2 c1                <chr [3]> outcome   original\n 3 wordembed_text_V2 <chr [2]> predictor derived \n 4 wordembed_text_V3 <chr [2]> predictor derived \n 5 wordembed_text_V4 <chr [2]> predictor derived \n 6 wordembed_text_V5 <chr [2]> predictor derived \n 7 wordembed_text_V6 <chr [2]> predictor derived \n 8 wordembed_text_V7 <chr [2]> predictor derived \n 9 wordembed_text_V8 <chr [2]> predictor derived \n10 wordembed_text_V9 <chr [2]> predictor derived \n# ℹ 92 more rows\n\n$steps\n$steps[[1]]\n\n$steps[[2]]\n\n$steps[[3]]\n\n$steps[[4]]\n$terms\n<list_of<quosure>>\n\n[[1]]\n<quosure>\nexpr: ^all_numeric_predictors()\nenv:  global\n\n\n$role\n[1] NA\n\n$trained\n[1] TRUE\n\n$means\n wordembed_text_V2  wordembed_text_V3  wordembed_text_V4  wordembed_text_V5 \n      -0.124011759       -0.081089957       -0.128388343       -0.075558929 \n wordembed_text_V6  wordembed_text_V7  wordembed_text_V8  wordembed_text_V9 \n       0.189031842        0.003937014       -0.221513564        0.365974836 \nwordembed_text_V10 wordembed_text_V11 \n       0.013272361       -0.312491743 \n [ reached getOption(\"max.print\") -- omitted 90 entries ]\n\n$sds\n wordembed_text_V2  wordembed_text_V3  wordembed_text_V4  wordembed_text_V5 \n        0.08420589         0.08519713         0.08789574         0.08494160 \n wordembed_text_V6  wordembed_text_V7  wordembed_text_V8  wordembed_text_V9 \n        0.08312099         0.08975037         0.10767634         0.08125405 \nwordembed_text_V10 wordembed_text_V11 \n        0.08721453         0.09460724 \n [ reached getOption(\"max.print\") -- omitted 90 entries ]\n\n$na_rm\n[1] TRUE\n\n$skip\n[1] FALSE\n\n$id\n[1] \"normalize_lqjIy\"\n\n$case_weights\nNULL\n\nattr(,\"class\")\n[1] \"step_normalize\" \"step\"          \n\n\n$template\n# A tibble: 5,009 × 102\n      id c1      wordembed_text_V2 wordembed_text_V3 wordembed_text_V4\n   <int> <fct>               <dbl>             <dbl>             <dbl>\n 1     1 OTHER              1.35             -1.41              1.53  \n 2     2 OTHER              0.0922            0.0249            0.161 \n 3     3 OTHER             -1.62              0.329            -0.419 \n 4     4 OTHER              0.399            -1.17              0.0215\n 5     5 OFFENSE            0.0778           -0.812            -0.414 \n 6     6 OTHER             -0.475             0.357             0.176 \n 7     7 OFFENSE            0.111             0.232             0.799 \n 8     8 OTHER             -2.53             -4.09              1.15  \n 9     9 OFFENSE           -1.77              1.12             -1.41  \n10    10 OFFENSE            0.779             0.987            -0.641 \n# ℹ 4,999 more rows\n# ℹ 97 more variables: wordembed_text_V5 <dbl>, wordembed_text_V6 <dbl>,\n#   wordembed_text_V7 <dbl>, wordembed_text_V8 <dbl>, wordembed_text_V9 <dbl>,\n#   wordembed_text_V10 <dbl>, wordembed_text_V11 <dbl>,\n#   wordembed_text_V12 <dbl>, wordembed_text_V13 <dbl>,\n#   wordembed_text_V14 <dbl>, wordembed_text_V15 <dbl>,\n#   wordembed_text_V16 <dbl>, wordembed_text_V17 <dbl>, …\n\n$levels\n$levels$id\n$levels$id$values\n[1] NA\n\n$levels$id$ordered\n[1] NA\n\n\n$levels$c1\n$levels$c1$values\n[1] \"OFFENSE\" \"OTHER\"  \n\n$levels$c1$ordered\n[1] FALSE\n\n$levels$c1$factor\n[1] TRUE\n\n\n$levels$wordembed_text_V2\n$levels$wordembed_text_V2$values\n[1] NA\n\n$levels$wordembed_text_V2$ordered\n[1] NA\n\n\n$levels$wordembed_text_V3\n$levels$wordembed_text_V3$values\n[1] NA\n\n$levels$wordembed_text_V3$ordered\n[1] NA\n\n\n$levels$wordembed_text_V4\n$levels$wordembed_text_V4$values\n[1] NA\n\n$levels$wordembed_text_V4$ordered\n[1] NA\n\n\n$levels$wordembed_text_V5\n$levels$wordembed_text_V5$values\n[1] NA\n\n$levels$wordembed_text_V5$ordered\n[1] NA\n\n\n$levels$wordembed_text_V6\n$levels$wordembed_text_V6$values\n[1] NA\n\n$levels$wordembed_text_V6$ordered\n[1] NA\n\n\n$levels$wordembed_text_V7\n$levels$wordembed_text_V7$values\n[1] NA\n\n$levels$wordembed_text_V7$ordered\n[1] NA\n\n\n$levels$wordembed_text_V8\n$levels$wordembed_text_V8$values\n[1] NA\n\n$levels$wordembed_text_V8$ordered\n[1] NA\n\n\n$levels$wordembed_text_V9\n$levels$wordembed_text_V9$values\n[1] NA\n\n$levels$wordembed_text_V9$ordered\n[1] NA\n\n\n [ reached getOption(\"max.print\") -- omitted 92 entries ]\n\n$retained\n[1] TRUE\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train_baked <-\n  bake(rec1_prepped, new_data = NULL)\n```\n:::\n\n\n\n\n## Tuninig/Fitting\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nwf1_fit <-\n  wf1 %>% \n  fit(data = d_train)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n10.373 sec elapsed\n```\n\n\n:::\n\n```{.r .cell-code}\nbeep()\n```\n:::\n\n\n\nAus Zeitgründen verzichten wir hier auf Tuning.\n\n\n## Test-Set-Güte\n\n\nVorhersagen im Test-Set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\npreds <-\n  predict(wf1_fit, new_data = germeval_test)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n7.212 sec elapsed\n```\n\n\n:::\n:::\n\n\nUnd die Vorhersagen zum Test-Set hinzufügen, damit man `TRUTH` und `ESTIMATE` vergleichen kann:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_test <-\n  germeval_test |> \n  bind_cols(preds) |> \n  mutate(c1 = as.factor(c1))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_metrics <- metric_set(accuracy, f_meas)\nmy_metrics(d_test,\n           truth = c1,\n           estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.715\n2 f_meas   binary         0.538\n```\n\n\n:::\n:::\n\n\n\n## Fazit\n\n\n`wikipedia2vec` ist für die deutsche Sprache vorgekocht. \nDas macht Sinn für einen deutschsprachigen Corpus.\n\n\n\n\n---\n\nCategories: \n\n- 2023\n- textmining\n- datawrangling\n- germeval\n- prediction\n- tidymodels\n- string\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}