{
  "hash": "3781543b74384f1a16ed0a62e7226274",
  "result": {
    "engine": "knitr",
    "markdown": "---\nextype: string\nexsolution: NA\nexname: twitter04\nexpoints: 1\ncategories:\n- textmining\n- twitter\ndate: '2022-10-28'\nslug: twitter04\ntitle: twitter04\n\n---\n\n\n\n\n\n\n\n\n\n# Exercise\n\n\nLaden Sie $n=10^k$ Tweets von Twitter herunter (mit $k=2$) via der Twitter API; Suchterm soll sein \"@karl_lauterbach\".\nBereiten Sie die Textdaten mit grundlegenden Methoden des Textminings auf (Tokenisieren, Stopwörter entfernen, Zahlen entfernen, ...).\nBerichten Sie dann die 10 häufigsten Wörter als Schätzer für die Dinge, die an Karl Lauterbach getweetet werden.\n\n\n\n\n\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n\n# Solution\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rtweet)\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()  masks stats::filter()\n✖ purrr::flatten() masks rtweet::flatten()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidytext)\nlibrary(lsa)  # Stopwörter\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: SnowballC\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(SnowballC)  # Stemming\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nauth <- rtweet_app(bearer_token = Bearer_Token)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nkarl1 <- search_tweets(\"@karl_lauterbach\", n = 1e2, include_rts = FALSE)\n#write_rds(karl1, file = \"karl1.rds\", compress = \"gz\")\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nkarl2 <- \n  karl1 %>% \n  select(full_text)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nkarl3 <- \n  karl2 %>% \n  unnest_tokens(output = word, input = full_text)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nkarl4 <- \nkarl3 %>% \n  anti_join(tibble(word = lsa::stopwords_de)) \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nJoining with `by = join_by(word)`\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nkarl5 <- \n  karl4 %>% \n  mutate(word = str_replace_na(word, \"^[:digit:]+$\")) %>% \n  mutate(word = str_replace_na(word, \"hptts?://\\\\w+\")) %>% \n  mutate(word = str_replace_na(word, \" +\")) %>% \n  drop_na()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nkarl6 <-\n  karl5 %>% \n  mutate(word = wordStem(word))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nkarl6 %>% \n  count(word, sort = TRUE) %>% \n  slice_head(n=10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 2\n   word                       n\n   <chr>                  <int>\n 1 karl_lauterbach          100\n 2 rt                        60\n 3 ultrakaerl                19\n 4 corona                    16\n 5 wirwollenmaskenpflicht    16\n 6 länder                    12\n 7 gesundheitsminist         11\n 8 polarstern64              11\n 9 schon                     11\n10 shomburg                  11\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n---\n\nCategories: \n\n- textmining\n- twitter\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}