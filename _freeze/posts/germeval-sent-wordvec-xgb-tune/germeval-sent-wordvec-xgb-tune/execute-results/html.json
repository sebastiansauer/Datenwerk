{
  "hash": "3fb697397a91cd3727cbe32d5d225321",
  "result": {
    "engine": "knitr",
    "markdown": "---\nexpoints: 1\nextype: string\nexsolution: NA\ncategories:\n- textmining\n- datawrangling\n- germeval\n- prediction\n- tidymodels\n- sentiment\n- string\n- xgb\n- tune\ndate: '2023-12-03'\ntitle: germeval03-sent-wordvec-xgb-tune\ndraft: false   # DRAFT TRUE\neval: true\nexecute:\n  cache: true\n---\n\n\n\n\n\n\n# Aufgabe\n\nErstellen Sie ein prädiktives Modell für Textdaten. Nutzen Sie Sentiments und TextFeatures im Rahmen von Feature-Engineering.\nNutzen Sie außerdem *deutsche Word-Vektoren* für das Feature-Engineering.\n\nAls Lernalgorithmus verwenden Sie XGB. Tunen Sie die Lernrate und die max. Tiefe (`max_depth`) des Modells.\n\n\n\n## Daten\n\nVerwenden Sie die [GermEval-2018-Daten](https://heidata.uni-heidelberg.de/dataset.xhtml?persistentId=doi:10.11588/data/0B5VML).\n\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\n\nDie Daten sind auch über das R-Paket [PradaData](https://github.com/sebastiansauer/pradadata/tree/master/data-raw/GermEval-2018-Data-master) zu beziehen.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ndata(\"germeval_train\", package = \"pradadata\")\ndata(\"germeval_test\", package = \"pradadata\")\n```\n:::\n\n\n## AV und UV\n\nDie AV lautet `c1`. Die (einzige) UV lautet: `text`.\n\n\n## Hinweise\n\n- Orientieren Sie sich im Übrigen an den [allgemeinen Hinweisen des Datenwerks](https://datenwerk.netlify.app/hinweise).\n- Nutzen Sie Tidymodels.\n- Nutzen Sie das `sentiws` Lexikon.\n- ❗ Achten Sie darauf, die Variable `c2` zu entfernen bzw. nicht zu verwenden.\n\n\n\n\n\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n\n# Lösung\n\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train <-\n  germeval_train |> \n  select(id, c1, text)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tictoc)\nlibrary(tidymodels)\n#library(syuzhet)\nlibrary(beepr)\nlibrary(finetune)  # anova race\nlibrary(lobstr)  # object size\nlibrary(visdat)  # footprint of csv\n#data(\"sentiws\", package = \"pradadata\")\n```\n:::\n\n\n\nEine [Vorlage für ein Tidymodels-Pipeline findet sich hier](https://datenwerk.netlify.app/posts/tidymodels-vorlage2/tidymodels-vorlage2.html).\n\n\n\n## Learner/Modell\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod <-\n  boost_tree(mode = \"classification\",\n             learn_rate = tune(), \n             tree_depth = tune()\n             )\n```\n:::\n\n\n\n\n## Gebackenen Datensatz als neue Grundlage\n\nWir importieren den schon an anderer Stelle aufbereiteten Datensatz.\nDas hat den Vorteil (hoffentlich), das die Datenvolumina viel kleiner sind.\nDie Arbeit des Feature Engineering wurde uns schon abgenommen.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train <-\n  read_csv(\"https://raw.githubusercontent.com/sebastiansauer/Datenwerk2/main/data/germeval/germeval_train_recipe_wordvec_senti.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 5009 Columns: 121\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (1): c1\ndbl (120): id, emo_count, schimpf_count, emoji_count, textfeature_text_copy_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nvis_dat(d_train) +\n  # remove axis labels:\n  theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank() \n        )\n```\n\n::: {.cell-output-display}\n![](germeval-sent-wordvec-xgb-tune_files/figure-html/vis-dat-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nd_test_baked <- read_csv(\"https://raw.githubusercontent.com/sebastiansauer/Datenwerk2/main/data/germeval/germeval_test_recipe_wordvec_senti.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 3532 Columns: 121\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (1): c1\ndbl (120): id, emo_count, schimpf_count, emoji_count, textfeature_text_copy_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n\n\n## Plain-Rezept\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec <- \n  recipe(c1 ~ ., data = d_train)\n```\n:::\n\n\n\n\n\n## Neuer Workflow mit plainem Rezept\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf <-\n  workflow() |> \n  add_recipe(rec) |> \n  add_model(mod)\n```\n:::\n\n\n\n\n\n\n## Parallelisierung über mehrere Kerne\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(parallel)\nall_cores <- detectCores(logical = FALSE)\n\nlibrary(doFuture)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: foreach\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'foreach'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: future\n```\n\n\n:::\n\n```{.r .cell-code}\nregisterDoFuture()\ncl <- makeCluster(3)\nplan(cluster, workers = cl)\n```\n:::\n\n\n\nAchtung: Viele Kerne brauchen auch viel Speicher.\n\n## Tune/Resample/Fit\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nfit_wordvec_senti_xgb <-\n  tune_race_anova(\n    wf,\n    grid = 30,\n    resamples = vfold_cv(d_train, v = 5),\n    control = control_race(verbose_elim = TRUE))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nℹ Racing will maximize the roc_auc metric.\nℹ Resamples are analyzed in a random order.\nℹ Fold5: 23 eliminated; 7 candidates remain.\n\nℹ Fold4: 2 eliminated; 5 candidates remain.\n```\n\n\n:::\n\n```{.r .cell-code}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n186.332 sec elapsed\n```\n\n\n:::\n\n```{.r .cell-code}\nbeep()\n```\n:::\n\n\n\n\nObjekt-Größe:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlobstr::obj_size(fit_wordvec_senti_xgb)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n5.10 MB\n```\n\n\n:::\n:::\n\n\n\nGroß!\n\nWie wir gesehen haben, ist das Rezept riesig.\n\n\n\n## Get best performance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(fit_wordvec_senti_xgb)\n```\n\n::: {.cell-output-display}\n![](germeval-sent-wordvec-xgb-tune_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(fit_wordvec_senti_xgb)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 8\n  tree_depth learn_rate .metric .estimator  mean     n std_err .config          \n       <int>      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>            \n1          7     0.255  roc_auc binary     0.765     5 0.00862 Preprocessor1_Mo…\n2         11     0.292  roc_auc binary     0.760     5 0.00859 Preprocessor1_Mo…\n3          9     0.126  roc_auc binary     0.756     5 0.00662 Preprocessor1_Mo…\n4          8     0.0796 roc_auc binary     0.755     5 0.00570 Preprocessor1_Mo…\n5         10     0.213  roc_auc binary     0.754     5 0.00628 Preprocessor1_Mo…\n```\n\n\n:::\n\n```{.r .cell-code}\nbest_params <- select_best(fit_wordvec_senti_xgb)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n```\n\n\n:::\n:::\n\n\n\n## Finalisieren\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_params <- select_best(fit_wordvec_senti_xgb)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n```\n\n\n:::\n\n```{.r .cell-code}\ntic()\nwf_finalized <- finalize_workflow(wf, best_params)\nlastfit_xgb <- fit(wf_finalized, data = d_train)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1.997 sec elapsed\n```\n\n\n:::\n:::\n\n\n\n## Test-Set-Güte\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\npreds <-\n  predict(lastfit_xgb, new_data = d_test_baked)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.219 sec elapsed\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nd_test <-\n  d_test_baked |> \n  bind_cols(preds) |> \n  mutate(c1 = as.factor(c1))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_metrics <- metric_set(accuracy, f_meas)\nmy_metrics(d_test,\n           truth = c1,\n           estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.715\n2 f_meas   binary         0.479\n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}