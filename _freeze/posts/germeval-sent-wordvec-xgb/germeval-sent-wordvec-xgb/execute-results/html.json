{
  "hash": "61d8e89728f84ab706fcb190584a753c",
  "result": {
    "engine": "knitr",
    "markdown": "---\nexname: germeval03-sent-wrodvec-xgb\nexpoints: 1\nextype: string\nexsolution: NA\ncategories:\n- textmining\n- datawrangling\n- germeval\n- prediction\n- tidymodels\n- sentiment\n- string\n- xgb\ndate: '2023-12-01'\ntitle: germeval03-sent-wordvec-xgb\n\n---\n\n\n\n\n\n\n# Aufgabe\n\nErstellen Sie ein prädiktives Modell für Textdaten. Nutzen Sie Sentiments und TextFeatures im Rahmen von Feature-Engineering.\nNutzen Sie außerdem *deutsche Word-Vektoren* für das Feature-Engineering.\n\nAls Lernalgorithmus verwenden Sie XGB.\n\nVerwenden Sie die [GermEval-2018-Daten](https://heidata.uni-heidelberg.de/dataset.xhtml?persistentId=doi:10.11588/data/0B5VML).\n\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\n\nDie Daten sind auch über das R-Paket [PradaData](https://github.com/sebastiansauer/pradadata/tree/master/data-raw/GermEval-2018-Data-master) zu beziehen.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ndata(\"germeval_train\", package = \"pradadata\")\ndata(\"germeval_test\", package = \"pradadata\")\n```\n:::\n\n\nDie AV lautet `c1`. Die (einzige) UV lautet: `text`.\n\n\nHinweise:\n\n- Orientieren Sie sich im Übrigen an den [allgemeinen Hinweisen des Datenwerks](https://datenwerk.netlify.app/hinweise).\n- Nutzen Sie Tidymodels.\n- Nutzen Sie das `sentiws` Lexikon.\n- ❗ Achten Sie darauf, die Variable `c2` zu entfernen bzw. nicht zu verwenden.\n\n\n\n\n\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n\n# Lösung\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train <-\n  germeval_train |> \n  select(id, c1, text)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tictoc)\nlibrary(tidymodels)\nlibrary(syuzhet)\nlibrary(beepr)\nlibrary(lobstr)  # object size\ndata(\"sentiws\", package = \"pradadata\")\n```\n:::\n\n\n\nEine [Vorlage für ein Tidymodels-Pipeline findet sich hier](https://datenwerk.netlify.app/posts/tidymodels-vorlage2/tidymodels-vorlage2.html).\n\n\n\n## Learner/Modell\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod <-\n  boost_tree(mode = \"classification\",\n             learn_rate = .01, \n             tree_depth = 5\n             )\n```\n:::\n\n\n## Rezept\n\nPfad zu den Wordvecktoren:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npath_wordvec <- \"/Users/sebastiansaueruser/datasets/word-embeddings/wikipedia2vec/part-0.arrow\"\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"https://raw.githubusercontent.com/sebastiansauer/Datenwerk2/main/funs/def_recipe_wordvec_senti.R\")\n\nrec <- def_recipe_wordvec_senti(data_train = d_train,\n                                path_wordvec = path_wordvec)\n```\n:::\n\n\n\n\n## Workflow\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"https://raw.githubusercontent.com/sebastiansauer/Datenwerk2/main/funs/def_df.R\")\nwf <- def_wf()\n\nwf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n13 Recipe Steps\n\n• step_text_normalization()\n• step_mutate()\n• step_mutate()\n• step_mutate()\n• step_mutate()\n• step_textfeature()\n• step_tokenize()\n• step_stopwords()\n• step_word_embeddings()\n• step_zv()\n• ...\n• and 3 more steps.\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  tree_depth = 5\n  learn_rate = 0.01\n\nComputational engine: xgboost \n```\n\n\n:::\n:::\n\n\n\n## Check\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nrec_prepped <- prep(rec)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n67.325 sec elapsed\n```\n\n\n:::\n\n```{.r .cell-code}\nrec_prepped\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nobj_size(rec_prepped)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n3.17 GB\n```\n\n\n:::\n:::\n\n\nGroß!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(rec_prepped)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 13 × 6\n   number operation type               trained skip  id                      \n    <int> <chr>     <chr>              <lgl>   <lgl> <chr>                   \n 1      1 step      text_normalization TRUE    FALSE text_normalization_QTRCS\n 2      2 step      mutate             TRUE    FALSE mutate_z4zTn            \n 3      3 step      mutate             TRUE    FALSE mutate_bjCuT            \n 4      4 step      mutate             TRUE    FALSE mutate_OVxpj            \n 5      5 step      mutate             TRUE    FALSE mutate_TRK3c            \n 6      6 step      textfeature        TRUE    FALSE textfeature_6BkkC       \n 7      7 step      tokenize           TRUE    FALSE tokenize_csz3N          \n 8      8 step      stopwords          TRUE    FALSE stopwords_HU9cX         \n 9      9 step      word_embeddings    TRUE    FALSE word_embeddings_2ZNxu   \n10     10 step      zv                 TRUE    FALSE zv_FNUiA                \n11     11 step      normalize          TRUE    FALSE normalize_bOlig         \n12     12 step      impute_mean        TRUE    FALSE impute_mean_kRaUZ       \n13     13 step      mutate             TRUE    FALSE mutate_PpudL            \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nd_rec_baked <- bake(rec_prepped, new_data = NULL)\n\nhead(d_rec_baked)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 121\n     id c1      emo_count schimpf_count emoji_count textfeature_text_copy_n_wo…¹\n  <dbl> <fct>       <dbl>         <dbl>       <dbl>                        <dbl>\n1     1 OTHER       0.575        -0.450      -0.353                      -0.495 \n2     2 OTHER      -1.11         -0.450      -0.353                      -0.0874\n3     3 OTHER       0.186        -0.450       0.774                      -0.903 \n4     4 OTHER       0.202        -0.450      -0.353                      -0.0874\n5     5 OFFENSE     0.168        -0.450      -0.353                      -0.393 \n6     6 OTHER      -1.12         -0.450      -0.353                       2.46  \n# ℹ abbreviated name: ¹​textfeature_text_copy_n_words\n# ℹ 115 more variables: textfeature_text_copy_n_uq_words <dbl>,\n#   textfeature_text_copy_n_charS <dbl>,\n#   textfeature_text_copy_n_uq_charS <dbl>,\n#   textfeature_text_copy_n_digits <dbl>,\n#   textfeature_text_copy_n_hashtags <dbl>,\n#   textfeature_text_copy_n_uq_hashtags <dbl>, …\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(is.na(d_rec_baked))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nobj_size(d_rec_baked)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n4.85 MB\n```\n\n\n:::\n:::\n\n\n\n\n## Fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nfit_wordvec_senti_xgb <-\n  fit(wf,\n      data = d_train)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n35.314 sec elapsed\n```\n\n\n:::\n\n```{.r .cell-code}\nbeep()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_wordvec_senti_xgb\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n13 Recipe Steps\n\n• step_text_normalization()\n• step_mutate()\n• step_mutate()\n• step_mutate()\n• step_mutate()\n• step_textfeature()\n• step_tokenize()\n• step_stopwords()\n• step_word_embeddings()\n• step_zv()\n• ...\n• and 3 more steps.\n\n── Model ───────────────────────────────────────────────────────────────────────\n##### xgb.Booster\nraw: 42.4 Kb \ncall:\n  xgboost::xgb.train(params = list(eta = 0.01, max_depth = 5, gamma = 0, \n    colsample_bytree = 1, colsample_bynode = 1, min_child_weight = 1, \n    subsample = 1), data = x$data, nrounds = 15, watchlist = x$watchlist, \n    verbose = 0, nthread = 1, objective = \"binary:logistic\")\nparams (as set within xgb.train):\n  eta = \"0.01\", max_depth = \"5\", gamma = \"0\", colsample_bytree = \"1\", colsample_bynode = \"1\", min_child_weight = \"1\", subsample = \"1\", nthread = \"1\", objective = \"binary:logistic\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.evaluation.log()\n# of features: 119 \nniter: 15\nnfeatures : 119 \nevaluation_log:\n    iter training_logloss\n       1        0.6904064\n       2        0.6877236\n---                      \n      14        0.6590144\n      15        0.6568817\n```\n\n\n:::\n:::\n\n\nObjekt-Größe:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlobstr::obj_size(fit_wordvec_senti_xgb)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n3.17 GB\n```\n\n\n:::\n:::\n\n\n\nGroß!\n\nWie wir gesehen haben, ist das Rezept riesig.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(butcher)\nout <- butcher(fit_wordvec_senti_xgb)\nlobstr::obj_size(out)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n3.16 GB\n```\n\n\n:::\n:::\n\n\n\n\n## Test-Set-Güte\n\n\nVorhersagen im Test-Set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\npreds <-\n  predict(fit_wordvec_senti_xgb, new_data = germeval_test)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n22.669 sec elapsed\n```\n\n\n:::\n:::\n\n\nUnd die Vorhersagen zum Test-Set hinzufügen, damit man `TRUTH` und `ESTIMATE` vergleichen kann:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_test <-\n  germeval_test |> \n  bind_cols(preds) |> \n  mutate(c1 = as.factor(c1))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_metrics <- metric_set(accuracy, f_meas)\nmy_metrics(d_test,\n           truth = c1,\n           estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.689\n2 f_meas   binary         0.400\n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}