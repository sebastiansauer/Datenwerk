{
  "hash": "4d4836040eb2b4f89d554f7b031cfbcb",
  "result": {
    "engine": "knitr",
    "markdown": "---\nexpoints: 1\nextype: string\nexsolution: NA\ncategories:\n- textmining\n- datawrangling\n- germeval\n- prediction\n- tidymodels\n- sentiment\n- string\n- xgb\n- tune\ndate: '2023-12-03'\ntitle: germeval03-sent-wordvec-xgb-plain\ndraft: false   # DRAFT TRUE\neval: true\n---\n\n\n\n\n\n\n# Aufgabe\n\nErstellen Sie ein prädiktives Modell für Textdaten. Nutzen Sie Sentiments und TextFeatures im Rahmen von Feature-Engineering.\nNutzen Sie außerdem *deutsche Word-Vektoren* für das Feature-Engineering.\n\nAls Lernalgorithmus verwenden Sie XGB. \n\nPreppen und Backen Sie das Rezept,\naber führen Sie die Pipelien mit dem gebackenen Datensatz und einem \"Plain-Rezept\" durch.\n\n\n## Daten\n\nVerwenden Sie die [GermEval-2018-Daten](https://heidata.uni-heidelberg.de/dataset.xhtml?persistentId=doi:10.11588/data/0B5VML).\n\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\n\nDie Daten sind auch über das R-Paket [PradaData](https://github.com/sebastiansauer/pradadata/tree/master/data-raw/GermEval-2018-Data-master) zu beziehen.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ndata(\"germeval_train\", package = \"pradadata\")\ndata(\"germeval_test\", package = \"pradadata\")\n```\n:::\n\n\n## AV und UV\n\nDie AV lautet `c1`. Die (einzige) UV lautet: `text`.\n\n\n## Hinweise\n\n- Orientieren Sie sich im Übrigen an den [allgemeinen Hinweisen des Datenwerks](https://datenwerk.netlify.app/hinweise).\n- Nutzen Sie Tidymodels.\n- Nutzen Sie das `sentiws` Lexikon.\n- ❗ Achten Sie darauf, die Variable `c2` zu entfernen bzw. nicht zu verwenden.\n\n\n\n\n\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n\n# Lösung\n\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train <-\n  germeval_train |> \n  select(id, c1, text)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tictoc)\nlibrary(tidymodels)\nlibrary(syuzhet)\nlibrary(beepr)\nlibrary(lobstr)  # object size\nlibrary(visdat)  # Fingerprint/footprint of dataset (CSV)\ndata(\"sentiws\", package = \"pradadata\")\n```\n:::\n\n\n\nEine [Vorlage für ein Tidymodels-Pipeline findet sich hier](https://datenwerk.netlify.app/posts/tidymodels-vorlage2/tidymodels-vorlage2.html).\n\n\n\n## Learner/Modell\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod <-\n  boost_tree(mode = \"classification\",\n             learn_rate = tune(), \n             tree_depth = tune()\n             )\n```\n:::\n\n\n## Rezept Workvektoren\n\nPfad zu den Wordvektoren:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npath_wordvec <- \"/Users/sebastiansaueruser/datasets/word-embeddings/wikipedia2vec/part-0.arrow\"\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"https://raw.githubusercontent.com/sebastiansauer/Datenwerk2/main/funs/def_recipe_wordvec_senti.R\")\n\nrec <- def_recipe_wordvec_senti(data_train = d_train,\n                                path_wordvec = path_wordvec)\n```\n:::\n\n\n\n\n## Prep/Bake Wordvektoren\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nrec_prepped <- prep(rec)\ntoc()\n```\n:::\n\n\n\n`78.021 sec elapsed`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_rec_baked <- bake(rec_prepped, new_data = NULL)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(is.na(d_rec_baked))\n```\n:::\n\n\n\n## Test-Set auch baken\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_test_baked <- bake(rec_prepped, new_data = germeval_test)\ndim(d_test_baked)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_csv(d_test_baked, \"data/germeval/germeval_test_recipe_wordvec_senti.csv\")\n```\n:::\n\n\nSpäter kann man es dann analog wieder importieren:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_test_baked <- read_csv(\"https://raw.githubusercontent.com/sebastiansauer/Datenwerk2/main/data/germeval/germeval_test_recipe_wordvec_senti.csv\")\n```\n:::\n\n\n\n## Gebackenen Datensatz als neue Grundlage\n\nDen gepreppten/gebackenen Datensatz speichern wir als Datensatz ab:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_csv(d_rec_baked, \"https://raw.githubusercontent.com/sebastiansauer/Datenwerk2/main/data/germeval/germeval_train_recipe_wordvec_senti.csv\")\n```\n:::\n\n\n\nSpäter können wir den Datensatz als \"neuen, frischen\" Datensatz für ein \"Plain-Rezept\", also ein ganz einfaches Rezept nutzen.\nDas hat den Vorteil (hoffentlich), das die Datenvolumina viel kleiner sind.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train_new <-\n  read_csv(\"https://raw.githubusercontent.com/sebastiansauer/Datenwerk2/main/data/germeval/germeval_train_recipe_wordvec_senti.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nvis_dat(d_train_new) +\n  # remove axis labels:\n  theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank() \n        )\n```\n\n::: {.cell-output-display}\n![](vis-dat-1.png){fig-pos='H' width=384}\n:::\n:::\n\n\n\n\n## Plain-Rezept\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec <- \n  recipe(c1 ~ ., data = d_train_new)\n```\n:::\n\n\n\n\n\n## Neuer Workflow mit plainem Rezept\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf <-\n  workflow() |> \n  add_recipe(rec) |> \n  add_model(mod)\n\nwf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  tree_depth = tune()\n  learn_rate = tune()\n\nComputational engine: xgboost \n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Parallelisierung über mehrere Kerne\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(parallel)\nall_cores <- detectCores(logical = FALSE)\n\nlibrary(doFuture)\nregisterDoFuture()\ncl <- makeCluster(2)\nplan(cluster, workers = cl)\n```\n:::\n\n\n\nAchtung: Viele Kerne brauchen auch viel Speicher.\n\n## Tune/Resample/Fit\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nfit_wordvec_senti_xgb <-\n  tune_grid(\n    wf,\n    grid = 50,\n    resamples = vfold_cv(d_train_new, v = 5))\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n285.723 sec elapsed\n```\n\n\n:::\n\n```{.r .cell-code}\nbeep()\n```\n:::\n\n\n\n\nObjekt-Größe:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlobstr::obj_size(fit_wordvec_senti_xgb)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n5.11 MB\n```\n\n\n:::\n:::\n\n\nAh! Angenehm klein.\n\n\n\n\n## Get best performance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(fit_wordvec_senti_xgb)\n```\n\n::: {.cell-output-display}\n![](unnamed-chunk-12-1.png){fig-pos='H' width=384}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(fit_wordvec_senti_xgb)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 8\n  tree_depth learn_rate .metric .estimator  mean     n std_err .config          \n       <int>      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>            \n1         10      0.252 roc_auc binary     0.761     5 0.0108  Preprocessor1_Mo…\n2          9      0.220 roc_auc binary     0.760     5 0.00759 Preprocessor1_Mo…\n3         10      0.179 roc_auc binary     0.758     5 0.0111  Preprocessor1_Mo…\n4          5      0.106 roc_auc binary     0.758     5 0.0115  Preprocessor1_Mo…\n5          2      0.286 roc_auc binary     0.757     5 0.00868 Preprocessor1_Mo…\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_params <- select_best(fit_wordvec_senti_xgb)\n```\n:::\n\n\n\n## Finalisieren\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_params <- select_best(fit_wordvec_senti_xgb)\ntic()\nwf_finalized <- finalize_workflow(wf, best_params)\nlastfit_xgb <- fit(wf_finalized, data = d_train_new)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n2.853 sec elapsed\n```\n\n\n:::\n:::\n\n\n\n## Test-Set-Güte\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\npreds <-\n  predict(lastfit_xgb, new_data = d_test_baked)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.035 sec elapsed\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nd_test <-\n  germeval_test |> \n  bind_cols(preds) |> \n  mutate(c1 = as.factor(c1))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_metrics <- metric_set(accuracy, f_meas)\nmy_metrics(d_test,\n           truth = c1,\n           estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.714\n2 f_meas   binary         0.484\n```\n\n\n:::\n:::\n\n\n\n## Fazit\n\nVerzichtet man auf ein Rezept mit viel Datenvolument (Wordvektoren blähen das Rezept mächtig auf), so wird das Fitten schlanker und schneller.\nSchneller auch deshalb, weil ggf. kein Swapping zwischen Speicher und Festplatte mehr nötig ist.\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "germeval-sent-wordvec-xgb-plain_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}