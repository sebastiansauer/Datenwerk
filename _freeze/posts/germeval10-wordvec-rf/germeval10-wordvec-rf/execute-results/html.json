{
  "hash": "a4aa0925237a5dcfb7c23913936bf1c9",
  "result": {
    "engine": "knitr",
    "markdown": "---\nexname: germeval10-wordvec-rf\nexpoints: 1\nextype: string\nexsolution: NA\ncategories:\n- textmining\n- datawrangling\n- germeval\n- prediction\n- tidymodels\n- string\ndate: '2023-11-17'\nslug: germeval10-wordvec-rf\ntitle: germeval10-wordvec-rf\n\n---\n\n\n\n\n\n\n# Aufgabe\n\nErstellen Sie ein prädiktives Modell für Textdaten. \nNutzen Sie *deutsche Word-Vektoren* für das Feature-Engineering.\n\nNutzen Sie die [GermEval-2018-Daten](https://heidata.uni-heidelberg.de/dataset.xhtml?persistentId=doi:10.11588/data/0B5VML).\n\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\n\nDie Daten sind auch über das R-Paket [PradaData](https://github.com/sebastiansauer/pradadata/tree/master/data-raw/GermEval-2018-Data-master) zu beziehen.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ndata(\"germeval_train\", package = \"pradadata\")\ndata(\"germeval_test\", package = \"pradadata\")\n```\n:::\n\n\nDie AV lautet `c1`. Die (einzige) UV lautet: `text`.\n\n\nHinweise:\n\n- Orientieren Sie sich im Übrigen an den [allgemeinen Hinweisen des Datenwerks](https://datenwerk.netlify.app/hinweise).\n- Nutzen Sie Tidymodels.\n- Nutzen Sie [Wikipedia2Vec](https://wikipedia2vec.github.io/wikipedia2vec/) als Grundlage für die Wordembeddings in deutscher Sprache. Laden Sie die Daten herunter (Achtung: ca. 2.8 GB).\n\n\n\n\n\n\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n\n\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n\n# Lösung\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train <-\n  germeval_train |> \n  select(id, c1, text)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tictoc)\nlibrary(tidymodels)\nlibrary(syuzhet)\nlibrary(beepr)\nlibrary(textrecipes)\n```\n:::\n\n\n\nEine [Vorlage für ein Tidymodels-Pipeline findet sich hier](https://datenwerk.netlify.app/posts/tidymodels-vorlage2/tidymodels-vorlage2.html).\n\n\n## Deutsche Textvektoren importieren\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nwiki_de_embeds <- arrow::read_feather(\n  file = \"/Users/sebastiansaueruser/datasets/word-embeddings/wikipedia2vec/part-0.arrow\")\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.743 sec elapsed\n```\n\n\n:::\n\n```{.r .cell-code}\nnames(wiki_de_embeds)[1] <- \"word\"\n\nwiki <- as_tibble(wiki_de_embeds)\n```\n:::\n\n\n## Workflow\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# model:\nmod1 <-\n  rand_forest(mode = \"classification\",\n              mtry = tune())\n\n# recipe:\nrec1 <-\n  recipe(c1 ~ ., data = d_train) |> \n  update_role(id, new_role = \"id\")  |> \n  #update_role(c2, new_role = \"ignore\") |> \n  step_tokenize(text) %>%\n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") |> \n  step_word_embeddings(text,\n                       embeddings = wiki,\n                       aggregation = \"mean\")\n\n# workflow:\nwf1 <-\n  workflow() %>% \n  add_model(mod1) %>% \n  add_recipe(rec1)\n```\n:::\n\n\n## Preppen/Baken\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nrec1_prepped <- prep(rec1)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n68.465 sec elapsed\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train_baked <-\n  bake(rec1_prepped, new_data = NULL)\nhead(d_train_baked)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 102\n     id c1      wordembed_text_V2 wordembed_text_V3 wordembed_text_V4\n  <int> <fct>               <dbl>             <dbl>             <dbl>\n1     1 OTHER             -0.0648          -0.0664             0.0242\n2     2 OTHER             -0.207           -0.0102            -0.118 \n3     3 OTHER             -0.246           -0.110             -0.195 \n4     4 OTHER             -0.0139          -0.241             -0.135 \n5     5 OFFENSE           -0.0803          -0.164             -0.160 \n6     6 OTHER             -0.195           -0.00456           -0.132 \n# ℹ 97 more variables: wordembed_text_V5 <dbl>, wordembed_text_V6 <dbl>,\n#   wordembed_text_V7 <dbl>, wordembed_text_V8 <dbl>, wordembed_text_V9 <dbl>,\n#   wordembed_text_V10 <dbl>, wordembed_text_V11 <dbl>,\n#   wordembed_text_V12 <dbl>, wordembed_text_V13 <dbl>,\n#   wordembed_text_V14 <dbl>, wordembed_text_V15 <dbl>,\n#   wordembed_text_V16 <dbl>, wordembed_text_V17 <dbl>,\n#   wordembed_text_V18 <dbl>, wordembed_text_V19 <dbl>, …\n```\n\n\n:::\n:::\n\n\n\n\n## Tuninig/Fitting\n\n\n\n::: {.cell hash='germeval10-wordvec-rf_cache/html/tune-fit_be10aefe5e3de9145b4d480e82093b1c'}\n\n```{.r .cell-code}\ntic()\nwf_fit <-\n  wf1 %>% \n  tune_grid(\n    grid = 5,\n    resamples = vfold_cv(strata = c1, \n                         v = 5,\n                         data = d_train),\n    control = control_grid(save_pred = TRUE,\n                           verbose = TRUE,\n                           save_workflow = FALSE)) \ntoc()\nbeep()\n```\n:::\n\n\n\nOder das schon in grauer Vorzeit berechnete Objekt importieren:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_fit <- read_rds(\"/Users/sebastiansaueruser/github-repos/rexams-exercises/objects/germeval10-wordvec-rf.rds\")\n```\n:::\n\n\n\n\n## Plot performance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(wf_fit)\n```\n\n::: {.cell-output-display}\n![](unnamed-chunk-5-1.png){fig-pos='H' width=384}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(wf_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 7\n   mtry .metric .estimator  mean     n std_err .config             \n  <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1     2 roc_auc binary     0.771     5 0.00846 Preprocessor1_Model3\n2    32 roc_auc binary     0.749     5 0.00878 Preprocessor1_Model4\n3    41 roc_auc binary     0.747     5 0.00859 Preprocessor1_Model2\n4    67 roc_auc binary     0.741     5 0.00952 Preprocessor1_Model1\n5    96 roc_auc binary     0.739     5 0.00883 Preprocessor1_Model5\n```\n\n\n:::\n:::\n\n\n\n## Finalisieren\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_params <- select_best(wf_fit)\ntic()\nwf_finalized <- finalize_workflow(wf1, best_params)\nlastfit1 <- fit(wf_finalized, data = d_train)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n21.916 sec elapsed\n```\n\n\n:::\n:::\n\n\n\n## Test-Set-Güte\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\npreds <-\n  predict(lastfit1, new_data = germeval_test)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n10.773 sec elapsed\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nd_test <-\n  germeval_test |> \n  bind_cols(preds) |> \n  mutate(c1 = as.factor(c1))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_metrics <- metric_set(accuracy, f_meas)\nmy_metrics(d_test,\n           truth = c1,\n           estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.689\n2 f_meas   binary         0.196\n```\n\n\n:::\n:::\n\n\n\n## Fazit\n\n\n`wikipedia2vec` ist für die deutsche Sprache vorgekocht. \nDas macht Sinn für einen deutschsprachigen Corpus.\n\nDas Modell braucht doch ganz schön viel Rechenzeit.\n\nAchtung: Mit dem Parameter `save_pred = TRUE` wird der Workflow größer als 3 GB.\n\n\n\n\n\n\n\n---\n\nCategories: \n\n- textmining\n- datawrangling\n- germeval\n- prediction\n- tidymodels\n- string\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}