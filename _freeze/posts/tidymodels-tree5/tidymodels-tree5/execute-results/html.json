{
  "hash": "e79fc7a34395138cea98e68f37284967",
  "result": {
    "engine": "knitr",
    "markdown": "---\nexname: tidymodels-tree5\nexpoints: 1\nextype: string\nexsolution: NA\ncategories:\n- statlearning\n- trees\n- tidymodels\n- speed\n- string\ndate: '2023-11-08'\nslug: tidymodels-tree5\ntitle: tidymodels-tree5\n\n---\n\n\n\n\n\n\n\n\n# Aufgabe\n\n\nBerechnen Sie folgendes einfache Modell:\n\n1. Random Forest mit `trees=50`\n\n\nModellformel: `body_mass_g ~ .` (Datensatz `palmerpenguins::penguins`)\n\nHier geht es darum, die Geschwindigkeit (und den Ressourcenverbrauch) beim Fitten zu verringern.\nBenutzen Sie dazu folgende Methoden\n\n- Auslassen gering performanter Tuningparameterwerte\n- Verwenden Sie ein Anova-Grid-Search!\n- Parallelisieren Sie auf mehrere Kerne (wenn möglich).\n\n\nHinweise:\n\n- Tunen Sie alle Parameter (die der Engine anbietet). \n- Verwenden Sie Defaults, wo nicht anders angegeben.\n- Beachten Sie die [üblichen Hinweise](https://datenwerk.netlify.app/hinweise).\n\n\n\n\n\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n\n# Lösung\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n✔ broom        1.0.5     ✔ recipes      1.0.8\n✔ dials        1.2.0     ✔ rsample      1.2.0\n✔ dplyr        1.1.3     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n```\n\n\n:::\n\n```{.r .cell-code}\ndata(\"penguins\", package = \"palmerpenguins\")\nlibrary(tictoc)  # Zeitmessung\nlibrary(finetune)  # tune_race_anova\nlibrary(doParallel)  # mehrere CPUs nutzen \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: foreach\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'foreach'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: iterators\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: parallel\n```\n\n\n:::\n\n```{.r .cell-code}\nset.seed(42)\n```\n:::\n\n\n\n\nEntfernen wir Fälle mit fehlenden Werten:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <-\n  penguins %>% \n  drop_na()\n```\n:::\n\n\n\n\n## Daten teilen\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nd_split <- initial_split(d)\nd_train <- training(d_split)\nd_test <- testing(d_split)\n```\n:::\n\n\n\n## Modell(e)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_rf <-\n  rand_forest(mode = \"regression\",\n              mtry = tune())\nmod_rf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n\nComputational engine: ranger \n```\n\n\n:::\n:::\n\n\n\n\n\n## Rezept(e)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_plain <- \n  recipe(body_mass_g ~ ., data = d_train) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_impute_knn(all_predictors())\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train_baked <-\n  bake(prep(rec_plain, d_train), new_data = NULL)\n\nhead(d_train_baked)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 10\n  bill_length_mm bill_depth_mm flipper_length_mm  year body_mass_g\n           <dbl>         <dbl>             <int> <int>       <int>\n1           34.5          18.1               187  2008        2900\n2           52.2          18.8               197  2009        3450\n3           45.4          14.6               211  2007        4800\n4           42.1          19.1               195  2008        4000\n5           50            15.9               224  2009        5350\n6           41.5          18.5               201  2009        4000\n# ℹ 5 more variables: species_Chinstrap <dbl>, species_Gentoo <dbl>,\n#   island_Dream <dbl>, island_Torgersen <dbl>, sex_male <dbl>\n```\n\n\n:::\n:::\n\n\nKeine fehlenden Werte mehr?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(is.na(d_train_baked))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\n\n## Resampling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nrsmpl <- vfold_cv(d_train)\n```\n:::\n\n\n\n## Workflows\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_rf <-\n  workflow() %>%  \n  add_recipe(rec_plain) %>% \n  add_model(mod_rf)\n\nwf_rf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_dummy()\n• step_impute_knn()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n\nComputational engine: ranger \n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Ohne Speed-up\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nfit_rf <-\n  tune_grid(\n    object = wf_rf,\n    resamples = rsmpl)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\ni Creating pre-processing data to finalize unknown parameter: mtry\n```\n\n\n:::\n\n```{.r .cell-code}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n13.061 sec elapsed\n```\n\n\n:::\n:::\n\n\nDie angegebene Rechenzeit bezieht sich auf einen 4-Kerne-MacBook Pro (2020).\n\n\n## Mit Speeed-up 1\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nfit_rf2 <-\n  tune_race_anova(\n    object = wf_rf,\n    resamples = rsmpl)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n17.925 sec elapsed\n```\n\n\n:::\n:::\n\n\n\n## Mit Speeed-up 2\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoParallel::registerDoParallel()\n\ntic()\nfit_tree2 <-\n  tune_race_anova(\n    object = wf_rf,\n    metrics = metric_set(rmse),\n    control = control_race(verbose = FALSE,\n                           pkgs = c(\"tidymodels\"),\n                           save_pred = TRUE),\n            resamples = rsmpl)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\ni Creating pre-processing data to finalize unknown parameter: mtry\n```\n\n\n:::\n\n```{.r .cell-code}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n12.339 sec elapsed\n```\n\n\n:::\n:::\n\n\n\n\n\n## Mit Speeed-up 3\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoParallel::registerDoParallel()\n\ntic()\nfit_tree2 <-\n  tune_grid(object = wf_rf,\n            metrics = metric_set(rmse),\n            control = control_grid(verbose = FALSE,\n                                   save_pred = TRUE),\n            resamples = rsmpl)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\ni Creating pre-processing data to finalize unknown parameter: mtry\n```\n\n\n:::\n\n```{.r .cell-code}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n4.53 sec elapsed\n```\n\n\n:::\n:::\n\n\n\n## Fazit\n\nMit Speed-up ist schneller also ohne.\nEin Random-Forest ist ein Modelltyp, der von Parallelisierung gut profitiert.\n\n\n\n\n\n---\n\nCategories: \n\n- statlearning\n- trees\n- tidymodels\n- speed\n- string\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}