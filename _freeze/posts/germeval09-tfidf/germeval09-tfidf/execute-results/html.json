{
  "hash": "24a3897c571d78775f02011443c3c111",
  "result": {
    "engine": "knitr",
    "markdown": "---\nexname: germeval09-tfidf\nexpoints: 1\nextype: string\nexsolution: NA\ncategories:\n- textmining\n- datawrangling\n- germeval\n- prediction\n- tidymodels\n- string\ndate: '2023-11-16'\nslug: germeval09-tfidf\ntitle: germeval09-tfidf\n\n---\n\n\n\n\n\n\n# Aufgabe\n\nErstellen Sie ein prädiktives Modell für Textdaten, nutzen Sie einen Entscheidungsbaum als Modell. Erstellen Sie pro Wort tfIDF-Kennwerte im Rahmen von Feature-Engineering.\n\nNutzen Sie die [GermEval-2018-Daten](https://heidata.uni-heidelberg.de/dataset.xhtml?persistentId=doi:10.11588/data/0B5VML).\n\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\n\nDie Daten sind auch über das R-Paket [PradaData](https://github.com/sebastiansauer/pradadata/tree/master/data-raw/GermEval-2018-Data-master) zu beziehen.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ndata(\"germeval_train\", package = \"pradadata\")\ndata(\"germeval_test\", package = \"pradadata\")\n```\n:::\n\n\nDie AV lautet `c1`. Die (einzige) UV lautet: `text`.\n\n\nHinweise:\n\n- Orientieren Sie sich im Übrigen an den [allgemeinen Hinweisen des Datenwerks](https://datenwerk.netlify.app/hinweise).\n- Nutzen Sie Tidymodels.\n- Nutzen Sie das `sentiws` Lexikon.\n\n\n\n\n\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n\n# Lösung\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train <-\n  germeval_train |> \n  select(id, c1, text)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tictoc)\nlibrary(tidymodels)\nlibrary(tidytext)\nlibrary(beepr)\nlibrary(textrecipes)  # step_tokenfilter, step_tokenize\n```\n:::\n\n\n\n\n\nEine [Vorlage für ein Tidymodels-Pipeline findet sich hier](https://datenwerk.netlify.app/posts/tidymodels-vorlage2/tidymodels-vorlage2.html).\n\n\n## Workflow\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# model:\nmod1 <-\n  decision_tree(mode = \"classification\")\n\n# recipe:\nrec1 <-\n  recipe(c1 ~ ., data = d_train) |> \n  update_role(id, new_role = \"id\")  |> \n  step_tokenize(text) %>%\n  step_tokenfilter(text, max_tokens = 1e3) %>%\n  step_tfidf(text) %>%\n  step_zv(all_predictors()) %>%\n  step_normalize(all_numeric_predictors())\n\n# workflow:\nwf1 <-\n  workflow() %>% \n  add_model(mod1) %>% \n  add_recipe(rec1)\n```\n:::\n\n\n\n\n## Fit\n\nOhne Tuning:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nfit1 <-\n  fit(wf1,\n      data = d_train)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n28.45 sec elapsed\n```\n\n\n:::\n\n```{.r .cell-code}\n#beep()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfit1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_tokenize()\n• step_tokenfilter()\n• step_tfidf()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 5009 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 5009 1688 OTHER (0.3369934 0.6630066)  \n  2) tfidf_text_merkel>=1.084677 279  117 OFFENSE (0.5806452 0.4193548) *\n  3) tfidf_text_merkel< 1.084677 4730 1526 OTHER (0.3226216 0.6773784) *\n```\n\n\n:::\n:::\n\n\n## Test-Set-Güte\n\n\nVorhersagen im Test-Set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\npreds <-\n  predict(fit1, new_data = germeval_test)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.453 sec elapsed\n```\n\n\n:::\n:::\n\n\nUnd die Vorhersagen zum Test-Set hinzufügen, damit man `TRUTH` und `ESTIMATE` vergleichen kann:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_test <-\n  germeval_test |> \n  bind_cols(preds) |> \n  mutate(c1 = as.factor(c1))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_metrics <- metric_set(accuracy, f_meas)\nmy_metrics(d_test,\n           truth = c1,\n           estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary        0.657 \n2 f_meas   binary        0.0706\n```\n\n\n:::\n:::\n\n\n\n## Prep/Bake\n\nAls Check: Das gepreppte/bebackene Rezept:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nrec1_prepped <- prep(rec1)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n3.376 sec elapsed\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nd_train_baked <- bake(rec1_prepped, new_data = NULL)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.026 sec elapsed\n```\n\n\n:::\n:::\n\n\n\n## Sehr viele Spalten\n\nDas Problem ist, dass dieses Rezept sehr viele Spalten erzeugt.\nDas ist (sehr) rechen- und speicherintensiv.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(d_train_baked)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5009 1002\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train_baked |> \n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 1,002\n     id c1      tfidf_text__macmike tfidf_text_1 tfidf_text_10 tfidf_text_100\n  <int> <fct>                 <dbl>        <dbl>         <dbl>          <dbl>\n1     1 OTHER                -0.137      -0.0692       -0.0531        -0.0475\n2     2 OTHER                -0.137      -0.0692       -0.0531        -0.0475\n3     3 OTHER                -0.137      -0.0692       -0.0531        -0.0475\n4     4 OTHER                -0.137      -0.0692       -0.0531        -0.0475\n5     5 OFFENSE              -0.137      -0.0692       -0.0531        -0.0475\n6     6 OTHER                -0.137      -0.0692       -0.0531        -0.0475\n# ℹ 996 more variables: tfidf_text_12 <dbl>, tfidf_text_14 <dbl>,\n#   tfidf_text_15 <dbl>, tfidf_text_19 <dbl>, tfidf_text_2 <dbl>,\n#   tfidf_text_20 <dbl>, tfidf_text_2017 <dbl>, tfidf_text_2018 <dbl>,\n#   tfidf_text_3 <dbl>, tfidf_text_30 <dbl>, tfidf_text_4 <dbl>,\n#   tfidf_text_5 <dbl>, tfidf_text_6 <dbl>, tfidf_text_66freedom66 <dbl>,\n#   tfidf_text_8 <dbl>, tfidf_text_90 <dbl>, tfidf_text_a <dbl>,\n#   tfidf_text_ab <dbl>, tfidf_text_abend <dbl>, tfidf_text_aber <dbl>, …\n```\n\n\n:::\n:::\n\n\n\n\n\n\n---\n\nCategories: \n\n- textmining\n- datawrangling\n- germeval\n- prediction\n- tidymodels\n- string\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}