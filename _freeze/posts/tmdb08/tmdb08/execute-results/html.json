{
  "hash": "73d61c6c059f920df323e69fc7523ab5",
  "result": {
    "markdown": "---\nexname: tmdb08\nextype: num\nexsolution: r sol\nextol: 0.2\nexpoints: 1\ncategories:\n- ds1\n- tidymodels\n- statlearning\n- tmdb\n- random-forest\n- num\ndate: '2023-05-17'\nslug: tmdb08\ntitle: tmdb08\n\n---\n\n\n\n\n\n\n\n\n\n\n\n# Aufgabe\n\nWir bearbeiten hier die Fallstudie [TMDB Box Office Prediction - \nCan you predict a movie's worldwide box office revenue?](https://www.kaggle.com/competitions/tmdb-box-office-prediction/overview),\nein [Kaggle](https://www.kaggle.com/)-Prognosewettbewerb.\n\nZiel ist es, genaue Vorhersagen zu machen,\nin diesem Fall für Filme.\n\n\nDie Daten können Sie von der Kaggle-Projektseite beziehen oder so:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train_path <- \"https://raw.githubusercontent.com/sebastiansauer/Lehre/main/data/tmdb-box-office-prediction/train.csv\"\nd_test_path <- \"https://raw.githubusercontent.com/sebastiansauer/Lehre/main/data/tmdb-box-office-prediction/test.csv\"\n```\n:::\n\n\n\n# Aufgabe\n\nReichen Sie bei Kaggle eine Submission für die Fallstudie ein! Berichten Sie den Score!\n\n\nHinweise:\n\n- Sie müssen sich bei Kaggle ein Konto anlegen (kostenlos und anonym möglich); alternativ können Sie sich mit einem Google-Konto anmelden.\n- Halten Sie das Modell so *einfach* wie möglich. Verwenden Sie als Algorithmus die *regularisierte lineare Regression* .\n- Minimieren Sie die Vorverarbeitung (`steps`) so weit als möglich.\n- Verwenden Sie `tidymodels`.\n\n\n\n\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n\n# Lösung\n\n\n# Vorbereitung\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(tictoc)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train_raw <- read_csv(d_train_path)\nd_test_raw <- read_csv(d_test_path)\n```\n:::\n\n\n\n\n\n\n## Train-Set verschlanken\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_train <-\n  d_train_raw %>% \n  select(id, popularity, runtime, revenue, budget) \n```\n:::\n\n\n\n## Test-Set verschlanken\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_test <-\n  d_test_raw %>% \n  select(id,popularity, runtime, budget) \n```\n:::\n\n\n\n# Rezept\n\n## Rezept definieren\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec2 <-\n  recipe(revenue ~ ., data = d_train) %>% \n  step_mutate(budget = ifelse(budget == 0, 1, budget)) %>%  # log mag keine 0\n  step_log(budget) %>% \n  step_impute_knn(all_predictors()) %>% \n  step_dummy(all_nominal_predictors())  %>% \n  update_role(id, new_role = \"id\")\n\nrec2\n```\n:::\n\n\n\n\n# Kreuzvalidierung / Resampling\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncv_scheme <- vfold_cv(d_train,\n                      v = 5)\n```\n:::\n\n\n\n# Modelle\n\n\n\n## LM regularisiert \n\nMit `mixture = 1` definieren wir ein Lasso.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_lm <-\n  linear_reg(penalty = tune(), mixture = 1) %>% \n  set_engine(\"glmnet\")\n```\n:::\n\n\nCheck:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_lm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n\n\n\n# Workflow-Set\n\nHier nur ein sehr kleiner Workflow-Set.\n\nDas ist übrigens eine gute Strategie: Erstmal mit einem kleinen Prozess anfangen,\nund dann sukzessive erweitern.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreproc2 <- list(rec1 = rec2)\nmodels2 <- list(lm1 = mod_lm)\n \nall_workflows2 <- workflow_set(preproc2, models2)\n```\n:::\n\n\n\n# Fitten und tunen\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_grid <- grid_max_entropy(penalty(), size = 10)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntmdb_model_set2 <-\n    all_workflows2 %>% \n    workflow_map(resamples = cv_scheme,\n                 verbose = TRUE,\n                 grid = my_grid\n                 )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(tmdb_model_set2)\n```\n\n::: {.cell-output-display}\n![](unnamed-chunk-11-1.png){fig-pos='H' width=384}\n:::\n:::\n\n\n\n\n\n# Finalisieren\n\nWir müssen uns leider händisch das beste Modell raussuchen:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmdb_model_set2 %>% \n  collect_metrics() %>% \n  arrange(mean) %>% \n  filter(.metric == \"rmse\") %>% \n  select(1,2, mean, std_err)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 4\n   wflow_id .config                     mean  std_err\n   <chr>    <chr>                      <dbl>    <dbl>\n 1 rec1_lm1 Preprocessor1_Model01 116759819. 5345245.\n 2 rec1_lm1 Preprocessor1_Model02 116759819. 5345245.\n 3 rec1_lm1 Preprocessor1_Model03 116759819. 5345245.\n 4 rec1_lm1 Preprocessor1_Model04 116759819. 5345245.\n 5 rec1_lm1 Preprocessor1_Model05 116759819. 5345245.\n 6 rec1_lm1 Preprocessor1_Model06 116759819. 5345245.\n 7 rec1_lm1 Preprocessor1_Model07 116759819. 5345245.\n 8 rec1_lm1 Preprocessor1_Model08 116759819. 5345245.\n 9 rec1_lm1 Preprocessor1_Model09 116759819. 5345245.\n10 rec1_lm1 Preprocessor1_Model10 116759819. 5345245.\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_model_params2 <-\nextract_workflow_set_result(tmdb_model_set2, \"rec1_lm1\") %>% \n  select_best()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n```\n:::\n\n```{.r .cell-code}\nbest_model_params2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n   penalty .config              \n     <dbl> <chr>                \n1 1.02e-10 Preprocessor1_Model01\n```\n:::\n:::\n\n\n\n\n## Finalisieren\n\nFinalisieren bedeutet:\n\n- Besten Workflow identifizieren (zur Erinnerung: Workflow = Rezept + Modell)\n- Den besten Workflow mit den optimalen Modell-Parametern ausstatten\n- Damit dann den ganzen Train-Datensatz fitten\n- Auf dieser Basis das Test-Sample vorhersagen\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_wf2 <- \nall_workflows2 %>% \n  extract_workflow(\"rec1_lm1\")\n\nbest_wf2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_mutate()\n• step_log()\n• step_impute_knn()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_wf_finalized2 <- \n  best_wf2 %>% \n  finalize_workflow(best_model_params2)\n\nbest_wf_finalized2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_mutate()\n• step_log()\n• step_impute_knn()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1.01692692955698e-10\n  mixture = 1\n\nComputational engine: glmnet \n```\n:::\n:::\n\n\n## Final Fit\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_final2 <-\n  best_wf_finalized2 %>% \n  fit(d_train)\n\nfit_final2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_mutate()\n• step_log()\n• step_impute_knn()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~1) \n\n   Df  %Dev   Lambda\n1   0  0.00 63460000\n2   1  3.62 57820000\n3   1  6.62 52680000\n4   1  9.11 48000000\n5   1 11.18 43740000\n6   1 12.90 39850000\n7   2 15.24 36310000\n8   2 17.19 33090000\n9   2 18.81 30150000\n10  2 20.16 27470000\n11  2 21.28 25030000\n12  2 22.21 22800000\n13  3 23.10 20780000\n14  3 23.95 18930000\n15  3 24.66 17250000\n16  3 25.25 15720000\n17  3 25.74 14320000\n18  3 26.15 13050000\n19  3 26.49 11890000\n20  3 26.77 10830000\n21  3 27.00  9872000\n22  3 27.20  8995000\n23  3 27.36  8196000\n24  3 27.49  7467000\n25  3 27.60  6804000\n26  3 27.69  6200000\n27  3 27.77  5649000\n28  3 27.83  5147000\n29  3 27.88  4690000\n30  3 27.93  4273000\n31  3 27.96  3894000\n32  3 27.99  3548000\n33  3 28.02  3232000\n34  3 28.04  2945000\n35  3 28.06  2684000\n36  3 28.07  2445000\n37  3 28.08  2228000\n38  3 28.09  2030000\n39  3 28.10  1850000\n40  3 28.11  1685000\n41  3 28.11  1536000\n42  3 28.12  1399000\n43  3 28.12  1275000\n44  3 28.13  1162000\n45  3 28.13  1058000\n46  3 28.13   964500\n\n...\nand 12 more lines.\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npreds <- \nfit_final2 %>% \n  predict(new_data = d_test)\n\nhead(preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 1\n       .pred\n       <dbl>\n1 -14840891.\n2  10804710.\n3  11698900.\n4  99190531.\n5  41798496.\n6  29974421.\n```\n:::\n:::\n\n\n\n## Submission df\n\nWir brauchen die ID-Spalte und die Vorhersagen für die Einreichung:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsubmission_df <-\n  d_test %>% \n  select(id) %>% \n  bind_cols(preds) %>% \n  rename(revenue = .pred)\n\nhead(submission_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 2\n     id    revenue\n  <dbl>      <dbl>\n1  3001 -14840891.\n2  3002  10804710.\n3  3003  11698900.\n4  3004  99190531.\n5  3005  41798496.\n6  3006  29974421.\n```\n:::\n:::\n\n\n\nAbspeichern und einreichen:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_csv(submission_df, file = \"submission_regul_lm.csv\")\n```\n:::\n\n\n\n\nLeider ein schlechter Score: `5.77945`.\n\n\n\n\n\n---\n\nCategories: \n\n- ds1\n- tidymodels\n- statlearning\n- tmdb\n- random-forest\n- num\n\n",
    "supporting": [
      "tmdb08_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}