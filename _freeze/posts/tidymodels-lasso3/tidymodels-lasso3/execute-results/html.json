{
  "hash": "d3fd3f0a49d72fbb1d2e8ca9d2b53609",
  "result": {
    "engine": "knitr",
    "markdown": "---\nexname: tidymodels-lasso3\nexpoints: 1\nextype: string\nexsolution: NA\ncategories:\n- tidymodels\n- statlearning\n- lasso\n- lm\n- string\ndate: '2023-05-17'\nslug: tidymodels-lasso3\ntitle: tidymodels-lasso3\nexecute:\n  cache: true\n  eval: false\n---\n\n\n\n\n# Aufgabe\n\n<!-- Schreiben Sie eine Vorlage für eine prädiktive Analyse mit Tidymodels! -->\n\nSchreiben Sie eine prototypische Analyse für ein Vorhersagemodell mit dem *Lasso*.\n\nBerichten Sie, welche Prädiktoren nach dem Lasso im Modell verbleiben.\n\nHinweise:\n\n- Tunen Sie die Penalisierung.\n- Verwenden Sie Kreuzvalidierung.\n- Verwenden Sie Standardwerte, wo nicht anders angegeben.\n- Fixieren Sie Zufallszahlen auf den Startwert 42.\n- Verwenden Sie den Datensatz `penguins`.\n- Modellformel: `body_mass_g ~ .`\n\n\n\n\n\n\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n\n# Lösung\n\n## Standardvorgehen\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 2023-05-14\n\n# Setup:\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(tictoc)  # Zeitmessung\nlibrary(vip)  # Variablenbedeutung\n\n# Data:\nd_path <- \"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\"\nd <- read_csv(d_path)\n\n# drop rows with NA in outcome variable:\nd <-\n  d %>% \n  drop_na(body_mass_g)\n\nset.seed(42)\nd_split <- initial_split(d)\nd_train <- training(d_split)\nd_test <- testing(d_split)\n\n\n# model:\nmod_lasso <-\n  linear_reg(mode = \"regression\",\n             penalty = tune(),\n             mixture = 1,\n             engine = \"glmnet\")\n\n# cv:\nset.seed(42)\nrsmpl <- vfold_cv(d_train)\n\n\n# recipe:\nrec1_plain <- \n  recipe(body_mass_g ~  ., data = d_train) %>% \n  update_role(\"rownames\", new_role = \"id\") %>% \n  step_normalize(all_numeric_predictors()) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_impute_bag(all_predictors())\n\n\n# check:\nd_train_baked <- \n  prep(rec1_plain) %>% bake(new_data = NULL)\n\nna_n <- sum(is.na(d_train_baked))\n\n\n# workflow:\nwf1 <-\n  workflow() %>% \n  add_model(mod_lasso) %>% \n  add_recipe(rec1_plain)\n\n\n# tuning:\ntic()\nwf1_fit <-\n  wf1 %>% \n  tune_grid(\n    resamples = rsmpl)\ntoc()\n\n# best candidate:\nshow_best(wf1_fit)\n\n\n# finalize wf:\nwf1_final <-\n  wf1 %>% \n  finalize_workflow(select_best(wf1_fit))\n\n\nwf1_fit_final <-\n  wf1_final %>% \n  last_fit(d_split)\n\n\n# Modellgüte im Test-Set:\ncollect_metrics(wf1_fit_final)\n```\n:::\n\n\n## Inspektion der Tuningparameter\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(wf1_fit)\n```\n:::\n\nDie Standard-Wahl der Tuningparameter-Werte war offenbar nicht so ideal,\nzumindest sieht man kaum Unterschiede zwischen der Modellgüte in Abhängigkeit von den Werten der Tuningparameter.\n\n\n## Variablenbedeutung\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(vip)\n\nvi_preds <- \nwf1_fit_final %>% \n  extract_fit_engine() %>% \n  vi()\n\nvi_preds\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvi_preds %>% \n  ggplot(aes(x = Importance, y = reorder(Variable, Importance), fill = Sign)) +\n  geom_col()\n```\n:::\n\n\n\n\n\n\nMan beachte: Für regulierte Modelle sind Zentrierung und Skalierung nötig.\n\n\n\n---\n\nCategories: \n\n- tidymodels\n- statlearning\n- lasso\n- lm\n- string\n- template\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}