{
  "hash": "4759ea7e72fd47f3b47e8fb6279255a3",
  "result": {
    "markdown": "---\nexname: tidymodels-tree3\nexpoints: 1\nextype: string\nexsolution: NA\ncategories:\n- statlearning\n- trees\n- tidymodels\n- speed\n- string\ndate: '2023-05-31'\nslug: tidymodels-tree3\ntitle: tidymodels-tree3\n\n---\n\n\n\n\n\n\n\n\n# Aufgabe\n\n\nBerechnen Sie folgendes einfache Modell:\n\n1. Entscheidungsbaum\n\n\nModellformel: `body_mass_g ~ .` (Datensatz `palmerpenguins::penguins`)\n\nHier geht es darum, die Geschwindigkeit (und den Ressourcenverbrauch) beim Fitten zu verringern.\nBenutzen Sie dazu folgende Methoden\n\n- Auslassen gering performanter Tuningparameterwerte\n\nHinweise:\n\n- Tunen Sie alle Parameter (die der Engine anbietet). \n- Verwenden Sie Defaults, wo nicht anders angegeben.\n- Beachten Sie die [üblichen Hinweise](https://datenwerk.netlify.app/hinweise).\n\n\n\n\n\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n</br>\n\n# Lösung\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.0 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ broom        1.0.4     ✔ recipes      1.0.6\n✔ dials        1.2.0     ✔ rsample      1.1.1\n✔ dplyr        1.1.2     ✔ tibble       3.2.1\n✔ ggplot2      3.4.2     ✔ tidyr        1.3.0\n✔ infer        1.0.4     ✔ tune         1.1.1\n✔ modeldata    1.1.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.0     ✔ workflowsets 1.0.1\n✔ purrr        1.0.1     ✔ yardstick    1.2.0\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n```\n:::\n\n```{.r .cell-code}\ndata(\"penguins\", package = \"palmerpenguins\")\nlibrary(tictoc)  # Zeitmessung\nlibrary(finetune)  # tune_race_anova\nset.seed(42)\n```\n:::\n\n\n\n\nEntfernen wir Fälle ohne y-Wert:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <-\n  penguins %>% \n  drop_na(body_mass_g)\n```\n:::\n\n\n\n\n## Daten teilen\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nd_split <- initial_split(d)\nd_train <- training(d_split)\nd_test <- testing(d_split)\n```\n:::\n\n\n\n## Modell(e)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_tree <-\n  decision_tree(mode = \"regression\",\n                cost_complexity = tune(),\n                tree_depth = tune(),\n                min_n = tune())\n```\n:::\n\n\n\n\n\n## Rezept(e)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_plain <- \n  recipe(body_mass_g ~ ., data = d_train)\n```\n:::\n\n\n\n\n## Resampling\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nrsmpl <- vfold_cv(d_train)\n```\n:::\n\n\n\n## Workflows\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwf_tree <-\n  workflow() %>%  \n  add_recipe(rec_plain) %>% \n  add_model(mod_tree)\n```\n:::\n\n\n\n\n\n\n## Tuning-Grid\n\nTuninggrid:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntune_grid <- grid_regular(extract_parameter_set_dials(mod_tree), levels = 5)\n```\n:::\n\n\n\nDie Zeilen im Tuninggrid zeigen uns, für wie viele Modellparameter ein Modell berechnet wird.\nNatürlich üblicherweise jedes Modell mit Resampling.\nDa kommt in Summe ein mitunter sehr große Menge an Modellberechnungen zusammen.\n\n## Ohne Speed-up\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nfit_tree <-\n  tune_grid(object = wf_tree,\n            grid = tune_grid,\n            metrics = metric_set(rmse),\n            resamples = rsmpl)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n95.238 sec elapsed\n```\n:::\n:::\n\n\nca.  auf meinem Rechner (4-Kerne-MacBook Pro 2020).\n\n\n## Mit geschicktem Weglassen von Tuningparametern\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nfit_tree2 <-\n  tune_race_anova(object = wf_tree,\n            grid = tune_grid,\n            metrics = metric_set(rmse),\n            resamples = rsmpl)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n87.989 sec elapsed\n```\n:::\n:::\n\n\n\nca.  - schneller!\n\n\n\n\n\n\n---\n\nCategories: \n\n- statlearning\n- trees\n- tidymodels\n- speed\n- string\n\n",
    "supporting": [
      "tidymodels-tree3_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}