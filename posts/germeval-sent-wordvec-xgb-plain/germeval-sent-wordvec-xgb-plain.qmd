---
expoints: 1
extype: string
exsolution: NA
categories:
- textmining
- datawrangling
- germeval
- prediction
- tidymodels
- sentiment
- string
- xgb
- tune
date: '2023-12-03'
title: germeval03-sent-wordvec-xgb-plain
draft: false   # DRAFT TRUE
execute: 
  eval: false
---



```{r global-knitr-options, include=FALSE, message=FALSE}
knitr::opts_chunk$set(fig.pos = 'H',
                      fig.asp = 0.618,
                      fig.width = 4,
                      fig.cap = "", 
                      fig.path = "",
                      echo = TRUE,  # ECHO IS FALSE!!!
                      message = FALSE,
                      warning = FALSE,
                      fig.show = "hold")

options(max.print = 10)
options(rstudio.help.showDataPreview = FALSE)
```



# Aufgabe

Erstellen Sie ein prädiktives Modell für Textdaten. Nutzen Sie Sentiments und TextFeatures im Rahmen von Feature-Engineering.
Nutzen Sie außerdem *deutsche Word-Vektoren* für das Feature-Engineering.

Als Lernalgorithmus verwenden Sie XGB. 

Preppen und Backen Sie das Rezept,
aber führen Sie die Pipelien mit dem gebackenen Datensatz und einem "Plain-Rezept" durch.


## Daten

Verwenden Sie die [GermEval-2018-Daten](https://heidata.uni-heidelberg.de/dataset.xhtml?persistentId=doi:10.11588/data/0B5VML).

Die Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),

Die Daten sind auch über das R-Paket [PradaData](https://github.com/sebastiansauer/pradadata/tree/master/data-raw/GermEval-2018-Data-master) zu beziehen.


```{r data}
library(tidyverse)
data("germeval_train", package = "pradadata")
data("germeval_test", package = "pradadata")
```

## AV und UV

Die AV lautet `c1`. Die (einzige) UV lautet: `text`.


## Hinweise

- Orientieren Sie sich im Übrigen an den [allgemeinen Hinweisen des Datenwerks](https://datenwerk.netlify.app/hinweise).
- Nutzen Sie Tidymodels.
- Nutzen Sie das `sentiws` Lexikon.
- ❗ Achten Sie darauf, die Variable `c2` zu entfernen bzw. nicht zu verwenden.





</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>

# Lösung


## Setup

```{r d-train}
d_train <-
  germeval_train |> 
  select(id, c1, text)
```


```{r libs}
library(tictoc)
library(tidymodels)
library(syuzhet)
library(beepr)
library(lobstr)  # object size
library(visdat)  # Fingerprint/footprint of dataset (CSV)
data("sentiws", package = "pradadata")
```


Eine [Vorlage für ein Tidymodels-Pipeline findet sich hier](https://datenwerk.netlify.app/posts/tidymodels-vorlage2/tidymodels-vorlage2.html).



## Learner/Modell

```{r}
mod <-
  boost_tree(mode = "classification",
             learn_rate = tune(), 
             tree_depth = tune()
             )
```

## Rezept Workvektoren

Pfad zu den Wordvektoren:

```{r}
path_wordvec <- "/Users/sebastiansaueruser/datasets/word-embeddings/wikipedia2vec/part-0.arrow"
```



```{r}
#| eval: false
source("https://raw.githubusercontent.com/sebastiansauer/Datenwerk2/main/funs/def_recipe_wordvec_senti.R")

rec <- def_recipe_wordvec_senti(data_train = d_train,
                                path_wordvec = path_wordvec)
```



## Prep/Bake Wordvektoren

```{r prep-rec}
#| eval: false
tic()
rec_prepped <- prep(rec)
toc()
```


`78.021 sec elapsed`



```{r bake}
#| eval: false
d_rec_baked <- bake(rec_prepped, new_data = NULL)
```


```{r}
#| eval: false
sum(is.na(d_rec_baked))
```


## Test-Set auch baken

```{r}
#| eval: false
d_test_baked <- bake(rec_prepped, new_data = germeval_test)
dim(d_test_baked)
```


```{r}
#| eval: false
write_csv(d_test_baked, "data/germeval/germeval_test_recipe_wordvec_senti.csv")
```

Später kann man es dann analog wieder importieren:

```{r}
d_test_baked <- read_csv("https://raw.githubusercontent.com/sebastiansauer/Datenwerk2/main/data/germeval/germeval_test_recipe_wordvec_senti.csv")
```


## Gebackenen Datensatz als neue Grundlage

Den gepreppten/gebackenen Datensatz speichern wir als Datensatz ab:

```{r}
#| eval: false
write_csv(d_rec_baked, "https://raw.githubusercontent.com/sebastiansauer/Datenwerk2/main/data/germeval/germeval_train_recipe_wordvec_senti.csv")
```


Später können wir den Datensatz als "neuen, frischen" Datensatz für ein "Plain-Rezept", also ein ganz einfaches Rezept nutzen.
Das hat den Vorteil (hoffentlich), das die Datenvolumina viel kleiner sind.

```{r import-train-data}
d_train_new <-
  read_csv("https://raw.githubusercontent.com/sebastiansauer/Datenwerk2/main/data/germeval/germeval_train_recipe_wordvec_senti.csv")
```


```{r vis-dat}
vis_dat(d_train_new) +
  # remove axis labels:
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank() 
        )
```



## Plain-Rezept

```{r}
rec <- 
  recipe(c1 ~ ., data = d_train_new)
```




## Neuer Workflow mit plainem Rezept

```{r wf-new}
wf <-
  workflow() |> 
  add_recipe(rec) |> 
  add_model(mod)

wf
```





## Parallelisierung über mehrere Kerne

```{r}
library(parallel)
all_cores <- detectCores(logical = FALSE)

library(doFuture)
registerDoFuture()
cl <- makeCluster(2)
plan(cluster, workers = cl)
```


Achtung: Viele Kerne brauchen auch viel Speicher.

## Tune/Resample/Fit



```{r fit}
tic()
fit_wordvec_senti_xgb <-
  tune_grid(
    wf,
    grid = 50,
    resamples = vfold_cv(d_train_new, v = 5))
toc()
beep()
```



Objekt-Größe:

```{r}
lobstr::obj_size(fit_wordvec_senti_xgb)
```

Ah! Angenehm klein.




## Get best performance

```{r}
autoplot(fit_wordvec_senti_xgb)
```


```{r show-best}
show_best(fit_wordvec_senti_xgb)
```


```{r select-best}
best_params <- select_best(fit_wordvec_senti_xgb)
```


## Finalisieren


```{r finalize-wf}
best_params <- select_best(fit_wordvec_senti_xgb)
tic()
wf_finalized <- finalize_workflow(wf, best_params)
lastfit_xgb <- fit(wf_finalized, data = d_train_new)
toc()
```


## Test-Set-Güte

```{r predict}
tic()
preds <-
  predict(lastfit_xgb, new_data = d_test_baked)
toc()
```


```{r bind-cols}
d_test <-
  germeval_test |> 
  bind_cols(preds) |> 
  mutate(c1 = as.factor(c1))
```


```{r metrics}
my_metrics <- metric_set(accuracy, f_meas)
my_metrics(d_test,
           truth = c1,
           estimate = .pred_class)
```


## Fazit

Verzichtet man auf ein Rezept mit viel Datenvolument (Wordvektoren blähen das Rezept mächtig auf), so wird das Fitten schlanker und schneller.
Schneller auch deshalb, weil ggf. kein Swapping zwischen Speicher und Festplatte mehr nötig ist.










