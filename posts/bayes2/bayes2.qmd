---
exname: bayes2
extype: num
exsolution: r exams::fmt(sol)
exshuffle: no
extol: 0.02
expoints: 1
categories:
- R
- bayes
- probability
- num
date: '2023-11-08'
slug: bayes2
title: bayes2

---



```{r global-knitr-options, include=FALSE, message=FALSE}
knitr::opts_chunk$set(fig.pos = 'H',
                      fig.asp = 0.618,
                      fig.width = 4,
                      fig.cap = "", 
                      fig.path = "",
                      echo = TRUE,  # ECHO IS TRUE!!!
                      message = FALSE,
                      warning = FALSE,
                      fig.show = "hold")
```







# Aufgabe



Wir haben eine Münze $n=10$ Mal geworfen. Unsere Daten ($D$) sind: 8 Mal lag "Kopf" oben. 
Gegeben dieser Datenlage, wie hoch ist die Wahrscheinlichkeit für das Ereignis $F$ (*F*alschspieler-Münze), dass die Münze also gezinkt ist auf $p=.8$? 
Apriori sind wir indifferent, ob die Münze gezinkt ist oder nicht ($\neg F$, also $p=.5$). 
Der Einfachheit halber gehen wir davon aus, dass es nur zwei Zustände für die Münze geben kann, gezinkt ($F$) oder nicht gezinkt ($\neg F$).

*Aufgabe*: Berechnen Sie die Wahrscheinlichkeit, dass die Münze gezinkt ist ($F$), gegeben der Datenlage $D$!

Hinweise:

- Orientieren Sie sich im Übrigen an den [allgemeinen Hinweisen des Datenwerks](https://datenwerk.netlify.app/hinweise).




</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>

# Lösung

Gesucht ist die Wahrscheinlichkeit, dass die Münze gezinkt ist, gegeben der beobachteten Daten: $Pr(F|D)$.

```{r}
p_gezinkt = .8
p_fair = .5
n = 10
k = 8
```


Wir können Bayes' Theorem zur Lösung nutzen:

$Pr(F|D) = \frac{L \times Priori}{Evidenz} = \frac{Pr(D|F) Pr(F)}{Pr(D)} =  \frac{Pr(D|F) Pr(F)}{Pr(D|F)Pr(F)  + {Pr(D|\neg F)Pr(\neg F)}}$

Die Priori-Wahrscheinlichkeit für die Hypothese, dass die Münze *gezinkt* ist, beträgt 1/2, da wir indifferent sind: $Pr(F) = 1/2$.

Die Likelihood, L, für die Hypothese einer *gezinkten* Münze, berechnet sich so:

```{r}
L_gezinkt <- dbinom(x = k, size = n, prob = p_gezinkt)
L_gezinkt
```

Der Zähler des Bruchs (unstand. Post) berechnet sich so:

```{r}
Post_unstand <- L_gezinkt * 1/2
Post_unstand
```




Die Likelihood für die Daten, wenn die Münze *fair* (nicht gezinkt ist), beträgt:

```{r}
L_fair <- dbinom(x = k, size = n, prob = p_fair)
L_fair
```

Die unstand. Post-Wahrscheinlichkeit für die Hypothese, dass die Münze fair ist, gegeben der Daten:

```{r}
Post_unstand2 <- L_fair * 1/2
Post_unstand2
```

Die Evidenz, E, berechnet sich als Summe aller unstand. Post-Wahrscheinlichkeiten (also über alle möglichen Hypothesen, d.h. $F$ und $\neg F$, also $L$ plus $L_2$):


```{r}
E <- Post_unstand + Post_unstand2
E
```



Die standardisierte Post-Wahrscheinlichkeit ist also die unstand. Post-Wahrscheinlichkeit geteilt durch die Evidenz:

```{r}
Post_std <- Post_unstand / E
Post_std
```



```{r}
#| echo: false

sol <- Post_std |> round(2)
```


*Antwort*: Die Lösung beträgt ``r sol``.


Alternativ zur Formel kann man auch eine Bayesbox verwenden, um die Aufgabe zu lösen.

```{r}
library(prada)
bayesbox(hyps = c("fair", "gezinkt"),
         priors = c(1/2, 1/2),
         liks = c(L_fair, L_gezinkt))
```
Übrigens würde sich die standardisierte Posteriori-Verteilung nicht ändern, 
wenn man als Priori bei den beiden Hypothesen jeweils 1 annehmen würden.



Die Likelihood für die Binomialverteilung kann man auch mit dem Taschenrechner (oder im Kopf...) berechnen:
gi
Likelihood für die Daten unter der Annahme einer *gezinkten* Münze:

```{r}
choose(n, k) * p_gezinkt^k * (1-p_gezinkt)^(n-k)
```

Likelihood für die Daten unter der Annahme einer *fairen* Münze:

```{r}
choose(n, (n-k)) * p_fair^(n-k) * (1-p_fair)^(k)
```

---

Categories: 

- R
- bayes
- probability
- num

