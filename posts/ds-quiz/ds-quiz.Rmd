---
exname: ds-quiz
extype: mchoice
exsolution: 10010
exshuffle: yes
categories:
- ds1
- tidymodels
- statlearning
- mchoice
date: '2023-05-17'
slug: ds-quiz
title: ds-quiz

---







# Aufgabe

Im Folgenden sind mehrere Aussagen zum Thema maschinelles Lernen dargestellt.
Wählen Sie alle korrekten Aussagen aus!

Hinweise:

- Alle Aussagen sind entweder richtig oder falsch, aber nicht beides.
- Beziehen Sie sich im Zweifel auf den Stoff wie im Unterricht dargestellt.



Answerlist
----------

* Decision Trees (Baummodelle) sind Overfitting (Überanpassung) mehr ausgesetzt als lineare Modelle.
* Ein Resampling-Schema mit $v=10$ Faltungen und $r=5$ Wiederholungen ist identisch zu einem Schema mit $v=50$ Faltungen und $r=1$ Wiederholungen.
* "Normale" (nicht regularisierte) lineare Modelle sind besser interpretierbar als L1-regularisierte lineare Modelle.
* "Normale" lineare Modelle verfügen nicht über Tuningparameter.
* Je größer die Anzahl der Bäume in einem Random Forest, desto größer die Gefahr des Overfittings.




</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>
</br>

# Lösung


Answerlist
----------


* Richtig. Decision Trees neigen zum Overfitting, sie sind sehr flexibel und haben daher viel Varianz in ihren Entscheidungen.
* Falsch. Bei 10 Faltungen beinhaltet das Test-Sample 1/10 der Beobachtungen und entsprechend bei 50 Faltungen nur 1/50 der Beobachtungen. Außerdem erreicht man durch die Wiederholungen eine robustere (präzisere) Schätzung der Vorhersagegüte.
* Falsch. Durch die L1-Regularisierung (Lasso) werden (oft) Prädiktoren entfernt (ihr Gewicht auf Null gesetzt), so dass das resultierende Modell einfacher und damit auch einfacher interpretierbarer ist.
* Richtig.
* Falsch. Normalerweise steigt die Modellgüte mit der Anzahl der Bäume bis zu einem Sättigungsniveau, ab welchem zusätzliche Bäume die Vorhersagegüte nicht mehr beeinflussen (allerdings die Rechenzeit schon).




---

Categories: 

- ds1
- tidymodels
- statlearning
- mchoice

