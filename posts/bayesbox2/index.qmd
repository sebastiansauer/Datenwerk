---
# gleich diese Datei in einem Ordner mit Namen der Aufgabe abspeichern!
date: 2024-11-03
draft: FALSE   # ACHTUNG DRAFT STEHT AUF TRUE!
title: bayesbox2  # HIER TITEL DES POSTS EINGEBEN.
execute: 
  eval: true 
highlight-style: arrow 
cache: true
toc: true
number-sections: true
extype: string
exsolution: ""
exshuffle: no
categories:
- bayes  # ENTER CATEGORIES HERE
- probability
- paper

bibliography: "../../library-ses.bib"


---


# Setup

```{r}
#| message: false
library(tidyverse)
library(ggpubr)
```




# Aufgabe 

Sie f√ºhren ein zweiwertiges (binomiales)
 Zufallsexperiment $n$-mal durch,
 dabei erzielen Sie $k$ Treffer.
 Die Wiederholungen sind unabh√§ngig voneinander, und die Trefferwahrscheinlichkeit $\pi$ bleibt konstant.
 
(Eine M√ºnze wiederholt werfen w√§re das typische Beispiel f√ºr ein solches Zufallexperiment.)

Gehen Sie von folgenden Parameterwerten aus:

```{r}
n <- 10
k <- 10
```

Welcher Parameterwert $\pi$ ist am wahrscheinlichsten, wenn Sie keine weiteren Informationen haben?

Sie √ºberpr√ºfen alle 11 Parameterwerte f√ºr $\pi$ von 0 bis 1 (in Schritten von 0.1.)

Um diese Frage zu beantworten, 
berechnen Sie die Wahrscheinlichkeiten f√ºr alle m√∂glichen Parameterwerte $\pi$ von 0 bis 1 in Schritten von 0.1 anhand einer Bayesbox.
Dabei gehen wir von einer Binomialverteilung aus:

$k \sim Bin(n, \pi)$.






```{r p-gitter}
#| lst-label: lst-wasseranteile
#| lst-cap: "Parameterwerte (Gitter) f√ºr Trefferwahrscheinlichkeit: 0, 0.1, 0.2, ..., 1"
pis <- seq(from = 0, to = 1, by = 0.1)  # Parameterwerte
pis
```


Dann berechnen wir schon mal die Wahrscheinlichkeit der Daten  gegeben jeweils eines Parameterwerts:

```{r lik-69}
Likelihood <- dbinom(k, size = n, prob = pis)
Likelihood
```

Auf dieser Basis erstellen wir eine Bayes-Box, um die Posteriori-Wahrscheinlichkeiten f√ºr alle Parameterwerte zu berechnen,
s. @lst-gitter1.


```{r QM2-Thema2-kleineModelle-28, echo = TRUE}
#| lst-cap: "Wir basteln uns eine Bayes-Box"
#| lst-label: lst-gitter1

d <-
  tibble(
    # definiere die Hypothesen (die Parameterwerte, p): 
    p = pis,
    # Lege den Priori-Wert fest:
    Priori  = 1/11) |> 
    mutate(
      # berechne Likelihood f√ºr jeden Wasseranteil (Parameterwert):
      Likelihood = Likelihood,
      # berechne unstand. Posteriori-Werte:
      unstd_Post = Likelihood * Priori,
      # berechne Evidenz, d.i. die Summe aller unstand. Post-Werte:
      Evidenz = sum(unstd_Post),
      # berechne stand. Posteriori-Werte (summiert zu 1):
      Post = unstd_Post / Evidenz)  
```

Die Bayes-Box (@tbl-globus) zeigt, wie sich die Post-Verteilung berechnet.

Leider sind die zentralen Spalten ausgeblendet. ü§¨



```{r QM2-Thema2-kleineModelle-29}
#| label: tbl-globus
#| tbl-cap: "Die Bayes-Box"
#| echo: false
d %>% 
  mutate(id = 1:nrow(d)) %>% 
  relocate(id, .before = 1) %>% 
  select(id, p, Priori ) |> 
  knitr::kable(digits = 2) 
```


**Aufgabe:** 
Welcher Parameterwert $\pi$ ist am wahrscheinlichsten laut der Bayesbox?

</br>
</br>
</br>

</br>
</br>
</br>

</br>
</br>
</br>

</br>
</br>
</br>











# L√∂sung 

Der wahrscheinlichste Parameterwert $\pi$ ist derjenige, der die h√∂chste Posteriori-Wahrscheinlichkeit hat.

Bei $k=n$ Treffer hat $p=1$ die h√∂chste Posteriori-Wahrscheinlichkeit.


```{r}
#| echo: false
d %>% 
  mutate(id = 1:nrow(d)) %>% 
  relocate(id, .before = 1) %>% 
  knitr::kable(digits = 2) 
```

Hier ist eine Visualisierung der Posteriori-Wahrscheinlichkeiten:


```{r}
ggline(d, x = "p", y = "Post", 
       xlab = "Trefferwahrscheinlichkeit", ylab = "Posteriori-Wahrscheinlichkeit",
       title = "Posteriori-Wahrscheinlichkeiten f√ºr Trefferwahrscheinlichkeit",
       add = c("point", "smooth"))
```

