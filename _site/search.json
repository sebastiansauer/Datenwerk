[
  {
    "objectID": "Hinweise.html",
    "href": "Hinweise.html",
    "title": "Hinweise",
    "section": "",
    "text": "Beachten Sie Hinweise aus dem Hinweisbuch, insbesondere:\n\nallgemeine Prüfungshinweise\nfür quantitative Prüfungen"
  },
  {
    "objectID": "Hinweise.html#bearbeitungshinweise",
    "href": "Hinweise.html#bearbeitungshinweise",
    "title": "Hinweise",
    "section": "",
    "text": "Beachten Sie Hinweise aus dem Hinweisbuch, insbesondere:\n\nallgemeine Prüfungshinweise\nfür quantitative Prüfungen"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Ans Werk, Daten!",
    "section": "",
    "text": "Datenwerk ist eine Sammlung von Aufgaben mit Bezug zur Datenanalyse.\nAutor: Sebastian Sauer (soweit nicht anders ausgewiesen)\nDer Quellcode dieser Webseite findet sich hier.\nDie Lizenz ist permissiv, s. Hinweise hier."
  },
  {
    "objectID": "posts/tidymodels-poly02/tidymodels-poly02.html",
    "href": "posts/tidymodels-poly02/tidymodels-poly02.html",
    "title": "tidymodels-poly02",
    "section": "",
    "text": "Aufgabe\nFitten Sie ein Polynomial-Modell für folgende Modellgleichung:\nbody_mass_g ~ bill_length_mm.\nGesucht ist der RMSE im Test-Set (optimal hinsichtlich minimalem Prognosefehler).\nHinweise:\n\nDatensatz penguins (palmerpenguins)\nVerwenden Sie Tidymodels\nFitten Sie Polynome des Grades 1 bis 10.\nDefinieren Sie die Polynomegrade als Tuningparameter.\nEntfernen Sie fehlende Werte in den Prädiktoren.\nWie immer gilt: Verwenden Sie die Standardeinstellungen der Funktionen, soweit nicht anders angegeben.\n\n         \n\n\nLösung\nSetup:\n\nlibrary(tidymodels)\ndata(penguins, package = \"palmerpenguins\")\n\nDatenaufteilung:\n\nd_split &lt;- initial_split(penguins)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\nRezept:\n\nrec1 &lt;- \n  recipe(body_mass_g ~ bill_length_mm, data = penguins) %&gt;% \n  step_naomit(all_predictors()) %&gt;% \n  step_poly(all_predictors(), degree = tune()) %&gt;% \n  update_role(contains(\"_poly_\"), new_role = \"predictor\")\n\nWarning: No columns were selected in `update_role()`.\n\n\nCheck:\n\nd_baked &lt;- bake(prep(rec1), new_data = NULL)\n\nRezepte mit Tuningparametern kann man nicht preppen/backen.\nWorkflow:\n\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(linear_reg()) %&gt;% \n  add_recipe(rec1)\n\nTuning:\n\nset.seed(42)\ntune1 &lt;-\n  tune_grid(\n    wf1,\n    resamples = vfold_cv(data = penguins),\n    metrics = metric_set(rmse),\n    grid = grid_regular(degree(range = c(1, 10)),\n                               levels = 10),\n    control = control_grid(save_workflow = TRUE)\n  )\n\n\nautoplot(tune1)\n\n\n\n\n\n\n\n\n\nshow_best(tune1)\n\n# A tibble: 5 × 7\n  degree .metric .estimator  mean     n std_err .config              \n   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1      2 rmse    standard    638.    10    22.7 Preprocessor02_Model1\n2      4 rmse    standard    641.    10    23.7 Preprocessor04_Model1\n3      1 rmse    standard    643.    10    21.8 Preprocessor01_Model1\n4      5 rmse    standard    643.    10    23.5 Preprocessor05_Model1\n5      3 rmse    standard    643.    10    24.2 Preprocessor03_Model1\n\n\nFinalisieren:\n\nbest1 &lt;- fit_best(tune1)\nbest1\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_naomit()\n• step_poly()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n          (Intercept)  bill_length_mm_poly_1  bill_length_mm_poly_2  \n                 4202                   8813                  -1708  \n\n\nPredicten:\n\nfinal1 &lt;- last_fit(best1, d_split)\ncollect_metrics(final1)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard     666.    Preprocessor1_Model1\n2 rsq     standard       0.287 Preprocessor1_Model1\n\n\nOder so:\n\nsol &lt;- \npredict(best1, new_data = d_test) %&gt;% \n  bind_cols(d_test) %&gt;% \n  rmse(truth = body_mass_g, estimate = .pred) %&gt;% \n  pull(.estimate) %&gt;% \n  pluck(1)\n\nsol\n\n[1] 657.5297\n\n\nDie Antwort lautet: 657.5296534.\n\nCategories:\n\nR\nstatlearning\ntidymodels\nnum"
  },
  {
    "objectID": "posts/summarise05/summarise05.html",
    "href": "posts/summarise05/summarise05.html",
    "title": "summarise05",
    "section": "",
    "text": "Aufgabe\nIm Datensatz mariokart:\n\nGruppieren Sie danach, wie viele Lenkräder bei der Auktion dabei waren.\nFassen Sie die Spalte total_pr zusammen und zwar zur MAA und zum IQR - pro Gruppe!\n\nGeben Sie den erste Wert des IQR als Antwort zurück!\n         \n\n\nLösung\nPakete starten:\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.6.0 (red = needs update)\n✔ bayestestR  0.13.1   ✔ correlation 0.8.4 \n✔ datawizard  0.9.0    ✔ effectsize  0.8.6 \n✔ insight     0.19.6   ✔ modelbased  0.8.6 \n✔ performance 0.10.8   ✔ parameters  0.21.3\n✔ report      0.5.7    ✖ see         0.8.0 \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`.\n\nlibrary(tidyverse)  # startet das Paket tidyverse\n\nDaten importieren:\n\nmariokart &lt;- data_read(\"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\")\n\nOder so:\n\ndata(mariokart, package = \"openintro\")  # aus dem Paket \"openintro\"\n\nDazu muss das Paket openintro auf Ihrem Computer installiert sein.\nZusammenfassen:\n\nlibrary(DescTools)\nmariokart_gruppiert &lt;- group_by(mariokart, wheels)  # Gruppieren\nmariokart_klein &lt;- summarise(mariokart_gruppiert, \n                             pr_iqr = IQR(total_pr),\n                             pr_maa = mean(abs(total_pr - mean(total_pr))),\n                             pr_maa2 = MeanAD(total_pr)\n                             )  # zusammenfassen\nmariokart_klein\n\n# A tibble: 5 × 4\n  wheels pr_iqr pr_maa pr_maa2\n   &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1      0   7      7.05    7.05\n2      1   5.32   3.25    3.25\n3      2   7.18  11.9    11.9 \n4      3   5.25   5.25    5.25\n5      4   0      0       0   \n\n\nMöchte man den MAA nicht von Hand ausrechnen, so kann man die Funktion MeanAD aus dem Paket DescTools nutzen (Denken Sie daran, dass Sie das Paket einmalig installiert haben müssen.)\nDie Lösung lautet: 7.00\n\nCategories:\n\ndatawrangling\neda\ntidyverse\ndplyr\nvariability\nnum"
  },
  {
    "objectID": "posts/tmdb03/tmdb03.html",
    "href": "posts/tmdb03/tmdb03.html",
    "title": "tmdb03",
    "section": "",
    "text": "Aufgabe\nWir bearbeiten hier die Fallstudie TMDB Box Office Prediction - Can you predict a movie’s worldwide box office revenue?, ein Kaggle-Prognosewettbewerb.\nZiel ist es, genaue Vorhersagen zu machen, in diesem Fall für Filme.\nDie Daten können Sie von der Kaggle-Projektseite beziehen oder so:\n\nd_train_path &lt;- \"https://raw.githubusercontent.com/sebastiansauer/Lehre/main/data/tmdb-box-office-prediction/train.csv\"\nd_test_path &lt;- \"https://raw.githubusercontent.com/sebastiansauer/Lehre/main/data/tmdb-box-office-prediction/test.csv\"\n\n\n\nAufgabe\nReichen Sie bei Kaggle eine Submission für die Fallstudie ein! Berichten Sie den Score!\nHinweise:\n\nSie müssen sich bei Kaggle ein Konto anlegen (kostenlos und anonym möglich); alternativ können Sie sich mit einem Google-Konto anmelden.\nVerwenden Sie mehrere, und zwar folgende Algorithmen: Random Forest, Boosting, lineare Regression. Tipp: Ein Workflow-Set ist hilfreich.\nLogarithmieren Sie budget.\nBetreiben Sie Feature Engineering, zumindest etwas. Insbesondere sollten Sie den Monat und das Jahr aus dem Datum extrahieren und als Features (Prädiktoren) nutzen.\nVerwenden Sie tidymodels.\nDie Zielgröße ist revenue in Dollars; nicht in “Log-Dollars”. Sie müssen also rücktransformieren, falls Sie revenue logarithmiert haben.\n\n         \n\n\nLösung\nVorbereitung\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(tictoc)  # Rechenzeit messen\n#library(Metrics)\nlibrary(lubridate)  # Datumsangaben\nlibrary(VIM)  # fehlende Werte\nlibrary(visdat)  # Datensatz visualisieren\nlibrary(lubridate)  # Datum/Uhrzeit verarbeiten\nlibrary(doParallel)  # mehrere CPUs nutzen\n\n\nd_train_raw &lt;- read_csv(d_train_path)\nd_test &lt;- read_csv(d_test_path)\n\nMal einen Blick werfen:\n\nglimpse(d_train_raw)\n\nRows: 3,000\nColumns: 23\n$ id                    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ belongs_to_collection &lt;chr&gt; \"[{'id': 313576, 'name': 'Hot Tub Time Machine C…\n$ budget                &lt;dbl&gt; 1.40e+07, 4.00e+07, 3.30e+06, 1.20e+06, 0.00e+00…\n$ genres                &lt;chr&gt; \"[{'id': 35, 'name': 'Comedy'}]\", \"[{'id': 35, '…\n$ homepage              &lt;chr&gt; NA, NA, \"http://sonyclassics.com/whiplash/\", \"ht…\n$ imdb_id               &lt;chr&gt; \"tt2637294\", \"tt0368933\", \"tt2582802\", \"tt182148…\n$ original_language     &lt;chr&gt; \"en\", \"en\", \"en\", \"hi\", \"ko\", \"en\", \"en\", \"en\", …\n$ original_title        &lt;chr&gt; \"Hot Tub Time Machine 2\", \"The Princess Diaries …\n$ overview              &lt;chr&gt; \"When Lou, who has become the \\\"father of the In…\n$ popularity            &lt;dbl&gt; 6.575393, 8.248895, 64.299990, 3.174936, 1.14807…\n$ poster_path           &lt;chr&gt; \"/tQtWuwvMf0hCc2QR2tkolwl7c3c.jpg\", \"/w9Z7A0GHEh…\n$ production_companies  &lt;chr&gt; \"[{'name': 'Paramount Pictures', 'id': 4}, {'nam…\n$ production_countries  &lt;chr&gt; \"[{'iso_3166_1': 'US', 'name': 'United States of…\n$ release_date          &lt;chr&gt; \"2/20/15\", \"8/6/04\", \"10/10/14\", \"3/9/12\", \"2/5/…\n$ runtime               &lt;dbl&gt; 93, 113, 105, 122, 118, 83, 92, 84, 100, 91, 119…\n$ spoken_languages      &lt;chr&gt; \"[{'iso_639_1': 'en', 'name': 'English'}]\", \"[{'…\n$ status                &lt;chr&gt; \"Released\", \"Released\", \"Released\", \"Released\", …\n$ tagline               &lt;chr&gt; \"The Laws of Space and Time are About to be Viol…\n$ title                 &lt;chr&gt; \"Hot Tub Time Machine 2\", \"The Princess Diaries …\n$ Keywords              &lt;chr&gt; \"[{'id': 4379, 'name': 'time travel'}, {'id': 96…\n$ cast                  &lt;chr&gt; \"[{'cast_id': 4, 'character': 'Lou', 'credit_id'…\n$ crew                  &lt;chr&gt; \"[{'credit_id': '59ac067c92514107af02c8c8', 'dep…\n$ revenue               &lt;dbl&gt; 12314651, 95149435, 13092000, 16000000, 3923970,…\n\nglimpse(d_test)\n\nRows: 4,398\nColumns: 22\n$ id                    &lt;dbl&gt; 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, …\n$ belongs_to_collection &lt;chr&gt; \"[{'id': 34055, 'name': 'Pokémon Collection', 'p…\n$ budget                &lt;dbl&gt; 0.00e+00, 8.80e+04, 0.00e+00, 6.80e+06, 2.00e+06…\n$ genres                &lt;chr&gt; \"[{'id': 12, 'name': 'Adventure'}, {'id': 16, 'n…\n$ homepage              &lt;chr&gt; \"http://www.pokemon.com/us/movies/movie-pokemon-…\n$ imdb_id               &lt;chr&gt; \"tt1226251\", \"tt0051380\", \"tt0118556\", \"tt125595…\n$ original_language     &lt;chr&gt; \"ja\", \"en\", \"en\", \"fr\", \"en\", \"en\", \"de\", \"en\", …\n$ original_title        &lt;chr&gt; \"ディアルガVSパルキアVSダークライ\", \"Attack of t…\n$ overview              &lt;chr&gt; \"Ash and friends (this time accompanied by newco…\n$ popularity            &lt;dbl&gt; 3.851534, 3.559789, 8.085194, 8.596012, 3.217680…\n$ poster_path           &lt;chr&gt; \"/tnftmLMemPLduW6MRyZE0ZUD19z.jpg\", \"/9MgBNBqlH1…\n$ production_companies  &lt;chr&gt; NA, \"[{'name': 'Woolner Brothers Pictures Inc.',…\n$ production_countries  &lt;chr&gt; \"[{'iso_3166_1': 'JP', 'name': 'Japan'}, {'iso_3…\n$ release_date          &lt;chr&gt; \"7/14/07\", \"5/19/58\", \"5/23/97\", \"9/4/10\", \"2/11…\n$ runtime               &lt;dbl&gt; 90, 65, 100, 130, 92, 121, 119, 77, 120, 92, 88,…\n$ spoken_languages      &lt;chr&gt; \"[{'iso_639_1': 'en', 'name': 'English'}, {'iso_…\n$ status                &lt;chr&gt; \"Released\", \"Released\", \"Released\", \"Released\", …\n$ tagline               &lt;chr&gt; \"Somewhere Between Time & Space... A Legend Is B…\n$ title                 &lt;chr&gt; \"Pokémon: The Rise of Darkrai\", \"Attack of the 5…\n$ Keywords              &lt;chr&gt; \"[{'id': 11451, 'name': 'pok√©mon'}, {'id': 1155…\n$ cast                  &lt;chr&gt; \"[{'cast_id': 3, 'character': 'Tonio', 'credit_i…\n$ crew                  &lt;chr&gt; \"[{'credit_id': '52fe44e7c3a368484e03d683', 'dep…\n\n\nTrain-Set verschlanken\n\nd_train &lt;-\n  d_train_raw %&gt;% \n  select(popularity, runtime, revenue, budget, release_date) \n\nDatensatz kennenlernen\n\nlibrary(visdat)\nvis_dat(d_train)\n\n\n\n\n\n\n\n\nFehlende Werte prüfen\nWelche Spalten haben viele fehlende Werte?\n\nvis_miss(d_train)\n\n\n\n\n\n\n\n\nMit {VIM} kann man einen Datensatz gut auf fehlende Werte hin untersuchen:\n\naggr(d_train)\n\n\n\n\n\n\n\n\nRezept definieren\n\nrec1 &lt;-\n  recipe(revenue ~ ., data = d_train) %&gt;% \n  #update_role(all_predictors(), new_role = \"id\") %&gt;% \n  #update_role(popularity, runtime, revenue, budget, original_language) %&gt;% \n  #update_role(revenue, new_role = \"outcome\") %&gt;% \n  step_mutate(budget = if_else(budget &lt; 10, 10, budget)) %&gt;% \n  step_log(budget) %&gt;% \n  step_mutate(release_date = mdy(release_date)) %&gt;% \n  step_date(release_date, features = c(\"year\"), keep_original_cols = FALSE) %&gt;% \n  step_impute_bag(all_predictors()) %&gt;% \n  step_dummy(all_nominal())\n\nrec1\n\n\ntidy(rec1)\n\n# A tibble: 6 × 6\n  number operation type       trained skip  id              \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;      &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;           \n1      1 step      mutate     FALSE   FALSE mutate_7GZRr    \n2      2 step      log        FALSE   FALSE log_9bqVu       \n3      3 step      mutate     FALSE   FALSE mutate_bGMkA    \n4      4 step      date       FALSE   FALSE date_mHs3V      \n5      5 step      impute_bag FALSE   FALSE impute_bag_57NLQ\n6      6 step      dummy      FALSE   FALSE dummy_l6So6     \n\n\nCheck das Rezept \n\nprep(rec1, verbose = TRUE)\n\noper 1 step mutate [training] \noper 2 step log [training] \noper 3 step mutate [training] \noper 4 step date [training] \noper 5 step impute bag [training] \noper 6 step dummy [training] \nThe retained training set is ~ 0.1 Mb  in memory.\n\n\n\nd_train_baked &lt;- \nprep(rec1) %&gt;% \n  bake(new_data = NULL) \n\nd_train_baked\n\n# A tibble: 3,000 × 5\n   popularity runtime budget  revenue release_date_year\n        &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;             &lt;int&gt;\n 1      6.58       93  16.5  12314651              2015\n 2      8.25      113  17.5  95149435              2004\n 3     64.3       105  15.0  13092000              2014\n 4      3.17      122  14.0  16000000              2012\n 5      1.15      118   2.30  3923970              2009\n 6      0.743      83  15.9   3261638              1987\n 7      7.29       92  16.5  85446075              2012\n 8      1.95       84   2.30  2586511              2004\n 9      6.90      100   2.30 34327391              1996\n10      4.67       91  15.6  18750246              2003\n# ℹ 2,990 more rows\n\n\n\nd_train_baked %&gt;% \n  map_df(~ sum(is.na(.)))\n\n# A tibble: 1 × 5\n  popularity runtime budget revenue release_date_year\n       &lt;int&gt;   &lt;int&gt;  &lt;int&gt;   &lt;int&gt;             &lt;int&gt;\n1          0       0      0       0                 0\n\n\nKeine fehlenden Werte mehr in den Prädiktoren.\nNach fehlenden Werten könnte man z.B. auch so suchen:\n\ndatawizard::describe_distribution(d_train_baked)\n\nVariable          |     Mean |       SD |      IQR |              Range | Skewness | Kurtosis |    n | n_Missing\n----------------------------------------------------------------------------------------------------------------\npopularity        |     8.46 |    12.10 |     6.88 | [1.00e-06, 294.34] |    14.38 |   280.10 | 3000 |         0\nruntime           |   107.85 |    22.08 |    24.00 |     [0.00, 338.00] |     1.02 |     8.20 | 3000 |         0\nbudget            |    12.51 |     6.44 |    14.88 |      [2.30, 19.76] |    -0.87 |    -1.09 | 3000 |         0\nrevenue           | 6.67e+07 | 1.38e+08 | 6.66e+07 |   [1.00, 1.52e+09] |     4.54 |    27.78 | 3000 |         0\nrelease_date_year |  2004.58 |    15.48 |    17.00 | [1969.00, 2068.00] |     1.22 |     3.94 | 3000 |         0\n\n\nSo bekommt man gleich noch ein paar Infos über die Verteilung der Variablen. Praktische Sache.\nCheck Test-Sample\nDas Test-Sample backen wir auch mal. Das hat nur den Zwecke, zu prüfen, ob unser Rezept auch richtig funktioniert. Das Preppen und Backen des Test-Samples wir automatisch von predict() bzw. last_fit() erledigt.\nWichtig: Wir preppen den Datensatz mit dem Train-Sample, auch wenn wir das Test-Sample backen wollen.\n\nrec1_prepped &lt;- prep(rec1)\n\nd_test_baked &lt;-\n  bake(rec1_prepped, new_data = d_test)\n\nd_test_baked %&gt;% \n  head()\n\n# A tibble: 6 × 4\n  popularity runtime budget release_date_year\n       &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;             &lt;int&gt;\n1       3.85      90   2.30              2007\n2       3.56      65  11.4               2058\n3       8.09     100   2.30              1997\n4       8.60     130  15.7               2010\n5       3.22      92  14.5               2005\n6       8.68     121   2.30              1996\n\n\n\n\nKreuzvalidierung\nNur aus Zeitgründen ist hier \\(v=5\\) eingestellt; besser wäre z.B. \\(v=10\\) und \\(r=3\\).\n\ncv_scheme &lt;- vfold_cv(d_train,\n                      v = 5, \n                      repeats = 1)\n\n\n\nModelle\nBaum\n\nmod_tree &lt;-\n  decision_tree(cost_complexity = tune(),\n                tree_depth = tune(),\n                mode = \"regression\")\n\nRandom Forest\n\nmod_rf &lt;-\n  rand_forest(mtry = tune(),\n              min_n = tune(),\n              trees = 1000,\n              mode = \"regression\") \n\nXGBoost\n\nmod_boost &lt;- boost_tree(mtry = tune(),\n                        min_n = tune(),\n                        trees = tune()) %&gt;% \n  set_mode(\"regression\")\n\nLM\n\nmod_lm &lt;-\n  linear_reg()\n\nWorkflow-Set\n\npreproc &lt;- list(rec1 = rec1)\nmodels &lt;- list(tree1 = mod_tree, \n               rf1 = mod_rf, \n               boost1 = mod_boost, \n               lm1 = mod_lm)\n \nall_workflows &lt;- workflow_set(preproc, models)\n\nFitten und tunen\n\n\nFitten/Tunen\nWenn man das Ergebnis-Objekt abgespeichert hat, dann kann man es einfach laden, spart Rechenzeit (der Tag ist kurz):\n\nresult_obj_file &lt;- \"tmdb_model_set.rds\"\n\n(Davon ausgehend, dass die Datei im Arbeitsverzeichnis liegt.)\nDann könnte man Folgendes machen:\n\nif (file.exists(result_obj_file)) {\n  tmdb_model_set &lt;- read_rds(result_obj_file)\n} else {\n  \n  &lt;computer_workflow_set_and_be_happy&gt;\n  \n}\n\nAchtung Gefährlich! Zwischenspeichern auf der Festplatte birgt die Gefahr, dass man vergisst, das Objekt auf der Festplatte zu aktualisieren und Sie noch in einem Jahr und nach 100 Updates Ihres Rezepts immer noch das uralte Objekt von der Festplatte laden …\nUm Rechenzeit zu sparen, kann man das Ergebnisobjekt abspeichern, dann muss man beim nächsten Mal nicht wieder von Neuem berechnen:\n\n#write_rds(tmdb_model_set, \"objects/tmdb_model_set.rds\")\n\nHier berechnen wir aber lieber das Modell neu:\n\ntic()\ntmdb_model_set &lt;-\n  all_workflows %&gt;% \n  workflow_map(\n    resamples = cv_scheme,\n    #grid = 10,\n    metrics = metric_set(rmse),\n    seed = 42,  # reproducibility\n    control = control_grid(verbose = FALSE))\ntoc()\n\n225.039 sec elapsed\n\n\nOhne Parallelisierung dauerte die Berechnung bei mir knapp 4 Minuten (225 Sec). Ich habe hier auf Parallelisierung verzichtet, da Tidymodels einen Fehler aufwarf mit der Begründung, dass das Paket lubridate in den parallel laufenden Instanzen nicht verfügbar sei (und der parameter pckgs = 'lubridate keine Linderung brachte).\nCheck:\n\ntmdb_model_set[[\"result\"]][[1]]\n\n# Tuning results\n# 5-fold cross-validation \n# A tibble: 5 × 4\n  splits             id    .metrics          .notes          \n  &lt;list&gt;             &lt;chr&gt; &lt;list&gt;            &lt;list&gt;          \n1 &lt;split [2400/600]&gt; Fold1 &lt;tibble [10 × 6]&gt; &lt;tibble [0 × 3]&gt;\n2 &lt;split [2400/600]&gt; Fold2 &lt;tibble [10 × 6]&gt; &lt;tibble [0 × 3]&gt;\n3 &lt;split [2400/600]&gt; Fold3 &lt;tibble [10 × 6]&gt; &lt;tibble [0 × 3]&gt;\n4 &lt;split [2400/600]&gt; Fold4 &lt;tibble [10 × 6]&gt; &lt;tibble [0 × 3]&gt;\n5 &lt;split [2400/600]&gt; Fold5 &lt;tibble [10 × 6]&gt; &lt;tibble [0 × 3]&gt;\n\n\nFinalisieren\nWelcher Algorithmus schneidet am besten ab?\nGenauer gesagt, welches Modell, denn es ist ja nicht nur ein Algorithmus, sondern ein Algorithmus plus ein Rezept plus die Parameterinstatiierung plus ein spezifischer Datensatz.\n\ntune::autoplot(tmdb_model_set)\n\n\n\n\n\n\n\n\nR-Quadrat ist nicht so entscheidend; rmse ist wichtiger.\nDie Ergebnislage ist nicht ganz klar, aber einiges spricht für das Random-Forest-Modell.\n\ntmdb_model_set %&gt;% \n  collect_metrics() %&gt;% \n  arrange(mean) %&gt;% \n  slice_head(n = 10)\n\n# A tibble: 10 × 9\n   wflow_id .config        preproc model .metric .estimator   mean     n std_err\n   &lt;chr&gt;    &lt;chr&gt;          &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1 rec1_rf1 Preprocessor1… recipe  rand… rmse    standard   8.10e7     5  3.37e6\n 2 rec1_rf1 Preprocessor1… recipe  rand… rmse    standard   8.12e7     5  3.37e6\n 3 rec1_rf1 Preprocessor1… recipe  rand… rmse    standard   8.14e7     5  3.29e6\n 4 rec1_rf1 Preprocessor1… recipe  rand… rmse    standard   8.14e7     5  3.56e6\n 5 rec1_rf1 Preprocessor1… recipe  rand… rmse    standard   8.14e7     5  3.42e6\n 6 rec1_rf1 Preprocessor1… recipe  rand… rmse    standard   8.15e7     5  3.43e6\n 7 rec1_rf1 Preprocessor1… recipe  rand… rmse    standard   8.19e7     5  3.55e6\n 8 rec1_rf1 Preprocessor1… recipe  rand… rmse    standard   8.23e7     5  3.67e6\n 9 rec1_rf1 Preprocessor1… recipe  rand… rmse    standard   8.28e7     5  3.66e6\n10 rec1_rf1 Preprocessor1… recipe  rand… rmse    standard   8.30e7     5  3.55e6\n\n\n\nbest_model_params &lt;-\nextract_workflow_set_result(tmdb_model_set, \"rec1_rf1\") %&gt;% \n  select_best()\n\nbest_model_params\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     2    24 Preprocessor1_Model06\n\n\nFinalisieren\n\nbest_wf &lt;- \nall_workflows %&gt;% \n  extract_workflow(\"rec1_rf1\")\n\nbest_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_mutate()\n• step_log()\n• step_mutate()\n• step_date()\n• step_impute_bag()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n  trees = 1000\n  min_n = tune()\n\nComputational engine: ranger \n\n\n\nbest_wf_finalized &lt;- \n  best_wf %&gt;% \n  finalize_workflow(best_model_params)\n\nbest_wf_finalized\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_mutate()\n• step_log()\n• step_mutate()\n• step_date()\n• step_impute_bag()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 2\n  trees = 1000\n  min_n = 24\n\nComputational engine: ranger \n\n\nFinal Fit\n\nfit_final &lt;-\n  best_wf_finalized %&gt;% \n  fit(d_train)\n\nfit_final\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_mutate()\n• step_log()\n• step_mutate()\n• step_date()\n• step_impute_bag()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~2L,      x), num.trees = ~1000, min.node.size = min_rows(~24L, x),      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1)) \n\nType:                             Regression \nNumber of trees:                  1000 \nSample size:                      3000 \nNumber of independent variables:  4 \nMtry:                             2 \nTarget node size:                 24 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       6.58465e+15 \nR squared (OOB):                  0.6518847 \n\n\n\nd_test$revenue &lt;- NA\n\nfinal_preds &lt;- \n  fit_final %&gt;% \n  predict(new_data = d_test) %&gt;% \n  bind_cols(d_test)\n\nSubmission\n\nsubmission_df &lt;-\n  final_preds %&gt;% \n  select(id, revenue = .pred)\n\nAbspeichern und einreichen:\n\n#write_csv(submission_df, file = \"submission.csv\")\n\nKaggle Score\nDiese Submission erzielte einen Score von 4.79227 (RMSLE).\n\nsol &lt;- 4.79227\n\n\nCategories:\n\nds1\ntidymodels\nstatlearning\ntmdb\nrandom-forest\nnum"
  },
  {
    "objectID": "posts/sicherheit/sicherheit.html",
    "href": "posts/sicherheit/sicherheit.html",
    "title": "sicherheit",
    "section": "",
    "text": "Aufgabe\nEin Betreiber eines komplexen technischen Geräts versucht, Sie zu beruhigen. Die Wahrscheinlichkeit eines Ausfalls (Ereignis \\(A\\)) betrage nur 0.001. Allerdings pro Komponente des Geräts. Das Gerät besteht aus 10 Komponenten.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\nUnterstellen Sie Unabhängkeit der einzelnen Ereignisse.\n\n         \n\n\nLösung\nDen Ausfall der Komponente \\(i\\) bezeichnen wir als \\(A_i\\) und entsprechend \\(Pr(A_i) = 0.001\\).\n\\(Pr(\\neg A_i) = 1- Pr(A_i)\\)\n\nPr_Ai &lt;- 0.001\nPr_negAi &lt;- 1 - Pr_Ai\nPr_negAi\n\n[1] 0.999\n\n\nDie Wahrscheinlichkeit, dass keine der Komponenten ausfällt, ist dann über den Multiplikationssatzu bestimmen:\n\nPr_negA &lt;- Pr_negAi^10\nPr_negA\n\n[1] 0.9900449\n\n\nDie Lösung lautet 0.9900449.\n\nCategories:\n\nR\nprobability\nnum"
  },
  {
    "objectID": "posts/tmdb04/tmdb04.html",
    "href": "posts/tmdb04/tmdb04.html",
    "title": "tmdb04",
    "section": "",
    "text": "Wir bearbeiten hier die Fallstudie TMDB Box Office Prediction - Can you predict a movie’s worldwide box office revenue?, ein Kaggle-Prognosewettbewerb.\nZiel ist es, genaue Vorhersagen zu machen, in diesem Fall für Filme.\nDie Daten können Sie von der Kaggle-Projektseite beziehen oder so:\n\nd_train_path &lt;- \"https://raw.githubusercontent.com/sebastiansauer/Lehre/main/data/tmdb-box-office-prediction/train.csv\"\nd_test_path &lt;- \"https://raw.githubusercontent.com/sebastiansauer/Lehre/main/data/tmdb-box-office-prediction/test.csv\""
  },
  {
    "objectID": "posts/tmdb04/tmdb04.html#train-set-verschlanken",
    "href": "posts/tmdb04/tmdb04.html#train-set-verschlanken",
    "title": "tmdb04",
    "section": "Train-Set verschlanken",
    "text": "Train-Set verschlanken\n\nd_train_raw_reduced &lt;-\n  d_train_raw %&gt;% \n  select(id, popularity, runtime, revenue, budget)"
  },
  {
    "objectID": "posts/tmdb04/tmdb04.html#test-set-verschlanken",
    "href": "posts/tmdb04/tmdb04.html#test-set-verschlanken",
    "title": "tmdb04",
    "section": "Test-Set verschlanken",
    "text": "Test-Set verschlanken\n\nd_test &lt;-\n  d_test_raw %&gt;% \n  select(id,popularity, runtime, budget)"
  },
  {
    "objectID": "posts/tmdb04/tmdb04.html#outcome-logarithmieren",
    "href": "posts/tmdb04/tmdb04.html#outcome-logarithmieren",
    "title": "tmdb04",
    "section": "Outcome logarithmieren",
    "text": "Outcome logarithmieren\nDer Outcome sollte nicht im Rezept transformiert werden (vgl. Part 3, S. 30, in dieser Unterlage).\n\nd_train &lt;-\n  d_train_raw_reduced %&gt;% \n  mutate(revenue = if_else(revenue &lt; 10, 10, revenue)) %&gt;% \n  mutate(revenue = log(revenue)) \n\nPrüfen, ob das funktioniert hat:\n\nd_train$revenue %&gt;% is.infinite() %&gt;% any()\n\n[1] FALSE\n\n\nKeine unendlichen Werte mehr, auf dieser Basis können wir weitermachen."
  },
  {
    "objectID": "posts/tmdb04/tmdb04.html#rezept-definieren",
    "href": "posts/tmdb04/tmdb04.html#rezept-definieren",
    "title": "tmdb04",
    "section": "Rezept definieren",
    "text": "Rezept definieren\n\nrec2 &lt;-\n  recipe(revenue ~ ., data = d_train) %&gt;% \n  step_mutate(budget = ifelse(budget == 0, NA, budget)) %&gt;%  # log mag keine 0\n  step_log(budget) %&gt;% \n  step_impute_knn(all_predictors()) %&gt;% \n  step_dummy(all_nominal_predictors())  %&gt;% \n  update_role(id, new_role = \"id\")\n\nrec2\n\nSchauen Sie mal, der Log mag keine Nullen:\n\nx &lt;- c(1,2, NA, 0)\n\nlog(x)\n\n[1] 0.0000000 0.6931472        NA      -Inf\n\n\nDa \\(log(0) = -\\infty\\). Aus dem Grund wandeln wir 0 lieber in NA um.\n\ntidy(rec2)\n\n# A tibble: 4 × 6\n  number operation type       trained skip  id              \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;      &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;           \n1      1 step      mutate     FALSE   FALSE mutate_5IvPK    \n2      2 step      log        FALSE   FALSE log_HuvzM       \n3      3 step      impute_knn FALSE   FALSE impute_knn_bzUap\n4      4 step      dummy      FALSE   FALSE dummy_Gm3kh"
  },
  {
    "objectID": "posts/tmdb04/tmdb04.html#check-das-rezept",
    "href": "posts/tmdb04/tmdb04.html#check-das-rezept",
    "title": "tmdb04",
    "section": "Check das Rezept",
    "text": "Check das Rezept\nWir berechnen das Rezept:\n\nrec2_prepped &lt;-\n  prep(rec2, verbose = TRUE)\n\noper 1 step mutate [training] \noper 2 step log [training] \noper 3 step impute knn [training] \noper 4 step dummy [training] \nThe retained training set is ~ 0.12 Mb  in memory.\n\nrec2_prepped\n\nDas ist noch nicht auf einen Datensatz angewendet! Lediglich die steps wurden vorbereitet, “präpariert”: z.B. “Diese Dummy-Variablen impliziert das Rezept”.\nSo sieht das dann aus, wenn man das präparierte Rezept auf das Train-Sample anwendet:\n\nd_train_baked2 &lt;-\n  rec2_prepped %&gt;% \n  bake(new_data = NULL) \n\nhead(d_train_baked2)\n\n# A tibble: 6 × 5\n     id popularity runtime budget revenue\n  &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1     1      6.58       93   16.5    16.3\n2     2      8.25      113   17.5    18.4\n3     3     64.3       105   15.0    16.4\n4     4      3.17      122   14.0    16.6\n5     5      1.15      118   15.8    15.2\n6     6      0.743      83   15.9    15.0\n\n\n\nd_train_baked2 %&gt;% \n  map_df(sum_isna)\n\n# A tibble: 1 × 5\n     id popularity runtime budget revenue\n  &lt;int&gt;      &lt;int&gt;   &lt;int&gt;  &lt;int&gt;   &lt;int&gt;\n1     0          0       0      0       0\n\n\nKeine fehlenden Werte mehr in den Prädiktoren.\nNach fehlenden Werten könnte man z.B. auch so suchen:\n\ndatawizard::describe_distribution(d_train_baked2)\n\nVariable   |    Mean |     SD |     IQR |              Range | Skewness | Kurtosis |    n | n_Missing\n-----------------------------------------------------------------------------------------------------\nid         | 1500.50 | 866.17 | 1500.50 |    [1.00, 3000.00] |     0.00 |    -1.20 | 3000 |         0\npopularity |    8.46 |  12.10 |    6.88 | [1.00e-06, 294.34] |    14.38 |   280.10 | 3000 |         0\nruntime    |  107.85 |  22.08 |   24.00 |     [0.00, 338.00] |     1.02 |     8.20 | 3000 |         0\nbudget     |   16.09 |   1.89 |    1.90 |      [0.00, 19.76] |    -2.93 |    18.71 | 3000 |         0\nrevenue    |   15.97 |   3.04 |    3.37 |      [2.30, 21.14] |    -1.60 |     3.82 | 3000 |         0\n\n\nSo bekommt man gleich noch ein paar Infos über die Verteilung der Variablen. Praktische Sache."
  },
  {
    "objectID": "posts/tmdb04/tmdb04.html#check-test-sample",
    "href": "posts/tmdb04/tmdb04.html#check-test-sample",
    "title": "tmdb04",
    "section": "Check Test-Sample",
    "text": "Check Test-Sample\nDas Test-Sample backen wir auch mal, um zu prüfen, das alles läuft:\n\nd_test_baked2 &lt;-\n  bake(rec2_prepped, new_data = d_test)\n\nd_test_baked2 %&gt;% \n  head()\n\n# A tibble: 6 × 4\n     id popularity runtime budget\n  &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1  3001       3.85      90   15.8\n2  3002       3.56      65   11.4\n3  3003       8.09     100   16.4\n4  3004       8.60     130   15.7\n5  3005       3.22      92   14.5\n6  3006       8.68     121   16.1\n\n\nSieht soweit gut aus."
  },
  {
    "objectID": "posts/tmdb04/tmdb04.html#lm",
    "href": "posts/tmdb04/tmdb04.html#lm",
    "title": "tmdb04",
    "section": "LM",
    "text": "LM\n\nmod_lm &lt;-\n  linear_reg()"
  },
  {
    "objectID": "posts/tmdb04/tmdb04.html#finalisieren-1",
    "href": "posts/tmdb04/tmdb04.html#finalisieren-1",
    "title": "tmdb04",
    "section": "Finalisieren",
    "text": "Finalisieren\nFinalisieren bedeutet:\n\nBesten Workflow identifizieren (zur Erinnerung: Workflow = Rezept + Modell)\nDen besten Workflow mit den optimalen Modell-Parametern ausstatten\nDamit dann den ganzen Train-Datensatz fitten\nAuf dieser Basis das Test-Sample vorhersagen\n\n\nbest_wf2 &lt;- \nall_workflows2 %&gt;% \n  extract_workflow(\"rec1_lm1\")\n\nbest_wf2\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_mutate()\n• step_log()\n• step_impute_knn()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\nbest_wf_finalized2 &lt;- \n  best_wf2 %&gt;% \n  finalize_workflow(best_model_params2)\n\nbest_wf_finalized2\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_mutate()\n• step_log()\n• step_impute_knn()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "posts/tmdb04/tmdb04.html#final-fit",
    "href": "posts/tmdb04/tmdb04.html#final-fit",
    "title": "tmdb04",
    "section": "Final Fit",
    "text": "Final Fit\n\nfit_final2 &lt;-\n  best_wf_finalized2 %&gt;% \n  fit(d_train)\n\nfit_final2\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_mutate()\n• step_log()\n• step_impute_knn()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)   popularity      runtime       budget  \n    1.26186      0.03755      0.01289      0.80752  \n\n\n\npreds &lt;- \nfit_final2 %&gt;% \n  predict(new_data = d_test)\n\nhead(preds)\n\n# A tibble: 6 × 1\n  .pred\n  &lt;dbl&gt;\n1  15.3\n2  11.4\n3  16.1\n4  16.0\n5  14.3\n6  16.1\n\n\nAchtung, wenn die Outcome-Variable im Rezept verändert wurde, dann würde obiger Code nicht durchlaufen.\nGrund ist hier beschrieben:\n\nWhen predict() is used, it only has access to the predictors (mirroring how this would work with new samples). Even if the outcome column is present, it is not exposed to the recipe. This is generally a good idea so that we can avoid information leakage.\n\n\nOne approach is the use the skip = TRUE option in step_log() so that it will avoid that step during predict() and/or bake(). However, if you are using this recipe with the tune package, there will still be an issue because the metric function(s) would get the predictions in log units and the observed outcome in the original units.\n\n\nThe better approach is, for simple transformations like yours, to log the outcome outside of the recipe (before data analysis and the initial split)."
  },
  {
    "objectID": "posts/tmdb04/tmdb04.html#submission-df",
    "href": "posts/tmdb04/tmdb04.html#submission-df",
    "title": "tmdb04",
    "section": "Submission df",
    "text": "Submission df\n\nsubmission_df &lt;-\n  d_test %&gt;% \n  select(id) %&gt;% \n  bind_cols(preds) %&gt;% \n  rename(revenue = .pred)\n\nhead(submission_df)\n\n# A tibble: 6 × 2\n     id revenue\n  &lt;dbl&gt;   &lt;dbl&gt;\n1  3001    15.3\n2  3002    11.4\n3  3003    16.1\n4  3004    16.0\n5  3005    14.3\n6  3006    16.1"
  },
  {
    "objectID": "posts/tmdb04/tmdb04.html#zurücktransformieren",
    "href": "posts/tmdb04/tmdb04.html#zurücktransformieren",
    "title": "tmdb04",
    "section": "Zurücktransformieren",
    "text": "Zurücktransformieren\n\nsubmission_df &lt;-\n  submission_df %&gt;% \n  mutate(revenue = exp(revenue)-1)\n\nhead(submission_df)\n\n# A tibble: 6 × 2\n     id   revenue\n  &lt;dbl&gt;     &lt;dbl&gt;\n1  3001  4435143.\n2  3002    91755.\n3  3003  9782986.\n4  3004  8573795.\n5  3005  1598106.\n6  3006 10061439.\n\n\nHier ein Beispiel, warum \\(e^x-1\\) genauer ist für kleine Zahlen als \\(e^x\\).\nAbspeichern und einreichen:\n\nwrite_csv(submission_df, file = \"submission.csv\")"
  },
  {
    "objectID": "posts/filter-na3/filter-na3.html",
    "href": "posts/filter-na3/filter-na3.html",
    "title": "filter-na3",
    "section": "",
    "text": "Filtern Sie alle Zeilen mit fehlende Werte im Datensatz penguins!\nLiefern Sie die Spalten zurück, die fehlende Werte aufweisen."
  },
  {
    "objectID": "posts/filter-na3/filter-na3.html#setup",
    "href": "posts/filter-na3/filter-na3.html#setup",
    "title": "filter-na3",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\"\nd &lt;- read_csv(d_path)\n\nRows: 344 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): species, island, sex\ndbl (6): rownames, bill_length_mm, bill_depth_mm, flipper_length_mm, body_ma...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnrow(d)\n\n[1] 344"
  },
  {
    "objectID": "posts/filter-na3/filter-na3.html#weg-1",
    "href": "posts/filter-na3/filter-na3.html#weg-1",
    "title": "filter-na3",
    "section": "Weg 1",
    "text": "Weg 1\n\nd_na_only &lt;-\n  d %&gt;% \n  filter(!complete.cases(.)) \n\nd_na_only %&gt;% \n  names()\n\n[1] \"rownames\"          \"species\"           \"island\"           \n[4] \"bill_length_mm\"    \"bill_depth_mm\"     \"flipper_length_mm\"\n[7] \"body_mass_g\"       \"sex\"               \"year\""
  },
  {
    "objectID": "posts/filter-na3/filter-na3.html#weg-2",
    "href": "posts/filter-na3/filter-na3.html#weg-2",
    "title": "filter-na3",
    "section": "Weg 2",
    "text": "Weg 2\n\nd %&gt;% \n  filter(if_any(everything(), ~ is.na(.))) %&gt;% \n  names()\n\n[1] \"rownames\"          \"species\"           \"island\"           \n[4] \"bill_length_mm\"    \"bill_depth_mm\"     \"flipper_length_mm\"\n[7] \"body_mass_g\"       \"sex\"               \"year\"             \n\n\n\nCategories:\n\n2023\neda\nna\nstring"
  },
  {
    "objectID": "posts/purrr-map03/purrr-map03.html",
    "href": "posts/purrr-map03/purrr-map03.html",
    "title": "purrr-map03",
    "section": "",
    "text": "Exercise\nImportieren Sie das Grundatzprogramm der Partei AfD (in der aktuellsten Version). Tokenisieren Sie nach Sätzen. Dann entfernen Sie alle Zahlen. Dann zählen Sie die Anzahl der Wörter pro Satz und berichten gängige deskriptive Statistiken dazu.\n         \n\n\nSolution\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nText aus PDF-Dateien kann man mit dem Paket pdftools einlesen:\n\nlibrary(pdftools)\n\nUsing poppler version 22.02.0\n\nd_path &lt;- \"~/Literatur/_Div/Politik/afd-grundsatzprogramm-2022.pdf\"\n\nd &lt;- tibble(text = pdf_text(d_path))\n\nDann erstellen wir eine Tidy-Version und tokenisieren nach Sätzen:\n\nlibrary(tidytext)\nd2 &lt;-\n  d %&gt;% \n  unnest_sentences(output = word, input = text)\n\nhead(d2)\n\n# A tibble: 6 × 1\n  word                                                                          \n  &lt;chr&gt;                                                                         \n1 programm für deutschland.                                                     \n2 das grundsatzprogramm der alternative für deutschland.                        \n3 2   programm für deutschland | inhalt         präambel                       …\n4 familien stärken        43             und parteiferne rechnungshöfe         …\n5 3   programm für deutschland | inhalt         7 | kultur, sprache und identit…\n6 förder- und                         10.10.3 deutsche literatur im inland digi…\n\n\nDann entfernen wir die Zahlen:\n\nd3 &lt;- \n  d2 %&gt;% \n  mutate(word = str_remove_all(word, pattern = \"[:digit:]+\"))\n\nPrüfen wir, ob es geklappt hat:\n\nd2$word[10]\n\n[1] \"weniger subventionen    88      13.7 fischerei, forst und jagd: im einklang mit der natur     88      13.8 flächenkonkurrenz:           nicht zu lasten der land- und forstwirtschaft            88\"\n\nd3$word[10]\n\n[1] \"weniger subventionen          . fischerei, forst und jagd: im einklang mit der natur           . flächenkonkurrenz:           nicht zu lasten der land- und forstwirtschaft            \"\n\n\nOk.\nDann zählen wir die Wörter pro Satz:\n\nd4 &lt;- \n  d3 %&gt;% \n  summarise(word_count_per_sentence = str_count(word, \"\\\\w+\"))\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\nhead(d4)\n\n# A tibble: 6 × 1\n  word_count_per_sentence\n                    &lt;int&gt;\n1                       3\n2                       6\n3                     196\n4                      40\n5                     254\n6                      15\n\n\nVisualisierung:\n\nd4 %&gt;% \n  ggplot(aes(x = word_count_per_sentence)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.6.0 (red = needs update)\n✔ bayestestR  0.13.1   ✔ correlation 0.8.4 \n✔ datawizard  0.9.0    ✔ effectsize  0.8.6 \n✔ insight     0.19.6   ✔ modelbased  0.8.6 \n✔ performance 0.10.8   ✔ parameters  0.21.3\n✔ report      0.5.7    ✖ see         0.8.0 \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`.\n\ndescribe_distribution(d4)\n\nVariable                |  Mean |    SD | IQR |          Range | Skewness | Kurtosis |    n | n_Missing\n-------------------------------------------------------------------------------------------------------\nword_count_per_sentence | 21.86 | 17.24 |  19 | [0.00, 254.00] |     3.84 |    37.52 | 1208 |         0\n\n\n\nCategories:\n\nR\nmap\ntidyverse"
  },
  {
    "objectID": "posts/ReThink4e1/ReThink4e1.html",
    "href": "posts/ReThink4e1/ReThink4e1.html",
    "title": "ReThink4e1",
    "section": "",
    "text": "Welche der folgenden Zeilen zeigt den Likelihood?\n\n\n\n\\(\\mu \\sim \\mathcal{N}(0, 10)\\)\n\\(\\sigma \\sim \\mathcal{U}(0, 1)\\)\n\\(y_i = \\beta_0 + \\beta_1\\cdot x\\)\n\\(y_i \\sim \\mathcal{N}(\\mu, \\sigma)\\)\n\nQuelle: McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Aufl.). Taylor and Francis, CRC Press."
  },
  {
    "objectID": "posts/ReThink4e1/ReThink4e1.html#answerlist",
    "href": "posts/ReThink4e1/ReThink4e1.html#answerlist",
    "title": "ReThink4e1",
    "section": "",
    "text": "\\(\\mu \\sim \\mathcal{N}(0, 10)\\)\n\\(\\sigma \\sim \\mathcal{U}(0, 1)\\)\n\\(y_i = \\beta_0 + \\beta_1\\cdot x\\)\n\\(y_i \\sim \\mathcal{N}(\\mu, \\sigma)\\)\n\nQuelle: McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Aufl.). Taylor and Francis, CRC Press."
  },
  {
    "objectID": "posts/ReThink4e1/ReThink4e1.html#answerlist-1",
    "href": "posts/ReThink4e1/ReThink4e1.html#answerlist-1",
    "title": "ReThink4e1",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch. Priori-Verteilung.\nFalsch. Priori-Verteilung.\nFalsch. Regressionsformel.\nWahr. Likelihood.\n\nMan könnte den Likelihood auch so schreiben:\n$L = y_i| , N(, ) $,\nwas noch deutlicher macht, dass die Likelihood die Wahrscheinlichkeit der Daten (y) ausdrückt, gegeben der Modellparameter (\\(\\mu, \\sigma)\\).\n\nCategories:\n\nprobability\nbayes\nschoice"
  },
  {
    "objectID": "posts/twitter02/twitter02.html",
    "href": "posts/twitter02/twitter02.html",
    "title": "twitter02",
    "section": "",
    "text": "Exercise\nLaden Sie die neuesten Tweets an karl_lauterbach herunter - und zwar so viele wie auf einmal möglich.\n         \n\n\nSolution\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rtweet)\n\n\nAttaching package: 'rtweet'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\n\nEinloggen bei Twitter; zuerst die Credentials bereithalten:\n\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\n\n\nauth &lt;- rtweet_app(bearer_token = Bearer_Token)\n\nAus der Hilfe zu search_tweets:\nDescription\nReturns Twitter statuses matching a user provided search query. ONLY RETURNS DATA FROM THE PAST 6-9 DAYS.\nTweets an Karl Lauterbach suchen:\n\nkarl_tweets &lt;- search_tweets(q = \"@karl_lauterbach\", n = 150000, retryonratelimit = TRUE)\n\nWir könnten n auch auf Inf setzen, aber, da wir auf das Refreshen des Rate Limits warten müssen, könnte sehr lange dauern. Daher nehmen wir hier nur einen kürzeren Wert.\n\ndim(karl_tweets)\n\n[1] 18000    43\n\nhead(karl_tweets)\n\n# A tibble: 6 × 43\n  created_at               id id_str      full_…¹ trunc…² displ…³ entities     metad…⁴\n  &lt;dttm&gt;                &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;   &lt;lgl&gt;     &lt;dbl&gt; &lt;list&gt;       &lt;list&gt; \n1 2022-10-23 13:30:18 1.58e18 1584145185… \"Bei ⁦@… FALSE       122 &lt;named list&gt; &lt;df&gt;   \n2 2022-10-22 18:34:37 1.58e18 1583859379… \"Es is… FALSE       263 &lt;named list&gt; &lt;df&gt;   \n3 2022-10-22 17:56:39 1.58e18 1583849826… \"Die S… FALSE       215 &lt;named list&gt; &lt;df&gt;   \n4 2022-10-24 08:10:35 1.58e18 1584427113… \"Zu we… FALSE       219 &lt;named list&gt; &lt;df&gt;   \n5 2022-10-24 08:10:35 1.58e18 1584427113… \"RT @K… FALSE       140 &lt;named list&gt; &lt;df&gt;   \n6 2022-10-24 08:10:25 1.58e18 1584427072… \"RT @U… FALSE       139 &lt;named list&gt; &lt;df&gt;   \n# … with 35 more variables: source &lt;chr&gt;, in_reply_to_status_id &lt;dbl&gt;,\n#   in_reply_to_status_id_str &lt;chr&gt;, in_reply_to_user_id &lt;dbl&gt;,\n#   in_reply_to_user_id_str &lt;chr&gt;, in_reply_to_screen_name &lt;chr&gt;, geo &lt;list&gt;,\n#   coordinates &lt;list&gt;, place &lt;list&gt;, contributors &lt;lgl&gt;, is_quote_status &lt;lgl&gt;,\n#   retweet_count &lt;int&gt;, favorite_count &lt;int&gt;, favorited &lt;lgl&gt;, retweeted &lt;lgl&gt;,\n#   possibly_sensitive &lt;lgl&gt;, lang &lt;chr&gt;, quoted_status_id &lt;dbl&gt;,\n#   quoted_status_id_str &lt;chr&gt;, quoted_status &lt;list&gt;, retweeted_status &lt;list&gt;, …\n# ℹ Use `colnames()` to see all variable names\n\nCategories:\n\ntextmining\ntwitter"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-rf-tune/germeval-sent-wordvec-rf-tune.html",
    "href": "posts/germeval-sent-wordvec-rf-tune/germeval-sent-wordvec-rf-tune.html",
    "title": "germeval03-sent-wordvec-rf-tune",
    "section": "",
    "text": "Erstellen Sie ein prädiktives Modell für Textdaten. Nutzen Sie Sentiments und TextFeatures im Rahmen von Feature-Engineering. Nutzen Sie außerdem deutsche Word-Vektoren für das Feature-Engineering.\nAls Lernalgorithmus verwenden Sie Random Forest (Ranger). Tunen Sie mtry und min_n.\n\n\nVerwenden Sie die GermEval-2018-Daten.\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\nDie Daten sind auch über das R-Paket PradaData zu beziehen.\n\nlibrary(tidyverse)\ndata(\"germeval_train\", package = \"pradadata\")\ndata(\"germeval_test\", package = \"pradadata\")\n\n\n\n\nDie AV lautet c1. Die (einzige) UV lautet: text.\n\n\n\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\nNutzen Sie Tidymodels.\nNutzen Sie das sentiws Lexikon.\n❗ Achten Sie darauf, die Variable c2 zu entfernen bzw. nicht zu verwenden."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-rf-tune/germeval-sent-wordvec-rf-tune.html#daten",
    "href": "posts/germeval-sent-wordvec-rf-tune/germeval-sent-wordvec-rf-tune.html#daten",
    "title": "germeval03-sent-wordvec-rf-tune",
    "section": "",
    "text": "Verwenden Sie die GermEval-2018-Daten.\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\nDie Daten sind auch über das R-Paket PradaData zu beziehen.\n\nlibrary(tidyverse)\ndata(\"germeval_train\", package = \"pradadata\")\ndata(\"germeval_test\", package = \"pradadata\")"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-rf-tune/germeval-sent-wordvec-rf-tune.html#av-und-uv",
    "href": "posts/germeval-sent-wordvec-rf-tune/germeval-sent-wordvec-rf-tune.html#av-und-uv",
    "title": "germeval03-sent-wordvec-rf-tune",
    "section": "",
    "text": "Die AV lautet c1. Die (einzige) UV lautet: text."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-rf-tune/germeval-sent-wordvec-rf-tune.html#hinweise",
    "href": "posts/germeval-sent-wordvec-rf-tune/germeval-sent-wordvec-rf-tune.html#hinweise",
    "title": "germeval03-sent-wordvec-rf-tune",
    "section": "",
    "text": "Orientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\nNutzen Sie Tidymodels.\nNutzen Sie das sentiws Lexikon.\n❗ Achten Sie darauf, die Variable c2 zu entfernen bzw. nicht zu verwenden."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-rf-tune/germeval-sent-wordvec-rf-tune.html#setup",
    "href": "posts/germeval-sent-wordvec-rf-tune/germeval-sent-wordvec-rf-tune.html#setup",
    "title": "germeval03-sent-wordvec-rf-tune",
    "section": "Setup",
    "text": "Setup\n\nd_train &lt;-\n  germeval_train |&gt; \n  select(id, c1, text)\n\n\nlibrary(tictoc)\nlibrary(tidymodels)\n#library(syuzhet)\nlibrary(beepr)\nlibrary(finetune)  # anova race\nlibrary(lobstr)  # object size\nlibrary(visdat)  # footprint of csv\n#data(\"sentiws\", package = \"pradadata\")\n\nEine Vorlage für ein Tidymodels-Pipeline findet sich hier."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-rf-tune/germeval-sent-wordvec-rf-tune.html#learnermodell",
    "href": "posts/germeval-sent-wordvec-rf-tune/germeval-sent-wordvec-rf-tune.html#learnermodell",
    "title": "germeval03-sent-wordvec-rf-tune",
    "section": "Learner/Modell",
    "text": "Learner/Modell\n\nmod &lt;-\n  rand_forest(mode = \"classification\",\n             mtry =  tune(), \n             min_n = tune()\n             )"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-rf-tune/germeval-sent-wordvec-rf-tune.html#gebackenen-datensatz-als-neue-grundlage",
    "href": "posts/germeval-sent-wordvec-rf-tune/germeval-sent-wordvec-rf-tune.html#gebackenen-datensatz-als-neue-grundlage",
    "title": "germeval03-sent-wordvec-rf-tune",
    "section": "Gebackenen Datensatz als neue Grundlage",
    "text": "Gebackenen Datensatz als neue Grundlage\nWir importieren den schon an anderer Stelle aufbereiteten Datensatz. Das hat den Vorteil (hoffentlich), das die Datenvolumina viel kleiner sind. Die Arbeit des Feature Engineering wurde uns schon abgenommen.\n\nd_train &lt;-\n  read_csv(\"https://raw.githubusercontent.com/sebastiansauer/Datenwerk2/main/data/germeval/germeval_train_recipe_wordvec_senti.csv\")\n\nRows: 5009 Columns: 121\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (1): c1\ndbl (120): id, emo_count, schimpf_count, emoji_count, textfeature_text_copy_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nvis_dat(d_train) +\n  # remove axis labels:\n  theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank() \n        )\n\n\n\n\n\n\n\n\n\nd_test_baked &lt;- read_csv(\"https://raw.githubusercontent.com/sebastiansauer/Datenwerk2/main/data/germeval/germeval_test_recipe_wordvec_senti.csv\")\n\nRows: 3532 Columns: 121\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (1): c1\ndbl (120): id, emo_count, schimpf_count, emoji_count, textfeature_text_copy_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-rf-tune/germeval-sent-wordvec-rf-tune.html#plain-rezept",
    "href": "posts/germeval-sent-wordvec-rf-tune/germeval-sent-wordvec-rf-tune.html#plain-rezept",
    "title": "germeval03-sent-wordvec-rf-tune",
    "section": "Plain-Rezept",
    "text": "Plain-Rezept\n\nrec &lt;- \n  recipe(c1 ~ ., data = d_train)"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-rf-tune/germeval-sent-wordvec-rf-tune.html#neuer-workflow-mit-plainem-rezept",
    "href": "posts/germeval-sent-wordvec-rf-tune/germeval-sent-wordvec-rf-tune.html#neuer-workflow-mit-plainem-rezept",
    "title": "germeval03-sent-wordvec-rf-tune",
    "section": "Neuer Workflow mit plainem Rezept",
    "text": "Neuer Workflow mit plainem Rezept\n\nwf &lt;-\n  workflow() |&gt; \n  add_recipe(rec) |&gt; \n  add_model(mod)"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-rf-tune/germeval-sent-wordvec-rf-tune.html#parallelisierung-über-mehrere-kerne",
    "href": "posts/germeval-sent-wordvec-rf-tune/germeval-sent-wordvec-rf-tune.html#parallelisierung-über-mehrere-kerne",
    "title": "germeval03-sent-wordvec-rf-tune",
    "section": "Parallelisierung über mehrere Kerne",
    "text": "Parallelisierung über mehrere Kerne\n\nlibrary(parallel)\nall_cores &lt;- detectCores(logical = FALSE)\n\nlibrary(doFuture)\nregisterDoFuture()\ncl &lt;- makeCluster(3)\nplan(cluster, workers = cl)\n\nAchtung: Viele Kerne brauchen auch viel Speicher."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-rf-tune/germeval-sent-wordvec-rf-tune.html#tuneresamplefit",
    "href": "posts/germeval-sent-wordvec-rf-tune/germeval-sent-wordvec-rf-tune.html#tuneresamplefit",
    "title": "germeval03-sent-wordvec-rf-tune",
    "section": "Tune/Resample/Fit",
    "text": "Tune/Resample/Fit\n\ntic()\nfit_wordvec_senti_rf &lt;-\n  tune_race_anova(\n    wf,\n    grid = 50,\n    resamples = vfold_cv(d_train, v = 5),\n    control = control_race(verbose_elim = TRUE))\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nℹ Racing will maximize the roc_auc metric.\nℹ Resamples are analyzed in a random order.\nℹ Fold5: 47 eliminated; 3 candidates remain.\n\nℹ Fold2: 0 eliminated; 3 candidates remain.\n\ntoc()\n\n3554.225 sec elapsed\n\nbeep()\n\nObjekt-Größe:\n\nlobstr::obj_size(fit_wordvec_senti_rf)\n\n5.06 MB"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-rf-tune/germeval-sent-wordvec-rf-tune.html#beste-performance",
    "href": "posts/germeval-sent-wordvec-rf-tune/germeval-sent-wordvec-rf-tune.html#beste-performance",
    "title": "germeval03-sent-wordvec-rf-tune",
    "section": "Beste Performance",
    "text": "Beste Performance\n\nautoplot(fit_wordvec_senti_rf)\n\n\n\n\n\n\n\n\n\nshow_best(fit_wordvec_senti_rf)\n\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n\n\n# A tibble: 3 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1     3    33 roc_auc binary     0.796     5 0.00659 Preprocessor1_Model01\n2     4    24 roc_auc binary     0.796     5 0.00716 Preprocessor1_Model09\n3     8    14 roc_auc binary     0.793     5 0.00722 Preprocessor1_Model43\n\nbest_params &lt;- select_best(fit_wordvec_senti_rf)\n\nWarning: No value of `metric` was given; metric 'roc_auc' will be used."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-rf-tune/germeval-sent-wordvec-rf-tune.html#finalisieren",
    "href": "posts/germeval-sent-wordvec-rf-tune/germeval-sent-wordvec-rf-tune.html#finalisieren",
    "title": "germeval03-sent-wordvec-rf-tune",
    "section": "Finalisieren",
    "text": "Finalisieren\n\nbest_params &lt;- select_best(fit_wordvec_senti_rf)\n\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n\ntic()\nwf_finalized &lt;- finalize_workflow(wf, best_params)\nlastfit_rf &lt;- fit(wf_finalized, data = d_train)\ntoc()\n\n5.687 sec elapsed"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-rf-tune/germeval-sent-wordvec-rf-tune.html#test-set-güte",
    "href": "posts/germeval-sent-wordvec-rf-tune/germeval-sent-wordvec-rf-tune.html#test-set-güte",
    "title": "germeval03-sent-wordvec-rf-tune",
    "section": "Test-Set-Güte",
    "text": "Test-Set-Güte\n\ntic()\npreds &lt;-\n  predict(lastfit_rf, new_data = d_test_baked)\ntoc()\n\n0.342 sec elapsed\n\n\n\nd_test &lt;-\n  d_test_baked |&gt; \n  bind_cols(preds) |&gt; \n  mutate(c1 = as.factor(c1))\n\n\nmy_metrics &lt;- metric_set(accuracy, f_meas)\nmy_metrics(d_test,\n           truth = c1,\n           estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.686\n2 f_meas   binary         0.177"
  },
  {
    "objectID": "posts/twitter05/twitter05.html",
    "href": "posts/twitter05/twitter05.html",
    "title": "twitter05",
    "section": "",
    "text": "Exercise\nLaden Sie \\(n=10^k\\) Tweets von Twitter herunter (mit \\(k=2\\)) via der Twitter API; Suchterm soll sein “(karl_lauterbach?)”. Bereiten Sie die Textdaten mit grundlegenden Methoden des Textminings auf (Tokenisieren, Stopwörter entfernen, Zahlen entfernen, …).\nNutzen Sie die Daten, um eine Sentimentanalyse zu erstellen.\n         \n\n\nSolution\nNutzen Sie die Daten der letzten Aufgabe, um eine Sentimentanalyse zu erstellen.\nZuerst muss man sich anmelden und die Tweets herunterladen; dieser Teil ist hier nicht aufgeführt (s. andere Aufgaben).\n\nlibrary(rtweet)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()  masks stats::filter()\n✖ purrr::flatten() masks rtweet::flatten()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidytext)\nlibrary(lsa)  # Stopwörter\n\nLoading required package: SnowballC\n\nlibrary(SnowballC)  # Stemming\n\nBeachten Sie, dass die Spalten je nach Funktion, die Sie zum Herunterladen der Tweets verwenden, unterschiedlich heißen können.\n\nkarl2 &lt;- \n  karl1 %&gt;% \n  select(contains(\"text\"))\n\n\nkarl3 &lt;- \n  karl2 %&gt;% \n  unnest_tokens(output = word, input = text)\n\n\nkarl4 &lt;- \nkarl3 %&gt;% \n  anti_join(tibble(word = lsa::stopwords_de)) \n\nJoining with `by = join_by(word)`\n\n\n\nkarl5 &lt;- \n  karl4 %&gt;% \n  mutate(word = str_replace_na(word, \"^[:digit:]+$\")) %&gt;% \n  mutate(word = str_replace_na(word, \"hptts?://\\\\w+\")) %&gt;% \n  mutate(word = str_replace_na(word, \" +\")) %&gt;% \n  drop_na()\n\n\ndata(sentiws, package = \"pradadata\")\n\n\nkarl7 &lt;-\n  karl5 %&gt;% \n  inner_join(sentiws)\n\nJoining with `by = join_by(word)`\n\n\n\nkarl7 %&gt;% \n  group_by(neg_pos) %&gt;% \n  summarise(senti_avg = mean(value, na.rm = TRUE),\n            senti_sd = sd(value, na.rm = TRUE),\n            senti_n = n())\n\n# A tibble: 2 × 4\n  neg_pos senti_avg senti_sd senti_n\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;int&gt;\n1 neg        -0.347    0.186       2\n2 pos         0.167    0.283       3\n\n\nAchtung, Sentimentanalyse sollte vor dem Stemming kommen.\n\nCategories:\n\ntextmining\ntwitter"
  },
  {
    "objectID": "posts/Wertberechnen/Wertberechnen.html",
    "href": "posts/Wertberechnen/Wertberechnen.html",
    "title": "Wertberechnen",
    "section": "",
    "text": "Aufgabe\nWelchen Wert bzw. welches Ergebnis liefert folgende R-Syntax für ergebnis zurück?\nx hat zu Beginn den Wert 3.\nHinweis: sqrt(x) liefert die (positive) Quadratwurzel von x zurück.\n         \n\n\nLösung\nEs wird 2 zurückgeliefert.\n\nCategories:\n\nR\ndyn\nnum"
  },
  {
    "objectID": "posts/tidymodels-penguins07/tidymodels-penguins07.html",
    "href": "posts/tidymodels-penguins07/tidymodels-penguins07.html",
    "title": "tidymodels-penguins07",
    "section": "",
    "text": "Berechnen Sie ein Entscheidungsbaum-Modell mit tidymodels und zwar anhand des penguins Datensatzes.\nModellgleichung: body_mass_g ~ bill_length_mm.\nBerichten Sie die RMSE!\nHinweise:\n\nTuning Sie \\(Cp\\) mit 20 verschiedenen den Werten.\nLöschen Sie alle Zeilen mit fehlenden Werten in den Prädiktoren.\nBeachten Sie die üblichen Hinweise.\nNatürlich gilt: Ceteris paribus. Halten Sie also die Modelle im Übrigen vergleichbar bzw. identisch."
  },
  {
    "objectID": "posts/tidymodels-penguins07/tidymodels-penguins07.html#setup",
    "href": "posts/tidymodels-penguins07/tidymodels-penguins07.html#setup",
    "title": "tidymodels-penguins07",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(tictoc)  # Rechenzeit messen, optional\n# data(penguins, package = \"palmerpenguins\")\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/modeldata/penguins.csv\"\nd &lt;- read_csv(d_path)\n\nWir dürfen keine fehlenden Werte in der Y-Variable haben (im Train-Set), sonst meckert Tidymodels:\n\nd2 &lt;- \n  d %&gt;% \n  drop_na(body_mass_g)"
  },
  {
    "objectID": "posts/tidymodels-penguins07/tidymodels-penguins07.html#daten-aufteilen",
    "href": "posts/tidymodels-penguins07/tidymodels-penguins07.html#daten-aufteilen",
    "title": "tidymodels-penguins07",
    "section": "Daten aufteilen:",
    "text": "Daten aufteilen:\n\nset.seed(42)\nd_split &lt;- initial_split(d2)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)"
  },
  {
    "objectID": "posts/tidymodels-penguins07/tidymodels-penguins07.html#cv-1",
    "href": "posts/tidymodels-penguins07/tidymodels-penguins07.html#cv-1",
    "title": "tidymodels-penguins07",
    "section": "CV",
    "text": "CV\n\nset.seed(42)\nfolds &lt;- vfold_cv(d_train, v = 10)"
  },
  {
    "objectID": "posts/tidymodels-penguins07/tidymodels-penguins07.html#workflow",
    "href": "posts/tidymodels-penguins07/tidymodels-penguins07.html#workflow",
    "title": "tidymodels-penguins07",
    "section": "Workflow",
    "text": "Workflow\n\nrec1 &lt;-\n  recipe(body_mass_g ~ bill_length_mm, data = d_train) %&gt;% \n  step_naomit(all_numeric_predictors())\n\nmod_tree &lt;- \ndecision_tree(\n  mode = \"regression\",\n  cost_complexity = tune()\n)\n\nwflow &lt;-\n  workflow() %&gt;%\n  add_recipe(rec1) %&gt;%\n  add_model(mod_tree)"
  },
  {
    "objectID": "posts/tidymodels-penguins07/tidymodels-penguins07.html#fitten",
    "href": "posts/tidymodels-penguins07/tidymodels-penguins07.html#fitten",
    "title": "tidymodels-penguins07",
    "section": "Fitten",
    "text": "Fitten\n\ntic()\nwflow_fit &lt;-\n  tune_grid(\n    wflow,\n    resamples = folds,\n    control = control_grid(save_workflow = TRUE),\n    grid = 20,\n    metrics = metric_set(rmse)\n    )\ntoc()\n\n12.189 sec elapsed"
  },
  {
    "objectID": "posts/tidymodels-penguins07/tidymodels-penguins07.html#modellgüte",
    "href": "posts/tidymodels-penguins07/tidymodels-penguins07.html#modellgüte",
    "title": "tidymodels-penguins07",
    "section": "Modellgüte",
    "text": "Modellgüte\n\nbestfit1 &lt;- fit_best(x = wflow_fit)\nlastfit1 &lt;- last_fit(bestfit1, d_split)\ncollect_metrics(lastfit1)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard     638.    Preprocessor1_Model1\n2 rsq     standard       0.315 Preprocessor1_Model1\n\n\n\nCategories:\n\ntidymodels\nstatlearning\ntrees\nschoice"
  },
  {
    "objectID": "posts/kausal24/kausal24.html",
    "href": "posts/kausal24/kausal24.html",
    "title": "kausal24",
    "section": "",
    "text": "Gegeben sei der DAG g (s. u.). Der DAG verfügt über mehrere Variablen, die als Knoten im Graph dargestellt sind und mit \\(x_1, x_2, \\ldots x_n\\) bezeichnet sind.\nWelche minimale Variablenmenge muss kontrolliert werden, um den kausalen Effekt von der UV zur AV zu identifizieren?\nUV: x7.\nAV: x8.\nHinweise:\n\nMengen sind mittels geschweifter Klammern gekennzeichnet, z.B. {x8, x9} meint die Menge mit den zwei Elementen x8 und x9.\nDie leere Menge { } bedeutet, dass keine Variable kontrolliert werden muss, um den kausalen Effekt zu identifizieren.\nAlle Variablen werden als gemessen vorausgesetzt.\nEs ist möglich, dass es keine Lösung gibt, dass es also keine Adjustierungsmenge gibt, um den kausalen Effekt zu identifizieren. Wenn dies der Fall sein sollte, wählen Sie “keine Lösung”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n{ x1 , x2 , x3 , x4 , x5 , x6 }\n{ x2, x4 }\n{ x5, x6 }\n{ x1, x6 }\n{ x4 }"
  },
  {
    "objectID": "posts/kausal24/kausal24.html#answerlist",
    "href": "posts/kausal24/kausal24.html#answerlist",
    "title": "kausal24",
    "section": "",
    "text": "{ x1 , x2 , x3 , x4 , x5 , x6 }\n{ x2, x4 }\n{ x5, x6 }\n{ x1, x6 }\n{ x4 }"
  },
  {
    "objectID": "posts/kausal24/kausal24.html#answerlist-1",
    "href": "posts/kausal24/kausal24.html#answerlist-1",
    "title": "kausal24",
    "section": "Answerlist",
    "text": "Answerlist\n\nRichtig\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/adjustieren2/adjustieren2.html",
    "href": "posts/adjustieren2/adjustieren2.html",
    "title": "adjustieren2",
    "section": "",
    "text": "Betrachten Sie folgendes Modell, das den Zusammenhang des Preises (price) und dem Gewicht (carat) von Diamanten untersucht (Datensatz diamonds).\n\nlibrary(tidyverse)\ndiamonds &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/ggplot2/diamonds.csv\")\n\nRows: 53940 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): cut, color, clarity\ndbl (8): rownames, carat, depth, table, price, x, y, z\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAber zuerst zentrieren wir den metrischen Prädiktor carat, um den Achsenabschnitt besser interpretieren zu können.\n\ndiamonds &lt;-\n  diamonds %&gt;% \n  mutate(carat_z = carat - mean(carat, na.rm = TRUE))\n\nDann berechnen wir ein (bayesianisches) Regressionsmodell, wobei wir auf die Standardwerte der Prior zurückgreifen.\n\nlibrary(rstanarm)\nlibrary(easystats)\nlm1 &lt;- stan_glm(price ~ carat_z, data = diamonds,\n                chains = 1,  # nur ein Mal Stichproben ziehen, spart Zeit (auf Kosten der Genauigkeit)\n                refresh = 0)\nparameters(lm1)\n\nParameter   |  Median |             95% CI |   pd |  Rhat |     ESS |                       Prior\n-------------------------------------------------------------------------------------------------\n(Intercept) | 3933.53 | [3919.66, 3944.37] | 100% | 1.008 |  346.00 | Normal (3932.80 +- 9973.60)\ncarat_z     | 7756.60 | [7728.28, 7786.16] | 100% | 0.999 | 1453.00 |   Normal (0.00 +- 21040.85)\n\n\nZur Verdeutlichung ein Diagramm zum Modell:\n\ndiamonds %&gt;% \n  ggplot() +\n  aes(x = carat_z, y = price) +\n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nWas kostet in Diamant mittlerer Größe laut Modell lm1? Runden Sie auf eine Dezimale. Geben Sie nur eine Zahl ein.\nGeben Sie eine Regressionsformel an, die lm1 ergänzt, so dass die Schliffart (cut) des Diamanten kontrolliert (adjustiert) wird. Anders gesagt: Das Modell soll die mittleren Preise für jede der fünf Schliffarten angeben. Geben Sie nur die Regressionsformel an. Lassen Sie zwischen Termen jeweils ein Leerzeichen Abstand.\n\nHinweis: Es gibt (laut Datensatz) folgende Schliffarten (und zwar in der folgenden Reihenfolge):\n\ndiamonds %&gt;% \n  distinct(cut)\n\n# A tibble: 5 × 1\n  cut      \n  &lt;chr&gt;    \n1 Ideal    \n2 Premium  \n3 Good     \n4 Very Good\n5 Fair"
  },
  {
    "objectID": "posts/adjustieren2/adjustieren2.html#answerlist",
    "href": "posts/adjustieren2/adjustieren2.html#answerlist",
    "title": "adjustieren2",
    "section": "",
    "text": "Was kostet in Diamant mittlerer Größe laut Modell lm1? Runden Sie auf eine Dezimale. Geben Sie nur eine Zahl ein.\nGeben Sie eine Regressionsformel an, die lm1 ergänzt, so dass die Schliffart (cut) des Diamanten kontrolliert (adjustiert) wird. Anders gesagt: Das Modell soll die mittleren Preise für jede der fünf Schliffarten angeben. Geben Sie nur die Regressionsformel an. Lassen Sie zwischen Termen jeweils ein Leerzeichen Abstand.\n\nHinweis: Es gibt (laut Datensatz) folgende Schliffarten (und zwar in der folgenden Reihenfolge):\n\ndiamonds %&gt;% \n  distinct(cut)\n\n# A tibble: 5 × 1\n  cut      \n  &lt;chr&gt;    \n1 Ideal    \n2 Premium  \n3 Good     \n4 Very Good\n5 Fair"
  },
  {
    "objectID": "posts/mariokart-sd3/mariokart-sd3.html",
    "href": "posts/mariokart-sd3/mariokart-sd3.html",
    "title": "mariokart-sd3",
    "section": "",
    "text": "Aufgabe\nImportieren Sie den Datensatz mariokart in R. Berechnen Sie die SD des Verkaufspreis (total_pr) für Spiele, die sowohl neu sind als auch über Lenkräder (wheels) verfügen.\nHinweise:\n\nRunden Sie auf 1 Dezimalstelle.\n\n         \n\n\nLösung\nPakete starten:\n\nlibrary(tidyverse)\nlibrary(easystats)\n\nDaten importieren:\n\nd_url &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\"\nmariokart &lt;- data_read(d_url)\n\n\nsolution &lt;-\nmariokart  %&gt;% \n  filter(cond == \"new\" & wheels &gt; 0) %&gt;% \n  summarise(pr_sd = sd(total_pr))\n\nsolution\n\n     pr_sd\n1 7.339186\n\n\nLösung: 7.3.\n\nCategories:\n\ndatawrangling\ndplyr\neda\nvariability\nnum"
  },
  {
    "objectID": "posts/Mediterran-Alk/Mediterran-Alk.html",
    "href": "posts/Mediterran-Alk/Mediterran-Alk.html",
    "title": "Mediterran-Alk",
    "section": "",
    "text": "Exercise\nAlkohol ist ein weit verbreites Genussmittel in vielen Gesellschaften. Insgesamt sind die negativen (kausalen) Konsequenzen für die Gesundheit unstrittig. So findet man etwa in dieser Studie:\n\nThis meta-analysis found that alcohol most strongly increased the risks for cancers of the oral cavity, pharynx, esophagus, and larynx. Statistically significant increases in risk also existed for cancers of the stomach, colon, rectum, liver, female breast, and ovaries.\n\nAllerdings gibt es auch Stimmen, die Alkohol mit gesundheitlich wünschenswerten Effekten in Verbindung bringen. Dabei wird in einigen Fällen die “mediterrane Ernährung” als Erkärungsnarrativ ins Spiel gebracht. So kann man etwa hier lesen:\n\nAdhering to a Mediterranean diet (…) were associated with a lower risk of all-cause mortality (…).\n\nSolche Befunde wurden von der Breiten- oder Boulevardpresse dankbar aufgenommen, wie man z.B. hier nachlesen kann:\n\nSmall Amounts of Alcohol in Mediterranean Diet Could Boost Brain Health, Claims Study\n\nMan beachte, dass “boost your health” eine kausale Aussage ist, die über einen reinen Zusammenhang hinausgeht. Nach dieser Lesart heißt es: Trink etwas Alkohol (A), das macht dich gesünder (G).\nIhre Aufgabe: Zeigen Sie ein alternatives Kausalmodell auf, das erklärt, warum ein Zusammenhang (wie eine Korrelation) zwischen A und G zu beobachten ist, aber ohne dass es einen (kausalen) Effekt zwischen beiden Größen gäbe!\n         \n\n\nSolution\nEine Erklärung lautet - frei erfunden! -, dass die Lebenszufriedenheit (L) jeweils einen (positiven, kausalen) Effekt auf Alkoholkonsum (A) und auf die Gesundheit (G) ausübt.\n\n\n\n\n\n\n\n\n\nÜbrigens: Eine Art von Diagramm, das Kausalbeziehungen zwischen Variablen aufzeigt, ist ein sog. Directed Acyclic Graph, oder kurz ein DAG. Hier ist so ein DAG gezeichnet.\n\nCategories:\n~"
  },
  {
    "objectID": "posts/log-y-regr1/log-y-regr1.html",
    "href": "posts/log-y-regr1/log-y-regr1.html",
    "title": "log-y-regression1",
    "section": "",
    "text": "Exercise\n\nlibrary(tidyverse)\nlibrary(easystats)\n\nIn dieser Aufgabe modellieren wir den (kausalen) Effekt von Schulbildung auf das Einkommen.\nImportieren Sie zunächst den Datensatz und verschaffen Sie sich einen Überblick.\n\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Treatment.csv\"\n\nd &lt;- data_read(d_path)\n\nDokumentation und Quellenangaben zum Datensatz finden sich hier.\n\nglimpse(d)\n\nRows: 2,675\nColumns: 11\n$ rownames &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ treat    &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n$ age      &lt;int&gt; 37, 30, 27, 33, 22, 23, 32, 22, 19, 21, 18, 27, 17, 19, 27, 2…\n$ educ     &lt;int&gt; 11, 12, 11, 8, 9, 12, 11, 16, 9, 13, 8, 10, 7, 10, 13, 10, 12…\n$ ethn     &lt;chr&gt; \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\"…\n$ married  &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ re74     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ re75     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ re78     &lt;dbl&gt; 9930.05, 24909.50, 7506.15, 289.79, 4056.49, 0.00, 8472.16, 2…\n$ u74      &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n$ u75      &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n\n\nModellieren Sie den Effekt der Bildungsdauer auf das Einkommen! Gehen Sie von einem exponenziellen Zusammenhang der beiden Variablen aus. Um welchen Faktor steigt das Einkommen pro Jahr Bildung (laut Modell)?\nHinweise:\n\nVerwenden Sie lm zur Modellierung.\nOperationalisieren Sie das Einkommen mit der Variable re74.\nFügen Sie keine weiteren Variablen dem Modell hinzu.\nGehen Sie von einem kausalen Effekt des Prädiktors aus.\n\n         \n\n\nSolution\n\nd2 &lt;-\n  d %&gt;% \n  filter(re74 &gt; 0) %&gt;% \n  mutate(re74_log = log(re74))\n\n\nm &lt;- lm(re74_log ~ educ, data = d2)\n\nHier sind die parameters des Modells.\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(2327)\np\n\n\n\n\n(Intercept)\n8.83\n0.06\n(8.70, 8.95)\n142.91\n&lt; .001\n\n\neduc\n0.07\n4.94e-03\n(0.07, 0.08)\n15.16\n&lt; .001\n\n\n\n\nFür jedes Jahr Bildung steigt das Einkommen also ca. um den Faktor 1.07.\nEtwas genauer:\n\\(\\hat{\\beta_1} = 0.07\\) bedeutet, dass ein Jahr Bildung zu einen erwarteten Unterschied im Einkommen in Höhe von 0.07 in Log-Einkommen führt. Anders gesagt wird das Einkommen um exp(0.07) erhöht. Dabei gilt \\(e^{0.07} \\approx 1.07\\):\n\nexp(0.07)\n\n[1] 1.072508\n\n\nDie Lösung lautet also: “Pro Jahr Bildung steigt das Einkommen - laut Modell um den Faktor ca. 1.07”.\nMan darf dabei nicht vergessen, dass wir wir uns hier auf die Schnelle ein Modell ausgedacht haben. Ob es in Wirklichkeit so ist, wie unser Modell meint, ist eine andere Sache!\n\nCategories:\n\nregression\nlm\nqm2\nstats-nutshell"
  },
  {
    "objectID": "posts/vis-penguins/vis-penguins.html",
    "href": "posts/vis-penguins/vis-penguins.html",
    "title": "vis-penguins",
    "section": "",
    "text": "In dieser Fallstudie (YACSDA: Yet another Case Study on Data Analysis) untersuchen wir den Datensatz penguins.\nSie können den Datensatz so beziehen:\n\n#install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\ndata(\"penguins\")\nd &lt;- penguins \n\nOder so:\n\nd &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\")\n\nEin Codebook finden Sie hier.\nDie Forschungsfrage lautet:\nWas ist der Einfluss der Spezies und der Schnabellänge auf das Körpergewicht?\n\nAbhängige Variable (metrisch), y: Körpergewicht\nUnabhängige Variable 1 (nominal), x1: Spezies\nUnabhängige Variable 2 (metrisch), x2: Schnabellänge\n\nVisualisieren Sie dazu folgende Aspekte der Forschungsfrage!\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks."
  },
  {
    "objectID": "posts/vis-penguins/vis-penguins.html#umbenennen",
    "href": "posts/vis-penguins/vis-penguins.html#umbenennen",
    "title": "vis-penguins",
    "section": "Umbenennen",
    "text": "Umbenennen\nFür weniger Tippen nenne ich die Variablen um:\n\nd &lt;-\n  d |&gt; \n  rename(y = body_mass_g, x1 = species, x2 = bill_length_mm)\n\nDas ist aber nicht unbedingt nötig und bringt auch vielleicht keinen Vorteil für Sie."
  },
  {
    "objectID": "posts/vis-penguins/vis-penguins.html#visualisieren-sie-die-verteilung-von-y-auf-zwei-verschiedene-arten.",
    "href": "posts/vis-penguins/vis-penguins.html#visualisieren-sie-die-verteilung-von-y-auf-zwei-verschiedene-arten.",
    "title": "vis-penguins",
    "section": "Visualisieren Sie die Verteilung von y auf zwei verschiedene Arten.",
    "text": "Visualisieren Sie die Verteilung von y auf zwei verschiedene Arten.\nDas R-Paket ggpubr erstellt schöne Diagramme (basierend auf ggplot) auf einfache Art. Nehmen wir ein Dichtediagramm; die Variable y soll auf der X-Achse stehen:\n\nggdensity(d, x = \"y\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\nBeachten Sie, dass die Variable in Anführungsstriche gesetzt werden muss: x = \"y\".\nOder ein Histogramm:\n\ngghistogram(d, x = \"y\")\n\nWarning: Using `bins = 30` by default. Pick better value with the argument\n`bins`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nAlternativ könnte man das R-Paket {DataExplorer} verwenden:\n\nd |&gt; \n  select(y) |&gt; \n  plot_density()"
  },
  {
    "objectID": "posts/vis-penguins/vis-penguins.html#fügen-sie-relevante-kennzahlen-hinzu.",
    "href": "posts/vis-penguins/vis-penguins.html#fügen-sie-relevante-kennzahlen-hinzu.",
    "title": "vis-penguins",
    "section": "Fügen Sie relevante Kennzahlen hinzu.",
    "text": "Fügen Sie relevante Kennzahlen hinzu.\nUm Diagramme mit Statistiken anzureichen, bietet sich das Paket {ggstatsplot} an:\n\ngghistostats(d, x = y)\n\n\n\n\n\n\n\n\nBeachten Sie, dass die Variable nicht in Anführungsstriche gesetzt werden darf: x = y.\nNatürlich könnte man sich typische deskriptive Statistiken auch anderweitig ausgeben lassen, etwa mit {easystats}:\n\nd |&gt; \n  select(y) |&gt; \n  describe_distribution()\n\nVariable |    Mean |     SD |     IQR |              Range | Skewness | Kurtosis |   n | n_Missing\n--------------------------------------------------------------------------------------------------\ny        | 4201.75 | 801.95 | 1206.25 | [2700.00, 6300.00] |     0.47 |    -0.72 | 342 |         2"
  },
  {
    "objectID": "posts/vis-penguins/vis-penguins.html#visualisieren-sie-die-verteilung-von-x1-und-x2.",
    "href": "posts/vis-penguins/vis-penguins.html#visualisieren-sie-die-verteilung-von-x1-und-x2.",
    "title": "vis-penguins",
    "section": "Visualisieren Sie die Verteilung von x1 und x2.",
    "text": "Visualisieren Sie die Verteilung von x1 und x2.\n\nx1\nMit ggpubr:\n\nd_counted &lt;- \n  d |&gt; \n  count(x1) \n\n\nggbarplot(data = d_counted, y = \"n\", x = \"x1\", label = TRUE)\n\n\n\n\n\n\n\n\nMit DataExplorer:\n\nd |&gt; \n  select(x1) |&gt; \n  plot_bar()\n\n\n\n\n\n\n\n\n\n\nx2\n\ngghistostats(d, x = x2)"
  },
  {
    "objectID": "posts/vis-penguins/vis-penguins.html#visualisieren-sie-die-verteilung-von-y-bedingt-auf-x1",
    "href": "posts/vis-penguins/vis-penguins.html#visualisieren-sie-die-verteilung-von-y-bedingt-auf-x1",
    "title": "vis-penguins",
    "section": "Visualisieren Sie die Verteilung von y bedingt auf x1",
    "text": "Visualisieren Sie die Verteilung von y bedingt auf x1\n\ngghistogram(d, x = \"y\", fill = \"x1\")\n\nWarning: Using `bins = 30` by default. Pick better value with the argument\n`bins`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nOder so:\n\ngghistogram(d, x = \"y\", facet.by = \"x1\")\n\nWarning: Using `bins = 30` by default. Pick better value with the argument\n`bins`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`)."
  },
  {
    "objectID": "posts/vis-penguins/vis-penguins.html#fügen-sie-relevante-kennzahlen-zur-letzten-visualisierung-hinzu",
    "href": "posts/vis-penguins/vis-penguins.html#fügen-sie-relevante-kennzahlen-zur-letzten-visualisierung-hinzu",
    "title": "vis-penguins",
    "section": "Fügen Sie relevante Kennzahlen zur letzten Visualisierung hinzu",
    "text": "Fügen Sie relevante Kennzahlen zur letzten Visualisierung hinzu\n\ngrouped_gghistostats(d, x = y, grouping.var = x1)"
  },
  {
    "objectID": "posts/vis-penguins/vis-penguins.html#visualisieren-sie-den-zusammenhang-von-y-und-x2",
    "href": "posts/vis-penguins/vis-penguins.html#visualisieren-sie-den-zusammenhang-von-y-und-x2",
    "title": "vis-penguins",
    "section": "Visualisieren Sie den Zusammenhang von y und x2",
    "text": "Visualisieren Sie den Zusammenhang von y und x2\n\nggscatter(d, x = \"x2\", y = \"y\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "posts/vis-penguins/vis-penguins.html#verbessern-sie-das-letzte-diagramm-so-dass-es-übersichtlicher-wird",
    "href": "posts/vis-penguins/vis-penguins.html#verbessern-sie-das-letzte-diagramm-so-dass-es-übersichtlicher-wird",
    "title": "vis-penguins",
    "section": "Verbessern Sie das letzte Diagramm, so dass es übersichtlicher wird",
    "text": "Verbessern Sie das letzte Diagramm, so dass es übersichtlicher wird\nEs gibt mehrere Wege, das Diagramm übersichtlicher zu machen. Logarithmieren ist ein Weg.\n\nd |&gt; \n  mutate(x2 = log(x2)) |&gt; \n  ggscatter(x = \"x2\", y = \"y\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nSynonym könnten wir schreiben:\n\nd_logged &lt;- \n  d |&gt; \n  mutate(x2 = log(x2))\n  \n\nggscatter(d_logged, x = \"x2\", y = \"y\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "posts/vis-penguins/vis-penguins.html#fügen-sie-dem-letzten-diagramm-relevante-kennzahlen-hinzu",
    "href": "posts/vis-penguins/vis-penguins.html#fügen-sie-dem-letzten-diagramm-relevante-kennzahlen-hinzu",
    "title": "vis-penguins",
    "section": "Fügen Sie dem letzten Diagramm relevante Kennzahlen hinzu",
    "text": "Fügen Sie dem letzten Diagramm relevante Kennzahlen hinzu\n\nggscatterstats(d_logged, x = x2, y = y)"
  },
  {
    "objectID": "posts/vis-penguins/vis-penguins.html#fügen-sie-dem-diagramm-zum-zusammenhang-von-y-und-x2-eine-regressionsgerade-hinzu",
    "href": "posts/vis-penguins/vis-penguins.html#fügen-sie-dem-diagramm-zum-zusammenhang-von-y-und-x2-eine-regressionsgerade-hinzu",
    "title": "vis-penguins",
    "section": "Fügen Sie dem Diagramm zum Zusammenhang von y und x2 eine Regressionsgerade hinzu",
    "text": "Fügen Sie dem Diagramm zum Zusammenhang von y und x2 eine Regressionsgerade hinzu\n\nggscatter(d_logged, x = \"x2\", y = \"y\", add = \"reg.line\", \n             add.params = list(color = \"blue\"))\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "posts/vis-penguins/vis-penguins.html#ersetzen-sie-die-regressionsgerade-durch-eine-loess-gerade",
    "href": "posts/vis-penguins/vis-penguins.html#ersetzen-sie-die-regressionsgerade-durch-eine-loess-gerade",
    "title": "vis-penguins",
    "section": "Ersetzen Sie die Regressionsgerade durch eine LOESS-Gerade",
    "text": "Ersetzen Sie die Regressionsgerade durch eine LOESS-Gerade\n\nggscatter(d_logged, x = \"x2\", y = \"y\", add = \"loess\", \n             add.params = list(color = \"blue\"))\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "posts/vis-penguins/vis-penguins.html#gruppieren-sie-das-letzte-diagramm-nach-x1",
    "href": "posts/vis-penguins/vis-penguins.html#gruppieren-sie-das-letzte-diagramm-nach-x1",
    "title": "vis-penguins",
    "section": "Gruppieren Sie das letzte Diagramm nach x1",
    "text": "Gruppieren Sie das letzte Diagramm nach x1\n\nggscatter(d_logged, x = \"x2\", y = \"y\", add = \"loess\", \n             add.params = list(color = \"blue\"),\n          facet.by = \"x1\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "posts/vis-penguins/vis-penguins.html#dichotomisieren-sie-y-und-zählen-sie-die-häufigkeiten",
    "href": "posts/vis-penguins/vis-penguins.html#dichotomisieren-sie-y-und-zählen-sie-die-häufigkeiten",
    "title": "vis-penguins",
    "section": "Dichotomisieren Sie y und zählen Sie die Häufigkeiten",
    "text": "Dichotomisieren Sie y und zählen Sie die Häufigkeiten\nNehmen wir einen Mediansplit, um zu dichotomisieren.\n\nd &lt;-\n  d |&gt; \n  mutate(y_dicho = ifelse(y &gt; median(y), \"high\", \"low\"))\n\n\nd |&gt; \n  count(y_dicho) |&gt; \n  ggbarplot(x = \"y_dicho\", y = \"n\")\n\n\n\n\n\n\n\n\nGleich viele! Das sollte nicht verwundern."
  },
  {
    "objectID": "posts/vis-penguins/vis-penguins.html#gruppieren-sie-das-letzte-diagramm-nach-den-stufen-von-x1",
    "href": "posts/vis-penguins/vis-penguins.html#gruppieren-sie-das-letzte-diagramm-nach-den-stufen-von-x1",
    "title": "vis-penguins",
    "section": "Gruppieren Sie das letzte Diagramm nach den Stufen von x1",
    "text": "Gruppieren Sie das letzte Diagramm nach den Stufen von x1\n\nd_count &lt;- \nd |&gt; \n  count(y_dicho, x1) \n\nd_count\n\n# A tibble: 3 × 3\n  y_dicho x1            n\n  &lt;lgl&gt;   &lt;fct&gt;     &lt;int&gt;\n1 NA      Adelie      152\n2 NA      Chinstrap    68\n3 NA      Gentoo      124\n\n\n\nggbarplot(d_count, x = \"y_dicho\", y = \"n\", facet.by = \"x1\", label = TRUE)"
  },
  {
    "objectID": "posts/vis-penguins/vis-penguins.html#variieren-sie-das-letzte-diagramm-so-dass-anteile-relative-häufigkeiten-statt-absoluter-häufigkeiten-gezeigt-werden",
    "href": "posts/vis-penguins/vis-penguins.html#variieren-sie-das-letzte-diagramm-so-dass-anteile-relative-häufigkeiten-statt-absoluter-häufigkeiten-gezeigt-werden",
    "title": "vis-penguins",
    "section": "Variieren Sie das letzte Diagramm so, dass Anteile (relative Häufigkeiten) statt absoluter Häufigkeiten gezeigt werden",
    "text": "Variieren Sie das letzte Diagramm so, dass Anteile (relative Häufigkeiten) statt absoluter Häufigkeiten gezeigt werden\n\nd_count &lt;-\n  d_count |&gt; \n  mutate(prop = n / sum(n)) |&gt; \n  mutate(prop = round(prop, 2))\n\nd_count\n\n# A tibble: 3 × 4\n  y_dicho x1            n  prop\n  &lt;lgl&gt;   &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt;\n1 NA      Adelie      152  0.44\n2 NA      Chinstrap    68  0.2 \n3 NA      Gentoo      124  0.36\n\n\nCheck:\n\nd_count |&gt; \n  summarise(sum(prop))\n\n# A tibble: 1 × 1\n  `sum(prop)`\n        &lt;dbl&gt;\n1           1\n\n\nGut! Die Anteile summieren sich zu ca. 1 (100 Prozent).\n\nggbarplot(d_count, x = \"y_dicho\", y = \"prop\", facet.by = \"x1\", label = TRUE)\n\n\n\n\n\n\n\n\nMan beachten, dass sich die Anteile auf das “Gesamt-N” beziehen.\n\nCategories:\n\nvis\nyacsda\nggquick\npenguins\nstring"
  },
  {
    "objectID": "posts/ttest-als-regr/ttest-als-regr.html",
    "href": "posts/ttest-als-regr/ttest-als-regr.html",
    "title": "ttest-als-regression",
    "section": "",
    "text": "Der t-Test kann als Spezialfall der Regressionsanalyse gedeutet werden.\nHierbei ist es wichtig, sich das Skalenniveau der Variablen, die ein t-Test verarbeitet, vor Augen zu führen.\nHinweisse:\n\nDie folgende Abbildung gibt Tipps.\nInformationen, die zur Lösung einer Aufgabe nicht nötig sind, sollte man ignorieren.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBenennen Sie die Skalenniveaus der UV eines t-Tests! Geben Sie nur ein Wort ein. Verwenden Sie nur Kleinbuchstaben (z.B. regression).\nBenennen Sie die Skalenniveaus der AV eines t-Tests! Geben Sie nur ein Wort ein. Verwenden Sie nur Kleinbuchstaben (z.B. regression).\nNennen Sie eine beispielhafte Forschungsfrage für einen t-Test.\nSkizzieren Sie ein Diagramm einer Regression, die analytisch identisch (oder sehr ähnlich) zu einem t-Test ist!"
  },
  {
    "objectID": "posts/ttest-als-regr/ttest-als-regr.html#answerlist",
    "href": "posts/ttest-als-regr/ttest-als-regr.html#answerlist",
    "title": "ttest-als-regression",
    "section": "",
    "text": "Benennen Sie die Skalenniveaus der UV eines t-Tests! Geben Sie nur ein Wort ein. Verwenden Sie nur Kleinbuchstaben (z.B. regression).\nBenennen Sie die Skalenniveaus der AV eines t-Tests! Geben Sie nur ein Wort ein. Verwenden Sie nur Kleinbuchstaben (z.B. regression).\nNennen Sie eine beispielhafte Forschungsfrage für einen t-Test.\nSkizzieren Sie ein Diagramm einer Regression, die analytisch identisch (oder sehr ähnlich) zu einem t-Test ist!"
  },
  {
    "objectID": "posts/ttest-als-regr/ttest-als-regr.html#answerlist-1",
    "href": "posts/ttest-als-regr/ttest-als-regr.html#answerlist-1",
    "title": "ttest-als-regression",
    "section": "Answerlist",
    "text": "Answerlist\n\nUV: binär\nAV: metrisch\nUnterscheiden sich die mittleren Einparkzeiten von Frauen und Männern?\nAus dem Datensatz mtcars:\n\n\ndata(mtcars)\nmtcars %&gt;% \n  ggplot() +\n  aes(x = am, y = mpg) +\n  geom_point(alpha = .5) +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nCategories:\n\nregression\nttest\nvariable-levels"
  },
  {
    "objectID": "posts/n-vars-diagram/n-vars-diagram.html",
    "href": "posts/n-vars-diagram/n-vars-diagram.html",
    "title": "n-vars-diagram",
    "section": "",
    "text": "Aufgabe\nWie viele Variablen sind in folgendem Diagramm dargestellt?\nDie Daten beziehen sich au den Datensatz mtcars; hier finden Sie Informationen zu dem Datensatz. Er ist in R “fest eingebaut”, also direkt verfügbar und muss daher nicht explizit geladen werden.\n\n\n\n\n\n\n\n\n\n         \n\n\nLösung\nEs sind 5 Variablen abgebildet:\n\nhp\nmpg\nam\ncyl\nvs\n\n\nCategories:\n\nvis\n‘2023’\nnum"
  },
  {
    "objectID": "posts/twitter04/twitter04.html",
    "href": "posts/twitter04/twitter04.html",
    "title": "twitter04",
    "section": "",
    "text": "Exercise\nLaden Sie \\(n=10^k\\) Tweets von Twitter herunter (mit \\(k=2\\)) via der Twitter API; Suchterm soll sein “(karl_lauterbach?)”. Bereiten Sie die Textdaten mit grundlegenden Methoden des Textminings auf (Tokenisieren, Stopwörter entfernen, Zahlen entfernen, …). Berichten Sie dann die 10 häufigsten Wörter als Schätzer für die Dinge, die an Karl Lauterbach getweetet werden.\n         \n\n\nSolution\n\nlibrary(rtweet)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()  masks stats::filter()\n✖ purrr::flatten() masks rtweet::flatten()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidytext)\nlibrary(lsa)  # Stopwörter\n\nLoading required package: SnowballC\n\nlibrary(SnowballC)  # Stemming\n\n\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\n\n\nauth &lt;- rtweet_app(bearer_token = Bearer_Token)\n\n\nkarl1 &lt;- search_tweets(\"@karl_lauterbach\", n = 1e2, include_rts = FALSE)\n#write_rds(karl1, file = \"karl1.rds\", compress = \"gz\")\n\n\nkarl2 &lt;- \n  karl1 %&gt;% \n  select(full_text)\n\n\nkarl3 &lt;- \n  karl2 %&gt;% \n  unnest_tokens(output = word, input = full_text)\n\n\nkarl4 &lt;- \nkarl3 %&gt;% \n  anti_join(tibble(word = lsa::stopwords_de)) \n\nJoining with `by = join_by(word)`\n\n\n\nkarl5 &lt;- \n  karl4 %&gt;% \n  mutate(word = str_replace_na(word, \"^[:digit:]+$\")) %&gt;% \n  mutate(word = str_replace_na(word, \"hptts?://\\\\w+\")) %&gt;% \n  mutate(word = str_replace_na(word, \" +\")) %&gt;% \n  drop_na()\n\n\nkarl6 &lt;-\n  karl5 %&gt;% \n  mutate(word = wordStem(word))\n\n\nkarl6 %&gt;% \n  count(word, sort = TRUE) %&gt;% \n  slice_head(n=10)\n\n# A tibble: 10 × 2\n   word                       n\n   &lt;chr&gt;                  &lt;int&gt;\n 1 karl_lauterbach          100\n 2 rt                        60\n 3 ultrakaerl                19\n 4 corona                    16\n 5 wirwollenmaskenpflicht    16\n 6 länder                    12\n 7 gesundheitsminist         11\n 8 polarstern64              11\n 9 schon                     11\n10 shomburg                  11\n\n\n\nCategories:\n\ntextmining\ntwitter"
  },
  {
    "objectID": "posts/tidymodels-remove-na2/tidymodels-remove-na2.html",
    "href": "posts/tidymodels-remove-na2/tidymodels-remove-na2.html",
    "title": "tidymodels-remove-na2",
    "section": "",
    "text": "Aufgabe\n\nDas folgende Rezept ist gedacht, fehlende Werte aus dem Datensatz penguins zu entfernen. Allerdings erfüllt es diese Aufgabe nicht.\nFinden Sie den Fehler und korrigieren Sie das Rezept.\nHinweise:\n\nVerwenden Sie tidymodels.\nVerwenden Sie Standardwerte, wo nicht anders angegeben.\nFixieren Sie Zufallszahlen auf den Startwert 42.\n\n         \n\n\nLösung\n\n# Setup:\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.0.8\n✔ dials        1.2.0     ✔ rsample      1.2.0\n✔ dplyr        1.1.3     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.4\n✔ lubridate 1.9.3     ✔ stringr   1.5.0\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.6.0 (red = needs update)\n✔ bayestestR  0.13.1   ✔ correlation 0.8.4 \n✔ datawizard  0.9.0    ✔ effectsize  0.8.6 \n✔ insight     0.19.6   ✔ modelbased  0.8.6 \n✔ performance 0.10.8   ✔ parameters  0.21.3\n✔ report      0.5.7    ✖ see         0.8.0 \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`.\n\n# Data:\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\"\nd &lt;- read_csv(d_path)\n\nRows: 344 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): species, island, sex\ndbl (6): rownames, bill_length_mm, bill_depth_mm, flipper_length_mm, body_ma...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# recipe:\nrec1 &lt;- recipe(body_mass_g ~  ., data = d) |&gt; \n  step_naomit() \n\nAls Check: Das gepreppte/bebackene Rezept:\n\nrec1_prepped &lt;- prep(rec1)\nd_train_baked &lt;- bake(rec1_prepped, new_data = NULL)\n\n\nd_train_baked |&gt; \n  head()\n\n# A tibble: 6 × 9\n  rownames species island   bill_length_mm bill_depth_mm flipper_length_mm sex  \n     &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt;             &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt; &lt;fct&gt;\n1        1 Adelie  Torgers…           39.1          18.7               181 male \n2        2 Adelie  Torgers…           39.5          17.4               186 fema…\n3        3 Adelie  Torgers…           40.3          18                 195 fema…\n4        4 Adelie  Torgers…           NA            NA                  NA &lt;NA&gt; \n5        5 Adelie  Torgers…           36.7          19.3               193 fema…\n6        6 Adelie  Torgers…           39.3          20.6               190 male \n# ℹ 2 more variables: year &lt;dbl&gt;, body_mass_g &lt;dbl&gt;\n\n\n\ndescribe_distribution(d_train_baked)\n\nVariable          |    Mean |     SD |     IQR |              Range | Skewness | Kurtosis |   n | n_Missing\n-----------------------------------------------------------------------------------------------------------\nrownames          |  172.50 |  99.45 |  172.50 |     [1.00, 344.00] |     0.00 |    -1.20 | 344 |         0\nbill_length_mm    |   43.92 |   5.46 |    9.30 |     [32.10, 59.60] |     0.05 |    -0.88 | 342 |         2\nbill_depth_mm     |   17.15 |   1.97 |    3.12 |     [13.10, 21.50] |    -0.14 |    -0.91 | 342 |         2\nflipper_length_mm |  200.92 |  14.06 |   23.25 |   [172.00, 231.00] |     0.35 |    -0.98 | 342 |         2\nyear              | 2008.03 |   0.82 |    2.00 | [2007.00, 2009.00] |    -0.05 |    -1.50 | 344 |         0\nbody_mass_g       | 4201.75 | 801.95 | 1206.25 | [2700.00, 6300.00] |     0.47 |    -0.72 | 342 |         2\n\n\n\nCategories:\n\ntidymodels\nstatlearning\ntemplate\nstring"
  },
  {
    "objectID": "posts/dplyr-mtcars01/index.html",
    "href": "posts/dplyr-mtcars01/index.html",
    "title": "dplyr-mtcars1",
    "section": "",
    "text": "Aufgabe\nIm Datensatz mtcars:\nGruppieren Sie den Datensatz in Autos mit bzw. ohne Automatikgetriebe.\nGeben Sie dann an, wieviel PS das Automatik-Auto mit der höchsten PS-Zahl hat.\nGeben Sie diese Zahl als Antwort zurück!\nHinweise:\n\nDer Datensatz mtcars ist in R “fest eingebaut”. Sie können ihn mit dem Befehl data(mtcars) verfügbar machen. Ein Herunterladen ist nicht nötig.\nHilfe zu einem Datensatz (oder einem anderen Objekt) bekommen Sie in R mit dem Befehl help(name_des_objekts).\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n         \n\n\nLösung\nPakete starten:\n\nlibrary(tidyverse)  # startet das Paket tidyverse\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nDaten importieren:\n\ndata(mtcars)\n\nZusammenfassen:\n\nmtcars |&gt; \n  group_by(am) |&gt; \n  summarise(max_ps = max(hp))\n\n# A tibble: 2 × 2\n     am max_ps\n  &lt;dbl&gt;  &lt;dbl&gt;\n1     0    245\n2     1    335\n\n\nDie Lösung lautet: 245 PS.\nAus der Hilfeseite können wir ablesen:\n\n[, 9] am Transmission (0 = automatic, 1 = manual)"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html",
    "href": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html",
    "title": "germeval03-sent-wordvec-xgb-plain",
    "section": "",
    "text": "Erstellen Sie ein prädiktives Modell für Textdaten. Nutzen Sie Sentiments und TextFeatures im Rahmen von Feature-Engineering. Nutzen Sie außerdem deutsche Word-Vektoren für das Feature-Engineering.\nAls Lernalgorithmus verwenden Sie XGB.\nPreppen und Backen Sie das Rezept, aber führen Sie die Pipelien mit dem gebackenen Datensatz und einem “Plain-Rezept” durch.\n\n\nVerwenden Sie die GermEval-2018-Daten.\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\nDie Daten sind auch über das R-Paket PradaData zu beziehen.\n\nlibrary(tidyverse)\ndata(\"germeval_train\", package = \"pradadata\")\ndata(\"germeval_test\", package = \"pradadata\")\n\n\n\n\nDie AV lautet c1. Die (einzige) UV lautet: text.\n\n\n\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\nNutzen Sie Tidymodels.\nNutzen Sie das sentiws Lexikon.\n❗ Achten Sie darauf, die Variable c2 zu entfernen bzw. nicht zu verwenden."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#daten",
    "href": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#daten",
    "title": "germeval03-sent-wordvec-xgb-plain",
    "section": "",
    "text": "Verwenden Sie die GermEval-2018-Daten.\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\nDie Daten sind auch über das R-Paket PradaData zu beziehen.\n\nlibrary(tidyverse)\ndata(\"germeval_train\", package = \"pradadata\")\ndata(\"germeval_test\", package = \"pradadata\")"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#av-und-uv",
    "href": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#av-und-uv",
    "title": "germeval03-sent-wordvec-xgb-plain",
    "section": "",
    "text": "Die AV lautet c1. Die (einzige) UV lautet: text."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#hinweise",
    "href": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#hinweise",
    "title": "germeval03-sent-wordvec-xgb-plain",
    "section": "",
    "text": "Orientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\nNutzen Sie Tidymodels.\nNutzen Sie das sentiws Lexikon.\n❗ Achten Sie darauf, die Variable c2 zu entfernen bzw. nicht zu verwenden."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#setup",
    "href": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#setup",
    "title": "germeval03-sent-wordvec-xgb-plain",
    "section": "Setup",
    "text": "Setup\n\nd_train &lt;-\n  germeval_train |&gt; \n  select(id, c1, text)\n\n\nlibrary(tictoc)\nlibrary(tidymodels)\nlibrary(syuzhet)\nlibrary(beepr)\nlibrary(lobstr)  # object size\nlibrary(visdat)  # Fingerprint/footprint of dataset (CSV)\ndata(\"sentiws\", package = \"pradadata\")\n\nEine Vorlage für ein Tidymodels-Pipeline findet sich hier."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#learnermodell",
    "href": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#learnermodell",
    "title": "germeval03-sent-wordvec-xgb-plain",
    "section": "Learner/Modell",
    "text": "Learner/Modell\n\nmod &lt;-\n  boost_tree(mode = \"classification\",\n             learn_rate = tune(), \n             tree_depth = tune()\n             )"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#rezept-workvektoren",
    "href": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#rezept-workvektoren",
    "title": "germeval03-sent-wordvec-xgb-plain",
    "section": "Rezept Workvektoren",
    "text": "Rezept Workvektoren\nPfad zu den Wordvektoren:\n\npath_wordvec &lt;- \"/Users/sebastiansaueruser/datasets/word-embeddings/wikipedia2vec/part-0.arrow\"\n\n\nsource(\"https://raw.githubusercontent.com/sebastiansauer/Datenwerk2/main/funs/def_recipe_wordvec_senti.R\")\n\nrec &lt;- def_recipe_wordvec_senti(data_train = d_train,\n                                path_wordvec = path_wordvec)"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#prepbake-wordvektoren",
    "href": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#prepbake-wordvektoren",
    "title": "germeval03-sent-wordvec-xgb-plain",
    "section": "Prep/Bake Wordvektoren",
    "text": "Prep/Bake Wordvektoren\n\ntic()\nrec_prepped &lt;- prep(rec)\ntoc()\n\n78.021 sec elapsed\n\nd_rec_baked &lt;- bake(rec_prepped, new_data = NULL)\n\n\nsum(is.na(d_rec_baked))"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#test-set-auch-baken",
    "href": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#test-set-auch-baken",
    "title": "germeval03-sent-wordvec-xgb-plain",
    "section": "Test-Set auch baken",
    "text": "Test-Set auch baken\n\nd_test_baked &lt;- bake(rec_prepped, new_data = germeval_test)\ndim(d_test_baked)\n\n\nwrite_csv(d_test_baked, \"data/germeval/germeval_test_recipe_wordvec_senti.csv\")\n\nSpäter kann man es dann analog wieder importieren:\n\nd_test_baked &lt;- read_csv(\"https://raw.githubusercontent.com/sebastiansauer/Datenwerk2/main/data/germeval/germeval_test_recipe_wordvec_senti.csv\")"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#gebackenen-datensatz-als-neue-grundlage",
    "href": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#gebackenen-datensatz-als-neue-grundlage",
    "title": "germeval03-sent-wordvec-xgb-plain",
    "section": "Gebackenen Datensatz als neue Grundlage",
    "text": "Gebackenen Datensatz als neue Grundlage\nDen gepreppten/gebackenen Datensatz speichern wir als Datensatz ab:\n\nwrite_csv(d_rec_baked, \"https://raw.githubusercontent.com/sebastiansauer/Datenwerk2/main/data/germeval/germeval_train_recipe_wordvec_senti.csv\")\n\nSpäter können wir den Datensatz als “neuen, frischen” Datensatz für ein “Plain-Rezept”, also ein ganz einfaches Rezept nutzen. Das hat den Vorteil (hoffentlich), das die Datenvolumina viel kleiner sind.\n\nd_train_new &lt;-\n  read_csv(\"https://raw.githubusercontent.com/sebastiansauer/Datenwerk2/main/data/germeval/germeval_train_recipe_wordvec_senti.csv\")\n\n\nvis_dat(d_train_new) +\n  # remove axis labels:\n  theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank() \n        )"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#plain-rezept",
    "href": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#plain-rezept",
    "title": "germeval03-sent-wordvec-xgb-plain",
    "section": "Plain-Rezept",
    "text": "Plain-Rezept\n\nrec &lt;- \n  recipe(c1 ~ ., data = d_train_new)"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#neuer-workflow-mit-plainem-rezept",
    "href": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#neuer-workflow-mit-plainem-rezept",
    "title": "germeval03-sent-wordvec-xgb-plain",
    "section": "Neuer Workflow mit plainem Rezept",
    "text": "Neuer Workflow mit plainem Rezept\n\nwf &lt;-\n  workflow() |&gt; \n  add_recipe(rec) |&gt; \n  add_model(mod)\n\nwf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  tree_depth = tune()\n  learn_rate = tune()\n\nComputational engine: xgboost"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#parallelisierung-über-mehrere-kerne",
    "href": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#parallelisierung-über-mehrere-kerne",
    "title": "germeval03-sent-wordvec-xgb-plain",
    "section": "Parallelisierung über mehrere Kerne",
    "text": "Parallelisierung über mehrere Kerne\n\nlibrary(parallel)\nall_cores &lt;- detectCores(logical = FALSE)\n\nlibrary(doFuture)\nregisterDoFuture()\ncl &lt;- makeCluster(2)\nplan(cluster, workers = cl)\n\nAchtung: Viele Kerne brauchen auch viel Speicher."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#tuneresamplefit",
    "href": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#tuneresamplefit",
    "title": "germeval03-sent-wordvec-xgb-plain",
    "section": "Tune/Resample/Fit",
    "text": "Tune/Resample/Fit\n\ntic()\nfit_wordvec_senti_xgb &lt;-\n  tune_grid(\n    wf,\n    grid = 50,\n    resamples = vfold_cv(d_train_new, v = 5))\ntoc()\n\n285.723 sec elapsed\n\nbeep()\n\nObjekt-Größe:\n\nlobstr::obj_size(fit_wordvec_senti_xgb)\n\n5.11 MB\n\n\nAh! Angenehm klein."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#get-best-performance",
    "href": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#get-best-performance",
    "title": "germeval03-sent-wordvec-xgb-plain",
    "section": "Get best performance",
    "text": "Get best performance\n\nautoplot(fit_wordvec_senti_xgb)\n\n\n\n\n\n\n\n\n\nshow_best(fit_wordvec_senti_xgb)\n\n# A tibble: 5 × 8\n  tree_depth learn_rate .metric .estimator  mean     n std_err .config          \n       &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;            \n1         10      0.252 roc_auc binary     0.761     5 0.0108  Preprocessor1_Mo…\n2          9      0.220 roc_auc binary     0.760     5 0.00759 Preprocessor1_Mo…\n3         10      0.179 roc_auc binary     0.758     5 0.0111  Preprocessor1_Mo…\n4          5      0.106 roc_auc binary     0.758     5 0.0115  Preprocessor1_Mo…\n5          2      0.286 roc_auc binary     0.757     5 0.00868 Preprocessor1_Mo…\n\n\n\nbest_params &lt;- select_best(fit_wordvec_senti_xgb)"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#finalisieren",
    "href": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#finalisieren",
    "title": "germeval03-sent-wordvec-xgb-plain",
    "section": "Finalisieren",
    "text": "Finalisieren\n\nbest_params &lt;- select_best(fit_wordvec_senti_xgb)\ntic()\nwf_finalized &lt;- finalize_workflow(wf, best_params)\nlastfit_xgb &lt;- fit(wf_finalized, data = d_train_new)\ntoc()\n\n2.853 sec elapsed"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#test-set-güte",
    "href": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#test-set-güte",
    "title": "germeval03-sent-wordvec-xgb-plain",
    "section": "Test-Set-Güte",
    "text": "Test-Set-Güte\n\ntic()\npreds &lt;-\n  predict(lastfit_xgb, new_data = d_test_baked)\ntoc()\n\n0.035 sec elapsed\n\n\n\nd_test &lt;-\n  germeval_test |&gt; \n  bind_cols(preds) |&gt; \n  mutate(c1 = as.factor(c1))\n\n\nmy_metrics &lt;- metric_set(accuracy, f_meas)\nmy_metrics(d_test,\n           truth = c1,\n           estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.714\n2 f_meas   binary         0.484"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#fazit",
    "href": "posts/germeval-sent-wordvec-xgb-plain/germeval-sent-wordvec-xgb-plain.html#fazit",
    "title": "germeval03-sent-wordvec-xgb-plain",
    "section": "Fazit",
    "text": "Fazit\nVerzichtet man auf ein Rezept mit viel Datenvolument (Wordvektoren blähen das Rezept mächtig auf), so wird das Fitten schlanker und schneller. Schneller auch deshalb, weil ggf. kein Swapping zwischen Speicher und Festplatte mehr nötig ist."
  },
  {
    "objectID": "posts/filter-na2/filter-na2.html",
    "href": "posts/filter-na2/filter-na2.html",
    "title": "filter-na2",
    "section": "",
    "text": "Filtern Sie alle Zeilen mit fehlende Werte im Datensatz penguins!"
  },
  {
    "objectID": "posts/filter-na2/filter-na2.html#setup",
    "href": "posts/filter-na2/filter-na2.html#setup",
    "title": "filter-na2",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\"\nd &lt;- read_csv(d_path)\n\nRows: 344 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): species, island, sex\ndbl (6): rownames, bill_length_mm, bill_depth_mm, flipper_length_mm, body_ma...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnrow(d)\n\n[1] 344"
  },
  {
    "objectID": "posts/filter-na2/filter-na2.html#weg-1",
    "href": "posts/filter-na2/filter-na2.html#weg-1",
    "title": "filter-na2",
    "section": "Weg 1",
    "text": "Weg 1\n\nd %&gt;% \n  filter(!complete.cases(.)) %&gt;% \n  nrow()\n\n[1] 11"
  },
  {
    "objectID": "posts/filter-na2/filter-na2.html#weg-2",
    "href": "posts/filter-na2/filter-na2.html#weg-2",
    "title": "filter-na2",
    "section": "Weg 2",
    "text": "Weg 2\n\nd %&gt;% \n  filter(if_any(everything(), ~ is.na(.))) %&gt;% \n  nrow()\n\n[1] 11\n\n\n\nCategories:\n\n2023\neda\nna\nstring"
  },
  {
    "objectID": "posts/summarise03/summarise03.html",
    "href": "posts/summarise03/summarise03.html",
    "title": "summarise03",
    "section": "",
    "text": "Aufgabe\nIm Datensatz mariokart:\n\nGruppieren Sie danach, wie viele Lenkräder bei der Auktion dabei waren.\nFassen Sie die Spalte total_pr zusammen und zwar zum Mittelwert - pro Gruppe!\nBerechnen Sie den Mittelwert dieser Zahlen!\n\nGeben Sie diese Zahl als Antwort zurück!\n         \n\n\nLösung\nPakete starten:\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.6.0 (red = needs update)\n✔ bayestestR  0.13.1   ✔ correlation 0.8.4 \n✔ datawizard  0.9.0    ✔ effectsize  0.8.6 \n✔ insight     0.19.6   ✔ modelbased  0.8.6 \n✔ performance 0.10.8   ✔ parameters  0.21.3\n✔ report      0.5.7    ✖ see         0.8.0 \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`.\n\nlibrary(tidyverse)  # startet das Paket tidyverse\n\nDaten importieren:\n\nmariokart &lt;- data_read(\"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\")\n\nZusammenfassen:\n\nmariokart_gruppiert &lt;- group_by(mariokart, wheels)  # Gruppieren\nmariokart_klein &lt;- summarise(mariokart_gruppiert, pr_mean = mean(total_pr))  # zusammenfassen\nmariokart_klein\n\n# A tibble: 5 × 2\n  wheels pr_mean\n   &lt;int&gt;   &lt;dbl&gt;\n1      0    41.1\n2      1    44.2\n3      2    61.0\n4      3    69.8\n5      4    65.0\n\n\n\nsummarise(mariokart_klein, pr_mean = mean(pr_mean))\n\n# A tibble: 1 × 1\n  pr_mean\n    &lt;dbl&gt;\n1    56.2\n\n\nmin analog.\nDie Lösung lautet: 56\n\nCategories:\n\ndatawrangling\neda\ntidyverse\ndplyr\nnum"
  },
  {
    "objectID": "posts/adjustieren2_var1/adjustieren2_var1.html",
    "href": "posts/adjustieren2_var1/adjustieren2_var1.html",
    "title": "adjustieren2_var1",
    "section": "",
    "text": "Aufgabe\nBetrachten Sie folgendes Modell, das den Zusammenhang des Preises (price) und dem Gewicht (carat) von Diamanten untersucht (Datensatz diamonds).\n\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(rstanarm)\ndiamonds &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/ggplot2/diamonds.csv\")\n\nRows: 53940 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): cut, color, clarity\ndbl (8): rownames, carat, depth, table, price, x, y, z\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAber zuerst zentrieren wir den metrischen Prädiktor carat, um den Achsenabschnitt besser interpretieren zu können.\n\ndiamonds2 &lt;-\n  diamonds %&gt;% \n  mutate(carat_z = carat - mean(carat, na.rm = TRUE))\n\nDann berechnen wir ein (bayesianisches) Regressionsmodell, wobei wir auf die Standardwerte der Prior zurückgreifen.\n\nlibrary(rstanarm)\nlm1 &lt;- stan_glm(price ~ carat_z, data = diamonds2,\n                refresh = 0)\nparameters(lm1)\n\nParameter   |  Median |             95% CI |   pd |  Rhat |     ESS |                       Prior\n-------------------------------------------------------------------------------------------------\n(Intercept) | 3932.64 | [3919.83, 3945.55] | 100% | 1.004 | 1178.00 | Normal (3932.80 +- 9973.60)\ncarat_z     | 7755.90 | [7727.59, 7783.52] | 100% | 1.000 | 4716.00 |   Normal (0.00 +- 21040.85)\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a MCMC distribution approximation.\n\n\nZur Verdeutlichung ein Diagramm zum Modell:\n\ndiamonds2 %&gt;% \n  ggplot() +\n  aes(x = carat_z, y = price) +\n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nOder so:\n\nestimate_relation(lm1) |&gt; plot()\n\n\n\n\n\n\n\n\nAufgabe:\nGeben Sie eine Regressionsformel an, die lm1 ergänzt, so dass die Schliffart (cut) des Diamanten kontrolliert (adjustiert) wird. Anders gesagt: Das Modell soll die mittleren Preise für jede der fünf Schliffarten angeben.\nHinweis:\n\nGeben Sie nur die Regressionsformel an.\nLassen Sie zwischen Termen der Regressionsformel jeweils ein Leerzeichen Abstand.\nBeziehen Sie sich auf das Modell bzw. die Angaben oben.\nEs gibt (laut Datensatz) folgende Schliffarten (und zwar in der folgenden Reihenfolge):\n\n\ndiamonds %&gt;% \n  distinct(cut)\n\n# A tibble: 5 × 1\n  cut      \n  &lt;chr&gt;    \n1 Ideal    \n2 Premium  \n3 Good     \n4 Very Good\n5 Fair     \n\n\n         \n\n\nLösung\nDie richtige Antwort lautet: price ~ carat_z + cut\nDas Modell könnten wir so berechnen:\n\nlm2 &lt;- stan_glm(price ~ carat_z + cut, data = diamonds2,\n                refresh = 0)  # verhindert einen Haufen unnötigen Output\nparameters(lm2)\n\nParameter    |  Median |             95% CI |   pd |  Rhat |     ESS |                       Prior\n--------------------------------------------------------------------------------------------------\n(Intercept)  | 2401.60 | [2326.67, 2475.70] | 100% | 1.000 | 1022.00 | Normal (3932.80 +- 9973.60)\ncarat_z      | 7871.06 | [7844.55, 7898.35] | 100% | 0.999 | 3734.00 |   Normal (0.00 +- 21040.85)\ncutGood      | 1124.68 | [1040.23, 1213.25] | 100% | 1.000 | 1224.00 |   Normal (0.00 +- 34685.38)\ncutIdeal     | 1804.40 | [1726.43, 1883.62] | 100% | 1.000 | 1083.00 |   Normal (0.00 +- 20362.28)\ncutPremium   | 1443.13 | [1365.40, 1520.66] | 100% | 1.000 | 1099.00 |   Normal (0.00 +- 22862.49)\ncutVery Good | 1513.52 | [1435.77, 1594.26] | 100% | 1.000 | 1092.00 |   Normal (0.00 +- 23922.15)\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a MCMC distribution approximation.\n\n\nOder auch so, mit der klassischen Regression:\nlm(price ~ carat_z + cut, data = diamonds2) gi```\nDas führt zu ähnlichen Ergebnissen.\nMan könnte hier noch einen Interaktionseffekt ergänzen.\n\nCategories:\n\nlm\nregression\nbayes\nadjust\nstring"
  },
  {
    "objectID": "posts/summarise04/summarise04.html",
    "href": "posts/summarise04/summarise04.html",
    "title": "summarise04",
    "section": "",
    "text": "Aufgabe\nIm Datensatz mariokart:\n\nGruppieren Sie danach, wie viele Lenkräder bei der Auktion dabei waren.\nFassen Sie die Spalte total_pr zusammen und zwar zur Standardabweichung (SD) - pro Gruppe!\n\nGeben Sie die erste Kennzahl als Antwort zurück!\n         \n\n\nLösung\nPakete starten:\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.6.0 (red = needs update)\n✔ bayestestR  0.13.1   ✔ correlation 0.8.4 \n✔ datawizard  0.9.0    ✔ effectsize  0.8.6 \n✔ insight     0.19.6   ✔ modelbased  0.8.6 \n✔ performance 0.10.8   ✔ parameters  0.21.3\n✔ report      0.5.7    ✖ see         0.8.0 \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`.\n\nlibrary(tidyverse)  # startet das Paket tidyverse\n\nDaten importieren:\n\nmariokart &lt;- data_read(\"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\")\n\nOder so:\n\ndata(mariokart, package = \"openintro\")  # aus dem Paket \"openintro\"\n\nDazu muss das Paket openintro auf Ihrem Computer installiert sein.\nZusammenfassen:\n\nmariokart_gruppiert &lt;- group_by(mariokart, wheels)  # Gruppieren\nmariokart_klein &lt;- summarise(mariokart_gruppiert, pr_sd = sd(total_pr))  # zusammenfassen\nmariokart_klein\n\n# A tibble: 5 × 2\n  wheels pr_sd\n   &lt;int&gt; &lt;dbl&gt;\n1      0 14.3 \n2      1  4.15\n3      2 38.3 \n4      3  7.42\n5      4 NA   \n\n\nDie Lösung lautet: 14.27\n\nCategories:\n\ndatawrangling\neda\ntidyverse\ndplyr\nvariability\nnum"
  },
  {
    "objectID": "posts/tmdb02/tmdb02.html",
    "href": "posts/tmdb02/tmdb02.html",
    "title": "tmdb02",
    "section": "",
    "text": "Wir bearbeiten hier die Fallstudie TMDB Box Office Prediction - Can you predict a movie’s worldwide box office revenue?, ein Kaggle-Prognosewettbewerb.\nZiel ist es, genaue Vorhersagen zu machen, in diesem Fall für Filme.\nDie Daten können Sie von der Kaggle-Projektseite beziehen oder so:\n\nd_train_path &lt;- \"https://raw.githubusercontent.com/sebastiansauer/Lehre/main/data/tmdb-box-office-prediction/train.csv\"\nd_test_path &lt;- \"https://raw.githubusercontent.com/sebastiansauer/Lehre/main/data/tmdb-box-office-prediction/test.csv\"\n\n\n\nReichen Sie bei Kaggle eine Submission für die Fallstudie ein! Berichten Sie den Kaggle-Score\nHinweise:\n\nSie müssen sich bei Kaggle ein Konto anlegen (kostenlos und anonym möglich); alternativ können Sie sich mit einem Google-Konto anmelden.\nBerechnen Sie einen Entscheidungsbaum und einen Random-Forest.\nTunen Sie nach Bedarf; verwenden Sie aber Default-Werte.\nVerwenden Sie Tidymodels."
  },
  {
    "objectID": "posts/tmdb02/tmdb02.html#vorbereitung",
    "href": "posts/tmdb02/tmdb02.html#vorbereitung",
    "title": "tmdb02",
    "section": "Vorbereitung",
    "text": "Vorbereitung\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(tictoc)\nlibrary(doParallel)  # mehrere CPUs nutzen\nlibrary(finetune)  # Tune Anova\n\n\nd_train &lt;- read_csv(d_train_path)\nd_test &lt;- read_csv(d_test_path)\n\nglimpse(d_train)\n\nRows: 3,000\nColumns: 23\n$ id                    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ belongs_to_collection &lt;chr&gt; \"[{'id': 313576, 'name': 'Hot Tub Time Machine C…\n$ budget                &lt;dbl&gt; 1.40e+07, 4.00e+07, 3.30e+06, 1.20e+06, 0.00e+00…\n$ genres                &lt;chr&gt; \"[{'id': 35, 'name': 'Comedy'}]\", \"[{'id': 35, '…\n$ homepage              &lt;chr&gt; NA, NA, \"http://sonyclassics.com/whiplash/\", \"ht…\n$ imdb_id               &lt;chr&gt; \"tt2637294\", \"tt0368933\", \"tt2582802\", \"tt182148…\n$ original_language     &lt;chr&gt; \"en\", \"en\", \"en\", \"hi\", \"ko\", \"en\", \"en\", \"en\", …\n$ original_title        &lt;chr&gt; \"Hot Tub Time Machine 2\", \"The Princess Diaries …\n$ overview              &lt;chr&gt; \"When Lou, who has become the \\\"father of the In…\n$ popularity            &lt;dbl&gt; 6.575393, 8.248895, 64.299990, 3.174936, 1.14807…\n$ poster_path           &lt;chr&gt; \"/tQtWuwvMf0hCc2QR2tkolwl7c3c.jpg\", \"/w9Z7A0GHEh…\n$ production_companies  &lt;chr&gt; \"[{'name': 'Paramount Pictures', 'id': 4}, {'nam…\n$ production_countries  &lt;chr&gt; \"[{'iso_3166_1': 'US', 'name': 'United States of…\n$ release_date          &lt;chr&gt; \"2/20/15\", \"8/6/04\", \"10/10/14\", \"3/9/12\", \"2/5/…\n$ runtime               &lt;dbl&gt; 93, 113, 105, 122, 118, 83, 92, 84, 100, 91, 119…\n$ spoken_languages      &lt;chr&gt; \"[{'iso_639_1': 'en', 'name': 'English'}]\", \"[{'…\n$ status                &lt;chr&gt; \"Released\", \"Released\", \"Released\", \"Released\", …\n$ tagline               &lt;chr&gt; \"The Laws of Space and Time are About to be Viol…\n$ title                 &lt;chr&gt; \"Hot Tub Time Machine 2\", \"The Princess Diaries …\n$ Keywords              &lt;chr&gt; \"[{'id': 4379, 'name': 'time travel'}, {'id': 96…\n$ cast                  &lt;chr&gt; \"[{'cast_id': 4, 'character': 'Lou', 'credit_id'…\n$ crew                  &lt;chr&gt; \"[{'credit_id': '59ac067c92514107af02c8c8', 'dep…\n$ revenue               &lt;dbl&gt; 12314651, 95149435, 13092000, 16000000, 3923970,…\n\nglimpse(d_test)\n\nRows: 4,398\nColumns: 22\n$ id                    &lt;dbl&gt; 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, …\n$ belongs_to_collection &lt;chr&gt; \"[{'id': 34055, 'name': 'Pokémon Collection', 'p…\n$ budget                &lt;dbl&gt; 0.00e+00, 8.80e+04, 0.00e+00, 6.80e+06, 2.00e+06…\n$ genres                &lt;chr&gt; \"[{'id': 12, 'name': 'Adventure'}, {'id': 16, 'n…\n$ homepage              &lt;chr&gt; \"http://www.pokemon.com/us/movies/movie-pokemon-…\n$ imdb_id               &lt;chr&gt; \"tt1226251\", \"tt0051380\", \"tt0118556\", \"tt125595…\n$ original_language     &lt;chr&gt; \"ja\", \"en\", \"en\", \"fr\", \"en\", \"en\", \"de\", \"en\", …\n$ original_title        &lt;chr&gt; \"ディアルガVSパルキアVSダークライ\", \"Attack of t…\n$ overview              &lt;chr&gt; \"Ash and friends (this time accompanied by newco…\n$ popularity            &lt;dbl&gt; 3.851534, 3.559789, 8.085194, 8.596012, 3.217680…\n$ poster_path           &lt;chr&gt; \"/tnftmLMemPLduW6MRyZE0ZUD19z.jpg\", \"/9MgBNBqlH1…\n$ production_companies  &lt;chr&gt; NA, \"[{'name': 'Woolner Brothers Pictures Inc.',…\n$ production_countries  &lt;chr&gt; \"[{'iso_3166_1': 'JP', 'name': 'Japan'}, {'iso_3…\n$ release_date          &lt;chr&gt; \"7/14/07\", \"5/19/58\", \"5/23/97\", \"9/4/10\", \"2/11…\n$ runtime               &lt;dbl&gt; 90, 65, 100, 130, 92, 121, 119, 77, 120, 92, 88,…\n$ spoken_languages      &lt;chr&gt; \"[{'iso_639_1': 'en', 'name': 'English'}, {'iso_…\n$ status                &lt;chr&gt; \"Released\", \"Released\", \"Released\", \"Released\", …\n$ tagline               &lt;chr&gt; \"Somewhere Between Time & Space... A Legend Is B…\n$ title                 &lt;chr&gt; \"Pokémon: The Rise of Darkrai\", \"Attack of the 5…\n$ Keywords              &lt;chr&gt; \"[{'id': 11451, 'name': 'pok√©mon'}, {'id': 1155…\n$ cast                  &lt;chr&gt; \"[{'cast_id': 3, 'character': 'Tonio', 'credit_i…\n$ crew                  &lt;chr&gt; \"[{'credit_id': '52fe44e7c3a368484e03d683', 'dep…"
  },
  {
    "objectID": "posts/tmdb02/tmdb02.html#rezept",
    "href": "posts/tmdb02/tmdb02.html#rezept",
    "title": "tmdb02",
    "section": "Rezept",
    "text": "Rezept\n\nRezept definieren\n\nrec1 &lt;-\n  recipe(revenue ~ ., data = d_train) %&gt;% \n  update_role(all_predictors(), new_role = \"id\") %&gt;% \n  update_role(popularity, runtime, revenue, budget) %&gt;% \n  update_role(revenue, new_role = \"outcome\") %&gt;% \n  step_mutate(budget = ifelse(budget &lt; 10, 10, budget)) %&gt;% \n  step_log(budget) %&gt;% \n  step_impute_knn(all_predictors())\n\nrec1\n\n\n\nCheck das Rezept\n\nrec1_prepped &lt;-\n  prep(rec1, verbose = TRUE)\n\noper 1 step mutate [training] \noper 2 step log [training] \noper 3 step impute knn [training] \nThe retained training set is ~ 28.71 Mb  in memory.\n\nrec1_prepped\n\n\nd_train_baked &lt;-\n  rec1_prepped %&gt;% \n  bake(new_data = NULL) \n\nhead(d_train_baked)\n\n# A tibble: 6 × 23\n     id belongs_to_collection   budget genres homepage imdb_id original_language\n  &lt;dbl&gt; &lt;fct&gt;                    &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;    &lt;fct&gt;   &lt;fct&gt;            \n1     1 [{'id': 313576, 'name'…  16.5  [{'id… &lt;NA&gt;     tt2637… en               \n2     2 [{'id': 107674, 'name'…  17.5  [{'id… &lt;NA&gt;     tt0368… en               \n3     3 &lt;NA&gt;                     15.0  [{'id… http://… tt2582… en               \n4     4 &lt;NA&gt;                     14.0  [{'id… http://… tt1821… hi               \n5     5 &lt;NA&gt;                      2.30 [{'id… &lt;NA&gt;     tt1380… ko               \n6     6 &lt;NA&gt;                     15.9  [{'id… &lt;NA&gt;     tt0093… en               \n# ℹ 16 more variables: original_title &lt;fct&gt;, overview &lt;fct&gt;, popularity &lt;dbl&gt;,\n#   poster_path &lt;fct&gt;, production_companies &lt;fct&gt;, production_countries &lt;fct&gt;,\n#   release_date &lt;fct&gt;, runtime &lt;dbl&gt;, spoken_languages &lt;fct&gt;, status &lt;fct&gt;,\n#   tagline &lt;fct&gt;, title &lt;fct&gt;, Keywords &lt;fct&gt;, cast &lt;fct&gt;, crew &lt;fct&gt;,\n#   revenue &lt;dbl&gt;\n\n\nDie AV-Spalte sollte leer sein:\n\nbake(rec1_prepped, new_data = head(d_test), all_outcomes())\n\n# A tibble: 6 × 0\n\n\n\nd_train_baked %&gt;% \n  map_df(~ sum(is.na(.)))\n\n# A tibble: 1 × 23\n     id belongs_to_collection budget genres homepage imdb_id original_language\n  &lt;int&gt;                 &lt;int&gt;  &lt;int&gt;  &lt;int&gt;    &lt;int&gt;   &lt;int&gt;             &lt;int&gt;\n1     0                  2396      0      7     2054       0                 0\n# ℹ 16 more variables: original_title &lt;int&gt;, overview &lt;int&gt;, popularity &lt;int&gt;,\n#   poster_path &lt;int&gt;, production_companies &lt;int&gt;, production_countries &lt;int&gt;,\n#   release_date &lt;int&gt;, runtime &lt;int&gt;, spoken_languages &lt;int&gt;, status &lt;int&gt;,\n#   tagline &lt;int&gt;, title &lt;int&gt;, Keywords &lt;int&gt;, cast &lt;int&gt;, crew &lt;int&gt;,\n#   revenue &lt;int&gt;\n\n\nKeine fehlenden Werte mehr in den Prädiktoren.\nNach fehlenden Werten könnte man z.B. auch so suchen:\n\ndatawizard::describe_distribution(d_train_baked)\n\nVariable   |     Mean |       SD |      IQR |              Range | Skewness | Kurtosis |    n | n_Missing\n---------------------------------------------------------------------------------------------------------\nid         |  1500.50 |   866.17 |  1500.50 |    [1.00, 3000.00] |     0.00 |    -1.20 | 3000 |         0\nbudget     |    12.51 |     6.44 |    14.88 |      [2.30, 19.76] |    -0.87 |    -1.09 | 3000 |         0\npopularity |     8.46 |    12.10 |     6.88 | [1.00e-06, 294.34] |    14.38 |   280.10 | 3000 |         0\nruntime    |   107.85 |    22.08 |    24.00 |     [0.00, 338.00] |     1.02 |     8.20 | 3000 |         0\nrevenue    | 6.67e+07 | 1.38e+08 | 6.66e+07 |   [1.00, 1.52e+09] |     4.54 |    27.78 | 3000 |         0\n\n\nSo bekommt man gleich noch ein paar Infos über die Verteilung der Variablen. Praktische Sache.\nDas Test-Sample backen wir auch mal:\n\nd_test_baked &lt;-\n  bake(rec1_prepped, new_data = d_test)\n\nd_test_baked %&gt;% \n  head()\n\n# A tibble: 6 × 22\n     id belongs_to_collection   budget genres homepage imdb_id original_language\n  &lt;dbl&gt; &lt;fct&gt;                    &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;    &lt;fct&gt;   &lt;fct&gt;            \n1  3001 [{'id': 34055, 'name':…   2.30 [{'id… &lt;NA&gt;     &lt;NA&gt;    ja               \n2  3002 &lt;NA&gt;                     11.4  [{'id… &lt;NA&gt;     &lt;NA&gt;    en               \n3  3003 &lt;NA&gt;                      2.30 [{'id… &lt;NA&gt;     &lt;NA&gt;    en               \n4  3004 &lt;NA&gt;                     15.7  &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;    fr               \n5  3005 &lt;NA&gt;                     14.5  [{'id… &lt;NA&gt;     &lt;NA&gt;    en               \n6  3006 &lt;NA&gt;                      2.30 [{'id… &lt;NA&gt;     &lt;NA&gt;    en               \n# ℹ 15 more variables: original_title &lt;fct&gt;, overview &lt;fct&gt;, popularity &lt;dbl&gt;,\n#   poster_path &lt;fct&gt;, production_companies &lt;fct&gt;, production_countries &lt;fct&gt;,\n#   release_date &lt;fct&gt;, runtime &lt;dbl&gt;, spoken_languages &lt;fct&gt;, status &lt;fct&gt;,\n#   tagline &lt;fct&gt;, title &lt;fct&gt;, Keywords &lt;fct&gt;, cast &lt;fct&gt;, crew &lt;fct&gt;"
  },
  {
    "objectID": "posts/tmdb02/tmdb02.html#kreuzvalidierung",
    "href": "posts/tmdb02/tmdb02.html#kreuzvalidierung",
    "title": "tmdb02",
    "section": "Kreuzvalidierung",
    "text": "Kreuzvalidierung\n\ncv_scheme &lt;- vfold_cv(d_train,\n                      v = 5, \n                      repeats = 1)"
  },
  {
    "objectID": "posts/tmdb02/tmdb02.html#modelle",
    "href": "posts/tmdb02/tmdb02.html#modelle",
    "title": "tmdb02",
    "section": "Modelle",
    "text": "Modelle\n\nBaum\n\nmod_tree &lt;-\n  decision_tree(cost_complexity = tune(),\n                tree_depth = tune(),\n                mode = \"regression\")\n\n\n\nRandom Forest\n\nmod_rf &lt;-\n  rand_forest(mtry = tune(),\n              min_n = tune(),\n              trees = 1000,\n              mode = \"regression\") %&gt;% \n  set_engine(\"ranger\", num.threads = 4)"
  },
  {
    "objectID": "posts/tmdb02/tmdb02.html#workflows",
    "href": "posts/tmdb02/tmdb02.html#workflows",
    "title": "tmdb02",
    "section": "Workflows",
    "text": "Workflows\n\nwf_tree &lt;-\n  workflow() %&gt;% \n  add_model(mod_tree) %&gt;% \n  add_recipe(rec1)\n\nwf_rf &lt;-\n  workflow() %&gt;% \n  add_model(mod_rf) %&gt;% \n  add_recipe(rec1)"
  },
  {
    "objectID": "posts/tmdb02/tmdb02.html#fitten-und-tunen",
    "href": "posts/tmdb02/tmdb02.html#fitten-und-tunen",
    "title": "tmdb02",
    "section": "Fitten und tunen",
    "text": "Fitten und tunen\nUm Rechenzeit zu sparen, kann man den Parameter grid bei tune_grid() auf einen kleinen Wert setzen. Der Default ist 10. Um gute Vorhersagen zu erzielen, sollte man den Wert tendenziell noch über 10 erhöhen.\n\nTree\nParallele Verarbeitung starten:\n\ncl &lt;- makePSOCKcluster(4)  # Create 4 clusters\nregisterDoParallel(cl)\n\n\ntic()\ntree_fit &lt;-\n  wf_tree %&gt;% \n  tune_race_anova(\n    resamples = cv_scheme,\n    #grid = 2\n  )\ntoc()\n\n37.736 sec elapsed\n\n\nHilfe zu tune_grid() bekommt man hier.\n\ntree_fit\n\n# Tuning results\n# 5-fold cross-validation \n# A tibble: 5 × 5\n  splits             id    .order .metrics          .notes          \n  &lt;list&gt;             &lt;chr&gt;  &lt;int&gt; &lt;list&gt;            &lt;list&gt;          \n1 &lt;split [2400/600]&gt; Fold1      3 &lt;tibble [20 × 6]&gt; &lt;tibble [0 × 3]&gt;\n2 &lt;split [2400/600]&gt; Fold2      1 &lt;tibble [20 × 6]&gt; &lt;tibble [0 × 3]&gt;\n3 &lt;split [2400/600]&gt; Fold3      2 &lt;tibble [20 × 6]&gt; &lt;tibble [0 × 3]&gt;\n4 &lt;split [2400/600]&gt; Fold5      4 &lt;tibble [16 × 6]&gt; &lt;tibble [0 × 3]&gt;\n5 &lt;split [2400/600]&gt; Fold4      5 &lt;tibble [14 × 6]&gt; &lt;tibble [0 × 3]&gt;\n\n\nSteht was in den .notes?\n\ntree_fit[[\".notes\"]][[2]]\n\n# A tibble: 0 × 3\n# ℹ 3 variables: location &lt;chr&gt;, type &lt;chr&gt;, note &lt;chr&gt;\n\n\nNein.\n\ncollect_metrics(tree_fit)\n\n# A tibble: 14 × 8\n   cost_complexity tree_depth .metric .estimator      mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1        1.56e- 5         14 rmse    standard     8.95e+7     5 4.65e+6 Prepro…\n 2        1.56e- 5         14 rsq     standard     5.82e-1     5 3.16e-2 Prepro…\n 3        9.32e- 5         10 rmse    standard     8.91e+7     5 4.66e+6 Prepro…\n 4        9.32e- 5         10 rsq     standard     5.85e-1     5 3.11e-2 Prepro…\n 5        2.36e-10          5 rmse    standard     8.80e+7     5 4.57e+6 Prepro…\n 6        2.36e-10          5 rsq     standard     5.92e-1     5 3.20e-2 Prepro…\n 7        2.29e- 8         11 rmse    standard     8.93e+7     5 4.67e+6 Prepro…\n 8        2.29e- 8         11 rsq     standard     5.83e-1     5 3.10e-2 Prepro…\n 9        9.60e- 4          9 rmse    standard     8.84e+7     5 5.00e+6 Prepro…\n10        9.60e- 4          9 rsq     standard     5.90e-1     5 3.22e-2 Prepro…\n11        1.94e- 9         12 rmse    standard     8.95e+7     5 4.64e+6 Prepro…\n12        1.94e- 9         12 rsq     standard     5.82e-1     5 3.10e-2 Prepro…\n13        5.72e- 7          7 rmse    standard     8.83e+7     5 4.73e+6 Prepro…\n14        5.72e- 7          7 rsq     standard     5.91e-1     5 3.38e-2 Prepro…\n\n\n\nshow_best(tree_fit)\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\n\n# A tibble: 5 × 8\n  cost_complexity tree_depth .metric .estimator      mean     n  std_err .config\n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;  \n1        2.36e-10          5 rmse    standard   88038619.     5 4572618. Prepro…\n2        5.72e- 7          7 rmse    standard   88262344.     5 4734314. Prepro…\n3        9.60e- 4          9 rmse    standard   88397994.     5 5003102. Prepro…\n4        9.32e- 5         10 rmse    standard   89140111.     5 4663576. Prepro…\n5        2.29e- 8         11 rmse    standard   89330466.     5 4668641. Prepro…"
  },
  {
    "objectID": "posts/tmdb02/tmdb02.html#finalisieren",
    "href": "posts/tmdb02/tmdb02.html#finalisieren",
    "title": "tmdb02",
    "section": "Finalisieren",
    "text": "Finalisieren\n\nbest_tree_wf &lt;-\n  wf_tree %&gt;% \n  finalize_workflow(select_best(tree_fit))\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\nbest_tree_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_mutate()\n• step_log()\n• step_impute_knn()\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (regression)\n\nMain Arguments:\n  cost_complexity = 2.36005153743282e-10\n  tree_depth = 5\n\nComputational engine: rpart \n\n\n\ntree_last_fit &lt;-\n  fit(best_tree_wf, data = d_train)\n\ntree_last_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_mutate()\n• step_log()\n• step_impute_knn()\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 3000 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 3000 5.672651e+19   66725850  \n   2) budget&lt; 18.32631 2845 1.958584e+19   46935270  \n     4) budget&lt; 17.19976 2252 5.443953e+18   25901120  \n       8) popularity&lt; 9.734966 1745 1.665118e+18   17076460  \n        16) popularity&lt; 5.761331 1019 3.184962e+17    8793730  \n          32) budget&lt; 15.44456 782 1.408243e+17    6074563 *\n          33) budget&gt;=15.44456 237 1.528117e+17   17765830 *\n        17) popularity&gt;=5.761331 726 1.178595e+18   28701940  \n          34) budget&lt; 16.15249 484 6.504138e+17   21093220 *\n          35) budget&gt;=16.15249 242 4.441208e+17   43919380 *\n       9) popularity&gt;=9.734966 507 3.175231e+18   56273980  \n        18) budget&lt; 15.36217 186 3.092335e+17   24880850  \n          36) popularity&lt; 14.04031 151 1.743659e+17   20728170 *\n          37) popularity&gt;=14.04031 35 1.210294e+17   42796710 *\n        19) budget&gt;=15.36217 321 2.576473e+18   74464390  \n          38) popularity&lt; 19.64394 300 2.025184e+18   68010500 *\n          39) popularity&gt;=19.64394 21 3.602808e+17  166662900 *\n     5) budget&gt;=17.19976 593 9.361685e+18  126815400  \n      10) popularity&lt; 19.63372 570 6.590372e+18  117422100  \n        20) budget&lt; 17.86726 374 2.692151e+18   94469490  \n          40) popularity&lt; 8.444193 149 6.363495e+17   68256660 *\n          41) popularity&gt;=8.444193 225 1.885623e+18  111828200 *\n        21) budget&gt;=17.86726 196 3.325222e+18  161219400  \n          42) popularity&lt; 11.60513 126 1.693483e+18  136587100 *\n          43) popularity&gt;=11.60513 70 1.417677e+18  205557600 *\n      11) popularity&gt;=19.63372 23 1.474624e+18  359605200  \n        22) runtime&gt;=109.5 16 9.882757e+17  299077200 *\n        23) runtime&lt; 109.5 7 2.937458e+17  497955000 *\n   3) budget&gt;=18.32631 155 1.557371e+19  429978800  \n     6) popularity&lt; 17.26579 101 4.711450e+18  299997300  \n      12) budget&lt; 18.73897 67 1.671489e+18  230290900  \n        24) popularity&lt; 12.66146 40 5.426991e+17  174328700  \n          48) budget&lt; 18.44536 18 1.099070e+17  134734600 *\n          49) budget&gt;=18.44536 22 3.814856e+17  206724000 *\n        25) popularity&gt;=12.66146 27 8.179336e+17  313197700  \n          50) budget&lt; 18.52944 13 1.273606e+17  234797100 *\n          51) budget&gt;=18.52944 14 5.364675e+17  385998300 *\n      13) budget&gt;=18.73897 34 2.072879e+18  437360100  \n        26) runtime&lt; 132.5 26 1.123840e+18  391271100  \n          52) popularity&lt; 11.34182 9 9.729505e+16  248614500 *\n          53) popularity&gt;=11.34182 17 7.464210e+17  466795200 *\n        27) runtime&gt;=132.5 8 7.143147e+17  587149400 *\n     7) popularity&gt;=17.26579 54 5.964228e+18  673092200  \n      14) budget&lt; 18.99438 33 2.082469e+18  534404700  \n        28) popularity&lt; 25.35778 19 5.425201e+17  416871200 *\n\n...\nand 4 more lines."
  },
  {
    "objectID": "posts/tmdb02/tmdb02.html#vorhersage-test-sample",
    "href": "posts/tmdb02/tmdb02.html#vorhersage-test-sample",
    "title": "tmdb02",
    "section": "Vorhersage Test-Sample",
    "text": "Vorhersage Test-Sample\n\npredict(tree_last_fit, new_data = d_test)\n\n# A tibble: 4,398 × 1\n        .pred\n        &lt;dbl&gt;\n 1   6074563.\n 2   6074563.\n 3  21093221.\n 4  21093221.\n 5   6074563.\n 6  21093221.\n 7   6074563.\n 8  68256659.\n 9  43919378.\n10 205557624.\n# ℹ 4,388 more rows\n\n\n\nRF"
  },
  {
    "objectID": "posts/tmdb02/tmdb02.html#fitten-und-tunen-1",
    "href": "posts/tmdb02/tmdb02.html#fitten-und-tunen-1",
    "title": "tmdb02",
    "section": "Fitten und Tunen",
    "text": "Fitten und Tunen\nUm Rechenzeit zu sparen, kann man das Objekt, wenn einmal berechnet, abspeichern unter result_obj_path auf der Festplatte und beim nächsten Mal importieren, das geht schneller als neu berechnen.\nDas könnte dann z.B. so aussehen:\n\nif (file.exists(result_obj_path)) {\n  rf_fit &lt;- read_rds(result_obj_path)\n} else {\n  tic()\n  rf_fit &lt;-\n    wf_rf %&gt;% \n    tune_grid(\n      resamples = cv_scheme)\n  toc()\n}\n\nAchtung Ein Ergebnisobjekt von der Festplatte zu laden ist gefährlich. Wenn Sie Ihr Modell verändern, aber vergessen, das Objekt auf der Festplatte zu aktualisieren, werden Ihre Ergebnisse falsch sein (da auf dem veralteten Objekt beruhend), ohne dass Sie durch eine Fehlermeldung von R gewarnt würden!\nSo kann man das Ergebnisobjekt auf die Festplatte schreiben:\n\n#write_rds(rf_fit, file = \"objects/tmbd_rf_fit1.rds\")\n\nAber wir berechnen lieber neu:\n\ntic()\nrf_fit &lt;-\n  wf_rf %&gt;% \n  tune_grid(\n    resamples = cv_scheme\n    #grid = 2\n    )\ntoc()\n\n34.282 sec elapsed\n\n\n\ncollect_metrics(rf_fit)\n\n# A tibble: 20 × 8\n    mtry min_n .metric .estimator         mean     n      std_err .config       \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;             &lt;dbl&gt; &lt;int&gt;        &lt;dbl&gt; &lt;chr&gt;         \n 1     3    26 rmse    standard   81496992.        5 4420334.     Preprocessor1…\n 2     3    26 rsq     standard          0.647     5       0.0319 Preprocessor1…\n 3     1     8 rmse    standard   81104914.        5 4249148.     Preprocessor1…\n 4     1     8 rsq     standard          0.651     5       0.0270 Preprocessor1…\n 5     3    13 rmse    standard   82253761.        5 4204371.     Preprocessor1…\n 6     3    13 rsq     standard          0.639     5       0.0316 Preprocessor1…\n 7     2    16 rmse    standard   81466291.        5 4103501.     Preprocessor1…\n 8     2    16 rsq     standard          0.646     5       0.0298 Preprocessor1…\n 9     2    36 rmse    standard   81355080.        5 4051776.     Preprocessor1…\n10     2    36 rsq     standard          0.649     5       0.0281 Preprocessor1…\n11     3     5 rmse    standard   84125788.        5 4113181.     Preprocessor1…\n12     3     5 rsq     standard          0.623     5       0.0347 Preprocessor1…\n13     1    32 rmse    standard   82381636.        5 4069505.     Preprocessor1…\n14     1    32 rsq     standard          0.645     5       0.0230 Preprocessor1…\n15     1    33 rmse    standard   82130106.        5 3978566.     Preprocessor1…\n16     1    33 rsq     standard          0.647     5       0.0231 Preprocessor1…\n17     2    20 rmse    standard   81547269.        5 4189669.     Preprocessor1…\n18     2    20 rsq     standard          0.647     5       0.0294 Preprocessor1…\n19     2    23 rmse    standard   81351141.        5 4073682.     Preprocessor1…\n20     2    23 rsq     standard          0.648     5       0.0285 Preprocessor1…\n\n\n\nselect_best(rf_fit)\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     1     8 Preprocessor1_Model02"
  },
  {
    "objectID": "posts/tmdb02/tmdb02.html#finalisieren-1",
    "href": "posts/tmdb02/tmdb02.html#finalisieren-1",
    "title": "tmdb02",
    "section": "Finalisieren",
    "text": "Finalisieren\n\nfinal_wf &lt;-\n  wf_rf %&gt;% \n  finalize_workflow(select_best(rf_fit))\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\n\n\nfinal_fit &lt;-\n  fit(final_wf, data = d_train)\n\n\nfinal_preds &lt;- \n  final_fit %&gt;% \n  predict(new_data = d_test) %&gt;% \n  bind_cols(d_test)\n\n\nsubmission &lt;-\n  final_preds %&gt;% \n  select(id, revenue = .pred)\n\nAbspeichern und einreichen:\n\nwrite_csv(submission, file = \"submission.csv\")"
  },
  {
    "objectID": "posts/tmdb02/tmdb02.html#kaggle-score",
    "href": "posts/tmdb02/tmdb02.html#kaggle-score",
    "title": "tmdb02",
    "section": "Kaggle Score",
    "text": "Kaggle Score\nDiese Submission erzielte einen Score von 2.7664 (RMSLE).\n\nsol &lt;- 2.7664\n\n\nCategories:\n\nds1\ntidymodels\nstatlearning\ntmdb\ntrees\nnum"
  },
  {
    "objectID": "posts/purrr-map05/purrr-map05.html",
    "href": "posts/purrr-map05/purrr-map05.html",
    "title": "purrr-map05",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nExercise\nErstellen Sie eine Tabelle mit mit folgenden Spalten:\n\nID-Spalte: \\(1,2,..., 10\\)\nEine Spalte, in der jede Zelle eine Tabelle mit einem Vektor \\(x\\), einer standardnormalverteilten Zufallszahlen (n=1000), enthält\n\nBerechnen Sie den Mittelwert von jedem \\(x\\)! Diese Ergebnisse sollen als weitere Spalte der Tabelle hinzugefügt werden.\n         \n\n\nSolution\n\nd &lt;- tibble(\n  id = 1:10) %&gt;% \n  mutate(x = map(id, ~ rnorm(n = 1e3))\n) \n\nstr(d)\n\ntibble [10 × 2] (S3: tbl_df/tbl/data.frame)\n $ id: int [1:10] 1 2 3 4 5 6 7 8 9 10\n $ x :List of 10\n  ..$ : num [1:1000] -0.114 0.607 1.288 0.372 -0.438 ...\n  ..$ : num [1:1000] 0.982 -0.376 0.421 -0.59 1.143 ...\n  ..$ : num [1:1000] 0.429 0.417 0.592 -1.735 -1.192 ...\n  ..$ : num [1:1000] 0.514 -0.244 1.358 0.432 1.227 ...\n  ..$ : num [1:1000] 0.47 -0.621 -0.15 -0.959 2.287 ...\n  ..$ : num [1:1000] 0.0743 0.1768 -0.8956 -0.029 1.1695 ...\n  ..$ : num [1:1000] 1.672 -0.976 1.019 -0.327 -0.165 ...\n  ..$ : num [1:1000] -0.0175 2.7151 -0.464 -0.4519 0.4463 ...\n  ..$ : num [1:1000] -0.18 0.729 -0.494 0.127 -0.391 ...\n  ..$ : num [1:1000] -1.058 0.898 1.085 0.142 3.171 ...\n\n\nSo kann man sich die Mittelwerte ausgeben lassen:\n\nd$x %&gt;% \n  map(mean)\n\n[[1]]\n[1] 0.01370096\n\n[[2]]\n[1] 0.005118962\n\n[[3]]\n[1] -0.001769942\n\n[[4]]\n[1] -0.01192552\n\n[[5]]\n[1] 0.01277106\n\n[[6]]\n[1] -0.04090293\n\n[[7]]\n[1] 0.05199444\n\n[[8]]\n[1] 0.01361518\n\n[[9]]\n[1] 0.001292623\n\n[[10]]\n[1] -0.005675222\n\n\nJetzt fügen wir den letzten Schritt als Spalte hinzu:\n\nd2 &lt;-\n  d %&gt;% \n  mutate(x_mean = map_dbl(x, ~ mean(.x))) \n\nhead(d2)\n\n# A tibble: 6 × 3\n     id x               x_mean\n  &lt;int&gt; &lt;list&gt;           &lt;dbl&gt;\n1     1 &lt;dbl [1,000]&gt;  0.0137 \n2     2 &lt;dbl [1,000]&gt;  0.00512\n3     3 &lt;dbl [1,000]&gt; -0.00177\n4     4 &lt;dbl [1,000]&gt; -0.0119 \n5     5 &lt;dbl [1,000]&gt;  0.0128 \n6     6 &lt;dbl [1,000]&gt; -0.0409 \n\n\nHier hätten wir auch schreiben können:\n\nd %&gt;% \n  mutate(x_mean = map(x, mean)) %&gt;% \n  unnest(x_mean) %&gt;% \n  head()\n\n# A tibble: 6 × 3\n     id x               x_mean\n  &lt;int&gt; &lt;list&gt;           &lt;dbl&gt;\n1     1 &lt;dbl [1,000]&gt;  0.0137 \n2     2 &lt;dbl [1,000]&gt;  0.00512\n3     3 &lt;dbl [1,000]&gt; -0.00177\n4     4 &lt;dbl [1,000]&gt; -0.0119 \n5     5 &lt;dbl [1,000]&gt;  0.0128 \n6     6 &lt;dbl [1,000]&gt; -0.0409 \n\n\n\nCategories:\n\nprogramming\nloop"
  },
  {
    "objectID": "posts/Zwielichter-Dozent-Bayes/Zwielichter-Dozent-Bayes.html",
    "href": "posts/Zwielichter-Dozent-Bayes/Zwielichter-Dozent-Bayes.html",
    "title": "Zwielichter-Dozent-Bayes",
    "section": "",
    "text": "options(digits = 2)\n\n\nExercise\nNach einem langen Unitag machen Sie sich auf den Weg nach Hause; ihr Weg führt Sie durch eine dunkle Ecke. Just dort regt sich auf einmal eine Gestalt in den Schatten. Die Person spricht Sie an: „Na, Lust auf ein Spielchen?“. Sie willigen sofort ein. Die Person stellt sich als ein Statistiker vor, dessen Namen nichts zur Sache tue; das Gesicht kommt Ihnen vage bekannt vor. „Pass auf“, erklärt der Statistiker, „wir werfen eine Münze, ich setze auf Zahl“. Dass er auf Zahl setzt, überrascht Sie nicht. „Wenn ich gewinne“, fährt der Statistiker fort, „bekomme ich 10 Euro von Dir, wenn Du gewinnst, bekommst Du 11 Euro von mir. Gutes Spiel, oder?“. Sie einigen sich auf 10 Durchgänge, in denen der Statistiker jedes Mal eine Münze wirft, fängt und dann die oben liegende Seite prüft. Erster Wurf: Zahl! Der Statistiker gewinnt. Pech für Sie. Zweiter Wurf: Zahl! Schon wieder 10 Euro für den Statistiker. Hm. Dritter Wurf: . . . Zahl! Schon wieder. Aber kann ja passieren, bei einer fairen Münze, oder? Vierter Wurf: Zahl! Langsam regen sich Zweifel bei Ihnen. Kann das noch mit rechten Dingen zugehen? Ist die Münze fair? Insgesamt gewinnt der zwielichte Statistiker 8 von 10 Durchgängen.\nUnter leisem Gelächter des Statistikers (und mit leeren Taschen) machen Sie sich von dannen. Hat er falsch gespielt? Wie plausibel ist es, bei 10 Würfen 8 Treffer zu erhalten, wenn die Münze fair ist? Ist das ein häufiges, ein typisches Ereignis oder ein seltenes, untypisches Ereignis bei einer fairen Münze? Wenn es ein einigermaßen häufiges Ereignis sein sollte, dann spricht das für die Fairness der Münze. Zumindest spricht ein Ereignis, welches von einer Hypothese als häufig vorausgesagt wird und schließlich eintritt, nicht gegen eine Hypothese. Zuhause angekommen, denken Sie sich, jetzt müssen Sie erstmal in Ruhe die Posteriori-Verteilung und die PPV ausrechnen!\n\nBerechnen Sie die Posteriori-Verteilung mit der Gittermethode! Gehen Sie von einer gleichverteilten Priori-Wahrscheinlichkeit aus. Visualisieren Sie sie. Alle folgenden Teil-Fragen bauen auf der Post-Verteilung auf.\nWie groß ist die Wahrscheinlichkeit, auf Basis der Post-Verteilung, dass die Münze zugunsten des Dozenten gezinkt ist?\nGeben Sie das 50%-PI und 50%-HDPI zum Parameterwert (\\(p\\) der Münze) an!\nMit welcher Wahrscheinlichkeit liegt die Trefferchance der Münze zwischen \\(p=.45\\) und \\(p=.55\\), ist also nicht “nennenswert” gezinkt?\nWas ist der wahrscheinlichste Parameterwert (Trefferchance der Münze)?\nGeben Sie das 90%-PI und 90%-HDI zu Parameterwert (\\(p\\) der Münze) an!\nBerechnen Sie die PPV! Visualisieren Sie sie. Interpretieren Sie die PPV.\nDiskutieren Sie die Annahme einer Gleichverteilung des Priori-Wertes von \\(p\\)!\n\n         \n\n\nSolution\n\nBerechnen Sie die Posteriori-Verteilung mit der Gittermethode! Visualisieren Sie sie. Alle folgenden Teil-Fragen bauen auf der Post-Verteilung auf.\n\n\np_grid &lt;- seq( from=0 , to=1 , length.out=1000 )  # Gitterwerte\n\nprior &lt;- rep( 1 , 1000 )  # Priori-Gewichte\n\nlikelihood &lt;- dbinom(8, size = 10, prob=p_grid) \n\nunstandardisierte_posterior &lt;- likelihood * prior \n\nposterior &lt;- unstandardisierte_posterior / sum(unstandardisierte_posterior)\n\n# Stichproben ziehen aus der Posteriori-Verteilung:\nsamples &lt;- \n  tibble(\n    gewinnchance_muenze = sample(p_grid , prob=posterior, size=1e4, replace=TRUE))\n\nVisualisierung:\n\nsamples %&gt;% \n  ggplot() +\n  aes(x = gewinnchance_muenze) +\n  geom_histogram() +\n  labs(title = \"Posterior-Verteilung\",\n       x = \"Gewinnchance der Münze (50%: faire Münze)\")\n\n\n\n\n\n\n\n\n\nWie groß ist die Wahrscheinlichkeit, auf Basis der Post-Verteilung, dass die Münze zugunsten des Dozenten gezinkt ist?\n\n\nsamples %&gt;% \n  count(gewinnchance_muenze &gt; .5) %&gt;% \n  mutate(prop = n / sum(n))\n\n# A tibble: 2 × 3\n  `gewinnchance_muenze &gt; 0.5`     n   prop\n  &lt;lgl&gt;                       &lt;int&gt;  &lt;dbl&gt;\n1 FALSE                         318 0.0318\n2 TRUE                         9682 0.968 \n\n\n\nGeben Sie das 50%-PI (Perzentilintervall) und 50%-HDI zum Parameterwert (\\(p\\) der Münze) an!\n\n\nlibrary(easystats)\neti(samples, ci = .5)\n\nEqual-Tailed Interval\n\nParameter           |      50% ETI\n----------------------------------\ngewinnchance_muenze | [0.67, 0.84]\n\nhdi(samples, ci = .5)\n\nHighest Density Interval\n\nParameter           |      50% HDI\n----------------------------------\ngewinnchance_muenze | [0.71, 0.87]\n\n\nEin PI wird auch equal tail interval genannt, weil die beiden “abgeschnitten Randbereiche” links und rechts die gleichen Flächenanteil (Wahrscheinlichkeitsmasse) aufweisen.\nInteresant ist, dass das PI und das HDI zu unterschiedlichen Ergebnissen kommen. Das lässt auf eine schiefe Verteilung schließen. Außerdem eröffnet es den Raum zur Diskussion, welches Intervall man berichtet. Um diese Frage besser zu verstehen, können wir die Intervalle visualisieren.\nBonus: Visualisieren wir die Intervalle:\nPI:\n\neti(samples, ci = .5) %&gt;% plot()\n\n\n\n\n\n\n\n\nHDI:\n\nhdi(samples, ci = .5) %&gt;% plot()\n\n\n\n\n\n\n\n\nDas HDI ist schmäler und liegt näher am Modus. Vermutlich ist das HDI zu bevorzugen.\n\nMit welcher Wahrscheinlichkeit liegt die Trefferchance der Münze zwischen \\(p=.45\\) und \\(p=.55\\), ist also nicht “nennenswert” gezinkt (auf Basis unserer Modellannahmen)?\n\n\nsamples %&gt;% \n  count(gewinnchance_muenze &gt;= 0.45 & gewinnchance_muenze &lt;= .55) %&gt;% \n  mutate(prop = n/sum(n))\n\n# A tibble: 2 × 3\n  `gewinnchance_muenze &gt;= 0.45 & gewinnchance_muenze &lt;= 0.55`     n   prop\n  &lt;lgl&gt;                                                       &lt;int&gt;  &lt;dbl&gt;\n1 FALSE                                                        9531 0.953 \n2 TRUE                                                          469 0.0469\n\n\nDie Wahrscheinlichkeit, dass die Münze nicht nennenswert gezinkt ist (nach unserer Definition), ist gering. Man sollte vielleicht erwähnen, dass unsere Definition von “nicht nennenswert gezinkt” plausibel ist, und andere (vernünftige) Definitionen zu einem sehr ähnlichen Ergebnis kämen.\n\nWas ist der wahrscheinlichste Parameterwert (Trefferchance der Münze)?\n\n\nsamples %&gt;% \n   map_estimate()\n\nMAP Estimate\n\nParameter           | MAP_Estimate\n----------------------------------\ngewinnchance_muenze |         0.82\n\n\nmap_estimate steht für …\n\nFind the Highest Maximum A Posteriori probability estimate (MAP) of a posterior, i.e., the value associated with the highest probability density (the “peak” of the posterior distribution). In other words, it is an estimation of the mode for continuous parameters.\n\n(aus der Hilfeseite der Funktion)\n\nGeben Sie das 90%-PI und 90%-HDI zum Parameterwert (\\(p\\) der Münze) an!\n\n\nlibrary(easystats)\neti(samples, ci = .9)\n\nEqual-Tailed Interval\n\nParameter           |      90% ETI\n----------------------------------\ngewinnchance_muenze | [0.53, 0.92]\n\nhdi(samples, ci = .9)\n\nHighest Density Interval\n\nParameter           |      90% HDI\n----------------------------------\ngewinnchance_muenze | [0.57, 0.94]\n\n\n\nBerechnen Sie die PPV! Visualisieren Sie sie. Interpretieren Sie die PPV.\n\n\nPPV &lt;-\n  samples %&gt;% \n  mutate(anzahl_kopf = rbinom(n = 1e4, size = 10, prob = gewinnchance_muenze))\n\nVisualisierung:\n\nPPV %&gt;% \n  ggplot() +\n  aes(x = anzahl_kopf) +\n  labs(title = \"PPV\") +\n  geom_bar()  # geom_bar() ginge auch, sieht aber bei wenig Balken nicht so gut aus.\n\n\n\n\n\n\n\n\nLaut der PPV sind 8 von 10 Treffern der Wert, der mit der höchsten Wahrscheinlichkeit zu beobachten sein wird. Allerdings sind 7 oder 9 Treffer fast genauso wahrscheinlich. Etwas genauer:\n\nPPV %&gt;% \n  count(between(anzahl_kopf, 7,9))   # \"zähle mir, wie oft ein Wert ZWISCHEN (between) 7 und 9 vorkommt\"\n\n# A tibble: 2 × 2\n  `between(anzahl_kopf, 7, 9)`     n\n  &lt;lgl&gt;                        &lt;int&gt;\n1 FALSE                         3840\n2 TRUE                          6160\n\n\nMit dieser Wahrscheinlichkeit ist ein Wert zwischen 7 und 9 zu beobachten, wenn man den Versuch wiederholt, laut dem Modell.\n\nPPV %&gt;% \n  eti(anzahl_kopf, ci = .9)\n\nEqual-Tailed Interval\n\nParameter           |       90% ETI\n-----------------------------------\ngewinnchance_muenze | [0.53,  0.92]\nanzahl_kopf         | [4.00, 10.00]\n\n\nUnser Modell sieht einen “Passungsbereich” (ein Perzentilintervall) von 4 bis 10 Treffern als mit 90% Wahrscheinlichkeit passend an.\n\nDiskutieren Sie die Annahme einer Gleichverteilung des Priori-Wertes von \\(p\\)!\n\nZwar hat eine Gleichverteilung der Priori-Werte den Vorteil, dass sie “objektiv” ist in dem Sinne, dass kein Wert “bevorteilt” wird; alle gelten als gleich wahrscheinlich. Aber das ist hochgradig unplausibel: So ist z.B. der Wert \\(p=1\\) logisch unmöglich, da wir nicht nur Treffer beobachtet haben. Ein Wert von z.B. \\(p=0.999\\) erscheint uns ebenfalls sehr unwahrscheinlich. Nützlicher erscheint daher vielleicht doch eine Priori-Verteilung, die extreme Werte von \\(p\\) als unwahrscheinlich bemisst.\n\nCategories:\n\nbayes\nprobability\nppv"
  },
  {
    "objectID": "posts/griech-buchstaben/griech-buchstaben.html",
    "href": "posts/griech-buchstaben/griech-buchstaben.html",
    "title": "Griech-Buchstaben-Inferenz",
    "section": "",
    "text": "Exercise\nFür Statistiken (Stichprobe) verwendet man meist lateinische Buchstaben; für Parameter (Population) verwendet man meist (die entsprechenden) griechischen Buchstaben.\nVervollständigen Sie folgende Tabelle entsprechend!\n\n\n\n\n\nKennwert\nStatistik\nParameter\n\n\n\n\nMittelwert\n\\(\\bar{X}\\)\nNA\n\n\nMittelwertsdifferenz\n\\(\\bar{X}_1-\\bar{X}_2\\)\nNA\n\n\nStreuung\nsd\nNA\n\n\nAnteil\np\nNA\n\n\nKorrelation\nr\nNA\n\n\nRegressionsgewicht\nb\nNA\n\n\n\n\n\n         \n\n\nSolution\n\n\n\n\n\nKennwert\nStatistik\nParameter\n\n\n\n\nMittelwert\n\\[\\bar{X}\\]\n\\[\\mu\\]\n\n\nMittelwertsdifferenz\n\\[d=\\bar{X}_1-\\bar{X}_2\\]\n\\[\\mu_1\\]- \\[\\mu_2\\]\n\n\nStreuung\nsd\n\\[\\sigma\\]\n\n\nAnteil\np\n\\[\\pi\\]\n\n\nKorrelation\nr\n\\[\\rho\\]\n\n\nRegressionsgewicht\nb\n\\[\\beta\\]\n\n\n\n\n\n\nCategories:\n\nqm2\ninference"
  },
  {
    "objectID": "posts/wozu-streudiagramm/wozu-streudiagramm.html",
    "href": "posts/wozu-streudiagramm/wozu-streudiagramm.html",
    "title": "wozu-streudiagramm",
    "section": "",
    "text": "Zu welchem Zweck ist ein Streudiagramm am besten geeignet?\n\n\n\nUm Verteilungen einer nominalen Variablen darzustellen.\nUm Verteilungen einer metrischen Variablen darzustellen.\nUm Verteilungen einer stetigen, metrischen Variablen darzustellen.\nUm Zusammenhänge zwischen zwei nominalen Variablen darzustellen.\nUm Zusammenhänge zwischen zwei metrischen Variablen darzustellen."
  },
  {
    "objectID": "posts/wozu-streudiagramm/wozu-streudiagramm.html#answerlist",
    "href": "posts/wozu-streudiagramm/wozu-streudiagramm.html#answerlist",
    "title": "wozu-streudiagramm",
    "section": "",
    "text": "Um Verteilungen einer nominalen Variablen darzustellen.\nUm Verteilungen einer metrischen Variablen darzustellen.\nUm Verteilungen einer stetigen, metrischen Variablen darzustellen.\nUm Zusammenhänge zwischen zwei nominalen Variablen darzustellen.\nUm Zusammenhänge zwischen zwei metrischen Variablen darzustellen."
  },
  {
    "objectID": "posts/wozu-streudiagramm/wozu-streudiagramm.html#answerlist-1",
    "href": "posts/wozu-streudiagramm/wozu-streudiagramm.html#answerlist-1",
    "title": "wozu-streudiagramm",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nFalsch\nWahr\n\n\nCategories:\n\nvis\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/haeufigkeit01/haeufigkeit01.html",
    "href": "posts/haeufigkeit01/haeufigkeit01.html",
    "title": "haeufigkeit01",
    "section": "",
    "text": "Aufgabe\nWerten Sie die Häufigkeiten (der Stufen) folgender Variablen aus wie unten beschrieben.\nDatensatz: mtcars.\nVariablen:\n\nam\ncyl\nvs\n\n\nErstellen Sie für jede der genannten Variablen eine univariate Häufigkeitsanalyse (also nur eine Variable).\nErstellen Sie dann für die ersten beiden genannten Variablen eine gemeinsame Häufigkeitsanalyse (bivariat).\nErstellen Sie dann für alle genannten Variablen eine gemeinsame Häufigkeitsanalyse.\nWie viele Gruppen (also Häufigkeitswerte) ergeben sich (theoretisch) in der letzten Auswertung?\n\n         \n\n\nLösung\n\nlibrary(\"tidyverse\")\ndata(\"mtcars\")\n\n\nunivariate Häufigkeitsanalyse\n\n\nmtcars %&gt;% \n  group_by(am) %&gt;% \n  summarise(zeilen_n = n())\n\n# A tibble: 2 × 2\n     am zeilen_n\n  &lt;dbl&gt;    &lt;int&gt;\n1     0       19\n2     1       13\n\n\nDer Befehle n() gibt die Anzahl der Zeilen (Reihen) zurück. Da in einem Dataframe alle Zeilen gleich lang sind, brauchen wir keine Spalte anzugeben.\nAlternativ könnte man auch schreiben:\n\nmtcars %&gt;% \n  count(am)\n\n  am  n\n1  0 19\n2  1 13\n\n\nDas ist haargenau der gleiche Effekt wie die vorherige Syntax.\nÜblich ist auch, eine Kontingenztabelle so darzustellen:\n\ngemeinsame Häufigkeitsanalyse (bivariat)\n\n\nmtcars %&gt;% \n  count(am, cyl)\n\n  am cyl  n\n1  0   4  3\n2  0   6  4\n3  0   8 12\n4  1   4  8\n5  1   6  3\n6  1   8  2\n\n\n\ntable(mtcars$am, mtcars$cyl)\n\n   \n     4  6  8\n  0  3  4 12\n  1  8  3  2\n\n\nWir sehen, dass wir \\(2\\cdot3=6\\) Gruppen haben, in denen sich die \\(n=32\\) Beobachtungen aufteilen.\n\nHäufigkeitsanalyse mit 3 Variablen\n\n\nmtcars %&gt;% \n  count(am, cyl, vs)\n\n  am cyl vs  n\n1  0   4  1  3\n2  0   6  1  4\n3  0   8  0 12\n4  1   4  0  1\n5  1   4  1  7\n6  1   6  0  3\n7  1   8  0  2\n\n\nDas sind drei Variablen mit \\(2 \\cdot 3 \\cdot 2 = 12\\) Gruppen.\nDa einige der 12 Gruppen in den Daten nicht vorkommen, sind sie in der Auszählung der Häufigkeiten nicht aufgenommen; in den Daten gibt es nur 7 der 12 Gruppen.\n\nCategories:\n\ndatawrangling\neda\ncount\nstring"
  },
  {
    "objectID": "posts/Boxplot-Aussagen/Boxplot-Aussagen.html",
    "href": "posts/Boxplot-Aussagen/Boxplot-Aussagen.html",
    "title": "Boxplot-Aussagen",
    "section": "",
    "text": "Hinweis: Female steht für Frauen; Male für Männer.\nDiese Aufgabe bezieht sich auf den Datensatz tips (aus dem R-Paket reshape2), den Sie ggf. hier herunterladen können. Ein Data-Dictionary findet sich hier.\n\n\n\n\n\n\n\n\n\n\n\n\nDer IQR der Männer ist größer als der der Frauen.\nDer Boxplot ist schlecht geeignet, um die Verteilung mehrerer Gruppen prägnant und übersichtlich zu visualisieren.\nDer Mittelwert der Männer ist kleiner als der der Frauen.\nDie Streuung in den RANDbereichen der Frauen ist größer als die der Männer.\nBei den Männern gibt es mehr Ausreißer als bei den Frauen.\nDie Streuung in den beiden äußeren Quartilen ist bei den Frauen größer als bei den Männern.\nDie Verteilungen sind beide nicht schief.\nDie Verteilungen sind beide symmetrisch.\nAuf der X-Achse steht eine metrische (quantitative) Variable.\nAuf der Y-Achse steht eine nominale (qualitative) Variable.\nAuf der Y-Achse steht eine metrische (quantitative) Variable.\nAuf der X-Achse steht eine nominale (qualitative) Variable."
  },
  {
    "objectID": "posts/Boxplot-Aussagen/Boxplot-Aussagen.html#answerlist",
    "href": "posts/Boxplot-Aussagen/Boxplot-Aussagen.html#answerlist",
    "title": "Boxplot-Aussagen",
    "section": "",
    "text": "Der IQR der Männer ist größer als der der Frauen.\nDer Boxplot ist schlecht geeignet, um die Verteilung mehrerer Gruppen prägnant und übersichtlich zu visualisieren.\nDer Mittelwert der Männer ist kleiner als der der Frauen.\nDie Streuung in den RANDbereichen der Frauen ist größer als die der Männer.\nBei den Männern gibt es mehr Ausreißer als bei den Frauen.\nDie Streuung in den beiden äußeren Quartilen ist bei den Frauen größer als bei den Männern.\nDie Verteilungen sind beide nicht schief.\nDie Verteilungen sind beide symmetrisch.\nAuf der X-Achse steht eine metrische (quantitative) Variable.\nAuf der Y-Achse steht eine nominale (qualitative) Variable.\nAuf der Y-Achse steht eine metrische (quantitative) Variable.\nAuf der X-Achse steht eine nominale (qualitative) Variable."
  },
  {
    "objectID": "posts/Boxplot-Aussagen/Boxplot-Aussagen.html#answerlist-1",
    "href": "posts/Boxplot-Aussagen/Boxplot-Aussagen.html#answerlist-1",
    "title": "Boxplot-Aussagen",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\nFalsch\nFalsch\nFalsch\nFalsch\nFalsch\nWahr\nWahr\n\n\nCategories:\n\nvis\neda\ndyn\nschoice"
  },
  {
    "objectID": "posts/kausal22/kausal22.html",
    "href": "posts/kausal22/kausal22.html",
    "title": "kausal22",
    "section": "",
    "text": "Gegeben sei der DAG g (s. u.). Der DAG verfügt über mehrere Variablen, die als Knoten im Graph dargestellt sind und mit \\(x_1, x_2, \\ldots x_n\\) bezeichnet sind.\nWelche minimale Variablenmenge muss kontrolliert werden, um den kausalen Effekt von der UV zur AV zu identifizieren?\nUV: x3.\nAV: x5.\nHinweise:\n\nMengen sind mittels geschweifter Klammern gekennzeichnet, z.B. {x8, x9} meint die Menge mit den zwei Elementen x8 und x9.\nDie leere Menge { } bedeutet, dass keine Variable kontrolliert werden muss, um den kausalen Effekt zu identifizieren.\nAlle Variablen werden als gemessen vorausgesetzt.\nEs ist möglich, dass es keine Lösung gibt, dass es also keine Adjustierungsmenge gibt, um den kausalen Effekt zu identifizieren. Wenn dies der Fall sein sollte, wählen Sie “keine Lösung”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n{ x2, x7 }\n{ x1, x5 }\n{ x2 }\n{ x4, x7 }\n{ x3, x5 }"
  },
  {
    "objectID": "posts/kausal22/kausal22.html#answerlist",
    "href": "posts/kausal22/kausal22.html#answerlist",
    "title": "kausal22",
    "section": "",
    "text": "{ x2, x7 }\n{ x1, x5 }\n{ x2 }\n{ x4, x7 }\n{ x3, x5 }"
  },
  {
    "objectID": "posts/kausal22/kausal22.html#answerlist-1",
    "href": "posts/kausal22/kausal22.html#answerlist-1",
    "title": "kausal22",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nRichtig\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/tidymodels-penguins01/tidymodels-penguins01.html",
    "href": "posts/tidymodels-penguins01/tidymodels-penguins01.html",
    "title": "tidymodels-penguins01",
    "section": "",
    "text": "Aufgabe\nBerechnen Sie ein lineares Modell mit tidymodels und zwar anhand des penguins Datensatzes.\nModellgleichung: body_mass_g ~ bill_length_mm, data = d_train.\nGesucht ist R-Quadrat als Maß für die Modellgüte im TEST-Sample.\nHinweise:\n\nFixieren Sie die Zufallszahlen auf den Startwert 42.\nNutzen Sie eine v=5,r=1 CV.\nEntfernen Sie fehlende Werte in den Variablen.\nVerzichten Sie auf weitere Schritte der Vorverarbeitung.\n\n         \n\n\nLösung\nSetup:\n\nlibrary(tidymodels)\nlibrary(tictoc)  # Rechenzeit messen, optional\n# data(penguins, package = \"palmerpenguins\")\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/modeldata/penguins.csv\"\nd &lt;- read.csv(d_path)\n\nDatensatz aufteilen:\n\nset.seed(42)\nd_split &lt;- initial_split(penguins)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\nWorkflow:\n\nrec1 &lt;-\n  recipe(body_mass_g ~ bill_length_mm, data = d_train) %&gt;% \n  step_naomit(all_numeric())\n\nlm_mod &lt;-\n  linear_reg()\n\nwflow &lt;-\n  workflow() %&gt;%\n  add_recipe(rec1) %&gt;%\n  add_model(lm_mod)\n\nwflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_naomit()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nBacken:\n\nd_baked &lt;- prep(rec1) %&gt;% bake(new_data = NULL)\nd_baked %&gt;% head()\n\n# A tibble: 6 × 2\n  bill_length_mm body_mass_g\n           &lt;dbl&gt;       &lt;int&gt;\n1           36          3450\n2           50.9        3675\n3           46.1        4500\n4           45.8        4150\n5           48.6        5800\n6           39          3650\n\n\nAuf NA prüfen:\n\nsum(is.na(d_baked))\n\n[1] 0\n\n\nCV:\n\nset.seed(42)\nfolds &lt;- vfold_cv(d_train, v = 5)\nfolds\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits           id   \n  &lt;list&gt;           &lt;chr&gt;\n1 &lt;split [206/52]&gt; Fold1\n2 &lt;split [206/52]&gt; Fold2\n3 &lt;split [206/52]&gt; Fold3\n4 &lt;split [207/51]&gt; Fold4\n5 &lt;split [207/51]&gt; Fold5\n\n\nResampling:\n\npenguins_resamples &lt;-\n  fit_resamples(\n    wflow,\n    resamples = folds\n  )\npenguins_resamples\n\n# Resampling results\n# 5-fold cross-validation \n# A tibble: 5 × 4\n  splits           id    .metrics         .notes          \n  &lt;list&gt;           &lt;chr&gt; &lt;list&gt;           &lt;list&gt;          \n1 &lt;split [206/52]&gt; Fold1 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n2 &lt;split [206/52]&gt; Fold2 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n3 &lt;split [206/52]&gt; Fold3 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n4 &lt;split [207/51]&gt; Fold4 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n5 &lt;split [207/51]&gt; Fold5 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n\n\nLast Fit:\n\npenguins_last &lt;- last_fit(wflow, d_split)\n\nModellgüte im Test-Sample:\n\npenguins_last %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard     652.    Preprocessor1_Model1\n2 rsq     standard       0.385 Preprocessor1_Model1\n\n\nR-Quadrat:\n\nsol &lt;-  collect_metrics(penguins_last)[[\".estimate\"]][2]\nsol\n\n[1] 0.3850608\n\n\n\nCategories:\n\nds1\ntidymodels\nprediction\nyacsda\nstatlearning\nnum"
  },
  {
    "objectID": "posts/zwert-berechnen/zwert-berechnen.html",
    "href": "posts/zwert-berechnen/zwert-berechnen.html",
    "title": "zwert-berechnen",
    "section": "",
    "text": "Exercise\nSei \\(X \\sim \\mathcal{N}(42, 7)\\) und \\(x_1 = 28\\).\nBerechnen Sie den z-Wert für \\(x_1\\)!\nHinweis:\n\nRunden Sie ggf. auf die nächste ganze Zahl.\n\n         \n\n\nSolution\n\nx1_z = (x1 - x_mw) / x_sd\n\n-2\n\nCategories:\n\nz-value\nR\nmath"
  },
  {
    "objectID": "posts/tidymodels-penguins06/tidymodels-penguins06.html",
    "href": "posts/tidymodels-penguins06/tidymodels-penguins06.html",
    "title": "tidymodels-penguins06",
    "section": "",
    "text": "Berechnen Sie ein kNN-Modell mit tidymodels und zwar anhand des penguins Datensatzes.\nModellgleichung: body_mass_g ~ bill_length_mm.\nVergleichen Sie die Testfehlerhöhe im Test-Sample in folgenden zwei Szenarien:\n\nTrain-Test-Aufspaltung, 10 Mal wiederholt\n10-fache Kreuzvalidierung (im Train-Sample) (\\(v=10, r= 1\\))\n\nHinweise:\n\nTuning Sie - nur im 2. Szenario - \\(k\\) mit den Werten 5, 10, 15.\nLöschen Sie alle Zeilen mit fehlenden Werten in den Prädiktoren.\nBeachten Sie die üblichen Hinweise.\nNatürlich gilt: Ceteris paribus. Halten Sie also die Modelle im Übrigen vergleichbar bzw. identisch.\n\n\n\n\nSzenario 1 hat den geringeren Vorhersagefehler.\nSzenario 2 hat den geringeren Vorhersagefehler.\nDer Vorhersagefehler ist in beiden Szenarien gleich.\nKeine Antwort möglich."
  },
  {
    "objectID": "posts/tidymodels-penguins06/tidymodels-penguins06.html#answerlist",
    "href": "posts/tidymodels-penguins06/tidymodels-penguins06.html#answerlist",
    "title": "tidymodels-penguins06",
    "section": "",
    "text": "Szenario 1 hat den geringeren Vorhersagefehler.\nSzenario 2 hat den geringeren Vorhersagefehler.\nDer Vorhersagefehler ist in beiden Szenarien gleich.\nKeine Antwort möglich."
  },
  {
    "objectID": "posts/tidymodels-penguins06/tidymodels-penguins06.html#setup",
    "href": "posts/tidymodels-penguins06/tidymodels-penguins06.html#setup",
    "title": "tidymodels-penguins06",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(tictoc)  # Rechenzeit messen, optional\n# data(penguins, package = \"palmerpenguins\")\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/modeldata/penguins.csv\"\nd &lt;- read_csv(d_path)\n\nWir dürfen keine fehlenden Werte in der Y-Variable haben (im Train-Set), sonst meckert Tidymodels:\n\nd2 &lt;- \n  d %&gt;% \n  drop_na(body_mass_g)"
  },
  {
    "objectID": "posts/tidymodels-penguins06/tidymodels-penguins06.html#daten-aufteilen",
    "href": "posts/tidymodels-penguins06/tidymodels-penguins06.html#daten-aufteilen",
    "title": "tidymodels-penguins06",
    "section": "Daten aufteilen:",
    "text": "Daten aufteilen:\n\nset.seed(42)\nd_split &lt;- initial_split(d2)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)"
  },
  {
    "objectID": "posts/tidymodels-penguins06/tidymodels-penguins06.html#cv-1",
    "href": "posts/tidymodels-penguins06/tidymodels-penguins06.html#cv-1",
    "title": "tidymodels-penguins06",
    "section": "CV",
    "text": "CV\n\nset.seed(42)\nfolds &lt;- vfold_cv(d_train, v = 10, repeats = 1)"
  },
  {
    "objectID": "posts/tidymodels-penguins06/tidymodels-penguins06.html#workflow",
    "href": "posts/tidymodels-penguins06/tidymodels-penguins06.html#workflow",
    "title": "tidymodels-penguins06",
    "section": "Workflow",
    "text": "Workflow\n\nrec1 &lt;-\n  recipe(body_mass_g ~ bill_length_mm, data = d_train) %&gt;% \n  step_naomit(all_numeric_predictors())\n\nknn_model &lt;-\n  nearest_neighbor(\n    mode = \"regression\",\n    neighbors = tune()\n  ) \n\nwflow &lt;-\n  workflow() %&gt;%\n  add_recipe(rec1) %&gt;%\n  add_model(knn_model)"
  },
  {
    "objectID": "posts/tidymodels-penguins06/tidymodels-penguins06.html#fitten",
    "href": "posts/tidymodels-penguins06/tidymodels-penguins06.html#fitten",
    "title": "tidymodels-penguins06",
    "section": "Fitten",
    "text": "Fitten\n\nd_resamples &lt;-\n  tune_grid(\n    wflow,\n    resamples = folds,\n    control = control_grid(save_workflow = TRUE),\n    grid = data.frame(neighbors = c(5, 10, 15)),\n    metrics = metric_set(rmse)\n    )"
  },
  {
    "objectID": "posts/tidymodels-penguins06/tidymodels-penguins06.html#modellgüte",
    "href": "posts/tidymodels-penguins06/tidymodels-penguins06.html#modellgüte",
    "title": "tidymodels-penguins06",
    "section": "Modellgüte",
    "text": "Modellgüte\n\nbestfit1 &lt;- fit_best(x = d_resamples)\nlastfit1 &lt;- last_fit(bestfit1, d_split)\ncollect_metrics(lastfit1)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard     586.    Preprocessor1_Model1\n2 rsq     standard       0.410 Preprocessor1_Model1"
  },
  {
    "objectID": "posts/tidymodels-penguins06/tidymodels-penguins06.html#cv-2",
    "href": "posts/tidymodels-penguins06/tidymodels-penguins06.html#cv-2",
    "title": "tidymodels-penguins06",
    "section": "CV",
    "text": "CV\nWir resamplen nicht über das Train-Sample, sondern über die ganze Stichprobe:\n\nset.seed(42)\nfolds2 &lt;- vfold_cv(d2, v = 2, repeats = 10)"
  },
  {
    "objectID": "posts/tidymodels-penguins06/tidymodels-penguins06.html#fitten-1",
    "href": "posts/tidymodels-penguins06/tidymodels-penguins06.html#fitten-1",
    "title": "tidymodels-penguins06",
    "section": "Fitten",
    "text": "Fitten\n\nd_resamples2 &lt;-\n  tune_grid(\n    wflow,\n    resamples = folds2,\n    control = control_grid(save_workflow = TRUE),\n    grid = data.frame(neighbors = c(5, 10, 15)),\n    metrics = metric_set(rmse)\n    )"
  },
  {
    "objectID": "posts/tidymodels-penguins06/tidymodels-penguins06.html#modellgüte-1",
    "href": "posts/tidymodels-penguins06/tidymodels-penguins06.html#modellgüte-1",
    "title": "tidymodels-penguins06",
    "section": "Modellgüte",
    "text": "Modellgüte\n\nbestfit2 &lt;- fit_best(x = d_resamples2)\nlastfit2 &lt;- last_fit(bestfit2, d_split)\ncollect_metrics(lastfit2)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard     586.    Preprocessor1_Model1\n2 rsq     standard       0.410 Preprocessor1_Model1\n\n\n\nCategories:\n\ntidymodels\nstatlearning\nschoice"
  },
  {
    "objectID": "posts/Urne1/Urne1.html",
    "href": "posts/Urne1/Urne1.html",
    "title": "Urne1",
    "section": "",
    "text": "Aufgabe\nIn einer Urne befinden sich fünf Kugeln, von denen 4 rot sind und 1 weiß.\nAufgabe: Wie groß ist die Wahrscheinlichkeit, dass bei 2 Ziehungen ohne Zurücklegen (ZoZ) genau 2 rote Kugeln gezogen werden?\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n         \n\n\nLösung\nSei \\(R1\\) “rote Kugel im 1. Zug gezogen”.\nSei \\(R2\\) “rote Kugel im 2. Zug gezogen”.\nGesucht ist die gemeinsame Wahrscheinlichkeit für R1 und R2: \\(Pr(R1 \\cap R2)\\), die Wahrscheinlichkeit also, dass beide Ereignisse (R1 und R2) eintreten.\nFür R1 gilt: \\(Pr(R1) = 4/5\\).\nFür R2 gilt: \\(Pr(R2|R1) = 3/4\\).\nMan beachte, dass R1 und R2 nicht unabhängig sind, d.h. sie sind abhängig (voneinander).\n\nPr_R1 &lt;- 4/5\nPr_R2_geg_R1 &lt;- 3/4\nPr_R1_R2 &lt;- Pr_R1 * Pr_R2_geg_R1\nPr_R1_R2\n\n[1] 0.6\n\n\nDie Lösung lautet 0.6.\n\nCategories:\n\nR\nprobability\nnum"
  },
  {
    "objectID": "posts/kausal_corona_glatze/kausal_corona_glatze.html",
    "href": "posts/kausal_corona_glatze/kausal_corona_glatze.html",
    "title": "kausal_corona_glatze",
    "section": "",
    "text": "Exercise\nVor einiger Zeit war in der Presse Folgendes zu lesen:\n\nRecent studies linking male sex hormones to severe coronavirus infections point to a potential predictor of disease severity: baldness. International researchers studying global data on COVID-19 patients have found that in general, the more male hormones called androgens someone has, the easier it is for SARS-CoV-2 to enter and take over their immune systems. And bald men have more of these hormones than men with full manes, and women.\n\nQuelle\nAllerdings versäumten es einige (viele) der Pressemeldungen, eine belastbare Quelle, also den Forschungsartikel, auf dem dieses Befund beruhen muss, zu zitieren.\nEine mögliche Kausalhypothese für die obige Pressemitteilung ist, dass männliche Sexualhormone eine gemeinsame Ursache für Haarausfall und auch die Schwere eine Covid19-Infektion darstellen.\nAllerdings sind auch andere, skeptischere, Hypothesen denkbar. Skeptisch meint dabei, dass auf komplexere Erklärungen zugunsten einfachere verzichtet werden kann.\nSo benötigt etwa die Hypothese “Störche bringen Babies” komplexe Erkärungsmodelle; eine skeptischere (einfachere) Erklärung wäre, dass die (geringe) Urbanisierung eines Landstrichs die gemeinsame Ursache für sowohl die Häufigkeit von Störchen als auch von Babies darstellt.\nGeben Sie eine skeptische Kausalerkärung an für den Befund, dass Haarausfall und Schwere eines Coronaverlaufs assoziiert sind!.\n         \n\n\nSolution\nAlter ist sowohl die Ursache von Haarausfall (bei Männern) als auch von der Schwere eines Corona-Verlaufs. Daher sind Haarausfall (bei Männern) und die Schwerer eines Corona-Verlaufs durch das Alter konfundiert. Ohne Berücksichtigung der gemeinsamen Ursache erscheint Haarausfall mit Corona-Schwere korreliert zu sein. Nach Kontrolle der konfundierenden Variablen verschwindet die Korrelation.\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/tidytext/tidytext.html",
    "href": "posts/tidytext/tidytext.html",
    "title": "tidytext",
    "section": "",
    "text": "library(tidytext)\nlibrary(tidyverse)\ntext_df %&gt;%\n  unnest_tokens(word, text) %&gt;% \n  filter(str_detect(word, \"[a-z]\"))\n\nWelche Aussage zu dieser Syntax ist korrekt?\n\n\n\nDer Text wird so “entschachtelt”, dass in jeder Zelle nur noch ein Wort steht. Dabei werden so viele Spalten angehängt, wie Wörter in der betreffenden Zelle standen.\nDurch filter() in Verbindung mit str_detect() werden alle Buchstaben von a bis z entfernt.\nEin Token bedeutet hier so viel wie eine numerische Analyseeinheit.\nDer Text wird in das lange Format umwandelt, so dass nur noch ein Wort pro Zeile steht."
  },
  {
    "objectID": "posts/tidytext/tidytext.html#answerlist",
    "href": "posts/tidytext/tidytext.html#answerlist",
    "title": "tidytext",
    "section": "",
    "text": "Der Text wird so “entschachtelt”, dass in jeder Zelle nur noch ein Wort steht. Dabei werden so viele Spalten angehängt, wie Wörter in der betreffenden Zelle standen.\nDurch filter() in Verbindung mit str_detect() werden alle Buchstaben von a bis z entfernt.\nEin Token bedeutet hier so viel wie eine numerische Analyseeinheit.\nDer Text wird in das lange Format umwandelt, so dass nur noch ein Wort pro Zeile steht."
  },
  {
    "objectID": "posts/ReThink3m4/ReThink3m4.html",
    "href": "posts/ReThink3m4/ReThink3m4.html",
    "title": "ReThink3m4",
    "section": "",
    "text": "Aufgabe\nNehmen wir an, wir haben 8 (Wasser-)“Treffer” (\\(W=8\\)) bei 15 Würfen (\\(N=15\\)) erhalten (wieder im Globusversuch).\nBerechnen Sie auf Basis dieser Posteriori-Verteilung (8 Treffer bei 15 Würfen) die Wahrscheinlichkeit für 6 Wasser bei 9 Würfen (\\(W=6, N=9\\)).\nHinweise:\n\nBerechnen Sie eine Bayes-Box (Gittermethode).\nVerwenden Sie 1000 Gitterwerte.\nFixieren Sie die Zufallszahlen mit dem Startwert 42, d.h. set.seed(42).\nGehen Sie von einem gleichverteilten Prior aus.\n\nQuelle: McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Aufl.). Taylor and Francis, CRC Press.\n         \n\n\nLösung\nErstellen wir zuerst wieder die Posteriori-Verteilung für den Globusversuch.\n\np_grid &lt;- seq( from=0 , to=1 , length.out=1000 )  # Gitterwerte\n\nprior &lt;- rep( 1 , 1000 )  # Priori-Gewichte\n\nset.seed(42)\nlikelihood &lt;- dbinom( 6 , size=9 , prob=p_grid ) \n\nunstandardisierte_posterior &lt;- likelihood * prior \n\nposterior &lt;- unstandardisierte_posterior / sum(unstandardisierte_posterior)\n\nDann ziehen wir unsere Stichproben daraus:\n\n# um die Zufallszahlen festzulegen, damit alle die gleichen Zufallswerte bekommen: \nset.seed(42) \n\n# Stichproben ziehen aus der Posteriori-Verteilung\nsamples &lt;- \n  tibble(\n    p = sample( p_grid , prob=posterior, size=1e4, replace=TRUE)) \n\nJetzt erstellen wir die PPV für einen anderen Versuch, nämlich mit 9 Zügen:\n\nPPV &lt;-\n  samples %&gt;% \n  mutate(anzahl_wasser2 = rbinom(1e4, size = 9, prob = p))\n\nSchließlich zählen wir, wie oft 6 Treffer beobachtet werden:\n\nPPV %&gt;% \n  count(anzahl_wasser2 == 6) \n\n# A tibble: 2 × 2\n  `anzahl_wasser2 == 6`     n\n  &lt;lgl&gt;                 &lt;int&gt;\n1 FALSE                  7972\n2 TRUE                   2028\n\n\nQuelle\n\nCategories:\n\nbayes\nppv\nprobability\nstring"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-rf-plain/germeval-sent-wordvec-rf-plain.html",
    "href": "posts/germeval-sent-wordvec-rf-plain/germeval-sent-wordvec-rf-plain.html",
    "title": "germeval03-sent-wordvec-rf-plain",
    "section": "",
    "text": "Erstellen Sie ein prädiktives Modell für Textdaten. Nutzen Sie Sentiments und TextFeatures im Rahmen von Feature-Engineering. Nutzen Sie außerdem deutsche Word-Vektoren für das Feature-Engineering.\nAls Lernalgorithmus verwenden Sie XGB.\nPreppen und Backen Sie das Rezept, aber führen Sie die Pipelien mit dem gebackenen Datensatz und einem “Plain-Rezept” durch.\n\n\nVerwenden Sie die GermEval-2018-Daten.\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\nDie Daten sind auch über das R-Paket PradaData zu beziehen.\n\nlibrary(tidyverse)\ndata(\"germeval_train\", package = \"pradadata\")\ndata(\"germeval_test\", package = \"pradadata\")\n\n\n\n\nDie AV lautet c1. Die (einzige) UV lautet: text.\n\n\n\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\nNutzen Sie Tidymodels.\nNutzen Sie das sentiws Lexikon.\n❗ Achten Sie darauf, die Variable c2 zu entfernen bzw. nicht zu verwenden."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-rf-plain/germeval-sent-wordvec-rf-plain.html#daten",
    "href": "posts/germeval-sent-wordvec-rf-plain/germeval-sent-wordvec-rf-plain.html#daten",
    "title": "germeval03-sent-wordvec-rf-plain",
    "section": "",
    "text": "Verwenden Sie die GermEval-2018-Daten.\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\nDie Daten sind auch über das R-Paket PradaData zu beziehen.\n\nlibrary(tidyverse)\ndata(\"germeval_train\", package = \"pradadata\")\ndata(\"germeval_test\", package = \"pradadata\")"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-rf-plain/germeval-sent-wordvec-rf-plain.html#av-und-uv",
    "href": "posts/germeval-sent-wordvec-rf-plain/germeval-sent-wordvec-rf-plain.html#av-und-uv",
    "title": "germeval03-sent-wordvec-rf-plain",
    "section": "",
    "text": "Die AV lautet c1. Die (einzige) UV lautet: text."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-rf-plain/germeval-sent-wordvec-rf-plain.html#hinweise",
    "href": "posts/germeval-sent-wordvec-rf-plain/germeval-sent-wordvec-rf-plain.html#hinweise",
    "title": "germeval03-sent-wordvec-rf-plain",
    "section": "",
    "text": "Orientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\nNutzen Sie Tidymodels.\nNutzen Sie das sentiws Lexikon.\n❗ Achten Sie darauf, die Variable c2 zu entfernen bzw. nicht zu verwenden."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-rf-plain/germeval-sent-wordvec-rf-plain.html#setup",
    "href": "posts/germeval-sent-wordvec-rf-plain/germeval-sent-wordvec-rf-plain.html#setup",
    "title": "germeval03-sent-wordvec-rf-plain",
    "section": "Setup",
    "text": "Setup\n\nd_train &lt;-\n  germeval_train |&gt; \n  select(id, c1, text)\n\n\nlibrary(tictoc)\nlibrary(tidymodels)\nlibrary(syuzhet)\nlibrary(beepr)\nlibrary(lobstr)  # object size\nlibrary(visdat)  # Fingerprint/footprint of dataset (CSV)\ndata(\"sentiws\", package = \"pradadata\")\n\nEine Vorlage für ein Tidymodels-Pipeline findet sich hier."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-rf-plain/germeval-sent-wordvec-rf-plain.html#learnermodell-rf",
    "href": "posts/germeval-sent-wordvec-rf-plain/germeval-sent-wordvec-rf-plain.html#learnermodell-rf",
    "title": "germeval03-sent-wordvec-rf-plain",
    "section": "Learner/Modell: RF",
    "text": "Learner/Modell: RF\n\nmod &lt;-\n  rand_forest(mode = \"classification\",\n           mtry = tune(), \n           min_n = tune()\n  )"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-rf-plain/germeval-sent-wordvec-rf-plain.html#gebackenen-datensatz-als-neue-grundlage",
    "href": "posts/germeval-sent-wordvec-rf-plain/germeval-sent-wordvec-rf-plain.html#gebackenen-datensatz-als-neue-grundlage",
    "title": "germeval03-sent-wordvec-rf-plain",
    "section": "Gebackenen Datensatz als neue Grundlage",
    "text": "Gebackenen Datensatz als neue Grundlage\nWir importieren den schon an anderer Stelle aufbereiteten Datensatz. Das hat den Vorteil (hoffentlich), das die Datenvolumina viel kleiner sind. Die Arbeit des Feature Engineering wurde uns schon abgenommen.\n\nd_train &lt;-\n  read_csv(\"https://raw.githubusercontent.com/sebastiansauer/Datenwerk2/main/data/germeval/germeval_train_recipe_wordvec_senti.csv\")\n\n\nvis_dat(d_train) +\n  # remove axis labels:\n  theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank() \n        )\n\n\n\n\n\n\n\n\n\nd_test_baked &lt;- read_csv(\"https://raw.githubusercontent.com/sebastiansauer/Datenwerk2/main/data/germeval/germeval_test_recipe_wordvec_senti.csv\")"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-rf-plain/germeval-sent-wordvec-rf-plain.html#plain-rezept",
    "href": "posts/germeval-sent-wordvec-rf-plain/germeval-sent-wordvec-rf-plain.html#plain-rezept",
    "title": "germeval03-sent-wordvec-rf-plain",
    "section": "Plain-Rezept",
    "text": "Plain-Rezept\n\nrec &lt;- \n  recipe(c1 ~ ., data = d_train)"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-rf-plain/germeval-sent-wordvec-rf-plain.html#neuer-workflow-mit-plainem-rezept",
    "href": "posts/germeval-sent-wordvec-rf-plain/germeval-sent-wordvec-rf-plain.html#neuer-workflow-mit-plainem-rezept",
    "title": "germeval03-sent-wordvec-rf-plain",
    "section": "Neuer Workflow mit plainem Rezept",
    "text": "Neuer Workflow mit plainem Rezept\n\nwf &lt;-\n  workflow() |&gt; \n  add_recipe(rec) |&gt; \n  add_model(mod)"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-rf-plain/germeval-sent-wordvec-rf-plain.html#parallelisierung-über-mehrere-kerne",
    "href": "posts/germeval-sent-wordvec-rf-plain/germeval-sent-wordvec-rf-plain.html#parallelisierung-über-mehrere-kerne",
    "title": "germeval03-sent-wordvec-rf-plain",
    "section": "Parallelisierung über mehrere Kerne",
    "text": "Parallelisierung über mehrere Kerne\n\nlibrary(parallel)\nall_cores &lt;- detectCores(logical = FALSE)\n\nlibrary(doFuture)\nregisterDoFuture()\ncl &lt;- makeCluster(3)\nplan(cluster, workers = cl)\n\nAchtung: Viele Kerne brauchen auch viel Speicher."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-rf-plain/germeval-sent-wordvec-rf-plain.html#tuneresamplefit",
    "href": "posts/germeval-sent-wordvec-rf-plain/germeval-sent-wordvec-rf-plain.html#tuneresamplefit",
    "title": "germeval03-sent-wordvec-rf-plain",
    "section": "Tune/Resample/Fit",
    "text": "Tune/Resample/Fit\n\ntic()\nfit_wordvec_senti_rf &lt;-\n  tune_grid(\n    wf,\n    grid = 10,\n    resamples = vfold_cv(d_train, v = 5))\ntoc()\n\n1556.058 sec elapsed\n\nbeep()\n\nModerate Größe:\n\nobj_size(fit_wordvec_senti_rf)\n\n5.05 MB"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-rf-plain/germeval-sent-wordvec-rf-plain.html#get-best-performance",
    "href": "posts/germeval-sent-wordvec-rf-plain/germeval-sent-wordvec-rf-plain.html#get-best-performance",
    "title": "germeval03-sent-wordvec-rf-plain",
    "section": "Get best performance",
    "text": "Get best performance\n\nautoplot(fit_wordvec_senti_rf)\n\n\n\n\n\n\n\n\n\nshow_best(fit_wordvec_senti_rf)\n\n# A tibble: 5 × 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1     8    26 roc_auc binary     0.792     5 0.00890 Preprocessor1_Model09\n2    19    33 roc_auc binary     0.782     5 0.00851 Preprocessor1_Model08\n3    32     8 roc_auc binary     0.777     5 0.00783 Preprocessor1_Model03\n4    41    17 roc_auc binary     0.773     5 0.00715 Preprocessor1_Model10\n5    74     5 roc_auc binary     0.768     5 0.00626 Preprocessor1_Model02\n\n\n\nbest_params &lt;- select_best(fit_wordvec_senti_rf)"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-rf-plain/germeval-sent-wordvec-rf-plain.html#finalisieren",
    "href": "posts/germeval-sent-wordvec-rf-plain/germeval-sent-wordvec-rf-plain.html#finalisieren",
    "title": "germeval03-sent-wordvec-rf-plain",
    "section": "Finalisieren",
    "text": "Finalisieren\n\nbest_params &lt;- select_best(fit_wordvec_senti_rf)\ntic()\nwf_finalized &lt;- finalize_workflow(wf, best_params)\nlastfit_rf &lt;- fit(wf_finalized, data = d_train)\ntoc()\n\n13.055 sec elapsed"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-rf-plain/germeval-sent-wordvec-rf-plain.html#test-set-güte",
    "href": "posts/germeval-sent-wordvec-rf-plain/germeval-sent-wordvec-rf-plain.html#test-set-güte",
    "title": "germeval03-sent-wordvec-rf-plain",
    "section": "Test-Set-Güte",
    "text": "Test-Set-Güte\n\ntic()\npreds &lt;-\n  predict(lastfit_rf, new_data = d_test_baked)\ntoc()\n\n0.338 sec elapsed\n\n\n\nd_test &lt;-\n  germeval_test |&gt; \n  bind_cols(preds) |&gt; \n  mutate(c1 = as.factor(c1))\n\nmy_metrics &lt;- metric_set(accuracy, f_meas)\nmy_metrics(d_test,\n           truth = c1,\n           estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.707\n2 f_meas   binary         0.303"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-rf-plain/germeval-sent-wordvec-rf-plain.html#fazit",
    "href": "posts/germeval-sent-wordvec-rf-plain/germeval-sent-wordvec-rf-plain.html#fazit",
    "title": "germeval03-sent-wordvec-rf-plain",
    "section": "Fazit",
    "text": "Fazit\nVerzichtet man auf ein Rezept mit viel Datenvolumen (Wordvektoren blähen das Rezept mächtig auf), so wird das Fitten schlanker und schneller. Schneller auch deshalb, weil ggf. kein Swapping zwischen Speicher und Festplatte mehr nötig ist."
  },
  {
    "objectID": "posts/wrangle5/wrangle5.html",
    "href": "posts/wrangle5/wrangle5.html",
    "title": "wrangle5",
    "section": "",
    "text": "Wie prüft man in R auf Gleichheit zweier Ausdrücke?\nWählen Sie die korrekte Aussage.\n\n\n\n!= – “definiert als”\n== Prüfung auf Gleichheit\n= – Prüfung auf Gleichheit\n&lt;- – Prüfung auf Gleichheit\n!= – Prüfung auf Gleichheit"
  },
  {
    "objectID": "posts/wrangle5/wrangle5.html#answerlist",
    "href": "posts/wrangle5/wrangle5.html#answerlist",
    "title": "wrangle5",
    "section": "",
    "text": "!= – “definiert als”\n== Prüfung auf Gleichheit\n= – Prüfung auf Gleichheit\n&lt;- – Prüfung auf Gleichheit\n!= – Prüfung auf Gleichheit"
  },
  {
    "objectID": "posts/wrangle5/wrangle5.html#answerlist-1",
    "href": "posts/wrangle5/wrangle5.html#answerlist-1",
    "title": "wrangle5",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\neda\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/germeval-senti01/germeval-senti01.html",
    "href": "posts/germeval-senti01/germeval-senti01.html",
    "title": "germeval-senti01",
    "section": "",
    "text": "Führen Sie eine Sentiment-Analyse als Teils eines Tidymodels-Rezept durch. Modellieren Sie dann mit einem einfachen linearen Modell die abhängige Variable.\nVerwenden Sie diesen Datensatz:\n\n# Analyse-Daten:\ndata(\"germeval_train\", package = \"pradadata\")\ndata(\"germeval_test\", package = \"pradadata\")\n# Sentiment-Daten\ndata(\"sentiws\", package = \"pradadata\")\n\nDie AV ist c1.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks."
  },
  {
    "objectID": "posts/germeval-senti01/germeval-senti01.html#setup",
    "href": "posts/germeval-senti01/germeval-senti01.html#setup",
    "title": "germeval-senti01",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(syuzhet)  # get_sentiment\nlibrary(tidymodels)\nlibrary(tictoc)"
  },
  {
    "objectID": "posts/germeval-senti01/germeval-senti01.html#daten",
    "href": "posts/germeval-senti01/germeval-senti01.html#daten",
    "title": "germeval-senti01",
    "section": "Daten",
    "text": "Daten\nc2 brauchen wir hier nicht:\n\nd_train &lt;-\n  germeval_train |&gt; \n  select(-c2) |&gt; \n  as_tibble()"
  },
  {
    "objectID": "posts/germeval-senti01/germeval-senti01.html#rezept",
    "href": "posts/germeval-senti01/germeval-senti01.html#rezept",
    "title": "germeval-senti01",
    "section": "Rezept",
    "text": "Rezept\nRezept definieren:\n\nrec &lt;-\n  recipe(c1 ~ ., data = d_train) |&gt; \n  update_role(id, new_role = \"id\")  |&gt; \n  #update_role(c2, new_role = \"ignore\") |&gt; \n  update_role(text, new_role = \"ignore\") |&gt; \n  step_mutate(n_emo = get_sentiment(text,  # aus `syuzhet`\n                                    method = \"custom\",\n                                    lexicon = sentiws))  |&gt; \n  step_rm(text)  # Datensatz verschlanken\n\nstep_mutate ergänzt für die erzeugte (mutierte) Variable automatisch eine Rolle im Rezept, nimmt sie also als Prädiktor auf.\nMal schauen:\n\nrec\n\n\ntidy(rec)\n\n# A tibble: 2 × 6\n  number operation type   trained skip  id          \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;       \n1      1 step      mutate FALSE   FALSE mutate_XC8s1\n2      2 step      rm     FALSE   FALSE rm_g6jF6    \n\n\nPreppen und backen:\n\ntic()\nrec_prepped &lt;- prep(rec)\ntoc()\n\n12.92 sec elapsed\n\n\n\nrec_prepped\n\n\nrec_baked &lt;- bake(rec_prepped, new_data = NULL)\nhead(rec_baked)\n\n# A tibble: 6 × 3\n     id c1       n_emo\n  &lt;int&gt; &lt;fct&gt;    &lt;dbl&gt;\n1     1 OTHER    0.004\n2     2 OTHER   -0.347\n3     3 OTHER    0    \n4     4 OTHER    0    \n5     5 OFFENSE  0    \n6     6 OTHER   -0.346"
  },
  {
    "objectID": "posts/germeval-senti01/germeval-senti01.html#model",
    "href": "posts/germeval-senti01/germeval-senti01.html#model",
    "title": "germeval-senti01",
    "section": "Model",
    "text": "Model\n\nmod &lt;-\n  logistic_reg()"
  },
  {
    "objectID": "posts/germeval-senti01/germeval-senti01.html#workflow",
    "href": "posts/germeval-senti01/germeval-senti01.html#workflow",
    "title": "germeval-senti01",
    "section": "Workflow",
    "text": "Workflow\n\nwf &lt;- workflow() |&gt; \n  add_recipe(rec) |&gt; \n  add_model(mod)"
  },
  {
    "objectID": "posts/germeval-senti01/germeval-senti01.html#fit",
    "href": "posts/germeval-senti01/germeval-senti01.html#fit",
    "title": "germeval-senti01",
    "section": "Fit",
    "text": "Fit\n\ntic()\nfit1 &lt;-\n  fit(wf,\n      data = d_train)\ntoc()\n\n12.64 sec elapsed\n\n\n\nfit1\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_mutate()\n• step_rm()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n(Intercept)        n_emo  \n     0.6819       0.4802  \n\nDegrees of Freedom: 5008 Total (i.e. Null);  5007 Residual\nNull Deviance:      6402 \nResidual Deviance: 6392     AIC: 6396"
  },
  {
    "objectID": "posts/germeval-senti01/germeval-senti01.html#test-set-güte",
    "href": "posts/germeval-senti01/germeval-senti01.html#test-set-güte",
    "title": "germeval-senti01",
    "section": "Test-Set-Güte",
    "text": "Test-Set-Güte\nVorhersagen im Test-Set:\n\ntic()\npreds &lt;-\n  predict(fit1, new_data = germeval_test)\ntoc()\n\n7.397 sec elapsed\n\n\nUnd die Vorhersagen zum Test-Set hinzufügen, damit man TRUTH und ESTIMATE vergleichen kann:\n\nd_test &lt;-\n  germeval_test |&gt; \n  bind_cols(preds) |&gt; \n  mutate(c1 = as.factor(c1))\n\n\nmetrics(d_test,\n        truth = c1,\n        estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.660\n2 kap      binary         0"
  },
  {
    "objectID": "posts/germeval-senti01/germeval-senti01.html#baseline",
    "href": "posts/germeval-senti01/germeval-senti01.html#baseline",
    "title": "germeval-senti01",
    "section": "Baseline",
    "text": "Baseline\nEin einfaches Referenzmodell ist, einfach die häufigste Kategorie vorherzusagen:\n\nd_train |&gt; \n  count(c1)\n\n# A tibble: 2 × 2\n  c1          n\n  &lt;chr&gt;   &lt;int&gt;\n1 OFFENSE  1688\n2 OTHER    3321\n\n\n\nCategories:\n\ntidymodels\ntextmining\nprediction\nsentimentanalysis\ngermeval\nstring"
  },
  {
    "objectID": "posts/wskt-quiz09/wskt-quiz09.html",
    "href": "posts/wskt-quiz09/wskt-quiz09.html",
    "title": "wskt-quiz09",
    "section": "",
    "text": "Sei \\(X \\sim U(0, 1)\\).\nBehauptung: Es gilt: \\(f(X=1) = .1\\).\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n\n\n\nFalsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz09/wskt-quiz09.html#answerlist",
    "href": "posts/wskt-quiz09/wskt-quiz09.html#answerlist",
    "title": "wskt-quiz09",
    "section": "",
    "text": "Falsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz09/wskt-quiz09.html#answerlist-1",
    "href": "posts/wskt-quiz09/wskt-quiz09.html#answerlist-1",
    "title": "wskt-quiz09",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\nquiz\nprobability\nbayes\ndistribution\nquiz1-qm2-ws23\nschoice"
  },
  {
    "objectID": "posts/rope2/rope2.html",
    "href": "posts/rope2/rope2.html",
    "title": "rope2",
    "section": "",
    "text": "Im Datensatz mtcars: Ist der (mittlere) Unterschied im Spritverbrauch (mpg) zwischen den beiden Gruppen Automatik vs. Schaltgetriebe vernachlässigbar?\nWir definieren “vernachlässigbar klein” als “höchstens eine Meile”.\nPrüfen Sie rechnerisch, anhand des angegebenen Datensatzes, folgende Behauptung:\nBehauptung: “Der Unterschied ist vernachlässigbar klein!”\nNutzen Sie das ROPE-Konzept mit den Standardwerten im Befehl rope aus {easystats}.\nWählen Sie die Antwortoption, die am besten zu der obigen Behauptung passt!\nHinweise\nAntwortoptionen:\n\n\n\nJa, die Behauptung ist korrekt.\nNein, die Behauptung ist falsch.\nDie Daten sind bzw. das Modell nicht konkludent; es ist keine Entscheidung über die Behauptung möglich.\nAuf Basis der bereitgestellten Informationen ist keine Antwort möglich."
  },
  {
    "objectID": "posts/rope2/rope2.html#answerlist",
    "href": "posts/rope2/rope2.html#answerlist",
    "title": "rope2",
    "section": "",
    "text": "Ja, die Behauptung ist korrekt.\nNein, die Behauptung ist falsch.\nDie Daten sind bzw. das Modell nicht konkludent; es ist keine Entscheidung über die Behauptung möglich.\nAuf Basis der bereitgestellten Informationen ist keine Antwort möglich."
  },
  {
    "objectID": "posts/rope2/rope2.html#answerlist-1",
    "href": "posts/rope2/rope2.html#answerlist-1",
    "title": "rope2",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nRichtig\nFalsch\nFalsch\n\n\nCategories:\n\nrope\nbayes\nregression\nexam-22"
  },
  {
    "objectID": "posts/rf-finalize3/rf-finalize3.html",
    "href": "posts/rf-finalize3/rf-finalize3.html",
    "title": "rf-finalize3",
    "section": "",
    "text": "Berechnen Sie ein prädiktives Modell (Random Forest) mit dieser Modellgleichung:\nbody_mass_g ~ . (Datensatz: palmerpenguins::penguins).\nZeigen Sie, welche Werte für mtry im Default von Tidymodels gesetzt werden!\nHinweise: - Tunen Sie alle Tuningparameter mit jeweils 3 Werten. - Verwenden Sie Kreuzvalidierung - Verwenden Sie Standardwerte, wo nicht anders angegeben. - Fixieren Sie Zufallszahlen auf den Startwert 42."
  },
  {
    "objectID": "posts/rf-finalize3/rf-finalize3.html#standard-start",
    "href": "posts/rf-finalize3/rf-finalize3.html#standard-start",
    "title": "rf-finalize3",
    "section": "Standard-Start",
    "text": "Standard-Start\nZuererst der Standardablauf:\n\n# Setup:\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.0.8\n✔ dials        1.2.0     ✔ rsample      1.2.0\n✔ dplyr        1.1.3     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.4\n✔ lubridate 1.9.3     ✔ stringr   1.5.0\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tictoc)  # Zeitmessung\nset.seed(42)\n\n\n# Data:\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\"\nd &lt;- read_csv(d_path)\n\nRows: 344 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): species, island, sex\ndbl (6): rownames, bill_length_mm, bill_depth_mm, flipper_length_mm, body_ma...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# rm NA in the dependent variable:\nd &lt;- d %&gt;% \n  drop_na(body_mass_g)\n\n\nset.seed(42)\nd_split &lt;- initial_split(d)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\n\n# model:\nmod_rf &lt;-\n  rand_forest(mode = \"regression\",\n           mtry = tune(),\n           min_n = tune(),\n           trees = tune())\n\n\n# cv:\nset.seed(42)\nrsmpl &lt;- vfold_cv(d_train)\n\n\n# recipe:\nrec_plain &lt;- \n  recipe(body_mass_g ~  ., data = d_train) %&gt;% \n  step_impute_bag(all_predictors())\n\n\n# workflow:\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(mod_rf) %&gt;% \n  add_recipe(rec_plain)"
  },
  {
    "objectID": "posts/rf-finalize3/rf-finalize3.html#tuninggrid",
    "href": "posts/rf-finalize3/rf-finalize3.html#tuninggrid",
    "title": "rf-finalize3",
    "section": "Tuninggrid",
    "text": "Tuninggrid\nWelche Tuningparameter hat unser Workflow?\n\nwf1_params_unclear &lt;- \n  extract_parameter_set_dials(wf1)\nwf1_params_unclear\n\nCollection of 3 parameters for tuning\n\n identifier  type    object\n       mtry  mtry nparam[?]\n      trees trees nparam[+]\n      min_n min_n nparam[+]\n\nModel parameters needing finalization:\n   # Randomly Selected Predictors ('mtry')\n\nSee `?dials::finalize` or `?dials::update.parameters` for more information.\n\n\nVerlangt waren 3 Tuningparameterwerte pro Parameter:\n\nmy_grid &lt;- grid_latin_hypercube(wf1_params_unclear, levels = 3)\n\nWarning: `levels` is not an argument to `grid_latin_hypercube()`. Did you mean\n`size`?\n\n\nError in `grid_latin_hypercube()`:\n! These arguments contain unknowns: `mtry`.\nℹ See the `finalize()` function.\n\nmy_grid\n\nError in eval(expr, envir, enclos): object 'my_grid' not found\n\n\nTidymodels weiß nicht, welche Werte für mtry benutzt werden sollen, da dieser Wert abhängig ist von der Anzahl der Spalten des Datensatzes, und damit unabhängig vom Modell.\nDie Ausgabe nparam[?] oben sagt uns, dass Tidymodels den Wertebereich des Tuningparameter nicht klären könnte, da er Daten abhängig ist.\nInformieren wir also Tidymodels zu diesem Wertebereich:\n\nwf1_params &lt;- \n  wf1 %&gt;% \n  extract_parameter_set_dials() %&gt;% \n  update(mtry = finalize(mtry(), d_train))\n\nwf1_params\n\nCollection of 3 parameters for tuning\n\n identifier  type    object\n       mtry  mtry nparam[+]\n      trees trees nparam[+]\n      min_n min_n nparam[+]\n\n\nSo, jetzt weiß Tidymodels, wie viele Werte für mtry benutzt werden können.\nWir können jetzt das Tuninggitter erstellen (das macht das Paket dials):\n\nmy_grid &lt;- grid_latin_hypercube(wf1_params, size = 125)\nmy_grid %&gt;% head()\n\n# A tibble: 6 × 3\n   mtry trees min_n\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     1   105    11\n2     5  1036    21\n3     3   325    16\n4     4  1375    28\n5     6  1405    21\n6     7   304    15\n\n\nWie viele verschiedene Werte gibt es in dem Tuningitter?\nSchauen wir es uns mal an.\n\nmy_grid %&gt;% \n  ggplot(aes(x = trees, y = mtry)) +\n  geom_point()\n\n\n\n\n\n\n\n\nWir können das Tuninggitter auch selber erstellen:\n\nmy_grid &lt;-\n  grid_latin_hypercube(mtry(range = c(1, ncol(d_train)-1)),\n                       trees(),\n                       min_n(),\n                       size = 60)\ndim(my_grid)\n\n[1] 60  3"
  },
  {
    "objectID": "posts/rf-finalize3/rf-finalize3.html#tuningfitting",
    "href": "posts/rf-finalize3/rf-finalize3.html#tuningfitting",
    "title": "rf-finalize3",
    "section": "Tuning/Fitting",
    "text": "Tuning/Fitting\n\n# tuning:\ntic()\nwf1_fit &lt;-\n  wf1 %&gt;% \n  tune_grid(\n    grid = my_grid,\n    resamples = rsmpl)\ntoc()\n\n145.827 sec elapsed\n\n\nDann schauen wir uns das Ergebnisobjekt vom Tuning an.\n\nwf1_fit %&gt;% \n  collect_metrics() %&gt;% \n  filter(.metric == \"rmse\") %&gt;% \n  arrange(mtry)\n\n# A tibble: 60 × 9\n    mtry trees min_n .metric .estimator  mean     n std_err .config             \n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n 1     1  1835    33 rmse    standard    332.    10    13.7 Preprocessor1_Model…\n 2     1  1742    14 rmse    standard    316.    10    13.1 Preprocessor1_Model…\n 3     1   510    29 rmse    standard    327.    10    14.1 Preprocessor1_Model…\n 4     1   826     3 rmse    standard    310.    10    13.0 Preprocessor1_Model…\n 5     2   672    15 rmse    standard    283.    10    11.2 Preprocessor1_Model…\n 6     2   147    22 rmse    standard    287.    10    10.7 Preprocessor1_Model…\n 7     2  1927     7 rmse    standard    283.    10    11.4 Preprocessor1_Model…\n 8     2   386    23 rmse    standard    288.    10    11.9 Preprocessor1_Model…\n 9     2    81    29 rmse    standard    292.    10    12.3 Preprocessor1_Model…\n10     2   359     6 rmse    standard    283.    10    11.1 Preprocessor1_Model…\n# ℹ 50 more rows\n\n\nIn der Hilfe ist zu lesen:\n\nIn some cases, the tuning parameter values depend on the dimensions of the data. For example, mtry in random forest models depends on the number of predictors. In this case, the default tuning parameter object requires an upper range. dials::finalize() can be used to derive the data-dependent parameters. Otherwise, a parameter set can be created (via dials::parameters()) and the dials update() function can be used to change the values. This updated parameter set can be passed to the function via the param_info argument.\n\nAchtung: step_impute_knn scheint Probleme zu haben, wenn es Charakter-Variablen gibt.\nPraktischerweise findet Tidymodels die Begrenzung von mtry selber heraus, wenn Sie kein Tuninggrid definieren. Das erkennen Sie daran, dass Tidymodels beim Tuning/Fitten die folgende Ausgabe zeigt:\ni Creating pre-processing data to finalize unknown parameter: mtry.\n\nCategories:\n\ntidymodels\nstatlearning\ntemplate\nstring"
  },
  {
    "objectID": "posts/mariokart-desk01/mariokart-desk01.html",
    "href": "posts/mariokart-desk01/mariokart-desk01.html",
    "title": "mariokart-desk01",
    "section": "",
    "text": "Aufgabe\nIm Datensatz mariokart:\nNutzen Sie describe_distribution um deskriptive Statistiken (Lagemaße, Streuungsmaße) für die Variable total_pr zu berechnen.\nWie viele Statistiken werden (im Default) berichtet?\n         \n\n\nLösung\nPakete starten:\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.6.0 (red = needs update)\n✔ bayestestR  0.13.1   ✔ correlation 0.8.4 \n✔ datawizard  0.9.0    ✔ effectsize  0.8.6 \n✔ insight     0.19.6   ✔ modelbased  0.8.6 \n✔ performance 0.10.8   ✔ parameters  0.21.3\n✔ report      0.5.7    ✖ see         0.8.0 \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`.\n\nlibrary(tidyverse)  # startet das Paket tidyverse\n\nDaten importieren:\n\nmariokart &lt;- data_read(\"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\")\n\nOder so:\n\ndata(mariokart, package = \"openintro\")  # aus dem Paket \"openintro\"\n\nDazu muss das Paket openintro auf Ihrem Computer installiert sein.\nDaten zusammenfassen zu deskriptiven Statistiken:\nMit dataExplorer:\n\nmariokart %&gt;% \n  select(total_pr) %&gt;% \n  describe_distribution()  \n\nVariable |  Mean |    SD |   IQR |           Range | Skewness | Kurtosis |   n | n_Missing\n------------------------------------------------------------------------------------------\ntotal_pr | 49.88 | 25.69 | 12.99 | [28.98, 326.51] |     9.04 |    96.14 | 143 |         0\n\n\nFalls Sie Teile der R-Syntax nicht kennen: Im Zweifel einfach ignorieren :-)\nAntwort: Es werden 8 Statistiken berichtet (im Default).\n\nCategories:\n\ndatawrangling\neda\ntidyverse\nvis\nvariability\nnum"
  },
  {
    "objectID": "posts/wskt-quiz07/wskt-quiz07.html",
    "href": "posts/wskt-quiz07/wskt-quiz07.html",
    "title": "wskt-quiz07",
    "section": "",
    "text": "Folgende Formel ist korrekt: \\(Pr(H|D) = Pr(D|H) \\cdot Pr(H)\\).\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n\n\n\nFalsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz07/wskt-quiz07.html#answerlist",
    "href": "posts/wskt-quiz07/wskt-quiz07.html#answerlist",
    "title": "wskt-quiz07",
    "section": "",
    "text": "Falsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz07/wskt-quiz07.html#answerlist-1",
    "href": "posts/wskt-quiz07/wskt-quiz07.html#answerlist-1",
    "title": "wskt-quiz07",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\nquiz\nprobability\nbayes\nquiz1-qm2-ws23\nschoice"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-12/Verteilungen-Quiz-12.html",
    "href": "posts/Verteilungen-Quiz-12/Verteilungen-Quiz-12.html",
    "title": "Verteilungen-Quiz-12",
    "section": "",
    "text": "Ist folgende Aussage wahr?\nBei einer symmetrischen Verteilung gilt: \\(\\bar{x} = Md = \\text{Modus}\\).\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-12/Verteilungen-Quiz-12.html#answerlist",
    "href": "posts/Verteilungen-Quiz-12/Verteilungen-Quiz-12.html#answerlist",
    "title": "Verteilungen-Quiz-12",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-12/Verteilungen-Quiz-12.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-12/Verteilungen-Quiz-12.html#answerlist-1",
    "title": "Verteilungen-Quiz-12",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-15/Verteilungen-Quiz-15.html",
    "href": "posts/Verteilungen-Quiz-15/Verteilungen-Quiz-15.html",
    "title": "Verteilungen-Quiz-15",
    "section": "",
    "text": "Ist folgende Aussage wahr?\nDas HDI schneidet auf beiden Seiten des Intervalls die gleiche Wahrscheinlichkeitsmasse ab.\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-15/Verteilungen-Quiz-15.html#answerlist",
    "href": "posts/Verteilungen-Quiz-15/Verteilungen-Quiz-15.html#answerlist",
    "title": "Verteilungen-Quiz-15",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-15/Verteilungen-Quiz-15.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-15/Verteilungen-Quiz-15.html#answerlist-1",
    "title": "Verteilungen-Quiz-15",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/wrangle3/wrangle3.html",
    "href": "posts/wrangle3/wrangle3.html",
    "title": "wrangle3",
    "section": "",
    "text": "Welche Aussage zu der Funktionsweise folgender Funktionen im R-Paket dplyr ist richtig?\n\nfilter\nselect\nsummarise\ncount\ngroup_by\n\n\n\n\nDas erste Argument darf nie ein Dataframe sein.\nDas erste Argument ist immer die zu analysierende Variable.\nSpaltennamen müssen mit Anführungsstrichen benannt werden.\nEs wird immer eine Tabelle ausgegeben.\nFunktionsnamen sind (zumeist) nicht als Verben formuliert."
  },
  {
    "objectID": "posts/wrangle3/wrangle3.html#answerlist",
    "href": "posts/wrangle3/wrangle3.html#answerlist",
    "title": "wrangle3",
    "section": "",
    "text": "Das erste Argument darf nie ein Dataframe sein.\nDas erste Argument ist immer die zu analysierende Variable.\nSpaltennamen müssen mit Anführungsstrichen benannt werden.\nEs wird immer eine Tabelle ausgegeben.\nFunktionsnamen sind (zumeist) nicht als Verben formuliert."
  },
  {
    "objectID": "posts/wrangle3/wrangle3.html#answerlist-1",
    "href": "posts/wrangle3/wrangle3.html#answerlist-1",
    "title": "wrangle3",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nWahr\nFalsch\n\n\nCategories:\n\ndatawrangling\neda\nschoice"
  },
  {
    "objectID": "posts/count-emoji-emo/count-emoji-emo.html",
    "href": "posts/count-emoji-emo/count-emoji-emo.html",
    "title": "count-emoji-emo",
    "section": "",
    "text": "Aufgabe\nGegeben eines (mehrelementigen) Strings, my_string, und eines Lexicons, my_lexicon, zählen Sie, wie häufig sich ein Emoji in einem Element des Strings wiederfindet.\n\nmy_string &lt;-\n  c(\"Heute ist ein schöner Tag 😄😄\", \"Was geht in dieser Woche?\", \"Super 🙂\")\n\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\nNutzen Sie die Funktion emo::ji.\n\n         \n\n\nLösung\n\nlibrary(emo)\nlibrary(purrr)\n\n\nmap_int(my_string, ji_count)\n\n[1] 2 0 1\n\n\n\nCategories:\n\ntextmining\nnlp\nemoji\nstring"
  },
  {
    "objectID": "posts/diamonds-nullhyp-mws/diamonds-nullhyp-mws.html",
    "href": "posts/diamonds-nullhyp-mws/diamonds-nullhyp-mws.html",
    "title": "diamonds-nullhyp-mws",
    "section": "",
    "text": "Betrachten Sie folgende Ausgabe eines Bayesmodells, das mit rstanarm “gefittet” wurde:\nstan_glm\n family:       gaussian [identity]\n formula:      price ~ cut\n observations: 53940\n predictors:   5\n------\n             Median MAD_SD\n(Intercept)  4358.6  100.7\ncutGood      -431.4  112.4\ncutIdeal     -901.9  104.3\ncutPremium    226.7  105.4\ncutVery Good -375.2  103.9\n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 3964.2   11.8\nWelche Aussage passt (am besten)?\nHinweise:\n\nMit “Nullhypothese” ist im Folgenden dieser Ausdruck gemeint: \\(\\mu_1 = \\mu_2 = \\ldots = \\mu_k\\).\nGehen Sie davon aus, dass die Posteriori-Verteilungen der Regressionskoeffizienten normalverteilt sind.\nBeziehen Sie sich bei den Antworten auf die oben dargestellten Daten.\n\n\n\n\nDie Nullhypothese ist (sicher) falsch und muss daher verworfen werden.\nDie Nullhypothese ist (sicher) wahr und muss daher beibehalten werden.\nMan kann schließen, dass beim Parameter von cutGood der Wert Null außerhalb des 95%-PI der Posteriori-Verteilung liegt.\nMan kann schließen, dass alle Parameter positiv sind."
  },
  {
    "objectID": "posts/diamonds-nullhyp-mws/diamonds-nullhyp-mws.html#answerlist",
    "href": "posts/diamonds-nullhyp-mws/diamonds-nullhyp-mws.html#answerlist",
    "title": "diamonds-nullhyp-mws",
    "section": "",
    "text": "Die Nullhypothese ist (sicher) falsch und muss daher verworfen werden.\nDie Nullhypothese ist (sicher) wahr und muss daher beibehalten werden.\nMan kann schließen, dass beim Parameter von cutGood der Wert Null außerhalb des 95%-PI der Posteriori-Verteilung liegt.\nMan kann schließen, dass alle Parameter positiv sind."
  },
  {
    "objectID": "posts/diamonds-nullhyp-mws/diamonds-nullhyp-mws.html#answerlist-1",
    "href": "posts/diamonds-nullhyp-mws/diamonds-nullhyp-mws.html#answerlist-1",
    "title": "diamonds-nullhyp-mws",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch. Streng genommen können wir nicht ganz sicher sein, ob eine Hypothese auf Basis eines Modells richtig oder falsch ist.\nFalsch. Streng genommen können wir nicht ganz sicher sein, ob eine Hypothese auf Basis eines Modells richtig oder falsch ist.\nRichtig. Mittelwert plus/minus 2 SD-Einheiten gibt bei einer Normalverteilung das 95%-ETI an.\nFalsch. cutGood hat z.B. negative Werte in seinem 95%-ETI der Postverteilung.\n\n\nCategories:\n\nbayes\nregression\nnull"
  },
  {
    "objectID": "posts/wrangle4/wrangle4.html",
    "href": "posts/wrangle4/wrangle4.html",
    "title": "wrangle4",
    "section": "",
    "text": "Welche Variante der folgenden Syntax-Beispiele ist richtig (formal korrekt)?\n\n\n\nfilter(flights, month = 1, day = 1)\nfilter(flights, day == 1)\nfilter(month == 1, day == 1)\nfilter(month = 1, day == 1)\nfilter(flights, month == 1, day == 1)"
  },
  {
    "objectID": "posts/wrangle4/wrangle4.html#answerlist",
    "href": "posts/wrangle4/wrangle4.html#answerlist",
    "title": "wrangle4",
    "section": "",
    "text": "filter(flights, month = 1, day = 1)\nfilter(flights, day == 1)\nfilter(month == 1, day == 1)\nfilter(month = 1, day == 1)\nfilter(flights, month == 1, day == 1)"
  },
  {
    "objectID": "posts/wrangle4/wrangle4.html#answerlist-1",
    "href": "posts/wrangle4/wrangle4.html#answerlist-1",
    "title": "wrangle4",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nFalsch\nWahr\n\n\nCategories:\n\neda\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/lm-mario1/lm-mario1.html",
    "href": "posts/lm-mario1/lm-mario1.html",
    "title": "lm-mario1",
    "section": "",
    "text": "Sagen Sie den Verkaufspreis vorher für ein Spiel mit 2 Lenkrädern!\nHinweise:\n\nEntfernen Sie Spiele mit einem Verkaufspreis von über 100 Euro aus dem Datensatz."
  },
  {
    "objectID": "posts/lm-mario1/lm-mario1.html#setup",
    "href": "posts/lm-mario1/lm-mario1.html#setup",
    "title": "lm-mario1",
    "section": "Setup",
    "text": "Setup\n\nmariokart &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\")\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.6.0 (red = needs update)\n✔ bayestestR  0.13.1   ✔ correlation 0.8.4 \n✔ datawizard  0.9.0    ✔ effectsize  0.8.6 \n✔ insight     0.19.6   ✔ modelbased  0.8.6 \n✔ performance 0.10.8   ✔ parameters  0.21.3\n✔ report      0.5.7    ✖ see         0.8.0 \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`."
  },
  {
    "objectID": "posts/lm-mario1/lm-mario1.html#bild-der-daten",
    "href": "posts/lm-mario1/lm-mario1.html#bild-der-daten",
    "title": "lm-mario1",
    "section": "Bild der Daten",
    "text": "Bild der Daten\n\nlibrary(ggpubr)  # einmalig vorab installieren, nicht vergessen\n\n\nAttaching package: 'ggpubr'\n\n\nThe following objects are masked from 'package:datawizard':\n\n    mean_sd, median_mad\n\nggscatter(mariokart, x = \"wheels\", y = \"total_pr\")  # aus ggpubr\n\n\n\n\n\n\n\n\nOder mit DataExplorer visualisieren:\n\nlibrary(DataExplorer)\n\nplot_scatterplot(mariokart, by = \"total_pr\")  # \"by\" ist Y-Achse"
  },
  {
    "objectID": "posts/lm-mario1/lm-mario1.html#extremwerte-entfernen",
    "href": "posts/lm-mario1/lm-mario1.html#extremwerte-entfernen",
    "title": "lm-mario1",
    "section": "Extremwerte entfernen",
    "text": "Extremwerte entfernen\n\nmariokart2 &lt;- \n  mariokart %&gt;% \n  filter(total_pr &lt; 100)  # alle Spiele teuerer als 100€ entfernen\n\nggscatter(mariokart2, x = \"wheels\", y = \"total_pr\")"
  },
  {
    "objectID": "posts/lm-mario1/lm-mario1.html#regressionsgerade-eintragen-in-das-diagramm",
    "href": "posts/lm-mario1/lm-mario1.html#regressionsgerade-eintragen-in-das-diagramm",
    "title": "lm-mario1",
    "section": "Regressionsgerade eintragen in das Diagramm",
    "text": "Regressionsgerade eintragen in das Diagramm\n\nggscatter(mariokart2, \n          x = \"wheels\", \n          y = \"total_pr\",\n          add = \"reg.line\")  # Dieser Schalter malt die Regr.gerade in das Diagramm"
  },
  {
    "objectID": "posts/lm-mario1/lm-mario1.html#regressionsgerade-berechnen",
    "href": "posts/lm-mario1/lm-mario1.html#regressionsgerade-berechnen",
    "title": "lm-mario1",
    "section": "Regressionsgerade berechnen",
    "text": "Regressionsgerade berechnen\n\nlm_mariokart &lt;- lm(total_pr ~ wheels, data = mariokart2)\nlm_mariokart\n\n\nCall:\nlm(formula = total_pr ~ wheels, data = mariokart2)\n\nCoefficients:\n(Intercept)       wheels  \n     37.502        8.643  \n\n\n“lm” wie llineares Modell, also eine Gerade."
  },
  {
    "objectID": "posts/lm-mario1/lm-mario1.html#vorhersagen",
    "href": "posts/lm-mario1/lm-mario1.html#vorhersagen",
    "title": "lm-mario1",
    "section": "Vorhersagen",
    "text": "Vorhersagen\nVorhersagen funktionieren mit dem Befehl predict.\n\nneues_spiel &lt;-\n  data.frame(\n    wheels = 2\n  )\n\nneues_spiel\n\n  wheels\n1      2\n\n\n\npredict(lm_mariokart, neues_spiel)  # predicte mir den Verkaufspreis\n\n       1 \n54.78743 \n\n\n\nCategories:\n\nR\nlm\npredict\nnum"
  },
  {
    "objectID": "posts/regex01/regex01.html",
    "href": "posts/regex01/regex01.html",
    "title": "regex01",
    "section": "",
    "text": "Aufgabe\nErstellen Sie einen Vektor mit den Namen aller CSV-Dateien eines beliebigen Ordners Ihres Computers.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n         \n\n\nLösung\nEinen Order auswählen:\nDateien als Strings einlesen:\n\n\ncharacter(0)\n\n\nDateien als Strings einlesen:\n\n\ncharacter(0)\n\n\nNur CSV-Dateien einlesen:\n\n\ncharacter(0)\n\n\n\nCategories:\n\ntextmining\nregex\nstring"
  },
  {
    "objectID": "posts/Aussagen-einfache-Regr/Aussagen-einfache-Regr.html",
    "href": "posts/Aussagen-einfache-Regr/Aussagen-einfache-Regr.html",
    "title": "Aussagen-einfache-Regr",
    "section": "",
    "text": "Im Hinblick auf die lineare Regression: Welche der folgenden Aussage passt am besten?\n\n\n\nDie einfache Regression - \\(y=\\alpha + \\beta_1x_1 + \\epsilon\\) - prüft, inwieweit zwei Variablen zusammenhängen (linear oder anderweitig).\nObwohl statistische Zusammenhänge nicht ohne Weiteres Kausalschlüsse erlauben, kann man die Regression für Vorhersagen gut nutzen.\nRegressionskoeffizienten kann man so interpretieren: “Erhöht man X um eine 1 Einheit, so steigt daraufhin Y um \\(\\beta_1\\) Einheiten” (\\(\\beta_1\\) sei der entsprechende Regressionskoeffizient).\n“Lineare Regression” bedeutet, dass z.B. keine Polynome wie \\(y= \\alpha + \\beta_1 x_1^2 + \\beta_2 x_1 + \\epsilon\\) berechnet werden dürfen, bzw. nicht zur linearen Regression zählen.\nZentrieren der Prädiktoren ist bei der linearen Regression nicht zulässig."
  },
  {
    "objectID": "posts/Aussagen-einfache-Regr/Aussagen-einfache-Regr.html#answerlist",
    "href": "posts/Aussagen-einfache-Regr/Aussagen-einfache-Regr.html#answerlist",
    "title": "Aussagen-einfache-Regr",
    "section": "",
    "text": "Die einfache Regression - \\(y=\\alpha + \\beta_1x_1 + \\epsilon\\) - prüft, inwieweit zwei Variablen zusammenhängen (linear oder anderweitig).\nObwohl statistische Zusammenhänge nicht ohne Weiteres Kausalschlüsse erlauben, kann man die Regression für Vorhersagen gut nutzen.\nRegressionskoeffizienten kann man so interpretieren: “Erhöht man X um eine 1 Einheit, so steigt daraufhin Y um \\(\\beta_1\\) Einheiten” (\\(\\beta_1\\) sei der entsprechende Regressionskoeffizient).\n“Lineare Regression” bedeutet, dass z.B. keine Polynome wie \\(y= \\alpha + \\beta_1 x_1^2 + \\beta_2 x_1 + \\epsilon\\) berechnet werden dürfen, bzw. nicht zur linearen Regression zählen.\nZentrieren der Prädiktoren ist bei der linearen Regression nicht zulässig."
  },
  {
    "objectID": "posts/Aussagen-einfache-Regr/Aussagen-einfache-Regr.html#answerlist-1",
    "href": "posts/Aussagen-einfache-Regr/Aussagen-einfache-Regr.html#answerlist-1",
    "title": "Aussagen-einfache-Regr",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch. Die lineare Regression \\(y=\\alpha + \\beta_1x_1 + \\epsilon\\) untersucht, wie die Korrelation, den Grad des linearen Zusammenhangs. Allerdings sind auch nicht-lineare Zusammenhänge von \\(y\\) und den Prädiktoren erlaubt, etwa \\(y=\\alpha + \\beta_1x_1^2 + \\beta_2x_2 + \\epsilon\\). Linear ist dabei so zu verstehen, dass \\(y\\) eine additive Funktion der Prädiktoren ist. Vielleicht wäre es daher besser, anstelle von “linearen” Modellen von “additiven” Modellen zu sprechen.\nRichtig. Für Vorhersagen ist Kenntnis einer Kausalstruktur nicht unbedingt nötig, kann aber sehr hilfreich sein.\nFalsch. Diese Interpretation suggeriert einen Kausaleffekt. Besser ist die Interpretation “Vergleicht man zwei Beobachtungen, die sich um 1 Einheit in X unterscheiden, so findet man im Durchschnitt einen Unterschied von \\(\\beta_1\\) in Y”.\nFalsch.Die Gleichung \\(y= \\alpha + \\beta_1 x_1^2 + \\beta_2 x_2 + \\epsilon\\) ist linear in ihren Summanden.\nFalsch. Zentrieren der Prädiktoren ist bei der linearen Regression zulässig und oft sinnvoll.\n\n\nCategories:\n\nregression\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/ReThink3m2/ReThink3m2.html",
    "href": "posts/ReThink3m2/ReThink3m2.html",
    "title": "ReThink3m2",
    "section": "",
    "text": "Exercise\nNehmen wir an, wir haben 8 (Wasser-)“Treffer” (\\(W=8\\)) bei 15 Würfen (\\(N=15\\)) erhalten (wieder im Globusversuch).\n\nZiehen Sie \\(10^4\\) Stichproben aus der Posteriori-Verteilung basierend auf der Gittermethode. Gehen Sie von einer gleichverteilung Priori-Wahrscheinlichkeit aus.\nVisualisieren Sie die Verteilung der Stichproben.\nBerechnen Sie ds 90%-HDI.\n\nHinweise:\n\nBerechnen Sie eine Bayes-Box (Gittermethode).\nVerwenden Sie 1000 Gitterwerte.\nFixieren Sie die Zufallszahlen mit dem Startwert 42, d.h. set.seed(42).\nGehen Sie von einem gleichverteilten Prior aus.\n\nQuelle: McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Aufl.). Taylor and Francis, CRC Press.\n         \n\n\nSolution\n\nlibrary(tidyverse)\n\n\n\n\nPost-Verteilung berechnen:\n\np_grid &lt;- seq(from = 0, to = 1, length.out = 1000)\nprior &lt;- rep(1, 1000)\nlikelihood &lt;- dbinom(8, size = 15, prob = p_grid)\nposterior &lt;- likelihood * prior\nposterior &lt;- posterior / sum(posterior)\n\nStichproben-Postverteilung erstellen:\n\nsamples &lt;- \n  tibble(anteil_wasser = sample(p_grid, prob = posterior, size = 1e4, replace = TRUE))\n\nhead(samples)\n\n# A tibble: 6 × 1\n  anteil_wasser\n          &lt;dbl&gt;\n1         0.543\n2         0.571\n3         0.375\n4         0.358\n5         0.337\n6         0.527\n\n\n\n\n\n\nsamples %&gt;% \n  ggplot() +\n  aes(x = anteil_wasser) +\n  geom_histogram() + \n  labs(title = \"Stichproben aus der Posteriori-Verteilung\")\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(easystats)\nhdi(samples, prob = 0.9)\n\nHighest Density Interval\n\nParameter     |      95% HDI\n----------------------------\nanteil_wasser | [0.32, 0.77]\n\n\n\nCategories:\n\nbayes\npost\nprobability"
  },
  {
    "objectID": "posts/wskt-quiz01/wskt-quiz01.html",
    "href": "posts/wskt-quiz01/wskt-quiz01.html",
    "title": "wskt-quiz01",
    "section": "",
    "text": "Sind die Ereignisse \\(A\\) und \\(B\\) unabhängig, so gilt \\(Pr(A|B) = P(A)\\).\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n\n\n\nFalsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz01/wskt-quiz01.html#answerlist",
    "href": "posts/wskt-quiz01/wskt-quiz01.html#answerlist",
    "title": "wskt-quiz01",
    "section": "",
    "text": "Falsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz01/wskt-quiz01.html#answerlist-1",
    "href": "posts/wskt-quiz01/wskt-quiz01.html#answerlist-1",
    "title": "wskt-quiz01",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch. Nein, die Aussage ist nicht falsch, sie ist wahr.\nWahr. Ja, die Aussage ist wahr.\n\n\nCategories:\n\nquiz\nprobability\nquiz1-qm2-ws23\nschoice"
  },
  {
    "objectID": "posts/Schiefe-erkennen/Schiefe-erkennen.html",
    "href": "posts/Schiefe-erkennen/Schiefe-erkennen.html",
    "title": "Schiefe-erkennen",
    "section": "",
    "text": "Wählen Sie das Histogramm, welches am deutlichsten die Eigenschaft “rechtsschief” aufweist!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE"
  },
  {
    "objectID": "posts/Schiefe-erkennen/Schiefe-erkennen.html#answerlist",
    "href": "posts/Schiefe-erkennen/Schiefe-erkennen.html#answerlist",
    "title": "Schiefe-erkennen",
    "section": "",
    "text": "A\nB\nC\nD\nE"
  },
  {
    "objectID": "posts/Schiefe-erkennen/Schiefe-erkennen.html#answerlist-1",
    "href": "posts/Schiefe-erkennen/Schiefe-erkennen.html#answerlist-1",
    "title": "Schiefe-erkennen",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nWahr\nFalsch\nFalsch\n\n\nCategories:\n\neda\ndistributions\nschoice"
  },
  {
    "objectID": "posts/tidymodels-lasso/tidymodels-lasso.html",
    "href": "posts/tidymodels-lasso/tidymodels-lasso.html",
    "title": "tidymodels-lasso",
    "section": "",
    "text": "Aufgabe\n\nSchreiben Sie eine prototypische Analyse für ein Vorhersagemodell mit dem Lasso.\nHinweise:\n\nTunen Sie die Penalisierung.\nVerwenden Sie Kreuzvalidierung.\nVerwenden Sie Standardwerte, wo nicht anders angegeben.\nFixieren Sie Zufallszahlen auf den Startwert 42.\nVerwenden Sie den Datensatz penguins.\nModellformel: body_mass_g ~ .\n\n         \n\n\nLösung\n\n# 2023-05-14\n\n# Setup:\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(tictoc)  # Zeitmessung\n\n\n# Data:\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\"\nd &lt;- read_csv(d_path)\n\n# drop rows with NA in outcome variable:\nd &lt;-\n  d %&gt;% \n  drop_na(body_mass_g)\n\nset.seed(42)\nd_split &lt;- initial_split(d)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\n\n# model:\nmod_lasso &lt;-\n  linear_reg(mode = \"regression\",\n             penalty = tune(),\n             mixture = 1,\n             engine = \"glmnet\")\n\n# cv:\nset.seed(42)\nrsmpl &lt;- vfold_cv(d_train)\n\n\n# recipe:\nrec1_plain &lt;- \n  recipe(body_mass_g ~  ., data = d_train) %&gt;% \n  update_role(\"rownames\", new_role = \"id\") %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_impute_bag(all_predictors())\n\n\n# check:\nd_train_baked &lt;- \n  prep(rec1_plain) %&gt;% bake(new_data = NULL)\n\nna_n &lt;- sum(is.na(d_train_baked))\n\n\n# workflow:\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(mod_lasso) %&gt;% \n  add_recipe(rec1_plain)\n\n\n# tuning:\ntic()\nwf1_fit &lt;-\n  wf1 %&gt;% \n  tune_grid(\n    resamples = rsmpl)\ntoc()\n\n15.016 sec elapsed\n\n# best candidate:\nshow_best(wf1_fit)\n\n# A tibble: 5 × 7\n   penalty .metric .estimator  mean     n std_err .config              \n     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1 1.97e-10 rmse    standard    281.    10    12.0 Preprocessor1_Model01\n2 4.54e- 9 rmse    standard    281.    10    12.0 Preprocessor1_Model02\n3 8.93e- 8 rmse    standard    281.    10    12.0 Preprocessor1_Model03\n4 1.75e- 7 rmse    standard    281.    10    12.0 Preprocessor1_Model04\n5 1.65e- 6 rmse    standard    281.    10    12.0 Preprocessor1_Model05\n\n# finalize wf:\nwf1_final &lt;-\n  wf1 %&gt;% \n  finalize_workflow(select_best(wf1_fit))\n\n\nwf1_fit_final &lt;-\n  wf1_final %&gt;% \n  last_fit(d_split)\n\n\n# Modellgüte im Test-Set:\ncollect_metrics(wf1_fit_final)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard     326.    Preprocessor1_Model1\n2 rsq     standard       0.819 Preprocessor1_Model1\n\n\nMan beachte: Für regulierte Modelle sind Zentrierung und Skalierung nötig.\n\nCategories:\n\ntidymodels\nstatlearning\nlasso\nlm\nstring\ntemplate"
  },
  {
    "objectID": "posts/rope3/rope3.html",
    "href": "posts/rope3/rope3.html",
    "title": "rope3",
    "section": "",
    "text": "Einer der (bisher) größten Studien der Untersuchung psychologischer Konsequenzen (oder Korrelate) der Covid-Zeit ist die Studie COVIDiStress.\nIm Folgenden sollen Sie folgende Forschungsfrage untersuchen:\nIst der Zusammenhang von Stress (PSS10_avg, AV) und Neurotizismus (neu, UV) vernachlässigbar klein?\nDen Datensatz können Sie so herunterladen (Achtung, groß):\n\nosf_d_path &lt;- \"https://osf.io/cjxua/?action=download\"\n\nd &lt;- read_csv(osf_d_path)\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nHinweise:\n\nSie benötigen einen Computer, um diese Aufgabe zu lösen.\nVerwenden Sie die statistischen Methoden, die im Unterricht behandelt wurden.\nVerwenden Sie Ansätze aus der Bayes-Statistik zur Lösung dieser Aufgabe.\nBei der Variable für Geschlecht können Sie sich auf Fälle begrenzen, die Männer und Frauen umfassen.\nWandeln Sie die die Variable für Geschlecht in eine binäre Variable - also Werte mit 0 und 1 - um.\nAlle Daten (und weitere Informationen) zum Projekt sind hier abgelegt.\nEine Beschreibung der Variablen der Studie finden Sie hier.\nDas Codebook findet sich hier.\nDer Datensatz ist recht groß (ca. 150 MB).\nFixieren Sie die Zufallszahlen auf den Startwert 42.\nBerechnen Sie 89%-PIs, wenn Sie Ungewissheit quantifizieren.\n\nAntwortoptionen\n\n\n\nJa\nNein\nDie Daten sind nicht konkludent; es ist keine Entscheidung möglich.\nAuf Basis der bereitgestellten Informationen ist keine Entscheidung möglich."
  },
  {
    "objectID": "posts/rope3/rope3.html#answerlist",
    "href": "posts/rope3/rope3.html#answerlist",
    "title": "rope3",
    "section": "",
    "text": "Ja\nNein\nDie Daten sind nicht konkludent; es ist keine Entscheidung möglich.\nAuf Basis der bereitgestellten Informationen ist keine Entscheidung möglich."
  },
  {
    "objectID": "posts/rope3/rope3.html#answerlist-1",
    "href": "posts/rope3/rope3.html#answerlist-1",
    "title": "rope3",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr. ROPE ist zu verwerfen, damit sind Werte um die Null herum nicht wahrscheinlich.\nFalsch\nFalsch\n\n\nCategories:\n\nrope\nbayes"
  },
  {
    "objectID": "posts/dplyr-uebersetzen/dplyr-uebersetzen.html",
    "href": "posts/dplyr-uebersetzen/dplyr-uebersetzen.html",
    "title": "dplyr-uebersetzen",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nAufgabe\nImportieren Sie den folgenden Datensatz in R:\n\nmtcars &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/datasets/mtcars.csv\")\n\nÜbersetzen Sie dann die folgende R-Sequenz ins Deutsche:\n\nmtcars %&gt;% \n  drop_na() %&gt;% \n  select(mpg, hp, cyl) %&gt;% \n  filter(hp &gt; 100, cyl &gt;= 6) %&gt;% \n  group_by(cyl) %&gt;% \n  summarise(mpg_mean = mean(mpg))\n\n# A tibble: 2 × 2\n    cyl mpg_mean\n  &lt;dbl&gt;    &lt;dbl&gt;\n1     6     19.7\n2     8     15.1\n\n\n         \n\n\nLösung\nHey R:\n\nNimm den Datensatz mtcars UND DANN\nhau alle Zeilen raus, in denen es fehlende Werte gibt UND DANN\nwähle (selektiere) die folgenden Spalten: Spritverbrauch, PS, Zylinder UND DANN\nfilter Autos mit mehr als 100 PS und mit mindestens 6 Zylindern UND DANN\ngruppiere nach der Zahl der Zylinder UND DANN\nfasse den Verbrauch zum Mittelwert zusammen.\n\n\nCategories:\n\ndatawrangling\ntidyverse\nstring"
  },
  {
    "objectID": "posts/Bed-Wskt1/Bed-Wskt1.html",
    "href": "posts/Bed-Wskt1/Bed-Wskt1.html",
    "title": "Bed-Wskt1",
    "section": "",
    "text": "Aufgabe\nProf. Süß-Sauer untersucht eine seiner Lieblingsfragen: Wie viel bringt das Lernen auf eine Klausur? Dabei konzentriert er sich auf das Fach Statistik (es gefällt ihm gut). In einer aktuellen Untersuchung hat er \\(n=42\\) Studierende untersucht (s. Tabelle und Diagramm) und jeweils erfasst, ob die Person die Klausur bestanden (b) hat oder durchgefallen (d) ist. Dabei hat er zwei Gruppen unterschieden: Die “Viel-Lerner” (VL) und die “Wenig-Lerner” (WL).\nBerechnen Sie die folgende bedingte Wahrscheinlichkeit: p(Bestehen|Viellerner).\nBeispiel: Wenn Sie ausrechnen, dass die Wahrscheinlichkeit bei 42 Prozentpunkten liegt, so geben Sie ein: 0,42 bzw. 0.42 (das Dezimalzeichen ist abhängig von Ihren Spracheinstellungen).\nHinweise:\n\nGeben Sie nur eine Zahl ein (ohne Prozentzeichen o.Ä.), z.B. 0,42.\nAndere Angaben können u.U. nicht gewertet werden.\nRunden Sie auf zwei Dezimalstellen.\nAchten Sie darauf, das korrekte Dezimaltrennzeichen einzugeben; auf Geräten mit deutscher Spracheinstellung ist dies oft ein Komma.\n\n\n\n\n\n\n\n\n\n\n\n\nErgebnisse der Studie\n\n\n\nViellerner\nWeniglerner\n\n\n\n\nBestehen\n12\n20\n\n\nDurchfallen\n5\n5\n\n\n\n\n\n\n         \n\n\nLösung\n\n\n\n\n\n\n\n\nLerntyp\nKlausurergebnis\nn\nn_group\nprop_conditional_group\nN_gesamt\n\n\n\n\nViellerner\nBestehen\n12\n17\n0.7058824\n42\n\n\n\n\n\n\n\nAntwort: Der gesuchte Wert liegt bei \\(12 / 17 =  0.71\\).\n\nCategories:\n\nprobability\nbayes\nnum"
  },
  {
    "objectID": "posts/Histogramm-in-Boxplot/Histogramm-in-Boxplot.html",
    "href": "posts/Histogramm-in-Boxplot/Histogramm-in-Boxplot.html",
    "title": "Histogramm-in-Boxplot",
    "section": "",
    "text": "Histogramm:\n\n\n\n\n\n\n\n\n\nBoxplots:\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n\n\n\n\n\nBoxplot A\nBoxplot B\nBoxplot C\nBoxplot D\nBoxplot E"
  },
  {
    "objectID": "posts/Histogramm-in-Boxplot/Histogramm-in-Boxplot.html#answerlist",
    "href": "posts/Histogramm-in-Boxplot/Histogramm-in-Boxplot.html#answerlist",
    "title": "Histogramm-in-Boxplot",
    "section": "",
    "text": "Boxplot A\nBoxplot B\nBoxplot C\nBoxplot D\nBoxplot E"
  },
  {
    "objectID": "posts/Histogramm-in-Boxplot/Histogramm-in-Boxplot.html#answerlist-1",
    "href": "posts/Histogramm-in-Boxplot/Histogramm-in-Boxplot.html#answerlist-1",
    "title": "Histogramm-in-Boxplot",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nvis\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/regression1/regression1.html",
    "href": "posts/regression1/regression1.html",
    "title": "regression1",
    "section": "",
    "text": "Die folgende Frage bezieht sich auf dieses Ergebnis einer Regressionsanalyse:\n\n\n\nCall:\nlm(formula = log(y) ~ log(x), data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9121 -0.9595 -0.1687  1.1232  3.6640 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.009234   0.217828  -0.042    0.966    \nlog(x)      -2.173359   0.223860  -9.709  1.8e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.822 on 68 degrees of freedom\nMultiple R-squared:  0.5809,    Adjusted R-squared:  0.5747 \nF-statistic: 94.26 on 1 and 68 DF,  p-value: 1.803e-14\n\n\nWelche der folgenden Aussagen ist korrekt?\n\n\n\nWenn x=0, dann ist ein Mittelwert von y in Höhe von etwa Inf zu erwarten.\nEs handelt sich um eine lineare Regression.\nWenn x um 1 Prozent steigt, dann kann eine Veränderung um etwa -0.01 Prozent in y erwartet werden.\nWenn x=1, dann ist ein Mittelwert von y in Höhe von ca. Inf zu erwarten.\nWenn x=2, dann ist ein Mittelwert von y in Höhe von ca. -0.01 zu erwarten."
  },
  {
    "objectID": "posts/regression1/regression1.html#answerlist",
    "href": "posts/regression1/regression1.html#answerlist",
    "title": "regression1",
    "section": "",
    "text": "Wenn x=0, dann ist ein Mittelwert von y in Höhe von etwa Inf zu erwarten.\nEs handelt sich um eine lineare Regression.\nWenn x um 1 Prozent steigt, dann kann eine Veränderung um etwa -0.01 Prozent in y erwartet werden.\nWenn x=1, dann ist ein Mittelwert von y in Höhe von ca. Inf zu erwarten.\nWenn x=2, dann ist ein Mittelwert von y in Höhe von ca. -0.01 zu erwarten."
  },
  {
    "objectID": "posts/regression1/regression1.html#answerlist-1",
    "href": "posts/regression1/regression1.html#answerlist-1",
    "title": "regression1",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nregression\ndyn\nlm\nschoice"
  },
  {
    "objectID": "posts/rope4/rope4.html",
    "href": "posts/rope4/rope4.html",
    "title": "rope4",
    "section": "",
    "text": "Einer der (bisher) größten Studien der Untersuchung psychologischer Konsequenzen (oder Korrelate) der Covid-Zeit ist die Studie COVIDiStress.\nIm Folgenden sollen Sie folgende Forschungsfrage untersuchen:\nForschungsfrage:\nIst der Unterschied zwischen Männern und Frauen (Dem_gender) im Hinblick zum Zusammenhang von Stress (PSS10_avg, AV) und Neurotizismus (neu, UV) vernachlässigbar klein?\nDen Datensatz können Sie so herunterladen (Achtung, groß):\n\nosf_d_path &lt;- \"https://osf.io/cjxua/?action=download\"\n\nd &lt;- read_csv(osf_d_path)\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nHinweise:\n\nSie benötigen einen Computer, um diese Aufgabe zu lösen.\nVerwenden Sie die statistischen Methoden, die im Unterricht behandelt wurden.\nVerwenden Sie Ansätze aus der Bayes-Statistik zur Lösung dieser Aufgabe.\nBei der Variable für Geschlecht können Sie sich auf Fälle begrenzen, die Männer und Frauen umfassen.\nWandeln Sie die die Variable für Geschlecht in eine binäre Variable - also Werte mit 0 und 1 - um.\nAlle Daten (und weitere Informationen) zum Projekt sind hier abgelegt.\nEine Beschreibung der Variablen der Studie finden Sie hier.\nFixieren Sie die Zufallszahlen auf den Startwert 42.\n\nAntwortoptionen:\n\n\n\nJa\nNein\nDie Daten sind nicht konkludent; es ist keine Entscheidung möglich.\nAuf Basis der bereitgestellten Informationen ist keine Entscheidung möglich."
  },
  {
    "objectID": "posts/rope4/rope4.html#answerlist",
    "href": "posts/rope4/rope4.html#answerlist",
    "title": "rope4",
    "section": "",
    "text": "Ja\nNein\nDie Daten sind nicht konkludent; es ist keine Entscheidung möglich.\nAuf Basis der bereitgestellten Informationen ist keine Entscheidung möglich."
  },
  {
    "objectID": "posts/rope4/rope4.html#answerlist-1",
    "href": "posts/rope4/rope4.html#answerlist-1",
    "title": "rope4",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\nFalsch\nFalsch\n\n\nCategories:\n\nrope\nbayes"
  },
  {
    "objectID": "posts/Krebs1/Krebs1.html",
    "href": "posts/Krebs1/Krebs1.html",
    "title": "Krebs1",
    "section": "",
    "text": "Aufgabe\nEin Krebstest (\\(T\\)) habe die Wahrscheinlichkeit von 0.9, einen vorhandenen Krebs (\\(K\\)) zu erkennen. Diese Wahrscheinlichkeit bezeichnen wir als \\(Pr(T+|K+)\\). Der Test erkennt also die meisten Krebsfälle, und ein paar werden übersehen.\nManchmal macht der Test auch den umgekehrten Fehler: Ein gesunder Mensch wird fälschlich Krebs diagnostiziert, \\(Pr(T+|K-)\\). Diese Wahrscheinlichkeit liegt bei dem Test bei 0.1, zum Glück also relativ gering.\nDie Grundrate dieser Krebsart belaufe sich in der Population auf 0.01, \\(Pr(K+)\\).\nAufgabe: Berechnen Sie die Wahrscheinlichkeit, dass ein Patient tatsächlich Krebs hat, wenn der Test positiv ist, also Krebs diagnostiziert hat!\n         \n\n\nLösung\nHier kann man Bayes Theorem anwenden:\n\\(Pr(K|T) = \\frac{Pr(K) \\cdot Pr(T|K) }{Pr(T)}\\).\n\nzaehler_bayes &lt;- Pr_Kpos * Pr_Tpos_geg_Kpos\nzaehler_bayes\n\n[1] 0.009\n\nPr_T &lt;- (zaehler_bayes + (1-Pr_Kpos) * Pr_Tpos_geg_Kneg)\nPr_T\n\n[1] 0.108\n\nsol &lt;- Pr_Kpos_geg_Tpos &lt;- zaehler_bayes / Pr_T \nsol &lt;- round(sol, 2)\nsol\n\n[1] 0.08\n\n\nDie Lösung beträgt also: 0.08.\nHier ist ein Baumdiagramm zur Visualisierung:\n\n\n\n\n\nflowchart LR\n  S[1000 Personen] --&gt; K[Krebs: 10]\n  S --&gt; NK[Nicht-Krebs: 990]\n  K --&gt; T[Test positiv: 9]\n  NK --&gt; NT[Nicht Test positiv: 1]\n  NK --&gt; TNK[Test positiv: 99]\n  NK --&gt; NTNK[Nicht Test positiv: 891]\n\n\n\n\n\n\n\nCategories:\n\nbayes\nprobability\nnum"
  },
  {
    "objectID": "posts/nasa03/nasa03.html",
    "href": "posts/nasa03/nasa03.html",
    "title": "nasa03",
    "section": "",
    "text": "Aufgabe\nViele Quellen berichten Klimadaten unserer Erde, z.B. auch National Aeronautics and Space Administration - Goddard Institute for Space Studies.\nVon dieser Quelle beziehen wir diesen Datensatz.\nDie Datensatz sind auf der Webseite wie folgt beschrieben:\nTables of Global and Hemispheric Monthly Means and Zonal Annual Means\nCombined Land-Surface Air and Sea-Surface Water Temperature Anomalies (Land-Ocean Temperature Index, L-OTI)\nThe following are plain-text files in tabular format of temperature anomalies, i.e. deviations from the corresponding 1951-1980 means.\n\nGlobal-mean monthly, seasonal, and annual means, 1880-present, updated through most recent month: TXT, CSV\n\nStarten Sie zunächst das R-Paket tidyverse falls noch nicht geschehen.\n\nlibrary(tidyverse)\n\nImportieren Sie dann die Daten:\n\ndata_path &lt;- \"https://data.giss.nasa.gov/gistemp/tabledata_v4/GLB.Ts+dSST.csv\"\nd &lt;- read_csv(data_path, skip = 1)\n\nWir lassen die 1. Zeile des Datensatzes aus (Argument skip), da dort Metadaten stehen, also keine Daten, sondern Informationen (Daten) zu den eigentlichen Daten.\nAufgaben\n\nErstellen Sie für jeden Januar eine Variable (temp_is_above), die ausgibt, ob die Temperatur über oder unter dem Durchschnitt liegt (d.h. eine negative oder positive Abweichung ist). Nutzen Sie als Ausprägungen die Werte “yes” und “no”.\nErstellen Sie dann eine Variable, die das Jahrhundert angibt (19. JH. vs. 20 JH).\nZählen Sie dann wie oft temp_is_above “yes” aufweist pro Jahrhundert (“erhöhter Temperatur”).\nBONUSAUFGABE: Berechnen Sie das Odds Ratio (Chancenverhältnis) von erhöhter Temperatur (vs. nicht erhöhter Temperatur) zwischen dem 19. und dem 20. Jahrhundert.\n\nHinweise:\nFür “Wenn-Dann-Abfragen” eignet sich folgender R-Befehl (als “Pseudocode” dargestellt):\n\nd %&gt;% \n  mutate(neue_spalte = case_when(\n    erste_bedingung_bzw_wenn_teil ~ dann_teil1,\n    zweite_bedingung_bzw_zweiter_wenn_teil ~ dann_teil2\n  ))\n\nEs finden sich online viele Hilfsangebote zu case_when, falls Sie weitere Informationen benötigen.\nUnter Odds Ratio versteht man den Quotienten zweier Quotienten. Das hört sich zu theoretisch an? Betrachten wir ein Beispiel. Sagen wir, mit Impfung sei das Risiko für eine bestimmte Erkrankung 1:1000; ohne Impfung aber 1:100.\nAlso: Hundert-zu-eins steht gegen tausend-zu-eins. Das ist ein Faktor von zehn bzw. von ein Zehntel, jenachdem wie rum man den Bruch (Quotient) betrachtet.\n\\[\\frac{\\frac{1}{100}}{\\frac{1}{1000}} = \\frac{1}{10}\\]\n         \n\n\nLösung\ntemp_is_above erstellen:\n\nd &lt;-\n  d %&gt;% \n  mutate(temp_is_above = case_when(\n    Jan &gt; 0 ~ \"yes\",\n    Jan &lt;= 0 ~ \"no\"\n  ))\n\nJahrhundert berechnen:\n\nd &lt;-\n  d %&gt;% \n  mutate(century = case_when(\n    Year &lt; 1900 ~ \"19th\",\n    Year &gt;= 1900 ~ \"20th\"\n  ))\n\nErhöhte Werte der Januar-Temperatur pro Jahrhundert berechnen:\n\nd_summarized &lt;- \nd %&gt;% \n  group_by(century) %&gt;% \n  count(temp_is_above)\n\nd_summarized\n\n# A tibble: 4 × 3\n# Groups:   century [2]\n  century temp_is_above     n\n  &lt;chr&gt;   &lt;chr&gt;         &lt;int&gt;\n1 19th    no               19\n2 19th    yes               1\n3 20th    no               56\n4 20th    yes              69\n\n\nDer Befehl count() zählt aus, wie häufig die Ausprägungen der angegebenen Variablen X sind, m.a.W. er gibt die Verteilung von X wieder.\nEs macht vermutlich Sinn, noch die Anteile (relative Häufigkeiten) zu den absoluten Häufigkeiten zu ergänzen:\n\nd_summarized %&gt;% \n  mutate(prop = n / sum(n))\n\n# A tibble: 4 × 4\n# Groups:   century [2]\n  century temp_is_above     n  prop\n  &lt;chr&gt;   &lt;chr&gt;         &lt;int&gt; &lt;dbl&gt;\n1 19th    no               19 0.95 \n2 19th    yes               1 0.05 \n3 20th    no               56 0.448\n4 20th    yes              69 0.552\n\n\nOdds Ratio berechnen:\nWir bezeichnen mit c19 (für “Chance 1”) das Verhältnis von erhöhter Temperatur zu nicht erhöhter Temperatur im 19. Jahrhundert.\n\nc19 &lt;- 1 / 19\n\nMit c20 bezeichnen wir die analoge Chance für das 20. Jahrhundert:\n\nc20 &lt;- 56 / 67\n\nDas Verhältnis der beiden Chancen gibt das Chancenverhältnis (Odds Ratio, OR):\n\nc19 / c20\n\n[1] 0.06296992\n\n\nGenauso gut kann man das OR von c20 zu c19 ausrechnen, der Effekt bleibt identisch:\n\nc20 / c19\n\n[1] 15.8806\n\n\nIn beiden Fällen ist es ein Faktor von knapp 16.\n\nCategories:\n\ndata\neda\nassociation\nstring"
  },
  {
    "objectID": "posts/wskt-quiz15/wskt-quiz15.html",
    "href": "posts/wskt-quiz15/wskt-quiz15.html",
    "title": "wskt-quiz15",
    "section": "",
    "text": "Behauptung:\n\\(Pr(BA) = Pr(B|A) \\cdot Pr(A)\\).\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n\n\n\nFalsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz15/wskt-quiz15.html#answerlist",
    "href": "posts/wskt-quiz15/wskt-quiz15.html#answerlist",
    "title": "wskt-quiz15",
    "section": "",
    "text": "Falsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz15/wskt-quiz15.html#answerlist-1",
    "href": "posts/wskt-quiz15/wskt-quiz15.html#answerlist-1",
    "title": "wskt-quiz15",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\nquiz\nprobability\nbayes\nquiz1-qm2-ws23\nschoice"
  },
  {
    "objectID": "posts/movie-sentiment1/movie-sentiment1.html",
    "href": "posts/movie-sentiment1/movie-sentiment1.html",
    "title": "movie-sentiment1",
    "section": "",
    "text": "Eine typische Aufgabe des Textminings ist die Sentimentanalyse. Betrachten wir dazu einen Datensatz des Filmbewertungsportal IMDB. Das Portal veröffentlicht Bewertungen (quantitativ und qualitativ, d.h. als Score oder Bewertung/Review) zu Filmen der Nutzerinnen und Nutzer. Der Datensatz kann über Kaggle bezogen werden.\nIm Rahmen einer Fallstudie soll eine Sentimentanalyse wie folgt abgearbeitet werden:\n\nDaten in R importieren\nRelevante Spalten auswählen (die die Reviews der Nutzer enthalten)\nDaten in das “Tidytext-Format” überführen\nNicht-Wörter (z.B. Zahlen) entfernen\nStopwörter entfernen\nSentimentanalyse durchführen zur Identifikation der Grundemotionen\nVisualisierung der Intensität der Emotionen der 10 häufigsten Wörter (sortierte Balken)\n\nHinweise:\n\nHier ist nur ein Teil des Datensatzes dargestellt (aus Gründen der Einfachheit).\nGehen Sie davon aus, dass die Daten unter dem Pfad verfügbar sind, der in dieser Variable gespeichert ist: path_to_data. Die relevanten Spalten sind dort schon ausgewählt.\n\nWelcher der folgenden R-Syntaxen führt diese Analyse korrekt aus? Wählen Sie die am besten passende Antwort!\n\n\nWarning in inner_join(., emo): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 18 of `x` matches multiple rows in `y`.\nℹ Row 11267 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\nOption A\n\n\n\nlibrary(tidytext) \nlibrary(tidyverse)\n\nd &lt;- read_csv(path_to_data)\n\nstopwords &lt;- get_stopwords()\nemo &lt;- get_sentiments('nrc')\n\nwordcount_plot1 &lt;- \nd %&gt;% \n  select(review) %&gt;% \n  unnest_tokens(word, review) %&gt;% \n  filter(str_detect(word, '[a-z]+')) %&gt;% \n  anti_join(stopwords) %&gt;% \n  inner_join(emo) %&gt;% \n  count(word, sort = TRUE) %&gt;% \n  slice_head(n = 10) %&gt;% \n  mutate(word = fct_reorder(word, n)) %&gt;% \n  ggplot(aes(n, word)) +\n  geom_col()\n\n\nOption B\n\n\n\nlibrary(tidytext)\nlibrary(tidyverse)\n\nd &lt;- read_csv(path_to_data)\n\nstopwords &lt;- get_stopwords()\nemo &lt;- get_sentiments('afinn')\n\nwordcount_plot1 &lt;- \nd %&gt;% \n  select(review) %&gt;% \n  unnest_tokens(output = word, input = review) %&gt;% \n  filter(str_detect(word, '[a-z]+')) %&gt;% \n  left_join(stopwords) %&gt;% \n  inner_join(emo) %&gt;% \n  count(word, sort=TRUE) %&gt;% \n  slice_head(n = 10) %&gt;% \n  mutate(word = fct_reorder(word, n)) %&gt;% \n  ggplot(aes(x = n, y = word)) +\n  geom_col()\n\n\nOption C\n\n\n\nlibrary(tidytext)\nlibrary(tidyverse)\n\nd &lt;- read_csv(path_to_data)\n\nstopwords &lt;- get_stopwords()\nemo &lt;- get_sentiments('loughran')\n\nwordcount_plot1 &lt;- \nd %&gt;% \n  select(review) %&gt;% \n  unnest_tokens(word, review) %&gt;% \n  filter(str_detect(word, '\\w.')) %&gt;% \n  anti_join(stopwords) %&gt;% \n  inner_join(emo) %&gt;% \n  count(word, sort = TRUE) %&gt;% \n  slice_head(n = 10) %&gt;% \n  mutate(word = fct_reorder(word, n)) %&gt;% \n  ggplot(aes(n, word)) +\n  geom_col()\n\n\nOption D\n\n\n\nlibrary(tidytext)\nlibrary(tidyverse)\n\nd &lt;- read_csv(path_to_data)\n\nstopwords &lt;- get_stopwords()\nemo &lt;- get_sentiments('nrc')\n\nwordcount_plot1 &lt;- \nd %&gt;% \n  select(review) %&gt;% \n  unnest_tokens(word, review) %&gt;% \n  filter(str_detect(word, '[a-z]+')) %&gt;% \n  left_join(stopwords) %&gt;% \n  inner_join(emo) %&gt;% \n  count(word, sort = TRUE) %&gt;% \n  slice(n = 10) %&gt;% \n  ggplot(aes(x = n, y = word)) +\n  geom_bar()\n  \n\n\nOption E\n\n\nkeine der genannten\n\n\n\n\n\nSyntax A\nSyntax B\nSyntax C\nSyntax D\nSyntax E"
  },
  {
    "objectID": "posts/movie-sentiment1/movie-sentiment1.html#answerlist",
    "href": "posts/movie-sentiment1/movie-sentiment1.html#answerlist",
    "title": "movie-sentiment1",
    "section": "",
    "text": "Syntax A\nSyntax B\nSyntax C\nSyntax D\nSyntax E"
  },
  {
    "objectID": "posts/movie-sentiment1/movie-sentiment1.html#answerlist-1",
    "href": "posts/movie-sentiment1/movie-sentiment1.html#answerlist-1",
    "title": "movie-sentiment1",
    "section": "Answerlist",
    "text": "Answerlist\n\nRichtig\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ntextmining\nimdb\nschoice"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-07/Verteilungen-Quiz-07.html",
    "href": "posts/Verteilungen-Quiz-07/Verteilungen-Quiz-07.html",
    "title": "Verteilungen-Quiz-07",
    "section": "",
    "text": "Ist folgende Aussage wahr?\nSei \\(X \\sim N(100,15)\\), dann ist \\(Pr(X \\le 100) \\ne 1/2\\).\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-07/Verteilungen-Quiz-07.html#answerlist",
    "href": "posts/Verteilungen-Quiz-07/Verteilungen-Quiz-07.html#answerlist",
    "title": "Verteilungen-Quiz-07",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-07/Verteilungen-Quiz-07.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-07/Verteilungen-Quiz-07.html#answerlist-1",
    "title": "Verteilungen-Quiz-07",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/posterior_interval/posterior_interval.html",
    "href": "posts/posterior_interval/posterior_interval.html",
    "title": "posterior_interval",
    "section": "",
    "text": "Welches Ergebnis hat der R-Befehl posterior_interval() (R-Paket rstanarm)?\nWählen Sie die (am besten) passende Antwort aus.\nHinweis:\n\nSoweit nicht anders benannt, ist immer die Voreinstellung der betreffenden Funktion gemeint.\n\n\n\n\nEr liefert einen Vorhersagewert aus der Posteriori-Verteilung.\nEr liefert ein Vorhersageintervall aus der Posteriori-Verteilung.\nEr liefert ein 90%-Vorhersageintervall aus der Posteriori-Verteilung.\nEr liefert ein 95%-Vorhersageintervall aus der Posteriori-Verteilung.\nEr liefert ein HDI-Vorhersageinterval aus der Posteriori-Verteilung."
  },
  {
    "objectID": "posts/posterior_interval/posterior_interval.html#answerlist",
    "href": "posts/posterior_interval/posterior_interval.html#answerlist",
    "title": "posterior_interval",
    "section": "",
    "text": "Er liefert einen Vorhersagewert aus der Posteriori-Verteilung.\nEr liefert ein Vorhersageintervall aus der Posteriori-Verteilung.\nEr liefert ein 90%-Vorhersageintervall aus der Posteriori-Verteilung.\nEr liefert ein 95%-Vorhersageintervall aus der Posteriori-Verteilung.\nEr liefert ein HDI-Vorhersageinterval aus der Posteriori-Verteilung."
  },
  {
    "objectID": "posts/posterior_interval/posterior_interval.html#answerlist-1",
    "href": "posts/posterior_interval/posterior_interval.html#answerlist-1",
    "title": "posterior_interval",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nWahr\nFalsch\nFalsch\n\n\nCategories:\n\nbayes\nregression\npost"
  },
  {
    "objectID": "posts/wuerfel05/wuerfel05.html",
    "href": "posts/wuerfel05/wuerfel05.html",
    "title": "wuerfel05",
    "section": "",
    "text": "Aufgabe\nWas ist die Wahrscheinlichkeit, mit zwei fairen Würfeln genau 12 Augen zu werfen?\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n         \n\n\nLösung\nDie Ereignisse A=“Wurf des 1. Würfels” und B=“Wurf des 2. Würfels” sind unabhängig voneinander.\nDaher gilt: \\(Pr(A\\cap B) = Pr(AB) = Pr(A) \\cdot Pr(B)\\).\n\nPr_AB &lt;- 1/6 * 1/6\nPr_AB\n\n[1] 0.02777778\n\n\nAufgrund der Unabhängigkeit gilt außerdem: \\(Pr(A|B) = Pr(A) = Pr(A|\\neg B).\\)\nDie Lösung lautet 0.0277778.\n\nCategories:\n\nR\nprobability\nnum"
  },
  {
    "objectID": "posts/CV1/CV1.html",
    "href": "posts/CV1/CV1.html",
    "title": "CV1",
    "section": "",
    "text": "Kreuzvalidierung (CV) ist ein gängiges Verfahren, um die Vorhersagegüte in einem Testdatensatz einzuschätzen und um Parameterwerte (Tuningparameter) auszuwählen. Welche der folgenden Aussagen zur Kreuzvalidierung ist richtig?\n\n\n\nBei der Kreuzvalidierung können unterschiedliche Fehlerwerte (z.B. \\(MSE\\)) resultieren in Abhängigkeit von der (zufälligen) Aufteilung in die jeweiligen Train- und Validierungsstichprobe.\nBei der Anzahl der Faltungen (folds) sollte stets \\(k=10\\) gewählt werden.\nBei der Anzahl der Faltungen (folds) sollte stets \\(k=5\\) gewählt werden.\nDie \\(k\\)-fache Kreuzvalidierung ist rechenintensiver als die LOOCV-Methode.\nDie wiederholte \\(k\\)-fache Kreuzvalidierung erhöht die Gefahr des Overfittings und sollte daher vermieden werden."
  },
  {
    "objectID": "posts/CV1/CV1.html#answerlist",
    "href": "posts/CV1/CV1.html#answerlist",
    "title": "CV1",
    "section": "",
    "text": "Bei der Kreuzvalidierung können unterschiedliche Fehlerwerte (z.B. \\(MSE\\)) resultieren in Abhängigkeit von der (zufälligen) Aufteilung in die jeweiligen Train- und Validierungsstichprobe.\nBei der Anzahl der Faltungen (folds) sollte stets \\(k=10\\) gewählt werden.\nBei der Anzahl der Faltungen (folds) sollte stets \\(k=5\\) gewählt werden.\nDie \\(k\\)-fache Kreuzvalidierung ist rechenintensiver als die LOOCV-Methode.\nDie wiederholte \\(k\\)-fache Kreuzvalidierung erhöht die Gefahr des Overfittings und sollte daher vermieden werden."
  },
  {
    "objectID": "posts/CV1/CV1.html#answerlist-1",
    "href": "posts/CV1/CV1.html#answerlist-1",
    "title": "CV1",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nstatlearning\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/wuerfel02/wuerfel02.html",
    "href": "posts/wuerfel02/wuerfel02.html",
    "title": "wuerfel02",
    "section": "",
    "text": "Exercise\nWas ist die Wahrscheinlichkeit, mit zwei fairen Würfeln mindestens 10 Augen zu werfen?\nHinweise:\n\nNutzen Sie exakte Methoden der Wahrscheinlichkeitsrechnung, keine Simulation.\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\n\n         \n\n\nSolution\nErstellen wir uns eine Tabelle, die alle Permutationen der beiden Würfelergebnisse fasst, das sind 36 Paare: (1,1), (1,2), …, (1,6), …, (6,6).\nDas kann man von Hand erstellen, halbautomatisch in Excel oder z.B. so:\n\nlibrary(tidyverse)\nd &lt;- expand_grid(wuerfel1 = 1:6,\n         wuerfel2 = 1:6)\n\nd\n\n# A tibble: 36 × 2\n   wuerfel1 wuerfel2\n      &lt;int&gt;    &lt;int&gt;\n 1        1        1\n 2        1        2\n 3        1        3\n 4        1        4\n 5        1        5\n 6        1        6\n 7        2        1\n 8        2        2\n 9        2        3\n10        2        4\n# ℹ 26 more rows\n\n\nJetzt ergänzen wir eine Spalte für die Wahrscheinlichkeit jeder Kombination, das ist einfach, denn \\(p(A \\cap B) = p(A) \\cdot p(B) = 1/36\\) gilt.\n\nd2 &lt;-\n  d %&gt;% \n  mutate(prob = 1/36)\n\nhead(d2)\n\n# A tibble: 6 × 3\n  wuerfel1 wuerfel2   prob\n     &lt;int&gt;    &lt;int&gt;  &lt;dbl&gt;\n1        1        1 0.0278\n2        1        2 0.0278\n3        1        3 0.0278\n4        1        4 0.0278\n5        1        5 0.0278\n6        1        6 0.0278\n\n\nAußerdem ergänzen wir die Summe der Augenzahlen, weil die Frage ja nach einer bestimmten Summe an Augenzahlen abzielt.\n\nd3 &lt;-\n  d2 %&gt;% \n  mutate(augensumme = wuerfel1 + wuerfel2)\n\nhead(d3)\n\n# A tibble: 6 × 4\n  wuerfel1 wuerfel2   prob augensumme\n     &lt;int&gt;    &lt;int&gt;  &lt;dbl&gt;      &lt;int&gt;\n1        1        1 0.0278          2\n2        1        2 0.0278          3\n3        1        3 0.0278          4\n4        1        4 0.0278          5\n5        1        5 0.0278          6\n6        1        6 0.0278          7\n\n\nFür manche Augensummen gibt es mehrere Möglichkeiten:\n\nd3 %&gt;% \n  filter(augensumme == 7)\n\n# A tibble: 6 × 4\n  wuerfel1 wuerfel2   prob augensumme\n     &lt;int&gt;    &lt;int&gt;  &lt;dbl&gt;      &lt;int&gt;\n1        1        6 0.0278          7\n2        2        5 0.0278          7\n3        3        4 0.0278          7\n4        4        3 0.0278          7\n5        5        2 0.0278          7\n6        6        1 0.0278          7\n\n\n… für andere weniger:\n\nd3 %&gt;% \n  filter(augensumme == 12)\n\n# A tibble: 1 × 4\n  wuerfel1 wuerfel2   prob augensumme\n     &lt;int&gt;    &lt;int&gt;  &lt;dbl&gt;      &lt;int&gt;\n1        6        6 0.0278         12\n\n\nJetzt summieren wir (nach dem Additionssatz der Wahrscheinlichkeit) die Wahrscheinlichkeiten pro Augenzahl:\n\nd4 &lt;- \n  d3 %&gt;% \n  group_by(augensumme) %&gt;% \n  summarise(totale_w_pro_augenzahl = sum(prob))\n\nd4\n\n# A tibble: 11 × 2\n   augensumme totale_w_pro_augenzahl\n        &lt;int&gt;                  &lt;dbl&gt;\n 1          2                 0.0278\n 2          3                 0.0556\n 3          4                 0.0833\n 4          5                 0.111 \n 5          6                 0.139 \n 6          7                 0.167 \n 7          8                 0.139 \n 8          9                 0.111 \n 9         10                 0.0833\n10         11                 0.0556\n11         12                 0.0278\n\n\nTest: Die Summe der Wahrscheinlichkeit muss insgesamt 1 sein.\n\nd4 %&gt;% \n  summarise(sum(totale_w_pro_augenzahl))\n\n# A tibble: 1 × 1\n  `sum(totale_w_pro_augenzahl)`\n                          &lt;dbl&gt;\n1                             1\n\n\nUnd:\n\nd2 %&gt;% \n  summarise(sum(prob))\n\n# A tibble: 1 × 1\n  `sum(prob)`\n        &lt;dbl&gt;\n1           1\n\n\nPasst!\nDie Wahrscheinlichkeit für die Augensumme von mind. 10 beträgt also:\n\nloesung &lt;-\n  d4 %&gt;% \n  filter(augensumme &gt;= 10) %&gt;% \n  summarise(prob_sum = sum(totale_w_pro_augenzahl)) %&gt;% \n  pull(prob_sum)\n\nloesung\n\n[1] 0.1666667\n\n\n\nCategories:\n\nprobability\ndice"
  },
  {
    "objectID": "posts/Typ-Fehler-R-04/Typ-Fehler-R-04.html",
    "href": "posts/Typ-Fehler-R-04/Typ-Fehler-R-04.html",
    "title": "Typ-Fehler-R-04",
    "section": "",
    "text": "Aufgabe\nGegeben sei diese Syntax, die einen Fehlermeldung ausgibt:\n\nmean(c(1,2,3,4). na.rm = TRUE)\n\nError: &lt;text&gt;:1:16: unexpected symbol\n1: mean(c(1,2,3,4).\n                   ^\n\n\nGeben Sie die korrekte Syntax ein, die nicht zu einer Fehlermeldung führt!\nBitte verwenden Sie keine Leerzeichen bei Ihrer Eingabe.\n         \n\n\nLösung\n\nmean(c(1,2,3,4), na.rm = TRUE)\n\n[1] 2.5\n\n\n\nsol &lt;- \"mean(c(1,2,3,4),na.rm=TRUE)\"\n\nDie Antwort lautet: mean(c(1,2,3,4),na.rm=TRUE).\n\nCategories:\n\nR\n‘2023’\nstring"
  },
  {
    "objectID": "posts/iq08/iq08.html",
    "href": "posts/iq08/iq08.html",
    "title": "iq08",
    "section": "",
    "text": "Aufgabe\nAn einer Elite-Hochschule wird man nur zugelassen, wenn man sowohl schön als auch schlau ist.\n“Schön” sei definiert als eine SD-Einheit über dem mittleren Aussehen, unter der Annahme, dass Aussehen normalverteilt ist.\n“Schlau” sei definiert als eine SD-Einheit über dem mittleren Wert, unter der Annahme, dass die Variable normalverteilt ist.\nWie hoch ist die Wahrscheinlichkeit, an dieser Elite-Uni zugelassen zu werden?\nHinweise:\n\nNutzen Sie Simulationsmethoden.\nGehen Sie von folgender Verteilung für Schönheit und für Schlauheit aus: \\(X \\sim N(0,1)\\)\nIntelligenz und Schönheit sollen als unabhängig angenommen werden.\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\nSimulieren Sie \\(n=10^4\\) Stichproben.\nNutzen Sie die Zahl 42 als Startwert für Ihre Zufallszahlen (um die Reproduzierbarkeit zu gewährleisten).\nWeitere Hinweise\n\n         \n\n\nLösung\nDie Wahrscheinlichkeit für “schön”, \\(S1\\) ist gleich der Wahrscheinlichkeit für “Schlau”, \\(S2\\).\n\nlibrary(tidyverse)\n\nWir simulieren die Daten:\n\nset.seed(42)\n\nd &lt;- tibble(\n  id = 1:10^4,\n  schoenheit = rnorm(n = 10^4, mean = 0, sd = 1),\n  schlauheit = rnorm(n = 10^4, mean = 0, sd = 1))\n\nDa es nur um Anteile (bzw. Wahrscheinlichkeiten) der Population geht, können wir mit z-Werten arbeiten.\nZur Erinnerung: Ein z-Wert von 1 bedeutet, dass der Messwert eine SD-Einheit größer ist als der Mittelwert der Verteilung.\nDann filtern wir wie in der Angabe gefragt:\n\nd2 &lt;-\n  d %&gt;% \n  count(schoenheit &gt; 1, schlauheit &gt; 1) %&gt;%  # Das Komma wird als logisches UND interpretiert\n  mutate(prop = n / sum(n))\n\nd2\n\n# A tibble: 4 × 4\n  `schoenheit &gt; 1` `schlauheit &gt; 1`     n  prop\n  &lt;lgl&gt;            &lt;lgl&gt;            &lt;int&gt; &lt;dbl&gt;\n1 FALSE            FALSE             7082 0.708\n2 FALSE            TRUE              1364 0.136\n3 TRUE             FALSE             1314 0.131\n4 TRUE             TRUE               240 0.024\n\n\nWieder nehmen wir den Anteil her und bezeichnen ihn als Wahrscheinlichkeit. Das ist eine schöne Sache dieser Simulationsmethoden: Es vereinfacht die Angelegenheit, denn mit Häufigkeiten lässt sich einfacher hantieren als mit Wahrscheinlichkeiten. Und die Anteile erfüllen die Kolmogorov-Axiome, wir können also beruhigt rechnen. Falls Sie also vor Sorge um die Reinheit der Mathematik nicht schlafen konnten, kann ich Sie insofern beruhigen :-)\nNatürlich könnte man die Frage auch analytisch lösen (mit dem Multiplikationssatz für unabhängige Ereignisse).\nDie Wahrscheinlichkeit für einen Wert \\(x &gt;= 115, X \\sim N(100,15)\\) beträgt:\n\npr_1sd_ueber_mw &lt;- 1- pnorm(115, 100, 15)\npr_1sd_ueber_mw\n\n[1] 0.1586553\n\n\nDannn:\n\npr_1sd_ueber_mw * pr_1sd_ueber_mw\n\n[1] 0.02517149\n\n\nAntwort: Die Lösung lautet also 0.024.\nInteressant ist es vielleicht, die Gesamtpopulation zu visualisieren:\n\nd %&gt;% \n  mutate(ist_schoen = if_else(schoenheit &gt; 1, TRUE, FALSE),\n         ist_schlau = if_else(schlauheit &gt; 1, TRUE, FALSE),\n         ist_schoen_schlau = if_else(ist_schoen & ist_schlau, TRUE, FALSE)) %&gt;% \n  ggplot() +\n  aes(x = schoenheit, y = schlauheit, color = ist_schoen_schlau, alpha = .1) +\n  geom_point()\n\n\n\n\n\n\n\n\nWäre die Aufnahmeregel, dass es reichte, entweder schön oder schlau (beides ist auch ok) zu sein, wäre der Anteil an zugelassenen Personen größer:\n\nd3 &lt;-\n  d %&gt;% \n  count(schoenheit &gt; 1 | schlauheit &gt; 1) %&gt;%  # der horizontale Balken steht für das logische ODER.\n  mutate(prop = n / sum(n))\n\nd3\n\n# A tibble: 2 × 3\n  `schoenheit &gt; 1 | schlauheit &gt; 1`     n  prop\n  &lt;lgl&gt;                             &lt;int&gt; &lt;dbl&gt;\n1 FALSE                              7082 0.708\n2 TRUE                               2918 0.292\n\n\n\nCategories:\n\nprobability\nsimulation\nnormal-distribution\nnum"
  },
  {
    "objectID": "posts/import-xls/import-xls.html",
    "href": "posts/import-xls/import-xls.html",
    "title": "import-xls",
    "section": "",
    "text": "Question\nImportieren Sie in R die Excel-Datei extra.xls.\nDie Daten liegen online z.B. hier: https://github.com/sebastiansauer/statistik1/raw/main/daten/extra.xls\nEs handelt sich um die Daten einer Umfrage zu den Korrelaten von Extraversion.\nEin Daten-Dictionary finden Sie hier.\nMehr Hinweise zu der zugrundeliegenden Studie finden Sie hier: https://osf.io/4kgzh.\n\n\nSolution\nEs gibt verschiedene Wege, Excel-Daten in R zu importieren. Hier ist ein Weg, mit Hilfe des Pakets {rio}:\n\nlibrary(rio)\nextra_path &lt;- \"https://github.com/sebastiansauer/statistik1/raw/main/daten/extra.xls\"\nextra &lt;- import(extra_path)\n\nTest:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nglimpse(extra)\n\nRows: 40\nColumns: 25\n$ X        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ i1       &lt;chr&gt; \"3/3/2016 15:33:23\", \"3/3/2016 15:33:28\", \"3/3/2016 15:34:43\"…\n$ i2       &lt;chr&gt; NA, \"FBR\", \"CZP\", NA, \"ism\", \"Asm\", \"TPF\", \"Oam\", \"AEM\", NA, …\n$ i3       &lt;dbl&gt; 3, 3, 4, 4, 4, 4, 3, 2, 3, 4, 3, 4, 3, 4, 3, 3, 3, 4, 4, 4, 4…\n$ i4       &lt;dbl&gt; 3, 3, 3, 3, 4, 3, 3, 2, 3, 3, 3, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4…\n$ i5       &lt;dbl&gt; 2, 3, 3, 3, 2, 3, 1, 3, 1, 2, 1, 1, 1, 3, 2, 4, 1, 2, 3, 3, 2…\n$ i6       &lt;dbl&gt; 2, 4, 4, 3, 4, 4, 3, 3, 4, 3, 3, 3, 4, 3, 4, 4, 2, 3, 3, NA, …\n$ i7       &lt;dbl&gt; 2, 3, 4, 3, 4, 3, 3, 3, 4, 3, 3, 2, 4, 4, 4, 3, 2, 3, 3, 4, 3…\n$ i8       &lt;dbl&gt; 3, 3, 4, 4, 3, 3, 4, 2, 3, 3, 3, 4, 3, 4, 4, 3, 4, 4, 3, 3, 3…\n$ i9       &lt;dbl&gt; 3, 2, 3, 3, 3, 4, 3, 1, 3, 3, 3, 2, 3, 3, 2, 4, 2, 4, 3, 3, 2…\n$ i10      &lt;dbl&gt; 2, 2, 3, 4, 3, 3, 4, 3, 3, 3, 4, 4, 4, 3, 3, 3, 4, 4, 3, 3, 2…\n$ i11      &lt;dbl&gt; 3, 3, 3, 4, 3, 4, 4, 1, 3, 4, 3, 4, 3, 4, 3, 3, 2, 3, 4, 3, 3…\n$ i12      &lt;dbl&gt; 1, 2, 2, 4, 2, 3, 2, 2, 2, 1, 3, 2, 3, 2, 2, 3, 2, 2, 2, 2, 1…\n$ i13      &lt;dbl&gt; 250, 294, 600, 500, 300, 350, 608, NA, 500, 521, 280, 250, 12…\n$ i14      &lt;dbl&gt; 1, 2, 0, 0, 0, 10, 2, 6, 10, 12, 12, 12, 20, 1, 3, 25, 0, 40,…\n$ i15      &lt;dbl&gt; 24, 28, 24, 20, 20, 23, 24, 24, 21, 25, 23, 22, 23, 21, 22, 2…\n$ i16      &lt;chr&gt; \"Frau\", \"Frau\", \"Frau\", \"Frau\", \"Frau\", \"Frau\", \"Frau\", \"Mann…\n$ i17      &lt;dbl&gt; 1, 3, 3, 3, 3, 3, 2, 3, 3, 2, 2, 2, 4, 3, 3, 4, 2, 3, 3, 3, 2…\n$ i18      &lt;dbl&gt; NA, 27, NA, 7, 30, 15, 10, 15, 10, 10, 10, 10, 15, 1, 5, 1, 1…\n$ i19      &lt;chr&gt; \"nein\", \"ja\", \"nein\", \"nein\", \"nein\", \"nein\", \"nein\", \"nein\",…\n$ i20      &lt;dbl&gt; 6, 1, 30, 4, 5, 20, 30, 6, 5, 12, 30, 12, 22, 5, 15, 40, 48, …\n$ i21      &lt;chr&gt; \"im Schnitt 1 Mal pro Quartal (oder weniger)\", \"im Schnitt me…\n$ i22      &lt;chr&gt; \"keine Antwort\", \"passt insgesamt nicht\", \"passt insgesamt\", …\n$ i23      &lt;dbl&gt; 6, 4, 3, 3, 3, 3, 5, 4, 6, 5, 4, 6, 2, 3, 5, 2, 8, 6, 2, 4, 5…\n$ extra_mw &lt;dbl&gt; 2.400000, 2.800000, 3.300000, 3.500000, 3.200000, 3.400000, 3…"
  },
  {
    "objectID": "posts/iq06/iq06.html",
    "href": "posts/iq06/iq06.html",
    "title": "iq06",
    "section": "",
    "text": "Aufgabe\nIntelligenz wird häufig mittels einem IQ-Test ermittelt.\nWie wahrscheinlich ist es, zur Gruppe der “durchschnittlich intelligenten” Menschen gehören?\nDabei sei “durchschnittlich intelligent” definiert als der Intelligenzwert \\(X\\), für den gilt \\(x-\\sigma &lt; x &lt; x + \\sigma\\).\nHinweise:\n\nNutzen Sie Simulationsmethoden.\nGehen Sie von folgender IQ-Verteilung aus: \\(IQ \\sim N(100,15)\\)\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\nSimulieren Sie \\(n=10^3\\) Stichproben.\nNutzen Sie die Zahl 42 als Startwert für Ihre Zufallszahlen (um die Reproduzierbarkeit zu gewährleisten)\n\n         \n\n\nLösung\n\nlibrary(tidyverse)\n\nWir simulieren die Daten:\n\nset.seed(42)\nk &lt;- 3\nd &lt;- tibble(\n  id = 1:10^3,\n  iq = rnorm(n = 10^3, mean = 100, sd = 15))\n\nWir filtern die schlauesten 0,1 Prozent:\n\nd %&gt;% \n  count(iq &gt; 85 & iq &lt; 115) \n\n# A tibble: 2 × 2\n  `iq &gt; 85 & iq &lt; 115`     n\n  &lt;lgl&gt;                &lt;int&gt;\n1 FALSE                  327\n2 TRUE                   673\n\n\nDie Antwort auf die Frage\nWie wahrscheinlich ist es, zur Gruppe der “durchschnittlich intelligenten” Menschen gehören?,\nlautet also 0.673.\n\nCategories:\n\nprobability\nsimulation\nnormal-distribution\nnum"
  },
  {
    "objectID": "posts/r-quiz/r-quiz.html",
    "href": "posts/r-quiz/r-quiz.html",
    "title": "r-quiz",
    "section": "",
    "text": "ExerciseSolution\n\n\nDefine in R the variable age and assign the value 42.\n\n\n\nage &lt;- 42\n\nNote that spaces here are not mandatory, but useful."
  },
  {
    "objectID": "posts/r-quiz/r-quiz.html#define-a-variable",
    "href": "posts/r-quiz/r-quiz.html#define-a-variable",
    "title": "r-quiz",
    "section": "",
    "text": "ExerciseSolution\n\n\nDefine in R the variable age and assign the value 42.\n\n\n\nage &lt;- 42\n\nNote that spaces here are not mandatory, but useful."
  },
  {
    "objectID": "posts/r-quiz/r-quiz.html#define-a-variable-as-a-string",
    "href": "posts/r-quiz/r-quiz.html#define-a-variable-as-a-string",
    "title": "r-quiz",
    "section": "2 Define a variable as a string",
    "text": "2 Define a variable as a string\n\nExerciseSolution\n\n\nDefine in R the variable name and assign the value me.\n\n\n\nname &lt;- \"me\""
  },
  {
    "objectID": "posts/r-quiz/r-quiz.html#define-a-variable-by-another-variable",
    "href": "posts/r-quiz/r-quiz.html#define-a-variable-by-another-variable",
    "title": "r-quiz",
    "section": "3 Define a variable by another variable",
    "text": "3 Define a variable by another variable\n\nExerciseSolution\n\n\nDefine in R the variable name and assign the variable age.\n\n\n\nname &lt;- age"
  },
  {
    "objectID": "posts/r-quiz/r-quiz.html#call-a-function",
    "href": "posts/r-quiz/r-quiz.html#call-a-function",
    "title": "r-quiz",
    "section": "4 Call a function",
    "text": "4 Call a function\n\nExerciseSolution\n\n\nAsk R what today’s date() is, that is, call a function.\n\n\n\ndate()\n\n[1] \"Mon Mar 11 12:38:04 2024\""
  },
  {
    "objectID": "posts/r-quiz/r-quiz.html#define-a-vector",
    "href": "posts/r-quiz/r-quiz.html#define-a-vector",
    "title": "r-quiz",
    "section": "5 Define a vector",
    "text": "5 Define a vector\n\nExerciseSolution\n\n\nDefine in R a vector x with the values 1,2,3 .\n\n\n\nx &lt;- c(1, 2, 3)"
  },
  {
    "objectID": "posts/r-quiz/r-quiz.html#sum-up-vector",
    "href": "posts/r-quiz/r-quiz.html#sum-up-vector",
    "title": "r-quiz",
    "section": "6 Sum up vector",
    "text": "6 Sum up vector\n\nExerciseSolution\n\n\nDefine in R a vector x with the values 1,2,3 . Then sum up its values.\n\n\n\nx &lt;- c(1, 2, 3)\nsum(x)\n\n[1] 6"
  },
  {
    "objectID": "posts/r-quiz/r-quiz.html#vector-wise-computation",
    "href": "posts/r-quiz/r-quiz.html#vector-wise-computation",
    "title": "r-quiz",
    "section": "7 Vector wise computation",
    "text": "7 Vector wise computation\n\nExerciseSolution\n\n\nSquare each value in the vector x.\n\n\n\nx^2\n\n[1] 1 4 9"
  },
  {
    "objectID": "posts/r-quiz/r-quiz.html#vector-wise-computation-2",
    "href": "posts/r-quiz/r-quiz.html#vector-wise-computation-2",
    "title": "r-quiz",
    "section": "8 Vector wise computation 2",
    "text": "8 Vector wise computation 2\n\nExerciseSolution\n\n\nSquare each value in the vector x and sum up the values.\n\n\n\nsum(x^2)\n\n[1] 14"
  },
  {
    "objectID": "posts/r-quiz/r-quiz.html#compute-the-variance",
    "href": "posts/r-quiz/r-quiz.html#compute-the-variance",
    "title": "r-quiz",
    "section": "9 Compute the variance",
    "text": "9 Compute the variance\n\nExerciseSolution\n\n\nCompute the variance of x using basic arithmetic.\n\n\n\n x &lt;- c(1, 2, 3)\n\nsum((x - mean(x))^2) / (length(x)-1)\n\n[1] 1\n\n # compare: \nvar(x) \n\n[1] 1"
  },
  {
    "objectID": "posts/r-quiz/r-quiz.html#work-with-na",
    "href": "posts/r-quiz/r-quiz.html#work-with-na",
    "title": "r-quiz",
    "section": "10 Work with NA",
    "text": "10 Work with NA\n\nExerciseSolution\n\n\nDefine the vector y with the values 1,2,NA. Compute the mean. Explain the results.\n\n\n\ny &lt;- c(1, 2, NA)\nmean(y)\n\n[1] NA\n\n\nNA (not available, ie., missing data) is contagious in R: If there’s a missing element, R will assume that something has gone wrong and will raise a red flag, i.e, give you a NA back."
  },
  {
    "objectID": "posts/germeval07/germeval07.html",
    "href": "posts/germeval07/germeval07.html",
    "title": "germeval07",
    "section": "",
    "text": "Erstellen Sie ein prädiktives Modell für Textdaten. Nutzen Sie deutsche Word-Vektoren für das Feature-Engineering.\nNutzen Sie die GermEval-2018-Daten.\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\nDie Daten sind auch über das R-Paket PradaData zu beziehen.\n\nlibrary(tidyverse)\ndata(\"germeval_train\", package = \"pradadata\")\ndata(\"germeval_test\", package = \"pradadata\")\n\nDie AV lautet c1. Die (einzige) UV lautet: text.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\nNutzen Sie Tidymodels.\nNutzen Sie Wikipedia2Vec als Grundlage für die Wordembeddings in deutscher Sprache. Laden Sie die Daten herunter (Achtung: ca. 2.8 GB)."
  },
  {
    "objectID": "posts/germeval07/germeval07.html#deutsche-textvektoren-importieren",
    "href": "posts/germeval07/germeval07.html#deutsche-textvektoren-importieren",
    "title": "germeval07",
    "section": "Deutsche Textvektoren importieren",
    "text": "Deutsche Textvektoren importieren\n\nwiki_de_embeds_path &lt;- \"/Users/sebastiansaueruser/datasets/word-embeddings/wikipedia2vec/dewiki_20180420_100d.txt\"\n\ntic()\nwiki_de_embeds &lt;- arrow::read_feather(file = \"/Users/sebastiansaueruser/datasets/word-embeddings/wikipedia2vec/part-0.arrow\")\ntoc()\n\n0.558 sec elapsed\n\nnames(wiki_de_embeds)[1] &lt;- \"word\"\n\nwiki &lt;- as_tibble(wiki_de_embeds)\n\nDie Arrow-Datei ist viel schneller zu importieren als die Text-Datei.\n\ntic()\nwiki_de_embeds &lt;-\n  data.table::fread(file = wiki_de_embeds_path,\n                    sep = \" \",\n                    header = FALSE,\n                    showProgress = FALSE)  # progressbar\ntoc()\n\nAls Parquet-Datei speichern (effizienter):\n\ntic()\narrow::write_dataset(wiki_de_embeds, path = \"/Users/sebastiansaueruser/datasets/word-embeddings/wikipedia2vec\",\n                     format = \"arrow\")\ntoc()"
  },
  {
    "objectID": "posts/germeval07/germeval07.html#workflow",
    "href": "posts/germeval07/germeval07.html#workflow",
    "title": "germeval07",
    "section": "Workflow",
    "text": "Workflow\n\n# model:\nmod1 &lt;-\n  logistic_reg()\n\n# recipe:\nrec1 &lt;-\n  recipe(c1 ~ ., data = d_train) |&gt; \n  update_role(id, new_role = \"id\")  |&gt; \n  #update_role(c2, new_role = \"ignore\") |&gt; \n  step_tokenize(text) %&gt;%\n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") |&gt; \n\n  step_word_embeddings(text,\n                       embeddings = wiki,\n                       aggregation = \"mean\") |&gt; \n  step_normalize(all_numeric_predictors()) \n\n# workflow:\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(mod1) %&gt;% \n  add_recipe(rec1)"
  },
  {
    "objectID": "posts/germeval07/germeval07.html#preppenbaken",
    "href": "posts/germeval07/germeval07.html#preppenbaken",
    "title": "germeval07",
    "section": "Preppen/Baken",
    "text": "Preppen/Baken\n\ntic()\nrec1_prepped &lt;- prep(rec1)\ntoc()\n\n25.165 sec elapsed\n\n\n\nd_train_baked &lt;-\n  bake(rec1_prepped, new_data = NULL)\n\nhead(d_train_baked)\n\n# A tibble: 6 × 102\n     id c1      wordembed_text_V2 wordembed_text_V3 wordembed_text_V4\n  &lt;int&gt; &lt;fct&gt;               &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n1     1 OTHER               0.557            -0.140            1.24  \n2     2 OTHER              -0.552             0.291            0.195 \n3     3 OTHER              -0.850            -0.473           -0.367 \n4     4 OTHER               0.953            -1.48             0.0741\n5     5 OFFENSE             0.436            -0.885           -0.112 \n6     6 OTHER              -0.453             0.333            0.0949\n# ℹ 97 more variables: wordembed_text_V5 &lt;dbl&gt;, wordembed_text_V6 &lt;dbl&gt;,\n#   wordembed_text_V7 &lt;dbl&gt;, wordembed_text_V8 &lt;dbl&gt;, wordembed_text_V9 &lt;dbl&gt;,\n#   wordembed_text_V10 &lt;dbl&gt;, wordembed_text_V11 &lt;dbl&gt;,\n#   wordembed_text_V12 &lt;dbl&gt;, wordembed_text_V13 &lt;dbl&gt;,\n#   wordembed_text_V14 &lt;dbl&gt;, wordembed_text_V15 &lt;dbl&gt;,\n#   wordembed_text_V16 &lt;dbl&gt;, wordembed_text_V17 &lt;dbl&gt;,\n#   wordembed_text_V18 &lt;dbl&gt;, wordembed_text_V19 &lt;dbl&gt;, …"
  },
  {
    "objectID": "posts/germeval07/germeval07.html#tuninigfitting",
    "href": "posts/germeval07/germeval07.html#tuninigfitting",
    "title": "germeval07",
    "section": "Tuninig/Fitting",
    "text": "Tuninig/Fitting\n\ntic()\nwf1_fit &lt;-\n  wf1 %&gt;% \n  fit(data = d_train)\ntoc()\n\n10.085 sec elapsed\n\nbeep()\n\nAus Zeitgründen verzichten wir hier auf Tuning."
  },
  {
    "objectID": "posts/germeval07/germeval07.html#test-set-güte",
    "href": "posts/germeval07/germeval07.html#test-set-güte",
    "title": "germeval07",
    "section": "Test-Set-Güte",
    "text": "Test-Set-Güte\nVorhersagen im Test-Set:\n\ntic()\npreds &lt;-\n  predict(wf1_fit, new_data = germeval_test)\ntoc()\n\n6.318 sec elapsed\n\n\nUnd die Vorhersagen zum Test-Set hinzufügen, damit man TRUTH und ESTIMATE vergleichen kann:\n\nd_test &lt;-\n  germeval_test |&gt; \n  bind_cols(preds) |&gt; \n  mutate(c1 = as.factor(c1))\n\n\nmy_metrics &lt;- metric_set(accuracy, f_meas)\nmy_metrics(d_test,\n           truth = c1,\n           estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.715\n2 f_meas   binary         0.533"
  },
  {
    "objectID": "posts/germeval07/germeval07.html#fazit",
    "href": "posts/germeval07/germeval07.html#fazit",
    "title": "germeval07",
    "section": "Fazit",
    "text": "Fazit\nwikipedia2vec ist für die deutsche Sprache vorgekocht. Das macht Sinn für einen deutschsprachigen Corpus.\n\nCategories:\n\n2023\ntextmining\ndatawrangling\ngermeval\nprediction\ntidymodels\nstring"
  },
  {
    "objectID": "posts/smartphone1/index.html",
    "href": "posts/smartphone1/index.html",
    "title": "smartphone1",
    "section": "",
    "text": "In dieser Fallstudie analysieren Sie die Ergebnisse einer Umfrage zum Thema Smartphone-Nutzung. \nKernstück der Umfrage ist die Smartphone-Sucht-Skala (kwon_smartphone_2013?). Eine Studie fand, dass ca. ein Siebtel der Studierenden süchtig nach ihrem Smartphone sind (Haug et al., 2015); demnach könnte dem Thema eine hohe Bedeutsamkeit zukommen.\n\nImportieren Sie den Datensatz zur Handynutzung von Google-Docs.\nBenennen Sie die Spalten um und zwar nach folgendem Muster: itemXY, wobei XY die Nummer der Spalte ist. Sichern Sie die ursprünglichen Spaltennamen in einem Vektor. Tipp: Der Funktion names(meine_tabelle) können Sie einen Vektor mit neuen Spaltennamen übergeben.\nBerechnen Sie für die Items der Smartphone-Addiction-Scale den Mittelwert pro Person. Tipp: Erstellen Sie einen Dataframe mit den entsprechenden Items und nutzen Sie dann die Funktion rowMeans(mein_dataframe), um den Mittelwert über mehrere Spalten für jede Zeile zu berechnen (“Score”).\nVisualisieren Sie die Verteilung des Scores getrennt für die Geschlechter.\nNach einer Quelle (kwon_smartphone_2013?) liegt der Cutoff-Wert für Sucht bei 3.1 (Männer) bzw. 3.3 (Frauen). Bestimmen Sie den Anteil abhängiger Personen (pro Geschlecht).\nDas Item i13 ist ein Versuch, mit einem einzelnen Item zu messen, ob jemand süchtig nach seinem Smartphone ist (Item-Label: “Ich.würde.sagen..dass.ich.smartphone.süchtig.bin.”). Visualisieren Sie den Zusammenhang dieses Items mit dem Score.\nVisualsieren Sie den Anteil abhängiger Personen.\nBerechnen Sie, wie viel Geld für das zuletzt gekaufte Handy im Schnitt ausgegeben wurde. Gruppieren Sie dabei nach dem Betriebsystem.\nWer gibt mehr Geld für das Handy aus: Frauen oder Männer? Beantworten Sie die Frage anhand des Medians.\nVisualisieren Sie den Median des Geldausgebens für das Handy, getrennt nach Geschlechtern\n\n\n\n\n\n\n\nTip\n\n\n\nNutzen Sie ChatGPT oder einen anderen Bot, um sich Hilfe mit dem R-Code zu holen. \\(\\square\\)\n\n\n\n\n\n\n\n\nTip\n\n\n\nEs wird (fast) nie von Ihnen verlangt, dass Sie eine Aufgabe mit einem bestimmten R-Befehl lösen. Wenn Ihnen ein bestimmter R-Befehl nicht zusagt (oder Sie ihn nicht kennen oder verstehen) – dann nehmen Sie einfach einen anderen R-Befehl, der Ihnen mehr zusagt. \\(\\square\\)\n\n\n\n\n\n\n\n\nNote\n\n\n\nBeachten Sie die Hinweise des Datenwerks. \\(\\square\\)"
  },
  {
    "objectID": "posts/smartphone1/index.html#daten-importieren",
    "href": "posts/smartphone1/index.html#daten-importieren",
    "title": "smartphone1",
    "section": "2.1 Daten importieren",
    "text": "2.1 Daten importieren\n\ndata_path &lt;- \"https://raw.githubusercontent.com/sebastiansauer/statistik1/main/daten/Smartphone-Nutzung%20(Responses)%20-%20Form%20responses%201.csv\"\nsmartphone_raw &lt;- read.csv(data_path)\n\nDie Anzahl der Spalten einer Tabelle kann man sich übrigens z.B. mit ncol ausgeben lassen:\n\nanz_spalten &lt;- ncol(smartphone_raw)\nanz_spalten\n\n[1] 18\n\n\nUnsere Datentabelle hat also 18 Spalten."
  },
  {
    "objectID": "posts/smartphone1/index.html#spalten-umbenennen",
    "href": "posts/smartphone1/index.html#spalten-umbenennen",
    "title": "smartphone1",
    "section": "2.2 Spalten umbenennen",
    "text": "2.2 Spalten umbenennen\nZunächst sichern wir die alten Spaltennamen in einen Vektor:\n\nitem_labels_old &lt;- names(smartphone_raw)\nitem_labels_old\n\n [1] \"Timestamp\"                                                                                                        \n [2] \"Wann.haben.Sie.heute.zum.letzten.Mal.Ihr.Handy.benutzt..Bitte.geben.Sie.die.Uhrzeit.an.\"                          \n [3] \"Aufgrund.meiner.Smartphone.Nutzung.erledige.ich.geplante.Aufgaben.nicht.\"                                         \n [4] \"Aufgrund.meiner.Smartphone.Nutzung.fällt.es.mir.schwer..mich.in.der.Schule.oder.Arbeit.zu.konzentrieren.\"         \n [5] \"Bei.der.Nutzung.des.Smartphones.bekomme.ich.Schmerzen.in.Handgelenk.oder.Nacken.\"                                 \n [6] \"Ich.würde.es.nicht.aushalten..kein.Smartphone.zu.haben.\"                                                          \n [7] \"Wenn.ich.mein.Smartphone.nicht.in.der.Hand.habe..fühle.ich.mich.unruhig.und.gereizt.\"                             \n [8] \"Ich.denke.ständig.an.mein.Smartphone..auch.wenn.ich.es.nicht.benutze.\"                                            \n [9] \"Ich.werde.nie.aufhören..mein.Smartphone.zu.benutzen..selbst.wenn.mein.Alltag.bereits.stark.davon.beeinflusst.ist.\"\n[10] \"Ich.schaue.ständig.auf.mein.Smartphone..um.keine.Neuigkeiten.zu.verpassen.\"                                       \n[11] \"Ich.benutze.mein.Smartphone.länger.als.ich.es.vorhabe.\"                                                           \n[12] \"Die.Menschen.in.meinem.Umfeld.sagen.mir..dass.ich.mein.Smartphone.zu.häufig.nutze.\"                               \n[13] \"Ich.würde.sagen..dass.ich.smartphone.süchtig.bin.\"                                                                \n[14] \"Hier.ist.Platz.für.Ihre.Kommentare\"                                                                               \n[15] \"Bitte.geben.Sie.Ihr.Geschlecht.an.\"                                                                               \n[16] \"Bitte.geben.Sie.Ihr.Alter.an.\"                                                                                    \n[17] \"Bitte.geben.Sie.das.Betriebssystem.Ihres..am.meisten.genutzten..Handies.an.\"                                      \n[18] \"Wie.viel.Geld.haben.Sie.für.Ihr.zuletzt.gekauftes.Handy.gezahlt.\"                                                 \n\n\nVariablen aus psychologischen Fragebögen nennt man übrigens oft items.\nUnd dann nennen wir die Spaltennamen um. Das geht mit der Funktion names(smartphone_raw) &lt;-, der wir einen Vektor mit neuen Spaltennamen übergeben, z.B. so:\n\nitem_labels_new &lt;- c( \"item1\",  \"item2\", \"item3\", \"item4\",\n                      \"item5\", \"item6\",  \"item7\", \"item8\",\n                      \"item9\",  \"item10\", \"item11\", \"item12\",\n                      \"item13\", \"item14\", \"item15\", \"item16\",\n                      \"item17\", \"item18\")\n\nWichtig ist, dass ihr Vektor item_labels_new genau so viele Elemente hat, wie die Datentabelle Spalten hat.\nJetzt können Sie der Funktion names() den neuen Vektor item_labels_new zuweisen und haben damit die Spaltenanmen geändert:\n\nnames(smartphone_raw) &lt;- item_labels_new\n\nDen Vektor item_labels_new zu erstellen, war Ihnen zu viel Tipperei? Ja, mir auch. Schneller geht’s mit der Funktion paste0. Das erklärt sich am einfachsten mit einem Beispiel:\n\npaste0(\"item\", 1:3)\n\n[1] \"item1\" \"item2\" \"item3\"\n\n\nSehen Sie, was paste0 macht? Es fügt zwei Vektoreneinen Reißverschluss zusammen. Da item nur aus einem Element besteht, wird es item einfach auf die richtige Länge erhöht.\nDer Doppelpunkt in 1:3 bedeutet “von 1 bis 3”.\nAlso:\n\nnames(smartphone_raw) &lt;- paste0(\"item\",1:anz_spalten)\n\nSie möchten lieber zweistellige Nummern für die Spalten, also 01, 02, …, 09, 10, …? Gute Idee. Aber wie macht man das? Eine einfache Lösung: Fragen Sie ChatGPT!\n\n👩‍🎓: I want a string of the type “itemXY”, where XY is a number between 0 and 18. Make sure to use two digits. Use the R function paste0.\n\n\n🤖: 😸"
  },
  {
    "objectID": "posts/smartphone1/index.html#vertiefung-fingerabdruck-der-datentabelle",
    "href": "posts/smartphone1/index.html#vertiefung-fingerabdruck-der-datentabelle",
    "title": "smartphone1",
    "section": "2.3 Vertiefung: Fingerabdruck der Datentabelle",
    "text": "2.3 Vertiefung: Fingerabdruck der Datentabelle\nMit dem R-Paket visdat bekommt man einen “Fingerabdruck” der Datentabelle.\nAm einfachsten erklärt sich das an einem Beispiel. Schauen Sie sich das folgende Diagramm mal an. Es zeigt Ihnen den Datentyp pro Spalte und außerdem fehlende Werte.\n\nlibrary(visdat)\n\nvis_dat(smartphone_raw)\n\n\n\n\n\n\n\n\nDas ist deutlich übersichtlicher als eine Excel-Tabelle, wenn es darum geht, die Datenstruktur grob zu verstehen."
  },
  {
    "objectID": "posts/smartphone1/index.html#mittelwert-der-smartphone-addiction-scale",
    "href": "posts/smartphone1/index.html#mittelwert-der-smartphone-addiction-scale",
    "title": "smartphone1",
    "section": "2.4 Mittelwert der Smartphone-Addiction-Scale",
    "text": "2.4 Mittelwert der Smartphone-Addiction-Scale\n\nsmartphone_addiction_mittelwert &lt;- \nsmartphone_raw |&gt; \n  select(item3:item12) |&gt; \n  rowMeans()\n\nWir erhalten einen Vektor mit den Mittwerten (Score) pro Person für die Skala Smartphone-Abhängigkeit:\n\nhead(smartphone_addiction_mittelwert)\n\n[1] 3.0 3.8 2.7 3.2 4.2 3.3\n\n\nDiesen Vektor fügen wir dann unseren Daten hinzu. Außerdem benennen wir die Spalte item15 in sex um, damit wir uns merken können, in welcher Spalte das Geschlecht codiert ist.\n\nsmartphone &lt;-\n  smartphone_raw |&gt; \n  mutate(smartphone_addiction_mean = smartphone_addiction_mittelwert) |&gt; \n  rename(sex = item15) |&gt;   \n  filter(sex == \"Frau\" | sex == \"Mann\") \n\nMit rename(neuer_name = alter_name) können Sie die Namen von Spalten Ihrer Datentabelle ändern.\n\n🧑‍🎓 Also das mit rename hätte ich jetzt nicht gewusst.\n\n\n👩‍🏫 Dann frag mal ChatGPT.\n\n\n🤖 Ja, bitte!!\n\nAlternativ können Sie die folgende, etwas fortgeschrittenere Syntax nutzen:\n\nsmartphone2 &lt;- \nsmartphone_raw |&gt; \n  rowwise() |&gt; \n  mutate(smartphone_addiction_mean = mean(c_across(item3:item12)))"
  },
  {
    "objectID": "posts/smartphone1/index.html#score-visualisieren",
    "href": "posts/smartphone1/index.html#score-visualisieren",
    "title": "smartphone1",
    "section": "2.5 Score visualisieren",
    "text": "2.5 Score visualisieren\nLeider kann DataExplorer nicht mehrere Gruppen in einem Dichtediagramm anzeigen. Wir müssten also mit DataExplorer zwei Diagramme erstellen, eines für Frauen und eines für Männer. Eleganter geht es mit dem Paket ggpubr\n\nsmartphone |&gt; \n  ggdensity(x = \"smartphone_addiction_mean\", \n            color = \"sex\")"
  },
  {
    "objectID": "posts/smartphone1/index.html#anteil-smartphone-abhängigkeit",
    "href": "posts/smartphone1/index.html#anteil-smartphone-abhängigkeit",
    "title": "smartphone1",
    "section": "2.6 Anteil Smartphone-Abhängigkeit",
    "text": "2.6 Anteil Smartphone-Abhängigkeit\nMit case_when erstellen wir folgende Regel:\n\n“addicted”: wenn der Score &gt; 3.1 und das Geschlecht “Mann” ist bzw.\n“addicted”: wenn der Score &gt; 3.3 und das Geschlecht “Frau” ist bzw.\n“nicht addicted”: ansonsten\n\n\nsmartphone &lt;-\n  smartphone |&gt; \n  mutate(is_addicted =\n           case_when(smartphone_addiction_mean &gt; 3.1 & sex == \"Mann\" ~ \"addicted\",\n                     smartphone_addiction_mean &gt; 3.3 & sex == \"Frau\" ~ \"addicted\",\n                     TRUE ~ \"not-addicted\"))\n\nJetzt haben wir die Spalte is_addicted, die für jede Person (Zeile) angibt, ob die Person addicted ist. Nun zählen wir die Anzahl (n) aus, und zwar pro Geschlechtsgruppe. Weil es praktisch ist, rechen wir die Anzahl noch in einen Anteil (proportion) um.\n\nsmartphone_count &lt;- \nsmartphone |&gt; \n  group_by(sex) |&gt; \n  count(is_addicted) |&gt; \n  mutate(is_addicted_proportion = n / sum(n))\n\nsmartphone_count\n\n# A tibble: 4 × 4\n# Groups:   sex [2]\n  sex   is_addicted      n is_addicted_proportion\n  &lt;chr&gt; &lt;chr&gt;        &lt;int&gt;                  &lt;dbl&gt;\n1 Frau  addicted        18                  0.462\n2 Frau  not-addicted    21                  0.538\n3 Mann  addicted         2                  0.333\n4 Mann  not-addicted     4                  0.667"
  },
  {
    "objectID": "posts/smartphone1/index.html#smartphone-sucht-mit-einzelnen-item-gemessen",
    "href": "posts/smartphone1/index.html#smartphone-sucht-mit-einzelnen-item-gemessen",
    "title": "smartphone1",
    "section": "2.7 Smartphone-Sucht, mit einzelnen Item gemessen",
    "text": "2.7 Smartphone-Sucht, mit einzelnen Item gemessen\n\nsmartphone |&gt; \n  select(smartphone_addiction_mean, item13) |&gt; \n  drop_na() |&gt; \n  plot_scatterplot(by = \"smartphone_addiction_mean\")\n\n\n\n\n\n\n\n\nEs scheint einen Zusammenhang zwischen item13 und smartphone_addiction_mean zu geben."
  },
  {
    "objectID": "posts/smartphone1/index.html#anteil-der-abhängigen-visualisieren",
    "href": "posts/smartphone1/index.html#anteil-der-abhängigen-visualisieren",
    "title": "smartphone1",
    "section": "2.8 Anteil der Abhängigen visualisieren",
    "text": "2.8 Anteil der Abhängigen visualisieren\nDer Anteil der abhängigen Personen ist in beiden Geschlechtern gleich hoch:\n\nsmartphone_count |&gt; \n  plot_bar(by = \"sex\")\n\n\n\n\n\n\n\n\nHier noch eine alternative Visualisierung mit dem Paket ggpubr:\n\nsmartphone_count |&gt; \n  ggbarplot(x = \"sex\", y = \"n\", fill = \"is_addicted\")"
  },
  {
    "objectID": "posts/smartphone1/index.html#kosten-nach-betriebssystem",
    "href": "posts/smartphone1/index.html#kosten-nach-betriebssystem",
    "title": "smartphone1",
    "section": "2.9 Kosten nach Betriebssystem",
    "text": "2.9 Kosten nach Betriebssystem\n\nsmartphone |&gt; \n  group_by(item17) |&gt; \n  summarise(price_mean = mean(item18, na.rm = TRUE))\n\n# A tibble: 2 × 2\n  item17  price_mean\n  &lt;chr&gt;        &lt;dbl&gt;\n1 Android       412 \n2 iOS           742."
  },
  {
    "objectID": "posts/smartphone1/index.html#kosten-nach-geschlecht",
    "href": "posts/smartphone1/index.html#kosten-nach-geschlecht",
    "title": "smartphone1",
    "section": "2.10 Kosten nach Geschlecht",
    "text": "2.10 Kosten nach Geschlecht\n\nsmartphone |&gt; \n  group_by(sex) |&gt; \n  summarise(price_median = median(item18, na.rm = TRUE))\n\n# A tibble: 2 × 2\n  sex   price_median\n  &lt;chr&gt;        &lt;dbl&gt;\n1 Frau           700\n2 Mann          1000\n\n\nMänner geben im Median 300 Euro mehr aus.\n\n\n\n\n\n\nTip\n\n\n\nWenn Sie nicht mehr wissen, was z.B. na.rm = TRUE bedeutet, dann einfach googeln oder einen ChatBot fragen. In der Regel ist die Frage dann in zwei Minuten beantwortet. \\(\\square\\)"
  },
  {
    "objectID": "posts/smartphone1/index.html#kosten-nach-geschlecht-visualisieren",
    "href": "posts/smartphone1/index.html#kosten-nach-geschlecht-visualisieren",
    "title": "smartphone1",
    "section": "2.11 Kosten nach Geschlecht visualisieren",
    "text": "2.11 Kosten nach Geschlecht visualisieren\n\nsmartphone |&gt; \n  select(sex, item18) |&gt; \n  plot_boxplot(by = \"sex\")\n\n\n\n\n\n\n\n\nWie man sieht ist der Median bei den Männern höher als bei den Frauen. Allerdings fällt der Median der Männer aus das dritte Quartil, was vermuten lässt, dass da irgendwas nicht stimmt. Schauen wir uns die Daten näher an:\n\nsmartphone |&gt; \n  filter(sex == \"Mann\")\n\n                item1    item2 item3 item4 item5 item6 item7 item8 item9 item10\n1 02/05/2024 16:29:51              4     4     3    NA     1    NA     1      4\n2 02/05/2024 16:30:37 16:20:00     2     5     2     4     3     3     3      3\n3 02/05/2024 16:30:42 16:20:00     1     2     2     4     1     1     6      5\n4 02/05/2024 16:30:48 16:30:00     5     5     2     6     2     2     5      4\n5 02/05/2024 16:30:58 16:00:00     4     2     3     5     2     2     3      4\n6 02/05/2024 16:31:28 16:28:00     6     6     1     6     3     2     6      3\n  item11 item12 item13 item14  sex item16  item17 item18\n1      4     NA      2        Mann     21 Android    400\n2      4      2      4        Mann     19     iOS   1000\n3      3      1      5        Mann     22     iOS   1200\n4      5      4      4        Mann     19 Android    300\n5      5      1      2        Mann     29     iOS   1000\n6      6      2      5        Mann     19     iOS   1000\n  smartphone_addiction_mean  is_addicted\n1                        NA not-addicted\n2                       3.1 not-addicted\n3                       2.6 not-addicted\n4                       4.0     addicted\n5                       3.1 not-addicted\n6                       4.1     addicted\n\n\nAh, es sind einfach sehr wenig Männer in diesem Datensatz enthalten."
  },
  {
    "objectID": "posts/mariokart-korr1/mariokart-korr1.html",
    "href": "posts/mariokart-korr1/mariokart-korr1.html",
    "title": "mariokart-korr1",
    "section": "",
    "text": "Aufgabe\nImportieren Sie den Datensatz mariokart in R. Berechnen Sie die Korrelation von Verkaufspreis (total_pr) und Startgebot (start_pr)!\nHinweise:\n\nRunden Sie auf 2 Dezimalstellen.\n\n         \n\n\nLösung\nPakete starten:\n\nlibrary(tidyverse)\nlibrary(easystats)\n\nDaten importieren:\n\nd_url &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\"\nd &lt;- data_read(d_url)\n\n\nsolution &lt;- \nd  %&gt;% \n  summarise(pr_cor = cor(total_pr, start_pr))\nsolution\n\n      pr_cor\n1 0.07340603\n\n\nAlternativ kann man (komfortabel) die Korrelation z.B. so berechnen:\n\nd %&gt;% \n  select(start_pr, total_pr) %&gt;% \n  correlation()\n\n# Correlation Matrix (pearson-method)\n\nParameter1 | Parameter2 |    r |        95% CI | t(141) |     p\n---------------------------------------------------------------\nstart_pr   |   total_pr | 0.07 | [-0.09, 0.23] |   0.87 | 0.384\n\np-value adjustment method: Holm (1979)\nObservations: 143\n\n\nMan kann das Ergebnis von correlation auch einfach in ein Diagramm überführen:\n\nmariokart_corr1 &lt;- \nd %&gt;% \n  select(start_pr, total_pr) %&gt;% \n  correlation()\n\nmariokart_corr1 %&gt;% plot()\n\n\n\n\n\n\n\n\nLösung: 0.07.\n\nCategories:\n\ndatawrangling\ndplyr\neda\nassociation\nnum"
  },
  {
    "objectID": "posts/wuerfel03/wuerfel03.html",
    "href": "posts/wuerfel03/wuerfel03.html",
    "title": "wuerfel03",
    "section": "",
    "text": "Exercise\nWas ist die Wahrscheinlichkeit, mit zwei fairen Würfeln höchstens 10 Augen zu werfen?\nHinweise:\n\nNutzen Sie exakte Methoden der Wahrscheinlichkeitsrechnung, keine Simulation.\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\n\n         \n\n\nSolution\nErstellen wir uns eine Tabelle, die alle Permutationen der beiden Würfelergebnisse fasst, das sind 36 Paare: (1,1), (1,2), …, (1,6), …, (6,6).\nDas kann man von Hand erstellen, halbautomatisch in Excel oder z.B. so:\n\nlibrary(tidyverse)\nd &lt;- expand_grid(wuerfel1 = 1:6,\n         wuerfel2 = 1:6)\n\nd\n\n# A tibble: 36 × 2\n   wuerfel1 wuerfel2\n      &lt;int&gt;    &lt;int&gt;\n 1        1        1\n 2        1        2\n 3        1        3\n 4        1        4\n 5        1        5\n 6        1        6\n 7        2        1\n 8        2        2\n 9        2        3\n10        2        4\n# ℹ 26 more rows\n\n\nJetzt ergänzen wir eine Spalte für die Wahrscheinlichkeit jeder Kombination, das ist einfach, denn \\(p(A \\cap B) = p(A) \\cdot p(B) = 1/36\\) gilt.\n\nd2 &lt;-\n  d %&gt;% \n  mutate(prob = 1/36)\n\nhead(d2)\n\n# A tibble: 6 × 3\n  wuerfel1 wuerfel2   prob\n     &lt;int&gt;    &lt;int&gt;  &lt;dbl&gt;\n1        1        1 0.0278\n2        1        2 0.0278\n3        1        3 0.0278\n4        1        4 0.0278\n5        1        5 0.0278\n6        1        6 0.0278\n\n\nAußerdem ergänzen wir die Summe der Augenzahlen, weil die Frage ja nach einer bestimmten Summe an Augenzahlen abzielt.\n\nd3 &lt;-\n  d2 %&gt;% \n  mutate(augensumme = wuerfel1 + wuerfel2)\n\nhead(d3)\n\n# A tibble: 6 × 4\n  wuerfel1 wuerfel2   prob augensumme\n     &lt;int&gt;    &lt;int&gt;  &lt;dbl&gt;      &lt;int&gt;\n1        1        1 0.0278          2\n2        1        2 0.0278          3\n3        1        3 0.0278          4\n4        1        4 0.0278          5\n5        1        5 0.0278          6\n6        1        6 0.0278          7\n\n\nFür manche Augensummen gibt es mehrere Möglichkeiten:\n\nd3 %&gt;% \n  filter(augensumme == 7)\n\n# A tibble: 6 × 4\n  wuerfel1 wuerfel2   prob augensumme\n     &lt;int&gt;    &lt;int&gt;  &lt;dbl&gt;      &lt;int&gt;\n1        1        6 0.0278          7\n2        2        5 0.0278          7\n3        3        4 0.0278          7\n4        4        3 0.0278          7\n5        5        2 0.0278          7\n6        6        1 0.0278          7\n\n\n… für andere weniger:\n\nd3 %&gt;% \n  filter(augensumme == 12)\n\n# A tibble: 1 × 4\n  wuerfel1 wuerfel2   prob augensumme\n     &lt;int&gt;    &lt;int&gt;  &lt;dbl&gt;      &lt;int&gt;\n1        6        6 0.0278         12\n\n\nJetzt summieren wir (nach dem Additionssatz der Wahrscheinlichkeit) die Wahrscheinlichkeiten pro Augenzahl:\n\nd4 &lt;- \n  d3 %&gt;% \n  group_by(augensumme) %&gt;% \n  summarise(totale_w_pro_augenzahl = sum(prob))\n\nd4\n\n# A tibble: 11 × 2\n   augensumme totale_w_pro_augenzahl\n        &lt;int&gt;                  &lt;dbl&gt;\n 1          2                 0.0278\n 2          3                 0.0556\n 3          4                 0.0833\n 4          5                 0.111 \n 5          6                 0.139 \n 6          7                 0.167 \n 7          8                 0.139 \n 8          9                 0.111 \n 9         10                 0.0833\n10         11                 0.0556\n11         12                 0.0278\n\n\nTest: Die Summe der Wahrscheinlichkeit muss insgesamt 1 sein.\n\nd4 %&gt;% \n  summarise(sum(totale_w_pro_augenzahl))\n\n# A tibble: 1 × 1\n  `sum(totale_w_pro_augenzahl)`\n                          &lt;dbl&gt;\n1                             1\n\n\nUnd:\n\nd2 %&gt;% \n  summarise(sum(prob))\n\n# A tibble: 1 × 1\n  `sum(prob)`\n        &lt;dbl&gt;\n1           1\n\n\nPasst!\nDie Wahrscheinlichkeit für die Augensumme von höchstens 10 beträgt also:\n\nloesung &lt;-\n  d4 %&gt;% \n  filter(augensumme &lt;= 10) %&gt;% \n  summarise(prob_sum = sum(totale_w_pro_augenzahl)) %&gt;% \n  pull(prob_sum)\n\nloesung\n\n[1] 0.9166667\n\n\n\nCategories:\n\nprobability\ndice"
  },
  {
    "objectID": "posts/wskt-mtcars-1l/wskt-mtcars-1l.html",
    "href": "posts/wskt-mtcars-1l/wskt-mtcars-1l.html",
    "title": "wskt-mtcars-1l",
    "section": "",
    "text": "Prüfen Sie folgende Hypothese:\n\nEin Auto mit manueller Schaltung hat pro Gallone Sprit mind. 5 Meilen mehr Reichweite als ein Auto mit Automatikschaltung (ceteris paribus).\n\nQuantifizieren Sie die Wahrscheinlichkeit dieser Hypothese!\nHinweise:\n\nNutzen Sie die Bayes-Statistik mit Stan.\nBeachten Sie die Standardhinweise des Datenwerks.\nVerwenden Sie den Datensatz mtcars."
  },
  {
    "objectID": "posts/wskt-mtcars-1l/wskt-mtcars-1l.html#setup",
    "href": "posts/wskt-mtcars-1l/wskt-mtcars-1l.html#setup",
    "title": "wskt-mtcars-1l",
    "section": "Setup",
    "text": "Setup\n\nlibrary(rstanarm)\nlibrary(easystats)\nlibrary(tidyverse)\n\n\ndata(mtcars)"
  },
  {
    "objectID": "posts/wskt-mtcars-1l/wskt-mtcars-1l.html#modell",
    "href": "posts/wskt-mtcars-1l/wskt-mtcars-1l.html#modell",
    "title": "wskt-mtcars-1l",
    "section": "Modell",
    "text": "Modell\nDie Hypothese kann man wie folgt formalisieren:\n\nDie Wahrscheinlichkeit, dass Manuellschalter eine höhere Reichweite haben, ist größer als die Wahrscheinlichkeit, dass Automatikschalter eine höhere Reichweite haben:\n\n\\[Pr(mpg_M &gt; mpg_A) &gt; Pr(mpg_M &lt;= mpg_A)\\]\n\nOder anders gesagt: Die Wahrscheinlichkeit, dass Automatikschalter eine höhere Reichweite haben (pro Gallone Sprit und im Vergleich zu Automatikschalter) ist größer als 50%.\n\n\\[Pr(mpg_M &gt; mpg_A) &gt; 1/2\\] 3. Möchte man noch hinzufügen, dass sich diese Behauptung auf ein bestimmtes, nämlich unser Regressionsmodell bezieht, kann man schreiben:\n\\[Pr(mpg_M &gt; mpg_A \\quad | \\beta_0, \\beta_1, \\sigma)\\]"
  },
  {
    "objectID": "posts/wskt-mtcars-1l/wskt-mtcars-1l.html#modell-berechnen",
    "href": "posts/wskt-mtcars-1l/wskt-mtcars-1l.html#modell-berechnen",
    "title": "wskt-mtcars-1l",
    "section": "Modell berechnen",
    "text": "Modell berechnen\n\nm &lt;- stan_glm(mpg ~ am,\n              data = mtcars,\n              refresh = 0,\n              seed = 42)\n\n\nparameters(m)\n\nParameter   | Median |         95% CI |     pd |  Rhat |     ESS |                   Prior\n------------------------------------------------------------------------------------------\n(Intercept) |  17.14 | [14.85, 19.51] |   100% | 0.999 | 3739.00 | Normal (20.09 +- 15.07)\nam          |   7.21 | [ 3.72, 10.70] | 99.95% | 0.999 | 3755.00 |  Normal (0.00 +- 30.20)\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a MCMC distribution approximation."
  },
  {
    "objectID": "posts/wskt-mtcars-1l/wskt-mtcars-1l.html#post-verteilung-auslesen",
    "href": "posts/wskt-mtcars-1l/wskt-mtcars-1l.html#post-verteilung-auslesen",
    "title": "wskt-mtcars-1l",
    "section": "Post-Verteilung auslesen",
    "text": "Post-Verteilung auslesen\n\nm_post &lt;-\n  m |&gt;\n  as_tibble()\n\nprop &lt;- \nm_post |&gt; \n  count(am &gt;= 5) |&gt; \n  mutate(prop = n/sum(n))\n\nprop\n\n# A tibble: 2 × 3\n  `am &gt;= 5`     n  prop\n  &lt;lgl&gt;     &lt;int&gt; &lt;dbl&gt;\n1 FALSE       431 0.108\n2 TRUE       3569 0.892"
  },
  {
    "objectID": "posts/wskt-mtcars-1l/wskt-mtcars-1l.html#antwort",
    "href": "posts/wskt-mtcars-1l/wskt-mtcars-1l.html#antwort",
    "title": "wskt-mtcars-1l",
    "section": "Antwort",
    "text": "Antwort\nLaut unserem Modell beträgt die Wahrscheinlichkeit für obige Hypothese 0.89."
  },
  {
    "objectID": "posts/wuerfel04/wuerfel04.html",
    "href": "posts/wuerfel04/wuerfel04.html",
    "title": "wuerfel04",
    "section": "",
    "text": "Exercise\nWas ist die Wahrscheinlichkeit, mit zwei fairen Würfeln genau 10 Augen zu werfen?\nHinweise:\n\nNutzen Sie Simulationsmnethoden der Wahrscheinlichkeitsrechnung, keine exakten Rechnung auf Basis der Wahrscheinlichkeitsrechnung.\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\nSetzen Sie bei Simulationsaufgaben immer die Zufallszahlen mit set.seed(). Sofern kein anderer Wert für set.seed() genannt, verwenden Sie die Zahl 42.\nDa es bei dieser Aufgabe nötig ist, zwei Mal Zufallszahlen zu berechnen (für zwei Würfel nämlich), verwenden Sie beim ersten Würfel die Zahl 42 und beim zweiten Würfel die Zahl 43.\n\n         \n\n\nSolution\n\nlibrary(tidyverse)\n\nEinen Würfelwurf in R kann man so simulieren:\n\nwuerfel &lt;- sample(x = c(1,2,3,4,5,6), size = 1, prob = c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))\nwuerfel\n\n[1] 5\n\n\nBei sample gibt x den Ereignisraum, \\(\\Omega\\), an, size die Stichprobengröße und prob gibt für jedes Element von x die Wahrscheinlichkeit an.\nDas machen wir jetzt 1000 Mal. Viel Spaß beim Tippen…\n… … …\nOkay, das sollten wir einfacher hinkriegen. Man kann R sagen, dass sie eine Funktion (wie sample) oft ausführen soll. Damit können wir viele Würfelwürfe simulieren. Diese “Wiederholungsfunktion” heißt replicate(n, expr); dabei gibt n an, wie oft die Funktion wiederholt werden soll, und expr ist der Ausdruck (die Funktion), die wiederholt werden soll, das ist bei uns die Funktion sample, wie oben dargestellt.\n\nzehn_wuerfel &lt;- replicate(n = 10, expr = sample(x = c(1,2,3,4,5,6), size = 1, prob = c(1/6, 1/6,1/6,1/6,1/6,1/6)))\nzehn_wuerfel\n\n [1] 2 6 3 2 6 3 6 5 3 1\n\n\nKönnen wir natürlich auch zich Mal wiederholen, nicht nur 10 Mal, sagen wir \\(10^4\\) Mal:\n\nset.seed(42)\nwuerfel1_oft &lt;- replicate(n = 10^4, expr = sample(x = c(1,2,3,4,5,6), size = 1, prob = c(1/6, 1/6,1/6,1/6,1/6,1/6)))\n\nmean(wuerfel1_oft)\n\n[1] 3.4968\n\n\nAh, interessant: Der Mittelwert ist etwa 3.5…\nJetzt werfen wir noch einen zweiten Würfel genau so oft:\n\nset.seed(43)\nwuerfel2_oft &lt;- replicate(n = 10^4, expr = sample(x = c(1,2,3,4,5,6), size = 1, prob = c(1/6, 1/6,1/6,1/6,1/6,1/6)))\n\nmean(wuerfel2_oft)\n\n[1] 3.4983\n\n\nDas packen wir jetzt in eine Tabelle und ergänzen die Augensumme für jede Wiederholung des Doppelwurfes:\n\nd &lt;-\n  tibble(w1 = wuerfel1_oft,\n         w2 = wuerfel2_oft,\n         w_sum = w1+w2)\n\nhead(d)\n\n# A tibble: 6 × 3\n     w1    w2 w_sum\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     4     5\n2     1     1     2\n3     3     2     5\n4     6     6    12\n5     5     3     8\n6     5     5    10\n\n\nJetzt ist es einfach:\nWir zählen einfach, wie oft das Ergebnis 10 vorkommt in der Tabelle.\n\nd %&gt;% \n  count(w_sum == 10)\n\n# A tibble: 2 × 2\n  `w_sum == 10`     n\n  &lt;lgl&gt;         &lt;int&gt;\n1 FALSE          9148\n2 TRUE            852\n\n\nErgänzen wir die Anteile dieser Anzahl:\n\nd %&gt;% \n  count(w_sum == 10) %&gt;% \n  mutate(Anteil = n/sum(n))\n\n# A tibble: 2 × 3\n  `w_sum == 10`     n Anteil\n  &lt;lgl&gt;         &lt;int&gt;  &lt;dbl&gt;\n1 FALSE          9148 0.915 \n2 TRUE            852 0.0852\n\n\nDie Lösung lautet also: 0.08 (gerundet auf zwei Dezimalen)\nAuf einfache Weise können wir entsprechend die Wahrscheinlichkeit für mindestens \\(k\\) Augen (bei zwei Würfelwürfen) ermitteln, mit \\(k\\) ist die gesuchte Augensumme, hier 10.\n\nd %&gt;% \n  count(w_sum &gt;= 10) %&gt;% \n  mutate(Anteil = n/sum(n))\n\n# A tibble: 2 × 3\n  `w_sum &gt;= 10`     n Anteil\n  &lt;lgl&gt;         &lt;int&gt;  &lt;dbl&gt;\n1 FALSE          8316  0.832\n2 TRUE           1684  0.168\n\n\nOder höchstens 10, ganz analog:\n\nd %&gt;% \n  count(w_sum &lt;= 10) %&gt;% \n  mutate(Anteil = n/sum(n))\n\n# A tibble: 2 × 3\n  `w_sum &lt;= 10`     n Anteil\n  &lt;lgl&gt;         &lt;int&gt;  &lt;dbl&gt;\n1 FALSE           832 0.0832\n2 TRUE           9168 0.917 \n\n\n\nCategories:\n\nprobability\ndice\nsimulation"
  },
  {
    "objectID": "posts/Cluster01/Cluster01.html",
    "href": "posts/Cluster01/Cluster01.html",
    "title": "Cluster01",
    "section": "",
    "text": "Aufgabe\nGegeben seien zwei Punkte im 3D-Raum:\n\\[P_1(0,0,0)\\] \\[P_2(3,4,12)\\]\nBerechnen Sie den euklidischen Abstand der beiden Punkte (bzw. den Abstand von \\(P_2\\) vom Ursprung)!\n         \n\n\nLösung\nDer Abstand ist so definiert:\n\\[d = \\sqrt{a^2 + b^2 + c+2}\\]\n\nergebnis &lt;- sqrt(3^2 + 4^2 + 12^2)\nergebnis\n\n[1] 13\n\n\n\nCategories:\nnum"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-06/Verteilungen-Quiz-06.html",
    "href": "posts/Verteilungen-Quiz-06/Verteilungen-Quiz-06.html",
    "title": "Verteilungen-Quiz-06",
    "section": "",
    "text": "Ist folgende Aussage wahr?\nSei \\(X \\sim N(100,15)\\), dann ist \\(Pr(X \\ge 100) \\ne 1/2\\).\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-06/Verteilungen-Quiz-06.html#answerlist",
    "href": "posts/Verteilungen-Quiz-06/Verteilungen-Quiz-06.html#answerlist",
    "title": "Verteilungen-Quiz-06",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-06/Verteilungen-Quiz-06.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-06/Verteilungen-Quiz-06.html#answerlist-1",
    "title": "Verteilungen-Quiz-06",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/fattails01/fattails01.html",
    "href": "posts/fattails01/fattails01.html",
    "title": "fattails01",
    "section": "",
    "text": "Exercise\nIn seinem Buch “Statistical Consequences of Fat Tails” schreibt der Autor, Nassim Taleb (S. 53):\n\nIn the summer of 1998, the hedge fund called “Long Term Capital Management” (LTCM) proved to have a very short life; it went bust from some deviations in the markets –those “of an unexpected nature”. The loss was a yuuuge deal because two of the partners received the Swedish Riksbank Prize, marketed as the “Nobel” in economics. (…) At least two of the partners made the statement that it was a “10 sigma” event (10 standard deviations), hence they should be absolved of all accusations of incompetence (I was ﬁrst hand witness of two such statements).\n\nWir testen in diesem Zusammenhang zwei Hypothesen: \\(H_N\\), dass der Finanzmarkt normalverteilt ist und \\(H_F\\), dass die Variable fat tailed ist, also nicht normalverteilt, sondernn einer Verteilung entspringt, in der “Extremereignisse” üblicher sind als in einer Normalverteilung.\nUm die Fat-Tails-Verteilung mit \\(n=10\\) zu simulieren, nutzen wir hier folgende Funktion:\n\nfat_tail_data &lt;- rt(n = 100, df = 2)\n\nDabei bedeutet df = 2, dass die Verteilung sehr randlastig (fat tailed) sein soll (genauer gesagt eine t-Verteilung mit zwei Freiheitsgraden). Details dazu sollen uns hier nicht interessieren. Nur für diejenigen, die neugierig sind: r steht für random, also eine Zufallszahl. Diese soll aus der sog. t-Verteilung mit df=1 stammen. Das ist, einfach gesagt, eine “plattgedrückte” Normalverteilung.\nBerechnen wir die Wahrscheinlichkeit, dass die Daten einer Normalverteilung entspringen (und nicht der Fat-Tail-Verteilung).\nDie Wahrscheinlichkeit eines 10-Sigma-Events ist übrigens … klein. Taleb berichtet sie mit \\(1.31 \\cdot 10^{-23}\\):\n\nL_norm &lt;- 1.31e-23\n\nFür die t-Verteilung ist der entsprechende Wert:\n\nL_fat &lt;- 1 - pt(q = 10, df = 2)\n\nAuch hier soll der Befehl pt nicht interessieren. Nur für die Neugierigen: p steht für probability, t für die t-Verteilung. Der Befehl gibt uns also die Wahrscheintlichkeit, \\(p\\), für ein bestimmten Quartil, \\(q\\), aus einer t-Verteilung mit 2 Freiheitsgraden.\nWie hoch ist die Post-Wahrscheinlichkeit, dass die Variable normalverteilt ist?\nHinweise:\n\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\nApriori sollen uns beide Hypothesen gleich plausibel sein.\n\n\n\nAnswerlist\n\nkleiner als 50%\nkleiner als 5%\nkleiner als 0.5%\nkleiner als 0.05%\nkleiner als 0.005%\n\n         \n\n\nSolution\n\nlibrary(tidyverse)\n\nErstellen wir erstmal den ersten Teil einer Bayes-Box:\n\nd &lt;-\n  tibble(H = c(\"Normalverteilt\", \"Randlastig verteilt\"),\n         Prior = c(1,1))\n\nd\n\n# A tibble: 2 × 2\n  H                   Prior\n  &lt;chr&gt;               &lt;dbl&gt;\n1 Normalverteilt          1\n2 Randlastig verteilt     1\n\n\nDann fügen wir den Likelihood jeder Hypothese dazu:\n\nd &lt;-\n  d %&gt;% \n  mutate(L = c(L_norm, L_fat))\n\nd\n\n# A tibble: 2 × 3\n  H                   Prior        L\n  &lt;chr&gt;               &lt;dbl&gt;    &lt;dbl&gt;\n1 Normalverteilt          1 1.31e-23\n2 Randlastig verteilt     1 4.93e- 3\n\n\nDann berechnen wir die Post-Wahrscheinlichkeit:\n\nd &lt;-\n  d %&gt;% \n  mutate(Post_unstand = Prior * L,\n         Post = Post_unstand / sum(Post_unstand))\nd\n\n# A tibble: 2 × 5\n  H                   Prior        L Post_unstand     Post\n  &lt;chr&gt;               &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 Normalverteilt          1 1.31e-23     1.31e-23 2.66e-21\n2 Randlastig verteilt     1 4.93e- 3     4.93e- 3 1   e+ 0\n\n\nDie Wahrscheinlichkeit, dass die Variable normalverteilt ist, ist seeeeehr klein, ca. \\(10^{-21}\\).\n\n\nAnswerlist\n\nFALSE\nFALSE\nFALSE\nFALSE\nTRUE\n\n\nCategories:\n\nprobability\nsimulation\nfat-tails\nnormal-distribution\nfat-tails"
  },
  {
    "objectID": "posts/punktschaetzer-reicht-nicht/punktschaetzer-reicht-nicht.html",
    "href": "posts/punktschaetzer-reicht-nicht/punktschaetzer-reicht-nicht.html",
    "title": "punktschaetzer-reicht-nicht",
    "section": "",
    "text": "Exercise\nZwei Modelle, m1 und m2 produzieren jeweils die gleiche Vorhersage (den gleichen Punktschätzer).\nm1:\n\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.25861 -0.06736  0.00040  0.08517  0.19928 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.005346   0.009649   0.554    0.581    \nx           1.016010   0.009614 105.684   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09643 on 98 degrees of freedom\nMultiple R-squared:  0.9913,    Adjusted R-squared:  0.9912 \nF-statistic: 1.117e+04 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nm2:\n\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1347 -0.6643 -0.0279  0.7704  2.3984 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.07032    0.10583  -0.665    0.508    \nx            0.92899    0.10879   8.539 1.77e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.057 on 98 degrees of freedom\nMultiple R-squared:  0.4266,    Adjusted R-squared:  0.4208 \nF-statistic: 72.92 on 1 and 98 DF,  p-value: 1.772e-13\n\n\nDie Modelle unterscheiden sich aber in ihrer Ungewissheit bezüglich \\(\\beta\\), wie in der Spalte Std. Error ausgedrückt.\nWelches der beiden Modelle ist zu bevorzugen? Begründen Sie.\n         \n\n\nSolution\nModell m1 hat eine kleinere Ungewissheit im Hinblick auf die Modellkoeffizienten \\(\\beta_0, \\beta_1\\) und ist daher gegenüber m2 zu bevorzugen.\n\nCategories:\n\nregression\nen\nbayes\nfrequentist\nqm1\nstats-nutshell\nqm2\nstats-nutshell"
  },
  {
    "objectID": "posts/Wertpruefen/Wertpruefen.html",
    "href": "posts/Wertpruefen/Wertpruefen.html",
    "title": "Wertpruefen",
    "section": "",
    "text": "Aufgabe\nGeben Sie die R-Syntax ein, um zu prüfen, dass die Variable loesung den Wert 42 hat.\nHinweis: Geben Sie Ihre Lösung ohne Leerzeichen an, da sonst eine richtige Lösung nicht erkannt werden kann.\n         \n\n\nLösung\nloesung==42\n\nCategories:\n\nR\n‘2023’\nstring"
  },
  {
    "objectID": "posts/wskt-quiz14/wskt-quiz14.html",
    "href": "posts/wskt-quiz14/wskt-quiz14.html",
    "title": "wskt-quiz14",
    "section": "",
    "text": "Behauptung:\n\\(Pr(AB) = Pr(A|B) \\cdot Pr(B)\\).\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n\n\n\nFalsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz14/wskt-quiz14.html#answerlist",
    "href": "posts/wskt-quiz14/wskt-quiz14.html#answerlist",
    "title": "wskt-quiz14",
    "section": "",
    "text": "Falsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz14/wskt-quiz14.html#answerlist-1",
    "href": "posts/wskt-quiz14/wskt-quiz14.html#answerlist-1",
    "title": "wskt-quiz14",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\nquiz\nprobability\nbayes\nquiz1-qm2-ws23\nschoice"
  },
  {
    "objectID": "posts/na-per-col/na-per-col.html",
    "href": "posts/na-per-col/na-per-col.html",
    "title": "na-per-col",
    "section": "",
    "text": "Zählen Sie die Anzahl der fehlenden Werte pro Spalte im Datensatz penguins.\nZeigen Sie einen prägnanten Weg.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n\ndata(\"penguins\", package = \"palmerpenguins\")  # ggf. das Paket vorab installieren"
  },
  {
    "objectID": "posts/na-per-col/na-per-col.html#weg-1",
    "href": "posts/na-per-col/na-per-col.html#weg-1",
    "title": "na-per-col",
    "section": "Weg 1",
    "text": "Weg 1\n\npenguins |&gt; \n  purrr::map_int(~ sum(is.na(.)))\n\n          species            island    bill_length_mm     bill_depth_mm \n                0                 0                 2                 2 \nflipper_length_mm       body_mass_g               sex              year \n                2                 2                11                 0"
  },
  {
    "objectID": "posts/na-per-col/na-per-col.html#weg-2",
    "href": "posts/na-per-col/na-per-col.html#weg-2",
    "title": "na-per-col",
    "section": "Weg 2",
    "text": "Weg 2\n\ncolSums(is.na(penguins))\n\n          species            island    bill_length_mm     bill_depth_mm \n                0                 0                 2                 2 \nflipper_length_mm       body_mass_g               sex              year \n                2                 2                11                 0"
  },
  {
    "objectID": "posts/na-per-col/na-per-col.html#weg-3",
    "href": "posts/na-per-col/na-per-col.html#weg-3",
    "title": "na-per-col",
    "section": "Weg 3",
    "text": "Weg 3\n\ndescribe_distribution(penguins)\n\nVariable          |    Mean |     SD |     IQR |              Range | Skewness | Kurtosis |   n | n_Missing\n-----------------------------------------------------------------------------------------------------------\nbill_length_mm    |   43.92 |   5.46 |    9.30 |     [32.10, 59.60] |     0.05 |    -0.88 | 342 |         2\nbill_depth_mm     |   17.15 |   1.97 |    3.12 |     [13.10, 21.50] |    -0.14 |    -0.91 | 342 |         2\nflipper_length_mm |  200.92 |  14.06 |   23.25 |   [172.00, 231.00] |     0.35 |    -0.98 | 342 |         2\nbody_mass_g       | 4201.75 | 801.95 | 1206.25 | [2700.00, 6300.00] |     0.47 |    -0.72 | 342 |         2\nyear              | 2008.03 |   0.82 |    2.00 | [2007.00, 2009.00] |    -0.05 |    -1.50 | 344 |         0\n\n\nAllerdings berücksichtigt describe_distribution nur metrische Spalten."
  },
  {
    "objectID": "posts/na-per-col/na-per-col.html#weg-4",
    "href": "posts/na-per-col/na-per-col.html#weg-4",
    "title": "na-per-col",
    "section": "Weg 4",
    "text": "Weg 4\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2"
  },
  {
    "objectID": "posts/na-per-col/na-per-col.html#weg-5",
    "href": "posts/na-per-col/na-per-col.html#weg-5",
    "title": "na-per-col",
    "section": "Weg 5",
    "text": "Weg 5\n\nskim(penguins)\n\n\nData summary\n\n\nName\npenguins\n\n\nNumber of rows\n344\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nbill_length_mm\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n▃▇▇▆▁\n\n\nbill_depth_mm\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n▅▅▇▇▂\n\n\nflipper_length_mm\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n▂▇▃▅▂\n\n\nbody_mass_g\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n▃▇▆▃▂\n\n\nyear\n0\n1.00\n2008.03\n0.82\n2007.0\n2007.00\n2008.00\n2009.0\n2009.0\n▇▁▇▁▇\n\n\n\n\n\n\nCategories:\n\nR\nwrangling\nna\nstring"
  },
  {
    "objectID": "posts/lm-Standardfehler/lm-Standardfehler.html",
    "href": "posts/lm-Standardfehler/lm-Standardfehler.html",
    "title": "lm-Standardfehler",
    "section": "",
    "text": "Man kann angeben, wie genau eine Schätzung von Regressionskoeffizienten die Grundgesamtheit widerspiegelt. Zumeist wird dazu der Standardfehler (engl. standard error, SE) verwendet.\nIn dieser Übung untersuchen wir, wie sich der SE als Funktion der Stichprobengröße, \\(n\\), verhält.\nErstellen Sie dazu folgenden Datensatz:\n\nlibrary(tidyverse)\n\nn &lt;- 2^4\n\nd &lt;-\n  tibble(x = rnorm(n = n),  # im Default: mean = 0, sd = 1\n         y = x + rnorm(n, mean = 0, sd = .5))\n\nHier ist das Ergebnis. Uns interessiert v.a. Std. Error für den Prädiktor x:\n\nlm(y ~ x, data = d) %&gt;% \nsummary()\n\n\nCall:\nlm(formula = y ~ x, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8039 -0.3986 -0.1500  0.6939  0.8013 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.1610     0.1446   1.114    0.284    \nx             1.1144     0.1660   6.711 9.92e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5777 on 14 degrees of freedom\nMultiple R-squared:  0.7629,    Adjusted R-squared:  0.746 \nF-statistic: 45.04 on 1 and 14 DF,  p-value: 9.919e-06\n\n\nHier haben wir eine Tabelle mit zwei Variablen, x und y, definiert mit n=16.\nVerdoppeln Sie die Stichprobengröße 5 Mal und betrachten Sie, wie sich die Schätzgenauigkeit, gemessen über den SE, verändert. Berechnen Sie dazu für jedes n eine Regression mit x als Prädiktor und y als AV!\nBei welcher Stichprobengröße ist SE am kleinsten?\n\n\n\n\\(2^5\\)\n\\(2^6\\)\n\\(2^7\\)\n\\(2^8\\)\n\\(2^9\\)"
  },
  {
    "objectID": "posts/lm-Standardfehler/lm-Standardfehler.html#answerlist",
    "href": "posts/lm-Standardfehler/lm-Standardfehler.html#answerlist",
    "title": "lm-Standardfehler",
    "section": "",
    "text": "\\(2^5\\)\n\\(2^6\\)\n\\(2^7\\)\n\\(2^8\\)\n\\(2^9\\)"
  },
  {
    "objectID": "posts/lm-Standardfehler/lm-Standardfehler.html#answerlist-1",
    "href": "posts/lm-Standardfehler/lm-Standardfehler.html#answerlist-1",
    "title": "lm-Standardfehler",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nFalsch\nWahr. Die größte Stichprobe impliziert den kleinsten SE, ceteris paribus.\n\n\nCategories:\n\ninference\nlm\nqm2"
  },
  {
    "objectID": "posts/iq07/iq07.html",
    "href": "posts/iq07/iq07.html",
    "title": "iq07",
    "section": "",
    "text": "Aufgabe\nIntelligenz wird häufig mittels einem IQ-Test ermittelt.\nIn einer Population gebe es zwei Subgruppen, für die gilt:\n\\(IQ_1 \\sim N(85, 15)\\) \\(IQ_2 \\sim N(115, 15)\\)\nWie groß ist die Wahrscheinlichkeit, dass eine zufällig gezogene Person einen IQ-Wert von mind. 115 Punkten hat?\nHinweise:\n\nNutzen Sie Simulationsmethoden.\nGehen Sie von folgender IQ-Verteilung aus: \\(IQ \\sim N(100,15)\\)\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\nSimulieren Sie \\(n=10^3\\) Stichproben pro Subpopulation.\nNutzen Sie die Zahl 42 als Startwert für Ihre Zufallszahlen (um die Reproduzierbarkeit zu gewährleisten)\n\n         \n\n\nLösung\n\nlibrary(tidyverse)\n\nWir simulieren die Daten; Subpopulation 1:\n\nset.seed(42)\n\nd1 &lt;- tibble(\n  id = 1:10^3,\n  iq = rnorm(n = 10^3, mean = 85, sd = 15))\n\nSubpopulation 2:\n\nset.seed(42)\n\nd2 &lt;- tibble(\n  id = 1:10^3,\n  iq = rnorm(n = 10^3, mean = 115, sd = 15))\n\nDann kombinieren wir die Daten zu einer Tabelle:\n\nd &lt;-\n  d1 %&gt;% \n  bind_rows(d2)\n\nDann filtern wir wie in der Angabe gefragt:\n\nsolution_d &lt;-\n  d %&gt;% \n  count(iq &gt; 115) %&gt;% \n  mutate(prop = n / sum(n))\n\nsolution_d\n\n# A tibble: 2 × 3\n  `iq &gt; 115`     n  prop\n  &lt;lgl&gt;      &lt;int&gt; &lt;dbl&gt;\n1 FALSE       1494 0.747\n2 TRUE         506 0.253\n\n\nDie Lösung lautet also 0.253.\nWenn Sie die Zufallszahlen mit set.seed fixiert haben, sollten Sie den exakt gleichen Wert gefunden haben.\nInteressant ist es vielleicht, die Gesamtpopulation zu visualisieren:\n\nggplot(d) +\n  aes(x = iq) +\n  geom_density()\n\n\n\n\n\n\n\n\nIm Vergleich dazu eine Normalverteilung mit MW=100 und SD=15:\n\n\n\n\n\n\n\n\n\nWir sehen, dass unsere Population über eine (deutlich) höhere Streuung verfügt:\n\nd %&gt;% \n  summarise(sd(iq))\n\n# A tibble: 1 × 1\n  `sd(iq)`\n     &lt;dbl&gt;\n1     21.2\n\n\n\nCategories:\n\nprobability\nsimulation\nnormal-distribution\nnum"
  },
  {
    "objectID": "posts/wfsets_penguins01/wfsets_penguins01.html",
    "href": "posts/wfsets_penguins01/wfsets_penguins01.html",
    "title": "wfsets_penguins01",
    "section": "",
    "text": "Berechnen Sie die Vorhersagegüte (RMSE) für folgende Lernalgorithmen:\n\nlineares Modell\nknn (neighbors: tune)\n\nModellgleichung: body_mass_g ~ bill_length_mm, data = d_train.\nNutzen Sie minimale Vorverarbeitung."
  },
  {
    "objectID": "posts/wfsets_penguins01/wfsets_penguins01.html#setup",
    "href": "posts/wfsets_penguins01/wfsets_penguins01.html#setup",
    "title": "wfsets_penguins01",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidymodels)\ndata(penguins, package = \"palmerpenguins\")"
  },
  {
    "objectID": "posts/wfsets_penguins01/wfsets_penguins01.html#daten",
    "href": "posts/wfsets_penguins01/wfsets_penguins01.html#daten",
    "title": "wfsets_penguins01",
    "section": "Daten",
    "text": "Daten\n\nd &lt;-\n  penguins %&gt;% \n  drop_na()\n\n\nd_split &lt;- initial_split(d)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)"
  },
  {
    "objectID": "posts/wfsets_penguins01/wfsets_penguins01.html#modelle",
    "href": "posts/wfsets_penguins01/wfsets_penguins01.html#modelle",
    "title": "wfsets_penguins01",
    "section": "Modelle",
    "text": "Modelle\nLineares Modell:\n\nmod_lin &lt;- linear_reg()\n\nmod_knn &lt;- nearest_neighbor(mode = \"regression\",\n                                  neighbors = tune())"
  },
  {
    "objectID": "posts/wfsets_penguins01/wfsets_penguins01.html#rezepte",
    "href": "posts/wfsets_penguins01/wfsets_penguins01.html#rezepte",
    "title": "wfsets_penguins01",
    "section": "Rezepte",
    "text": "Rezepte\n\nrec_basic &lt;- recipe(body_mass_g ~ bill_length_mm, data = d_train) %&gt;% \n         step_normalize(all_predictors())\n\nrec_basic"
  },
  {
    "objectID": "posts/wfsets_penguins01/wfsets_penguins01.html#resampling",
    "href": "posts/wfsets_penguins01/wfsets_penguins01.html#resampling",
    "title": "wfsets_penguins01",
    "section": "Resampling",
    "text": "Resampling\n\nrsmpls &lt;- vfold_cv(d_train)"
  },
  {
    "objectID": "posts/wfsets_penguins01/wfsets_penguins01.html#workflow-set",
    "href": "posts/wfsets_penguins01/wfsets_penguins01.html#workflow-set",
    "title": "wfsets_penguins01",
    "section": "Workflow Set",
    "text": "Workflow Set\n\nwf_set &lt;-\n  workflow_set(\n    preproc = list(rec_simple = rec_basic),\n    models = list(mod_lm = mod_lin,\n                  mod_nn = mod_knn)\n  )\n\nwf_set\n\n# A workflow set/tibble: 2 × 4\n  wflow_id          info             option    result    \n  &lt;chr&gt;             &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 rec_simple_mod_lm &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 rec_simple_mod_nn &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;"
  },
  {
    "objectID": "posts/wfsets_penguins01/wfsets_penguins01.html#fitten",
    "href": "posts/wfsets_penguins01/wfsets_penguins01.html#fitten",
    "title": "wfsets_penguins01",
    "section": "Fitten",
    "text": "Fitten\n\nwf_fit &lt;-\n  wf_set %&gt;% \n  workflow_map(resamples = rsmpls)\n\nwf_fit\n\n# A workflow set/tibble: 2 × 4\n  wflow_id          info             option    result   \n  &lt;chr&gt;             &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n1 rec_simple_mod_lm &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n2 rec_simple_mod_nn &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;tune[+]&gt;\n\n\nCheck:\n\nwf_fit %&gt;% pluck(\"result\")\n\n[[1]]\n# Resampling results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits           id     .metrics         .notes          \n   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;          \n 1 &lt;split [224/25]&gt; Fold01 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [224/25]&gt; Fold02 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [224/25]&gt; Fold03 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [224/25]&gt; Fold04 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [224/25]&gt; Fold05 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [224/25]&gt; Fold06 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [224/25]&gt; Fold07 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [224/25]&gt; Fold08 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [224/25]&gt; Fold09 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [225/24]&gt; Fold10 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n\n[[2]]\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits           id     .metrics          .notes          \n   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n 1 &lt;split [224/25]&gt; Fold01 &lt;tibble [16 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [224/25]&gt; Fold02 &lt;tibble [16 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [224/25]&gt; Fold03 &lt;tibble [16 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [224/25]&gt; Fold04 &lt;tibble [16 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [224/25]&gt; Fold05 &lt;tibble [16 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [224/25]&gt; Fold06 &lt;tibble [16 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [224/25]&gt; Fold07 &lt;tibble [16 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [224/25]&gt; Fold08 &lt;tibble [16 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [224/25]&gt; Fold09 &lt;tibble [16 × 5]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [225/24]&gt; Fold10 &lt;tibble [16 × 5]&gt; &lt;tibble [0 × 3]&gt;"
  },
  {
    "objectID": "posts/wfsets_penguins01/wfsets_penguins01.html#bester-kandidat",
    "href": "posts/wfsets_penguins01/wfsets_penguins01.html#bester-kandidat",
    "title": "wfsets_penguins01",
    "section": "Bester Kandidat",
    "text": "Bester Kandidat\n\nautoplot(wf_fit)\n\n\n\n\n\n\n\n\n\nautoplot(wf_fit, select_best = TRUE)\n\n\n\n\n\n\n\n\n\nrank_results(wf_fit, rank_metric = \"rmse\") %&gt;% \n  filter(.metric == \"rmse\")\n\n# A tibble: 9 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 rec_simple_mod_nn Prepro… rmse     642.    31.3    10 recipe       near…     1\n2 rec_simple_mod_nn Prepro… rmse     646.    30.9    10 recipe       near…     2\n3 rec_simple_mod_lm Prepro… rmse     647.    24.0    10 recipe       line…     3\n4 rec_simple_mod_nn Prepro… rmse     648.    32.2    10 recipe       near…     4\n5 rec_simple_mod_nn Prepro… rmse     659.    31.7    10 recipe       near…     5\n6 rec_simple_mod_nn Prepro… rmse     660.    32.2    10 recipe       near…     6\n7 rec_simple_mod_nn Prepro… rmse     687.    36.4    10 recipe       near…     7\n8 rec_simple_mod_nn Prepro… rmse     729.    39.7    10 recipe       near…     8\n9 rec_simple_mod_nn Prepro… rmse     786.    47.6    10 recipe       near…     9\n\n\nAm besten war das lineare Modell, aber schauen wir uns auch mal das knn-Modell an, v.a. um zu wissen, wie man den besten Tuningparameter-Wert sieht:\n\nextract_workflow_set_result(wf_fit, \"rec_simple_mod_nn\") %&gt;% \n  select_best()\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\n\n# A tibble: 1 × 2\n  neighbors .config             \n      &lt;int&gt; &lt;chr&gt;               \n1        14 Preprocessor1_Model8"
  },
  {
    "objectID": "posts/wfsets_penguins01/wfsets_penguins01.html#last-fit",
    "href": "posts/wfsets_penguins01/wfsets_penguins01.html#last-fit",
    "title": "wfsets_penguins01",
    "section": "Last Fit",
    "text": "Last Fit\n\nbest_wf &lt;-\n  wf_fit %&gt;% \n  extract_workflow(\"rec_simple_mod_lm\")\n\nFinalisieren müssen wir diesen Workflow nicht, da er keine Tuningparameter hatte.\n\nfit_final &lt;-\n  best_wf %&gt;% \n  last_fit(d_split)"
  },
  {
    "objectID": "posts/wfsets_penguins01/wfsets_penguins01.html#modellgüte-im-test-set",
    "href": "posts/wfsets_penguins01/wfsets_penguins01.html#modellgüte-im-test-set",
    "title": "wfsets_penguins01",
    "section": "Modellgüte im Test-Set",
    "text": "Modellgüte im Test-Set\n\ncollect_metrics(fit_final)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard     658.    Preprocessor1_Model1\n2 rsq     standard       0.342 Preprocessor1_Model1\n\n\n\nCategories:\n\nR\nstatlearning\ntidymodels\nnum"
  },
  {
    "objectID": "posts/Kaefer1/Kaefer1.html",
    "href": "posts/Kaefer1/Kaefer1.html",
    "title": "Kaefer1",
    "section": "",
    "text": "Weltsensation?! Der Insektenforscher Prof. Mügge ist der Meinung, eine bislang unbekannte Käferart entdeckt zu haben. Nach nur 18 Monaten Feldforschung im brasilianischen Regenwald gelang ihm dieser Durchbruch. Wenn es denn nun wirklich eine neue Art ist. Gerade untersucht er ein Exemplar unter dem Mikroskop. Hm, was ist das für ein Tier? 🐛 🔬\nDrei Arten kommen in Frage, \\(A_1, A_2, A_3\\).\nDabei ist die Art \\(A_1\\) sehr verbreitet und schon längst bekannt, \\(A_2\\) ist die neue Art, Exemplare dieser Art sind selten und \\(A_3\\) ist auch bekannt und eher häufig anzutreffen. Allerdings spricht das Aussehen am ehesten für \\(A_2\\), der seltenen Art.\n👉 Aufgabe: Wie groß ist die Wahrscheinlichkeit, dass Prof. Mügge wirklich einen großen Fang gemacht hat und einen unbekannten Käfer entdeckt hat?\nHier sind die genauen Vorkommenshäufigkeiten:\n\nPr_A1 &lt;- .6\nPr_A2 &lt;- .1\nPr_A3 &lt;- .4\n\nUnd hier die genauen Wahrscheinlichkeiten, wie typisch das beobachtete Objekt für einen Vertreter der jeweiligen Art ist:\n\nL_A1 &lt;- .5\nL_A2 &lt;- .9\nL_A3 &lt;- .4\n\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks."
  },
  {
    "objectID": "posts/Kaefer1/Kaefer1.html#setup",
    "href": "posts/Kaefer1/Kaefer1.html#setup",
    "title": "Kaefer1",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(prada)  # für Funktion `bayesbox`\n\n\nbb &lt;- bayesbox(hyps = c(\"A\", \"B\", \"C\"),\n               priors = c(Pr_A1, Pr_A2, Pr_A3),\n               liks = c(L_A1, L_A2, L_A3))\n\nbb\n\n  hyps priors liks post_unstand  post_std\n1    A    0.6  0.5         0.30 0.5454545\n2    B    0.1  0.9         0.09 0.1636364\n3    C    0.4  0.4         0.16 0.2909091\n\n\n\nsol &lt;- .164\n\nDie Wahrscheinlichkeit, dass der Käfer zur Art “B” gehört, ist relativ klein: 16%.\n\nCategories:\n\nR\nbayes\nbayesbox\nnum"
  },
  {
    "objectID": "posts/nichtlineare-regr1/nichtlineare-regr1.html",
    "href": "posts/nichtlineare-regr1/nichtlineare-regr1.html",
    "title": "nichtlineare-regr1",
    "section": "",
    "text": "Aufgabe\nWir suchen ein Modell, das einen nichtlinearen Zusammenhang von PS-Zahl und Spritverbrauch darstellt (Datensatz mtcars).\nGeben Sie dafür ein mögliches Modell an! Nutzen Sie den R-Befehl lm.\n         \n\n\nLösung\n\nmtcars &lt;-\n  mtcars %&gt;% \n  mutate(mpg_log = log(mpg)) \n\nlm1 &lt;- lm(mpg_log ~ hp, data = mtcars)\nsummary(lm1)\n\n\nCall:\nlm(formula = mpg_log ~ hp, data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.41577 -0.06583 -0.01737  0.09827  0.39621 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.4604669  0.0785838  44.035  &lt; 2e-16 ***\nhp          -0.0034287  0.0004867  -7.045 7.85e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1858 on 30 degrees of freedom\nMultiple R-squared:  0.6233,    Adjusted R-squared:  0.6107 \nF-statistic: 49.63 on 1 and 30 DF,  p-value: 7.853e-08\n\n\nVisualisieren wir die Vorhersagen des Modells:\n\nmtcars &lt;-\n  mtcars %&gt;% \n  mutate(pred = predict(lm1))\n\n\nmtcars %&gt;% \n  ggplot() +\n  aes(x = hp) +\n  geom_line(aes( y = pred), color = \"blue\") +\n  geom_point(aes(y = mpg_log)) +\n  labs(y = \"log(mpg)\",\n       title = \"Vorhersage von log-mpg in einem Log-Y-Modell\")\n\n\n\n\n\n\n\n\nOder so visualisieren:\n\nlibrary(easystats)\nestimate_expectation(lm1) %&gt;% plot()\n\n\n\n\n\n\n\n\nMöchte man auf der Y-Achse mpg und nicht log(mpg) anzeigen, muss man den Logarithmus wieder “auflösen”, das erreicht man mit der Umkehrfunktion des Logarithmus, das Exponentieren (man “delogarithmiert”):\n\\[\\begin{aligned}\nlog(y) &= x \\qquad | \\text{Y in Log-Form}\\\\\n    exp(log(y)) &= exp(x)  \\qquad | \\text{Jetzt exponenzieren wir beide Seiten}\\\\\n    y = exp(x)\n\\end{aligned}\\]\nDabei gilt \\(exp(x) = e^x\\), mit \\(e\\) als Eulersche Zahl (2.71…).\n\nmtcars &lt;-\n  mtcars %&gt;% \n  mutate(pred_delog = exp(pred))  # delogarithmieren\n\n\nmtcars %&gt;% \n  ggplot() +\n  aes(x = hp) +\n  geom_line(aes( y = pred_delog), color = \"blue\") +\n  geom_point(aes(y = mpg_log)) +\n  labs(y = \"mpg\",\n       title = \"Vorhersage von mpg in einem Log-Y-Modell\")\n\n\n\n\n\n\n\n\n\nCategories:\n\nlm\nvis\nqm2\nregression\nstring"
  },
  {
    "objectID": "posts/stan_glm_parameterzahl/stan_glm_parameterzahl.html",
    "href": "posts/stan_glm_parameterzahl/stan_glm_parameterzahl.html",
    "title": "stan_glm_parameterzahl",
    "section": "",
    "text": "Exercise\nBerechnet man eine Posteriori-Verteilung mit stan_glm(), so kann man entweder die schwach informativen Prioriwerte der Standardeinstellung verwenden, oder selber Prioriwerte definieren.\nBetrachten Sie dazu dieses Modell:\nstan_glm(price ~ cut, data = diamonds, \n                   prior = normal(location = c(100, 100, 100, 100),\n                                  scale = c(100, 100, 100, 100)),\n                   prior_aux = exponential(1),\n                   prior_intercept = normal(3000, 500))\nWie viele Parameter gibt es in diesem Modell?\nHinweise:\n\nGeben Sie nur eine (ganze) Zahl ein.\n\n         \n\n\nSolution\nGrundsätzlich hat ein Regressionsmodell die folgenden Parameter:\n\neinen Parameter für den Intercept, \\(\\beta_0\\)\npro UV ein weiterer Parameter, \\(\\beta_1, \\beta_2, \\ldots\\)\nfür sigma (\\(\\sigma\\)) noch ein zusätzlicher Parameter\n\nZu beachten ist aber, dass bei einer nominalen Variablen mit zwei Stufen nur ein Regressionsgewicht (\\(\\beta_1\\)) berechnet wird. Allgemein gilt bei nominalen also, dass bei \\(k\\) Stufen nur \\(k-1\\) Regressionsgewichte berechnet werden.\nIm vorliegenden Fall hat die Variable cut 5 Stufen, also werden 4 Regressiongewiche berechnet.\nIn Summe werden also 6 Parameter berechnet.\nBerechnet man das Modell, so kann man sich auch Infos über die Prioris ausgeben lassen:\n\nm1 &lt;- stan_glm(price ~ cut, data = diamonds, \n               prior = normal(location = c(100, 100, 100, 100),\n                              scale = c(100, 100, 100, 100)),\n               prior_intercept = normal(3000, 500),\n               prior_aux = exponential(1),\n               refresh = 0)\n\nprior_summary(m1)\n\nPriors for model 'm1' \n------\nIntercept (after predictors centered)\n ~ normal(location = 3000, scale = 500)\n\nCoefficients\n ~ normal(location = [100,100,100,...], scale = [100,100,100,...])\n\nAuxiliary (sigma)\n ~ exponential(rate = 1)\n------\nSee help('prior_summary.stanreg') for more details\n\n\nWie man sieht, wird für die Streuung im Standard eine Exponentialverteilung verwendet von stan_glm(). Gibt man also nicht an - wie im Beispiel m1 oben, so wird stan_glm() für die Streuung, d.h. prior_aux eine Exponentialverteilung verwenden. Zu beachten ist, dass stan_glm() ein automatische Skalierung vornimmt.\nS. hier für weitere Erläuterung.\nMöchte man den Prior für die Streuung direkt ansprechen, so kann man das so formulieren:\n\nm2 &lt;- stan_glm(price ~ cut, data = diamonds, \n               prior = normal(location = c(100, 100, 100, 100),\n                              scale = c(100, 100, 100, 100)),\n               prior_intercept = normal(3000, 500),\n               prior_aux = exponential(1),\n               refresh = 0)\n\nprior_summary(m1)\n\nPriors for model 'm1' \n------\nIntercept (after predictors centered)\n ~ normal(location = 3000, scale = 500)\n\nCoefficients\n ~ normal(location = [100,100,100,...], scale = [100,100,100,...])\n\nAuxiliary (sigma)\n ~ exponential(rate = 1)\n------\nSee help('prior_summary.stanreg') for more details\n\n\nZu beachten ist beim selber definieren der Prioris, dass dann keine Auto-Skalierung von stan_glm() vorgenommen wird, es sei denn, man weist es explizit an:\n\nm3 &lt;- stan_glm(price ~ cut, data = diamonds, \n               prior = normal(location = c(100, 100, 100, 100),\n                              scale = c(100, 100, 100, 100),\n                              autoscale = TRUE),\n               prior_intercept = normal(3000, 500, autoscale = TRUE),\n               prior_aux = exponential(1, autoscale = TRUE),\n               chain = 1,  # nur 1 mal Stichproben ziehen, um Zeit zu sparen\n               refresh = 0)\n\nprior_summary(m3)\n\nPriors for model 'm3' \n------\nIntercept (after predictors centered)\n  Specified prior:\n    ~ normal(location = 3000, scale = 500)\n  Adjusted prior:\n    ~ normal(location = 3000, scale = 2e+06)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = [100,100,100,...], scale = [100,100,100,...])\n  Adjusted prior:\n    ~ normal(location = [100,100,100,...], scale = [1129833.17, 868199.02, 936606.47,...])\n\nAuxiliary (sigma)\n  Specified prior:\n    ~ exponential(rate = 1)\n  Adjusted prior:\n    ~ exponential(rate = 0.00025)\n------\nSee help('prior_summary.stanreg') for more details\n\n\nGrundsätzlich ist es nützlich für die numerische Stabilität, dass die Zahlen (hier die Parameterwerte) etwa die gleiche Größenordnung haben, am besten um die 0-1 herum. Daher bietet sich oft eine z-Standardisierung an.\nUnabhängig von der der Art der Parameter ist die Anzahl immer gleich.\nDie Anzahl der geschätzten Parameter werden im Modell-Summary unter Estimates gezeigt:\n\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      price ~ cut\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 53940\n predictors:   5\n\nEstimates:\n              mean   sd     10%    50%    90% \n(Intercept) 4027.0   22.1 3998.3 4026.8 4055.8\ncut.L       -248.8   52.7 -316.9 -249.5 -180.9\ncut.Q       -283.6   46.9 -342.3 -283.4 -223.2\ncut.C       -580.5   41.8 -633.6 -580.1 -527.2\ncut^4       -266.4   36.8 -314.6 -266.5 -218.6\nsigma       3830.6   11.1 3816.4 3830.4 3845.4\n\nFit Diagnostics:\n           mean   sd     10%    50%    90% \nmean_PPD 3931.9   23.1 3902.3 3931.8 3962.0\n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.4  1.0  3025 \ncut.L         1.1  1.0  2393 \ncut.Q         1.0  1.0  2135 \ncut.C         0.8  1.0  2550 \ncut^4         0.7  1.0  3132 \nsigma         0.1  1.0  7241 \nmean_PPD      0.4  1.0  3673 \nlog-posterior 0.0  1.0  1883 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\nDas sind:\n\n1 Intercept (Achsenabschnitt) - prior_intercept\n4 Gruppen (zusätzlich zur Referenzgruppe, die mit dem Achsenabschnitt dargestellt ist) - prior_normal\n1 Sigma (Ungewissheit “innerhalb des Modells”) - prior_aux\n\n\nCategories:\n~"
  },
  {
    "objectID": "posts/wskt-df-r/wskt-df-r.html",
    "href": "posts/wskt-df-r/wskt-df-r.html",
    "title": "wskt-df-r",
    "section": "",
    "text": "In dieser Aufgabe betrachten wir typische Relationen von Ereignissen, um typische Fragen der Wahrscheinlichkeitsrechnung zu beantworten.\nGegeben sei folgender Datensatz:\n\nd &lt;-\n  data.frame(\n    A = c(1, 1, 0, 0),\n    B = c(1, 0, 1, 0)\n  )\n\nd\n\n  A B\n1 1 1\n2 1 0\n3 0 1\n4 0 0\n\n\nDer Datensatz d stellt alle vier Kombinationen der beiden Variablen A und B da (wir gehen davon aus, dass es sich um binäre Variablen, wie Ereignisse, handelt, der Einfachheit halber).\nDabei steht A == 1 für \\(A\\) (Ereignis \\(A\\) ist der Fall) und A == 0 für \\(\\neg A\\), A tritt nicht ein, ist nicht der Fall.\nGenerell wird in der Wissenschaft und Technik 0 für “nein, falsch” und 1 für “ja, wahr, richtig” verwendet.\nAufgabe: Berechnen Sie mit R \\(Pr(A\\cap B), Pr(\\neg A\\cap B), Pr(A\\cup B), Pr(A|B), Pr(B|A)\\)!"
  },
  {
    "objectID": "posts/wskt-df-r/wskt-df-r.html#setup",
    "href": "posts/wskt-df-r/wskt-df-r.html#setup",
    "title": "wskt-df-r",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\n\nHier ist unserer Datentabelle:\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n1\n1\n\n\n1\n0\n\n\n0\n1\n\n\n0\n0"
  },
  {
    "objectID": "posts/wskt-df-r/wskt-df-r.html#pracap-b",
    "href": "posts/wskt-df-r/wskt-df-r.html#pracap-b",
    "title": "wskt-df-r",
    "section": "\\(Pr(A\\cap B)\\)",
    "text": "\\(Pr(A\\cap B)\\)\n\nd |&gt; \n  filter(A == 1 & B == 1)\n\n  A B\n1 1 1\n\n\nAlso 1 von 4 Zeilen, das heißt 1/4 oder .25.\nMan kann das auch mit count ausrechnen:\n\nd |&gt; \n  count(A == 1 & B == 1)\n\n  A == 1 & B == 1 n\n1           FALSE 3\n2            TRUE 1\n\n\nDer Operator & steht für das logische “UND” (Schnitt, intersect).\nUnd so kann man sich noch die Anteile ausrechnen lassen:\n\nd |&gt; \n  count(A == 1 & B == 1) |&gt; \n  mutate(Anteil = n / sum(n))\n\n  A == 1 & B == 1 n Anteil\n1           FALSE 3   0.75\n2            TRUE 1   0.25"
  },
  {
    "objectID": "posts/wskt-df-r/wskt-df-r.html#prneg-acap-b",
    "href": "posts/wskt-df-r/wskt-df-r.html#prneg-acap-b",
    "title": "wskt-df-r",
    "section": "\\(Pr(\\neg A\\cap B)\\)",
    "text": "\\(Pr(\\neg A\\cap B)\\)\n\\(Pr(\\neg A\\cap B)\\) ist im Prinzip identisch zum Schnitt ohne Negation:\n\nd |&gt; \n  count(A == 0 & B == 1) |&gt; \n  mutate(Anteil = n / sum(n))\n\n  A == 0 & B == 1 n Anteil\n1           FALSE 3   0.75\n2            TRUE 1   0.25\n\n\nOder so:\n\nd |&gt; \n  count(!(A == 1) & B == 1) |&gt; \n  mutate(Anteil = n / sum(n))\n\n  !(A == 1) & B == 1 n Anteil\n1              FALSE 3   0.75\n2               TRUE 1   0.25\n\n\nDer Operator ! entspricht der logischen Negation."
  },
  {
    "objectID": "posts/wskt-df-r/wskt-df-r.html#pracup-b",
    "href": "posts/wskt-df-r/wskt-df-r.html#pracup-b",
    "title": "wskt-df-r",
    "section": "\\(Pr(A\\cup B)\\)",
    "text": "\\(Pr(A\\cup B)\\)\nKommen wir zu \\(Pr(A\\cup B)\\), der logischen Vereinigung, auch logisches “ODER” genannt.\n\nd |&gt; \n  count((A == 1) | (B == 1)) |&gt; \n  mutate(Anteil = n / sum(n))\n\n  (A == 1) | (B == 1) n Anteil\n1               FALSE 1   0.25\n2                TRUE 3   0.75\n\n\nDer Operator | steht in R für das logische ODER.\nWie man sieht, kann man die Klammern um (A == 1) | (B == 1) verwenden für bessere Sichtbarkeit. Es ist aber nicht nötig."
  },
  {
    "objectID": "posts/wskt-df-r/wskt-df-r.html#prab",
    "href": "posts/wskt-df-r/wskt-df-r.html#prab",
    "title": "wskt-df-r",
    "section": "\\(Pr(A|B)\\)",
    "text": "\\(Pr(A|B)\\)\n\\(Pr(A|B)\\) entspricht einem Filtern, d.h. bedingen auf B entspricht einem Filtern, so dass nur noch \\(B\\) und nicht \\(\\neg B\\) übrig bleibt.\n\nd |&gt; \n  filter(B == 1) |&gt; \n  count(A)\n\n  A n\n1 0 1\n2 1 1\n\n\n1 Fall von 2 erfüllt die Bedingung A == 1, also 50%."
  },
  {
    "objectID": "posts/wskt-df-r/wskt-df-r.html#prba",
    "href": "posts/wskt-df-r/wskt-df-r.html#prba",
    "title": "wskt-df-r",
    "section": "\\(Pr(B|A)\\)",
    "text": "\\(Pr(B|A)\\)\nDiese Aufgabe ist analog zu \\(Pr(A|B)\\):\n\nd |&gt; \n  filter(A == 1) |&gt; \n  count(B)\n\n  B n\n1 0 1\n2 1 1"
  },
  {
    "objectID": "posts/wskt-df-r/wskt-df-r.html#bonus-prbneg-a",
    "href": "posts/wskt-df-r/wskt-df-r.html#bonus-prbneg-a",
    "title": "wskt-df-r",
    "section": "Bonus \\(Pr(B|\\neg A)\\)",
    "text": "Bonus \\(Pr(B|\\neg A)\\)\n\\(Pr(B|\\neg A)\\) - Eigentlich nichts Neues:\n\nd |&gt; \n  filter(!(A == 1)) |&gt; \n  count(B)\n\n  B n\n1 0 1\n2 1 1"
  },
  {
    "objectID": "posts/Diamonds-Histogramm-Vergleich/Diamonds-Histogramm-Vergleich.html",
    "href": "posts/Diamonds-Histogramm-Vergleich/Diamonds-Histogramm-Vergleich.html",
    "title": "Diamonds-Histogramm-Vergleich",
    "section": "",
    "text": "Die Daten beziehen sich auf den Datensatz diamonds und sind hier einzusehen bzw. können bei Interesse dort heruntergeladen werden.\n\n\n\n\n\n\n\n\n\nHinweise:\n\nMit “Facette” sind “Teilbilder” gemeint, die jeweils eine Teilgruppe der Daten visualisieren, z.B. könnte die Variable “Geschlecht” zwei Facetten (Frauen, Männer) beinhalten.\n\n\n\n\nDer vertikale Strich passt nicht auf den Median.\nEs ist nicht sinnvoll, die Gesamtverteilung zusätzlich zur Verteilung pro Gruppe in jeder Facette darzustellen.\nDen globalen Median (für den gesamten Datensatz, also über alle Gruppen hinweg) in jeder Facette darzustellen, ist redundant. Daher ist es besser, in jeder Facetten den Median pro Gruppe darzustellen.\nDie Verwendung einer Füllfarbe (Diagramm B) ist hier nicht sinnvoll."
  },
  {
    "objectID": "posts/Diamonds-Histogramm-Vergleich/Diamonds-Histogramm-Vergleich.html#answerlist",
    "href": "posts/Diamonds-Histogramm-Vergleich/Diamonds-Histogramm-Vergleich.html#answerlist",
    "title": "Diamonds-Histogramm-Vergleich",
    "section": "",
    "text": "Der vertikale Strich passt nicht auf den Median.\nEs ist nicht sinnvoll, die Gesamtverteilung zusätzlich zur Verteilung pro Gruppe in jeder Facette darzustellen.\nDen globalen Median (für den gesamten Datensatz, also über alle Gruppen hinweg) in jeder Facette darzustellen, ist redundant. Daher ist es besser, in jeder Facetten den Median pro Gruppe darzustellen.\nDie Verwendung einer Füllfarbe (Diagramm B) ist hier nicht sinnvoll."
  },
  {
    "objectID": "posts/Diamonds-Histogramm-Vergleich/Diamonds-Histogramm-Vergleich.html#answerlist-1",
    "href": "posts/Diamonds-Histogramm-Vergleich/Diamonds-Histogramm-Vergleich.html#answerlist-1",
    "title": "Diamonds-Histogramm-Vergleich",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nWahr\nFalsch\n\n\nCategories:\n\nvis\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/mutate01/mutate01.html",
    "href": "posts/mutate01/mutate01.html",
    "title": "mutate01",
    "section": "",
    "text": "Aufgabe\nIm Datensatz mariokart:\nErzeugen Sie eine Spalte zu_teuer, die folgende Prüfung durchführt: total_pr &gt; 100.\n         \n\n\nLösung\nPakete starten:\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.6.0 (red = needs update)\n✔ bayestestR  0.13.1   ✔ correlation 0.8.4 \n✔ datawizard  0.9.0    ✔ effectsize  0.8.6 \n✔ insight     0.19.6   ✔ modelbased  0.8.6 \n✔ performance 0.10.8   ✔ parameters  0.21.3\n✔ report      0.5.7    ✖ see         0.8.0 \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`.\n\nlibrary(tidyverse)  # startet das Paket tidyverse\n\nDaten importieren:\n\nmariokart &lt;- data_read(\"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\")\n\n\nmariokart &lt;- \n  mutate(mariokart, zu_teuer = total_pr &gt; 100)\n\nmariokart2 &lt;-\n  select(mariokart, total_pr, zu_teuer)\n\nhead(mariokart2)\n\n  total_pr zu_teuer\n1    51.55    FALSE\n2    37.04    FALSE\n3    45.50    FALSE\n4    44.00    FALSE\n5    71.00    FALSE\n6    45.00    FALSE\n\n\n\nCategories:\n\ndatawrangling\neda\ntidyverse\ndplyr\nnum"
  },
  {
    "objectID": "posts/kausal09/kausal09.html",
    "href": "posts/kausal09/kausal09.html",
    "title": "kausal09",
    "section": "",
    "text": "Ein Forschungsteam aus Epidemiologen untersucht den (möglicherweise kausalen) Zusammenhang von Erziehung (education) und Diabetes (diabetes). Das Team schlägt folgendes Modell zur Erklärung des Zusammenhangs vor (s. DAG).\n\n\n\n\n\n\n\n\n\nNochmal den gleich DAG ohne “Schilder”, damit man die Pfeilspitzen besser sieht:\n\n\n\n\n\n\n\n\n\nSollte die Krankengeschichte der Mutter hinsichtlich Diabetes kontrolliert werden, um den kausalen Effekt von Erziehung auf Diabetes zu identifizieren?\n\n\n\nNein, Mother's Diabetes sollte nicht kontrolliert werden, da so ein Collider Bias (Kollisionsverzerrung) resultiert.\nNein, Mother's Diabetes sollte nicht kontrolliert werden, da so eine Konfundierung resultiert.\nNein, Mother's Diabetes sollte nicht kontrolliert werden, da zwar keine Verzerrung entsteht, es aber auch nicht nötig ist.\nJa, Mother's Diabetes sollte kontrolliert werden, da so ein Collider Bias (Kollisionsverzerrung) vermieden wird.\nJa, Mother's Diabetes sollte kontrolliert werden, da so eine Konfundierung vermieden wird."
  },
  {
    "objectID": "posts/kausal09/kausal09.html#answerlist",
    "href": "posts/kausal09/kausal09.html#answerlist",
    "title": "kausal09",
    "section": "",
    "text": "Nein, Mother's Diabetes sollte nicht kontrolliert werden, da so ein Collider Bias (Kollisionsverzerrung) resultiert.\nNein, Mother's Diabetes sollte nicht kontrolliert werden, da so eine Konfundierung resultiert.\nNein, Mother's Diabetes sollte nicht kontrolliert werden, da zwar keine Verzerrung entsteht, es aber auch nicht nötig ist.\nJa, Mother's Diabetes sollte kontrolliert werden, da so ein Collider Bias (Kollisionsverzerrung) vermieden wird.\nJa, Mother's Diabetes sollte kontrolliert werden, da so eine Konfundierung vermieden wird."
  },
  {
    "objectID": "posts/kausal09/kausal09.html#answerlist-1",
    "href": "posts/kausal09/kausal09.html#answerlist-1",
    "title": "kausal09",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/MWberechnen/MWberechnen.html",
    "href": "posts/MWberechnen/MWberechnen.html",
    "title": "MWberechnen",
    "section": "",
    "text": "Question\n\nAufgabe\nBerechnen Sie den Mittelwert folgender Zahlenreihe; ignorieren sie etwaige fehlende Werte. Runden Sie auf zwei Dezimalstellen.\n\n\n[1]  0.38  0.49 -0.84 -1.64  0.04\n\n\n         \n\n\nLösung\nDer Mittelwert liegt bei -0.31.\nDie Antwort lautet -0.31.\nIn R kann man den Mittelwert z.B. so berechnen:\n\nmean(zahlenreihe, na.rm = TRUE)\n\n[1] -0.314\n\n\nDas Argument na.rm = TRUE sorgt dafür, dass R auch bei Vorhandensein fehlender Werte ein Ergebnis ausgibt. Ohne dieses Argument würde R ein sprödes NA zurückgeben, falls fehlende Werte vorliegen. Dieses Verhalten von R ist recht defensiv, getreu dem Motto: Wenn es ein Problem gibt, sollte man so früh wie möglich darüber deutlich informiert werden (und nicht erst, wenn die Marsrakete gestartet ist…).\n\nCategories:\n\neda\ndatawrangling\nnum\ndyn"
  },
  {
    "objectID": "posts/voll-normal/voll-normal.html",
    "href": "posts/voll-normal/voll-normal.html",
    "title": "voll-normal",
    "section": "",
    "text": "Exercise\nNehmen wir an, \\(k=10\\) voneinander unabhängige Eigenschaften \\(E_1, E_2, \\ldots, E_{10}\\) bestimmen, ob eine Person als “normal” angesehen wird. Jede dieser Eigenschaften kann entweder mit “normal” (n) oder aber “nichtnormal” (nn) ausgeprägt sein, wobei wir nicht genau vorhersagen können, wie diese Eigenschaften bei einer Person bestellt sein werden.\nAls Zufallsexperiment ausgedrückt: \\(\\Omega_E := \\{n, nn\\}\\) mit den zwei Ergebnissen \\(n\\) und \\(nn\\).\nMit der Wahrscheinlichkeit \\(Pr_{E_i} = 0.9\\) treffe das Ereignis \\(N_i := E_i = \\{n\\}\\) (für alle \\(i = 1, \\ldots, k\\)) zu.\nNehmen wir weiter an, als “voll normal” (\\(VN\\)) wird eine Person genau dann angesehen, wenn sie in allen \\(k\\) Eigenschaften “normal” ausgeprägt ist, das Ereignis \\(N\\) also für alle \\(k\\) Eigenschaften auftritt.\n\nNennen Sie Beispiele für mögliche Eigenschaften \\(E\\)!\nWie groß ist die Wahrscheinlichkeit - unter den hier geschilderten Annahmen -, dass eine Person “voll normal” ist?\nDiskutieren Sie die Plausibilität der Annahmen!\n\n         \n\n\nSolution\n\nIntelligenz, Aussehen, Gesundheit, Herkunft, Hautfarbe, sexuelle Identität oder Neigung, …\nFür unabhängige Ereignisse ist die Wahrscheinlichkeit, dass sie alle eintreten, gleich dem Produkt ihrer Einzelwahrscheinlichkeiten:\n\n\\(VN = Pr(E_i)^{10} = 0.9^{10} \\approx 0.3486784\\)\nDie Wahrscheinlichkeit, dass \\(VN\\) nicht eintritt (Nicht-Voll-Normal, NVN), ist dann die Gegenwahrscheinlichkeit: \\(NVN = 1- VN\\).\n\nMehrere der Annahmen sind diskutabel. So könnten die Eigenschaften nicht unabhängig sein, dann wäre der hier gezeigte Rechenweg nicht anwendbar. Die Wahrscheinlichkeit für “normal” könnte höher oder niedriger sein, wobei 90% nicht ganz unplausibel ist. Schließlich unterliegt das Ereignis \\(E_N\\) mit den Ergebnissen \\(n\\) bzw. \\(nn\\) sozialpsychologischen bzw. soziologischen Einflüssen und kann variieren.\n\n\nCategories:\n\nprobability\nmeta"
  },
  {
    "objectID": "posts/tidymodels3/tidymodels3.html",
    "href": "posts/tidymodels3/tidymodels3.html",
    "title": "tidymodels3",
    "section": "",
    "text": "Aufgabe\nBerechnen Sie ein lineares Modell mit tidymodels und zwar anhand des ames Datensatzes.\nModellgleichung: Sale_Price ~ Gr_Liv_Area, data = ames.\nBerechnen Sie ein multiplikatives (exponenzielles) Modell.\nRücktransformieren Sie die Log-Werte in “Roh-Dollar”.\n         \n\n\nLösung\nMultiplikatives Modell:\nNicht vergessen: AV-Transformation in beiden Samples!\nDatensatz aufteilen:\nModell definieren:\nModell fitten:\n\n\n\nCall:\nstats::lm(formula = Sale_Price ~ Gr_Liv_Area, data = data)\n\nCoefficients:\n(Intercept)  Gr_Liv_Area  \n  4.8552133    0.0002437  \n\n\nModellgüte im Train-Sample:\nModellgüte im Train-Sample:\n\n\n\nCall:\nstats::lm(formula = Sale_Price ~ Gr_Liv_Area, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.02587 -0.06577  0.01342  0.07202  0.39231 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 4.855e+00  7.355e-03  660.12   &lt;2e-16 ***\nGr_Liv_Area 2.437e-04  4.648e-06   52.43   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1271 on 2928 degrees of freedom\nMultiple R-squared:  0.4842,    Adjusted R-squared:  0.484 \nF-statistic:  2749 on 1 and 2928 DF,  p-value: &lt; 2.2e-16\n\n\nR-Quadrat via easystats:\n\n\n# R2 for Linear Regression\n       R2: 0.484\n  adj. R2: 0.484\n\n\n\n\n# A tibble: 2 × 5\n  term        estimate  std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) 4.86     0.00736        660.        0\n2 Gr_Liv_Area 0.000244 0.00000465      52.4       0\n\n\nVorhersagen im Test-Sample:\n\n\n# A tibble: 6 × 1\n  .pred\n  &lt;dbl&gt;\n1  5.07\n2  5.18\n3  5.31\n4  5.11\n5  5.18\n6  5.10\n\n\npreds ist ein Tibble, also müssen wir noch die Spalte .pred. herausziehen, z.B. mit pluck(preds, \".pred\"):\n\n\n# A tibble: 6 × 4\n  Sale_Price Gr_Liv_Area preds .pred\n       &lt;dbl&gt;       &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1       5.02         896  5.07  5.07\n2       5.24        1329  5.18  5.18\n3       5.60        1856  5.31  5.31\n4       5.15        1056  5.11  5.11\n5       5.26        1337  5.18  5.18\n6       4.98         987  5.10  5.10\n\n\nOder mit unnest:\n\n\n# A tibble: 6 × 3\n  Sale_Price Gr_Liv_Area .pred\n       &lt;dbl&gt;       &lt;int&gt; &lt;dbl&gt;\n1       5.02         896  5.07\n2       5.24        1329  5.18\n3       5.60        1856  5.31\n4       5.15        1056  5.11\n5       5.26        1337  5.18\n6       4.98         987  5.10\n\n\nOder wir binden einfach die Spalte an den Tibble:\n\n\n# A tibble: 6 × 3\n  Sale_Price Gr_Liv_Area .pred\n       &lt;dbl&gt;       &lt;int&gt; &lt;dbl&gt;\n1       5.02         896  5.07\n2       5.24        1329  5.18\n3       5.60        1856  5.31\n4       5.15        1056  5.11\n5       5.26        1337  5.18\n6       4.98         987  5.10\n\n\nModellgüte im Test-Sample:\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.517\n\n\nZur Interpretation von Log10-Werten\n\n\n[1] 5e+05\n\n\n[1] 0\n\n\nRücktransformation (ohne Bias-Korrektur):\n\nCategories:\n\nds1\ntidymodels\nprediction\nyacsda\nstatlearning\nlm\nnum"
  },
  {
    "objectID": "posts/abh-ereignisse/abh-ereignisse.html",
    "href": "posts/abh-ereignisse/abh-ereignisse.html",
    "title": "abh-ereignisse",
    "section": "",
    "text": "Aufgabe\n\n\n\n\n\n\n\n\n\nLesen Sie folgende Wahrscheinlichkeiten aus dem Diagramm ab:\n\n\\(Pr(A)\\)\n\\(Pr(B)\\)\n\\(Pr(AB)\\)\n\\(Pr(BA)\\)\n\\(Pr(B|A)\\)\n\\(Ür(A|B)\\)\n\nHinweise:\n\nDas Ereignis “B tritt ein” ist mit “B+” im Diagramm eingezeichnet (entsprechend für A). Analog ist das Ereignis “B tritt nicht ein” mit “B-” eingezeichnet (entsprechend für A).\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n         \n\n\nLösung\n\n\\(Pr(A) = 1/2 + 1/8 = 5/8\\)\n\\(Pr(B) = 1/2\\)\n\\(Pr(AB) = 1/2 \\cdot 3/4 = 3/8\\)\n\\(Pr(BA) = Pr(AB) = 3/8\\)\n\\(Pr(B|A) = 3/5\\)\n\\(Ür(A|B) = 3/4\\)\n\n\nCategories:\n\nR\nprobability\nstring"
  },
  {
    "objectID": "posts/vorhersageintervall1/vorhersageintervall1.html",
    "href": "posts/vorhersageintervall1/vorhersageintervall1.html",
    "title": "vorhersageintervall1",
    "section": "",
    "text": "Vorhersagen, etwa in einem Regressionsmodell, sind mit mehreren Arten von Unsicherheit konfrontiert.\nBerechnen Sie dazu ein Regressionsmodell, Datensatz mtcars, mit hp als Prädiktor (UV) und mpg als AV (Kriterium)!\nDann sagen Sie bitte den Wert der AV für eine Beobachtungseinheit mit mittlerer Ausprägung im Präktor vorher:\nEinmal nur unter Berücksichtigung der Unsicherheit innerhalb des Modells (“Konfidenzintervall”); einmal unter Berücksichtigung der Unsicherheit innerhalb des Modells sowie die Unsicherheit durch die Koffizienten (“Vohersageintervall”).\nHinweise:\n\npredict() ist eine Funktion, die Sie zur Vorhersage von Regressionsmodellen verwenden können.\nVerwenden Sie lm() zur Berechnung eines Regressionsmodells.\nDas Argument type von predict() erlaubt Ihnen die Wahl der Art der Vorhersage, betrachten Sie Hilfe der Funktion z.B. hier.\n\nBei welchem Intervall ist die Ungewissheit in der Vorhersage größer?\n\n\n\nKonfidenzintervall\nVohersageintervall\nGleich groß\nKommt auf weitere Faktoren an, keine pauschale Antwort möglich"
  },
  {
    "objectID": "posts/vorhersageintervall1/vorhersageintervall1.html#answerlist",
    "href": "posts/vorhersageintervall1/vorhersageintervall1.html#answerlist",
    "title": "vorhersageintervall1",
    "section": "",
    "text": "Konfidenzintervall\nVohersageintervall\nGleich groß\nKommt auf weitere Faktoren an, keine pauschale Antwort möglich"
  },
  {
    "objectID": "posts/vorhersageintervall1/vorhersageintervall1.html#answerlist-1",
    "href": "posts/vorhersageintervall1/vorhersageintervall1.html#answerlist-1",
    "title": "vorhersageintervall1",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\nFalsch\nFalsch\n\n\nCategories:\n\nlm\ninference\nqm2"
  },
  {
    "objectID": "posts/variation02/variation02.html",
    "href": "posts/variation02/variation02.html",
    "title": "variability02",
    "section": "",
    "text": "In welchem Datensatz (x1-x4) gibt es am meisten Variation?\nDatensatz A:\n\n\n\n\n\n\n\n\n\nDatensatz B:\n\n\n\n\n\n\n\n\n\nDatensatz C:\n\n\n\n\n\n\n\n\n\nDatensatz D:\n\n\n\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD"
  },
  {
    "objectID": "posts/variation02/variation02.html#answerlist",
    "href": "posts/variation02/variation02.html#answerlist",
    "title": "variability02",
    "section": "",
    "text": "A\nB\nC\nD"
  },
  {
    "objectID": "posts/variation02/variation02.html#answerlist-1",
    "href": "posts/variation02/variation02.html#answerlist-1",
    "title": "variability02",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nWahr\n\n\nCategories:\n\nvariablity\nbasics\nschoice"
  },
  {
    "objectID": "posts/Rethink2m7/Rethink2m7.html",
    "href": "posts/Rethink2m7/Rethink2m7.html",
    "title": "Rethink2m7",
    "section": "",
    "text": "Aufgabe\nThis question is taken from McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Ed.). Taylor and Francis, CRC Press.\n2M7. Assume again the original card problem, with a single card showing a black side face up. Before looking at the other side, we draw another card from the bag and lay it face up on the table. The face that is shown on the new card is white. Show that the probability that the first card, the one showing a black side, has black on its other side is now 0.75. Use the counting method, if you can. Hint: Treat this like the sequence of globe tosses, counting all the ways to see each observation, for each possiible first card.\n         \n\n\nLösung\nLet’s label the cards bb (black on both sides), bw (black on one, white on the other), and ww (both sides are white), respectively.\nTo keep things straight, here’s a visualization of our data.\n\n\n\n\n\n\n\n\n\nWanted is the probability \\(Pr(c1=bb|1b,2w)\\), the probability of drawing (as card 1) a bb card, given that we observerd b in the first draw, denoted as 1b, and a white card in the second draw, denoted as 2w.\nLet’s draw a tree diagram for easier comprehension.\n\n\n\n\n\n\nIn the diagram, the symbol “_b_w” means that black face of a the bw-card (one black, one white face) was drawn. Similarly, “_b_b” means that one (of the two) black faces of the bb-card (two black faces) was drawn.\nHere, we have to consider two cards. Let’s use this notation ww-bb for the sequence “first card is white on both sides, second card is black on both sides”.\nThe data observed is: first card has one black side, the second card has one white side, i.,e b-w.\nLooking at the tree, we realize that out of all 8 paths, 6 feature the bb card as first card:\n\\(Pr(1bb|b,w) = 6/8 = 3/4 = 0.75\\)\nwhere 1bb means “card 1 is black on both sides”, and b,w means “first draw showed a black face, and second card showed a white face”.\nIn other words, there are 8 valid paths in the tree diagram, out of which 6 belong the the hypothesis that the first card is all black.\nUsing a Bayes-Grid (or “Bayes-Box”), we can depict the situation like this:\n\n\n\n\n\nHyp\nPrior\nL\nunstand_Post\nPost\n\n\n\n\nbb\n1\n6\n6\n6/8 = 3/4\n\n\nbw\n1\n2\n2\n2/8 = 1/4\n\n\nww\n1\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOr, using probability, and not counts:\n\n\n\n\n\nHyp\nPrior\nL\nunstand_Post\nPost\n\n\n\n\nbb\n2\n3/4\n6/4\n3/4\n\n\nbw\n1\n2/4\n2/4\n1/4\n\n\n\n\n\nWhenever the probability of all paths (in a tree diagram) is the same, as it is the case in the present example, we do not need to write down the probability of the path for the likelihood. It is enough to write the number of paths (of course we can if we want).\n\nCategories:\n\nprobability\nbayes\nbayes-grid\nrethink-chap2\nstring"
  },
  {
    "objectID": "posts/germeval10-wordvec-rf/germeval10-wordvec-rf.html",
    "href": "posts/germeval10-wordvec-rf/germeval10-wordvec-rf.html",
    "title": "germeval10-wordvec-rf",
    "section": "",
    "text": "Erstellen Sie ein prädiktives Modell für Textdaten. Nutzen Sie deutsche Word-Vektoren für das Feature-Engineering.\nNutzen Sie die GermEval-2018-Daten.\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\nDie Daten sind auch über das R-Paket PradaData zu beziehen.\n\nlibrary(tidyverse)\ndata(\"germeval_train\", package = \"pradadata\")\ndata(\"germeval_test\", package = \"pradadata\")\n\nDie AV lautet c1. Die (einzige) UV lautet: text.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\nNutzen Sie Tidymodels.\nNutzen Sie Wikipedia2Vec als Grundlage für die Wordembeddings in deutscher Sprache. Laden Sie die Daten herunter (Achtung: ca. 2.8 GB)."
  },
  {
    "objectID": "posts/germeval10-wordvec-rf/germeval10-wordvec-rf.html#deutsche-textvektoren-importieren",
    "href": "posts/germeval10-wordvec-rf/germeval10-wordvec-rf.html#deutsche-textvektoren-importieren",
    "title": "germeval10-wordvec-rf",
    "section": "Deutsche Textvektoren importieren",
    "text": "Deutsche Textvektoren importieren\n\ntic()\nwiki_de_embeds &lt;- arrow::read_feather(\n  file = \"/Users/sebastiansaueruser/datasets/word-embeddings/wikipedia2vec/part-0.arrow\")\ntoc()\n\n0.743 sec elapsed\n\nnames(wiki_de_embeds)[1] &lt;- \"word\"\n\nwiki &lt;- as_tibble(wiki_de_embeds)"
  },
  {
    "objectID": "posts/germeval10-wordvec-rf/germeval10-wordvec-rf.html#workflow",
    "href": "posts/germeval10-wordvec-rf/germeval10-wordvec-rf.html#workflow",
    "title": "germeval10-wordvec-rf",
    "section": "Workflow",
    "text": "Workflow\n\n# model:\nmod1 &lt;-\n  rand_forest(mode = \"classification\",\n              mtry = tune())\n\n# recipe:\nrec1 &lt;-\n  recipe(c1 ~ ., data = d_train) |&gt; \n  update_role(id, new_role = \"id\")  |&gt; \n  #update_role(c2, new_role = \"ignore\") |&gt; \n  step_tokenize(text) %&gt;%\n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") |&gt; \n  step_word_embeddings(text,\n                       embeddings = wiki,\n                       aggregation = \"mean\")\n\n# workflow:\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(mod1) %&gt;% \n  add_recipe(rec1)"
  },
  {
    "objectID": "posts/germeval10-wordvec-rf/germeval10-wordvec-rf.html#preppenbaken",
    "href": "posts/germeval10-wordvec-rf/germeval10-wordvec-rf.html#preppenbaken",
    "title": "germeval10-wordvec-rf",
    "section": "Preppen/Baken",
    "text": "Preppen/Baken\n\ntic()\nrec1_prepped &lt;- prep(rec1)\ntoc()\n\n68.465 sec elapsed\n\n\n\nd_train_baked &lt;-\n  bake(rec1_prepped, new_data = NULL)\nhead(d_train_baked)\n\n# A tibble: 6 × 102\n     id c1      wordembed_text_V2 wordembed_text_V3 wordembed_text_V4\n  &lt;int&gt; &lt;fct&gt;               &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n1     1 OTHER             -0.0648          -0.0664             0.0242\n2     2 OTHER             -0.207           -0.0102            -0.118 \n3     3 OTHER             -0.246           -0.110             -0.195 \n4     4 OTHER             -0.0139          -0.241             -0.135 \n5     5 OFFENSE           -0.0803          -0.164             -0.160 \n6     6 OTHER             -0.195           -0.00456           -0.132 \n# ℹ 97 more variables: wordembed_text_V5 &lt;dbl&gt;, wordembed_text_V6 &lt;dbl&gt;,\n#   wordembed_text_V7 &lt;dbl&gt;, wordembed_text_V8 &lt;dbl&gt;, wordembed_text_V9 &lt;dbl&gt;,\n#   wordembed_text_V10 &lt;dbl&gt;, wordembed_text_V11 &lt;dbl&gt;,\n#   wordembed_text_V12 &lt;dbl&gt;, wordembed_text_V13 &lt;dbl&gt;,\n#   wordembed_text_V14 &lt;dbl&gt;, wordembed_text_V15 &lt;dbl&gt;,\n#   wordembed_text_V16 &lt;dbl&gt;, wordembed_text_V17 &lt;dbl&gt;,\n#   wordembed_text_V18 &lt;dbl&gt;, wordembed_text_V19 &lt;dbl&gt;, …"
  },
  {
    "objectID": "posts/germeval10-wordvec-rf/germeval10-wordvec-rf.html#tuninigfitting",
    "href": "posts/germeval10-wordvec-rf/germeval10-wordvec-rf.html#tuninigfitting",
    "title": "germeval10-wordvec-rf",
    "section": "Tuninig/Fitting",
    "text": "Tuninig/Fitting\n\ntic()\nwf_fit &lt;-\n  wf1 %&gt;% \n  tune_grid(\n    grid = 5,\n    resamples = vfold_cv(strata = c1, \n                         v = 5,\n                         data = d_train),\n    control = control_grid(save_pred = TRUE,\n                           verbose = TRUE,\n                           save_workflow = FALSE)) \ntoc()\nbeep()\n\nOder das schon in grauer Vorzeit berechnete Objekt importieren:\n\nwf_fit &lt;- read_rds(\"/Users/sebastiansaueruser/github-repos/rexams-exercises/objects/germeval10-wordvec-rf.rds\")"
  },
  {
    "objectID": "posts/germeval10-wordvec-rf/germeval10-wordvec-rf.html#plot-performance",
    "href": "posts/germeval10-wordvec-rf/germeval10-wordvec-rf.html#plot-performance",
    "title": "germeval10-wordvec-rf",
    "section": "Plot performance",
    "text": "Plot performance\n\nautoplot(wf_fit)\n\n\n\n\n\n\n\n\n\nshow_best(wf_fit)\n\n# A tibble: 5 × 7\n   mtry .metric .estimator  mean     n std_err .config             \n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1     2 roc_auc binary     0.771     5 0.00846 Preprocessor1_Model3\n2    32 roc_auc binary     0.749     5 0.00878 Preprocessor1_Model4\n3    41 roc_auc binary     0.747     5 0.00859 Preprocessor1_Model2\n4    67 roc_auc binary     0.741     5 0.00952 Preprocessor1_Model1\n5    96 roc_auc binary     0.739     5 0.00883 Preprocessor1_Model5"
  },
  {
    "objectID": "posts/germeval10-wordvec-rf/germeval10-wordvec-rf.html#finalisieren",
    "href": "posts/germeval10-wordvec-rf/germeval10-wordvec-rf.html#finalisieren",
    "title": "germeval10-wordvec-rf",
    "section": "Finalisieren",
    "text": "Finalisieren\n\nbest_params &lt;- select_best(wf_fit)\ntic()\nwf_finalized &lt;- finalize_workflow(wf1, best_params)\nlastfit1 &lt;- fit(wf_finalized, data = d_train)\ntoc()\n\n21.916 sec elapsed"
  },
  {
    "objectID": "posts/germeval10-wordvec-rf/germeval10-wordvec-rf.html#test-set-güte",
    "href": "posts/germeval10-wordvec-rf/germeval10-wordvec-rf.html#test-set-güte",
    "title": "germeval10-wordvec-rf",
    "section": "Test-Set-Güte",
    "text": "Test-Set-Güte\n\ntic()\npreds &lt;-\n  predict(lastfit1, new_data = germeval_test)\ntoc()\n\n10.773 sec elapsed\n\n\n\nd_test &lt;-\n  germeval_test |&gt; \n  bind_cols(preds) |&gt; \n  mutate(c1 = as.factor(c1))\n\n\nmy_metrics &lt;- metric_set(accuracy, f_meas)\nmy_metrics(d_test,\n           truth = c1,\n           estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.689\n2 f_meas   binary         0.196"
  },
  {
    "objectID": "posts/germeval10-wordvec-rf/germeval10-wordvec-rf.html#fazit",
    "href": "posts/germeval10-wordvec-rf/germeval10-wordvec-rf.html#fazit",
    "title": "germeval10-wordvec-rf",
    "section": "Fazit",
    "text": "Fazit\nwikipedia2vec ist für die deutsche Sprache vorgekocht. Das macht Sinn für einen deutschsprachigen Corpus.\nDas Modell braucht doch ganz schön viel Rechenzeit.\nAchtung: Mit dem Parameter save_pred = TRUE wird der Workflow größer als 3 GB.\n\nCategories:\n\ntextmining\ndatawrangling\ngermeval\nprediction\ntidymodels\nstring"
  },
  {
    "objectID": "posts/count-emoji/count-emoji.html",
    "href": "posts/count-emoji/count-emoji.html",
    "title": "count-emoji",
    "section": "",
    "text": "Aufgabe\nGegeben eines (mehrelementigen) Strings, my_string, und eines Lexicons, my_lexicon, zählen Sie, wie häufig sich ein Emoji in einem Element des Strings wiederfindet.\n\nmy_string &lt;-\n  c(\"Heute ist ein schöner Tag 😄😄\", \"Was geht in dieser Woche?\", \"Super 🙂\")\n\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\nNutzen Sie die Funktion emo::ji.\n\n         \n\n\nLösung\n\nlibrary(emo)\nlibrary(purrr)\n\n\nmap_int(my_string, ji_count)\n\n[1] 2 0 1\n\n\n\nCategories:\n\ntextmining\nnlp\nstring"
  },
  {
    "objectID": "posts/stan_glm01/stan_glm01.html",
    "href": "posts/stan_glm01/stan_glm01.html",
    "title": "stan_glm01",
    "section": "",
    "text": "Exercise\nGegeben dem folgenden Modell, geben Sie den Befehl mit stan_glm() an, um die Posteriori-Verteilung zu berechnen.\nLikelihood: \\(h_i \\sim \\mathcal{N}(\\mu, \\sigma)\\)\nPrior für \\(\\mu\\): \\(\\mu \\sim \\mathcal{N}(0, 1)\\) k\n         \n\n\nSolution\n\nlibrary(rstanarm)\n\n\nmodel &lt;-\n  stan_glm(h ~ 1,\n           prior_intercept = normal(0,1),\n           prior_aux = exponential(0.1),\n           daten = meine_Daten\n  )\n\n\nCategories:\n\nprobability\nbayes"
  },
  {
    "objectID": "posts/kausal08/kausal08.html",
    "href": "posts/kausal08/kausal08.html",
    "title": "kausal08",
    "section": "",
    "text": "Gegeben sei die Theorie (oder schlichter: das Modell), demzufolge eine Anlage zu Suchtverhalten die Ursache von sowohl Rauchen als auch Kaffeegewohnheit darstellt. Lungenkrebs wiederum hat als (alleinige) Ursache Rauchen (laut diesem Modell).\nDaten zeigen, dass Kaffeegenuss und Lungenkrebs assoziiert sind: Bei Kaffeetrinkern ist die Lungenkrebsrate höher als bei Nichttrinkern (von Kaffee). Ob Kaffeegebrauch Lungenkrebs erzeugt?\nEine alternative Erklärung bietet folgender DAG.\n\n\n\n\n\n\n\n\n\nWelche Variablenmenge muss mindestens kontrolliert werden, um Konfundierung auszuschließen und damit den kausalen Effekt von Kaffee auf Lungenkrebs zu identifizieren?\n\n\n\n{Addictive Behavior oder aber Rauchen}\n{Rauchen}\n{Addictive Behavior}\n{Addictive Behavior und Rauchen}\n{Addictive Behavior und Lungenkrebs}"
  },
  {
    "objectID": "posts/kausal08/kausal08.html#answerlist",
    "href": "posts/kausal08/kausal08.html#answerlist",
    "title": "kausal08",
    "section": "",
    "text": "{Addictive Behavior oder aber Rauchen}\n{Rauchen}\n{Addictive Behavior}\n{Addictive Behavior und Rauchen}\n{Addictive Behavior und Lungenkrebs}"
  },
  {
    "objectID": "posts/kausal08/kausal08.html#answerlist-1",
    "href": "posts/kausal08/kausal08.html#answerlist-1",
    "title": "kausal08",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/Bayes-Theorem1/Bayes-Theorem1.html",
    "href": "posts/Bayes-Theorem1/Bayes-Theorem1.html",
    "title": "Bayes-Theorem1",
    "section": "",
    "text": "Aufgabe\nEin Krebstest (\\(T\\)) habe die Wahrscheinlichkeit von 0.85, einen vorhandenen Krebs (\\(K\\)) zu erkennen. Diese Wahrscheinlichkeit bezeichnen wir als \\(Pr(T+|K+)\\). Der Test erkennt also die meisten Krebsfälle, und ein paar werden übersehen.\nManchmal macht der Test auch den umgekehrten Fehler: Ein gesunder Mensch wird fälschlich Krebs diagnostiziert, \\(Pr(T+|K-)\\). Diese Wahrscheinlichkeit liegt bei dem Test bei 0.04, zum Glück also relativ gering.\nDie Grundrate dieser Krebsart belaufe sich in der Population auf 0.003, \\(Pr(K+)\\).\nAufgabe: Berechnen Sie die Wahrscheinlichkeit, dass ein Patient tatsächlich Krebs hat, wenn der Test positiv ist, also Krebs diagnostiziert hat!\n         \n\n\nLösung\n\nzaehler_bayes &lt;- Pr_Tpos_geg_Kpos * Pr_Kpos\nPr_Tpos &lt;- (zaehler_bayes + (1-Pr_Kpos) * Pr_Tpos_geg_Kneg)\n\nsol &lt;- Pr_Kpos_geg_Tpos &lt;- zaehler_bayes / Pr_Tpos \nsol &lt;- round(sol, 2)\nsol\n\n[1] 0.06\n\n\nDie Lösung beträgt also: 0.06.\n\nCategories:\n\nbayes\nprobability\nnum"
  },
  {
    "objectID": "posts/ds-quiz2/ds-quiz2.html",
    "href": "posts/ds-quiz2/ds-quiz2.html",
    "title": "ds-quiz2",
    "section": "",
    "text": "Im Folgenden sind mehrere Aussagen zum Thema maschinelles Lernen dargestellt. Wählen Sie alle korrekten Aussagen aus!\nHinweise:\n\nAlle Aussagen sind entweder richtig oder falsch, aber nicht beides.\nBeziehen Sie sich im Zweifel auf den Stoff wie im Unterricht dargestellt.\n\n\n\n\nJe größer der Wert von mtry in einem Random-Forest-Modell, desto besser die Modellgüte in der Regel.\nRandom-Forest-Modelle und Baginng-Modelle basieren auf einem Bootstrapping-Verfahren.\nBeim kNN-Modell ist ein Distanzmaß \\(d\\) die euklidische Distanz, die sich im 2D-Fall wie folgt berechnet: \\(d = \\sqrt{a^2 + b^2}\\). Dabei sind \\(a\\) und \\(b\\) die Distanz zwischen zwei Punkten \\(x\\) und \\(y\\) in den Dimensionen \\(A\\) und \\(B\\).\nBeim Random-Forest-Modell nennt man den Teil der Train-Stichprobe, der nicht in die Berechnung des jeweiligen Baumes einfließt, die “OOB-Stichprobe”.\nOverfitting tritt bei linearen Modellen nicht auf.\nBerechnet man die Vorhersagegüte eines Modells in mehreren Stichproben, so kann man die Vorhersagegüte für ein Test-Sample präziser bestimmen.\nBei baumbasierten Klassifikationsmodellen ist es dazu Ziel, die Homogenität (hinsichtlich der AV) in jedem Endknoten (“Blatt”) zu maximieren."
  },
  {
    "objectID": "posts/ds-quiz2/ds-quiz2.html#answerlist",
    "href": "posts/ds-quiz2/ds-quiz2.html#answerlist",
    "title": "ds-quiz2",
    "section": "",
    "text": "Je größer der Wert von mtry in einem Random-Forest-Modell, desto besser die Modellgüte in der Regel.\nRandom-Forest-Modelle und Baginng-Modelle basieren auf einem Bootstrapping-Verfahren.\nBeim kNN-Modell ist ein Distanzmaß \\(d\\) die euklidische Distanz, die sich im 2D-Fall wie folgt berechnet: \\(d = \\sqrt{a^2 + b^2}\\). Dabei sind \\(a\\) und \\(b\\) die Distanz zwischen zwei Punkten \\(x\\) und \\(y\\) in den Dimensionen \\(A\\) und \\(B\\).\nBeim Random-Forest-Modell nennt man den Teil der Train-Stichprobe, der nicht in die Berechnung des jeweiligen Baumes einfließt, die “OOB-Stichprobe”.\nOverfitting tritt bei linearen Modellen nicht auf.\nBerechnet man die Vorhersagegüte eines Modells in mehreren Stichproben, so kann man die Vorhersagegüte für ein Test-Sample präziser bestimmen.\nBei baumbasierten Klassifikationsmodellen ist es dazu Ziel, die Homogenität (hinsichtlich der AV) in jedem Endknoten (“Blatt”) zu maximieren."
  },
  {
    "objectID": "posts/ds-quiz2/ds-quiz2.html#answerlist-1",
    "href": "posts/ds-quiz2/ds-quiz2.html#answerlist-1",
    "title": "ds-quiz2",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch. Die obige Behauptung stimmt oft nicht.\nRichtig.\nRichtig.\nRichtig.\nFalsch. Zwar tritt Overfitting bei (einfachen) linearen Modellen oft weniger auf, aber gerade bei Verwendung von Polynomen ist die Gefahr des Overfitting hoch.\nRichtig.\nRichtig\n\n\nCategories:\n\nds1\ntidymodels\nstatlearning\nmchoice"
  },
  {
    "objectID": "posts/rope-regr/rope-regr.html",
    "href": "posts/rope-regr/rope-regr.html",
    "title": "rope-regression",
    "section": "",
    "text": "Exercise\nJohn Kruschke hat einen (Absolut-)Wert vorschlagen, als Grenze für Regressionskoeffizienten “vernachlässigbarer” Größe.\nNennen Sie diesen Wert!\nHinweise:\n\nGeben Sie nur Zahlen ein (und ggf. Dezimaltrennzeichen).\nFührende Nullen dürfen auch bei Zahlen kleiner als 1 nicht weggelassen werden.\n\n         \n\n\nSolution\n0.05\n\nCategories:\n\nbayes\nregression\nrope"
  },
  {
    "objectID": "posts/kausal01/kausal01.html",
    "href": "posts/kausal01/kausal01.html",
    "title": "kausal01",
    "section": "",
    "text": "Gegeben sei der DAG g (s.u.). Welche Variable/n sind zu kontrollieren, um den kausalen Effekt von x auf y zu identifizieren?\n\n\n\n\n\n\n\n\n\nGegeben sei der DAG g (s.o.). Dabei ist zu beachten, dass die gebogene Kurve (keine Gerade) mit zwei Pfeilspitzen keinen Kausaleffekt beschreibt, sondern eine Assoziation. Die dahinterstehende kausale Struktur ist eine Konfundierung. Daher ist der “Doppelpfeil” als Abkürzung für eine Konfundierung zu verstehen.\nWelche Variable/n sind zu kontrollieren, um den kausalen Effekt von x auf y zu identifizieren?\n\n\n\nkeine, bereits identifiziert\nx\ny\nm\nkeine, nicht identifizierbar"
  },
  {
    "objectID": "posts/kausal01/kausal01.html#answerlist",
    "href": "posts/kausal01/kausal01.html#answerlist",
    "title": "kausal01",
    "section": "",
    "text": "keine, bereits identifiziert\nx\ny\nm\nkeine, nicht identifizierbar"
  },
  {
    "objectID": "posts/kausal01/kausal01.html#answerlist-1",
    "href": "posts/kausal01/kausal01.html#answerlist-1",
    "title": "kausal01",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal\nexam-22"
  },
  {
    "objectID": "posts/import-mtcars/import-mtcars.html",
    "href": "posts/import-mtcars/import-mtcars.html",
    "title": "import-mtcars",
    "section": "",
    "text": "Aufgabe\nFinden Sie den Datensatz “mtcars” online! “mtcars.csv” Tipp: Die Webseite “vincentarelbundock” ist ein guter Ort zum Suchen. Importieren Sie dann den Datensatz in R.\nSagen Sie mir den Namen der letzten Spalte und dort den 1. Wert!\n         \n\n\nLösung\n\nlibrary(easystats)  # in diesem Paket \"wohnt\" data_read. \nmtcars_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/datasets/mtcars.csv\"\n\nmtcars &lt;- data_read(mtcars_path)\n\nAntwort: Die letzte Spalte heißt carb und der 1. Wert ist 4.\nAnstelle von data_read aus easystats könnte man auch read.csv verwenden, das ist ein “eingebauter” Befehl in R, für den man kein Paket gestartet haben muss.\n\nCategories:\n\nR\ndata\nnum"
  },
  {
    "objectID": "posts/germeval09-tfidf/germeval09-tfidf.html",
    "href": "posts/germeval09-tfidf/germeval09-tfidf.html",
    "title": "germeval09-tfidf",
    "section": "",
    "text": "Erstellen Sie ein prädiktives Modell für Textdaten, nutzen Sie einen Entscheidungsbaum als Modell. Erstellen Sie pro Wort tfIDF-Kennwerte im Rahmen von Feature-Engineering.\nNutzen Sie die GermEval-2018-Daten.\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\nDie Daten sind auch über das R-Paket PradaData zu beziehen.\n\nlibrary(tidyverse)\ndata(\"germeval_train\", package = \"pradadata\")\ndata(\"germeval_test\", package = \"pradadata\")\n\nDie AV lautet c1. Die (einzige) UV lautet: text.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\nNutzen Sie Tidymodels.\nNutzen Sie das sentiws Lexikon."
  },
  {
    "objectID": "posts/germeval09-tfidf/germeval09-tfidf.html#workflow",
    "href": "posts/germeval09-tfidf/germeval09-tfidf.html#workflow",
    "title": "germeval09-tfidf",
    "section": "Workflow",
    "text": "Workflow\n\n# model:\nmod1 &lt;-\n  decision_tree(mode = \"classification\")\n\n# recipe:\nrec1 &lt;-\n  recipe(c1 ~ ., data = d_train) |&gt; \n  update_role(id, new_role = \"id\")  |&gt; \n  step_tokenize(text) %&gt;%\n  step_tokenfilter(text, max_tokens = 1e3) %&gt;%\n  step_tfidf(text) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\n\n# workflow:\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(mod1) %&gt;% \n  add_recipe(rec1)"
  },
  {
    "objectID": "posts/germeval09-tfidf/germeval09-tfidf.html#fit",
    "href": "posts/germeval09-tfidf/germeval09-tfidf.html#fit",
    "title": "germeval09-tfidf",
    "section": "Fit",
    "text": "Fit\nOhne Tuning:\n\ntic()\nfit1 &lt;-\n  fit(wf1,\n      data = d_train)\ntoc()\n\n28.45 sec elapsed\n\n#beep()\n\n\nfit1\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_tokenize()\n• step_tokenfilter()\n• step_tfidf()\n• step_zv()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 5009 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 5009 1688 OTHER (0.3369934 0.6630066)  \n  2) tfidf_text_merkel&gt;=1.084677 279  117 OFFENSE (0.5806452 0.4193548) *\n  3) tfidf_text_merkel&lt; 1.084677 4730 1526 OTHER (0.3226216 0.6773784) *"
  },
  {
    "objectID": "posts/germeval09-tfidf/germeval09-tfidf.html#test-set-güte",
    "href": "posts/germeval09-tfidf/germeval09-tfidf.html#test-set-güte",
    "title": "germeval09-tfidf",
    "section": "Test-Set-Güte",
    "text": "Test-Set-Güte\nVorhersagen im Test-Set:\n\ntic()\npreds &lt;-\n  predict(fit1, new_data = germeval_test)\ntoc()\n\n0.453 sec elapsed\n\n\nUnd die Vorhersagen zum Test-Set hinzufügen, damit man TRUTH und ESTIMATE vergleichen kann:\n\nd_test &lt;-\n  germeval_test |&gt; \n  bind_cols(preds) |&gt; \n  mutate(c1 = as.factor(c1))\n\n\nmy_metrics &lt;- metric_set(accuracy, f_meas)\nmy_metrics(d_test,\n           truth = c1,\n           estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary        0.657 \n2 f_meas   binary        0.0706"
  },
  {
    "objectID": "posts/germeval09-tfidf/germeval09-tfidf.html#prepbake",
    "href": "posts/germeval09-tfidf/germeval09-tfidf.html#prepbake",
    "title": "germeval09-tfidf",
    "section": "Prep/Bake",
    "text": "Prep/Bake\nAls Check: Das gepreppte/bebackene Rezept:\n\ntic()\nrec1_prepped &lt;- prep(rec1)\ntoc()\n\n3.376 sec elapsed\n\n\n\ntic()\nd_train_baked &lt;- bake(rec1_prepped, new_data = NULL)\ntoc()\n\n0.026 sec elapsed"
  },
  {
    "objectID": "posts/germeval09-tfidf/germeval09-tfidf.html#sehr-viele-spalten",
    "href": "posts/germeval09-tfidf/germeval09-tfidf.html#sehr-viele-spalten",
    "title": "germeval09-tfidf",
    "section": "Sehr viele Spalten",
    "text": "Sehr viele Spalten\nDas Problem ist, dass dieses Rezept sehr viele Spalten erzeugt. Das ist (sehr) rechen- und speicherintensiv.\n\ndim(d_train_baked)\n\n[1] 5009 1002\n\n\n\nd_train_baked |&gt; \n  head()\n\n# A tibble: 6 × 1,002\n     id c1      tfidf_text__macmike tfidf_text_1 tfidf_text_10 tfidf_text_100\n  &lt;int&gt; &lt;fct&gt;                 &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n1     1 OTHER                -0.137      -0.0692       -0.0531        -0.0475\n2     2 OTHER                -0.137      -0.0692       -0.0531        -0.0475\n3     3 OTHER                -0.137      -0.0692       -0.0531        -0.0475\n4     4 OTHER                -0.137      -0.0692       -0.0531        -0.0475\n5     5 OFFENSE              -0.137      -0.0692       -0.0531        -0.0475\n6     6 OTHER                -0.137      -0.0692       -0.0531        -0.0475\n# ℹ 996 more variables: tfidf_text_12 &lt;dbl&gt;, tfidf_text_14 &lt;dbl&gt;,\n#   tfidf_text_15 &lt;dbl&gt;, tfidf_text_19 &lt;dbl&gt;, tfidf_text_2 &lt;dbl&gt;,\n#   tfidf_text_20 &lt;dbl&gt;, tfidf_text_2017 &lt;dbl&gt;, tfidf_text_2018 &lt;dbl&gt;,\n#   tfidf_text_3 &lt;dbl&gt;, tfidf_text_30 &lt;dbl&gt;, tfidf_text_4 &lt;dbl&gt;,\n#   tfidf_text_5 &lt;dbl&gt;, tfidf_text_6 &lt;dbl&gt;, tfidf_text_66freedom66 &lt;dbl&gt;,\n#   tfidf_text_8 &lt;dbl&gt;, tfidf_text_90 &lt;dbl&gt;, tfidf_text_a &lt;dbl&gt;,\n#   tfidf_text_ab &lt;dbl&gt;, tfidf_text_abend &lt;dbl&gt;, tfidf_text_aber &lt;dbl&gt;, …\n\n\n\nCategories:\n\ntextmining\ndatawrangling\ngermeval\nprediction\ntidymodels\nstring"
  },
  {
    "objectID": "posts/mtcars-abhaengig_var3a/mtcars-abhaengig_var3a.html",
    "href": "posts/mtcars-abhaengig_var3a/mtcars-abhaengig_var3a.html",
    "title": "mtcars-abhaengig_var3a",
    "section": "",
    "text": "Aufgabe\nIm Folgenden ist der Datensatz mtcars zu analysieren.\nDer Datensatz ist z.B. als CSV-Datei von dieser Webseite abrufbar.\nHilfe zum Datensatz ist via dieser Webseite abrufbar.\nOb die Variable hp (UV) und Spritverbrauch (mpg; AV) wohl voneinander abhängig sind? Was meinen Sie? Was ist Ihre Einschätzung dazu? Vermutlich haben Sie ein (wenn vielleicht auch implizites) Vorab-Wissen zu dieser Frage. Lassen wir dieses Vorab-Wissen aber einmal außen vor und schauen uns rein Daten dazu an. Vereinfachen wir die Frage etwas, indem wir beide Variablen am Mittelwert aufteilen: Wenn eine Beobachtung (d.h. ein Auto) einen Wert in der jeweiligen Variablen höchstens so groß wie der Mittelwert der Variable aufweist, geben wir der Beobachtung der Wert 0, ansonsten den Wert 1. Das Ereignis \\(A\\) sei “hoher Spritverbrauch”, mpg_high == 1. Das Ereignis \\(B\\) sei “hohe PS_Zahl”, hp_high == 1.\nBerechnen Sie: \\(Pr(\\neg \\text{uv high} \\, | \\,  \\text{av high})\\)\nHinweise:\n\nDas “Ellbogen-Zeichen” \\(\\neg\\) kennzeichnet eine logische Negierung (das Gegenteil oder Komplement).\nDie angegebene Wahrscheinlichkeit ist eine bedingte Wahrscheinlichkeit.\nWeitere Hinweise\n\n         \n\n\nLösung\nDieser Prädiktor wurde als UV bestimmt: hp.\nSchauen wir zuerst mal in den Datensatz:\n\ndata(mtcars)  # mtcars importieren\n\nmtcars %&gt;% \n  select(mpg, hp) %&gt;% \n  slice_head(n = 5)\n\n                   mpg  hp\nMazda RX4         21.0 110\nMazda RX4 Wag     21.0 110\nDatsun 710        22.8  93\nHornet 4 Drive    21.4 110\nHornet Sportabout 18.7 175\n\n\nDann berechnen wir die binären Variablen:\n\nmtcars2 &lt;-\n  mtcars %&gt;% \n  select(mpg, hp) %&gt;% \n  mutate(mpg_high = case_when(\n    mpg &lt;= mean(mpg) ~ 0,\n    mpg &gt; mean(mpg) ~ 1\n  )) %&gt;% \n  select(-mpg) \n\nmtcars3 &lt;-  # Jetzt analog für die UV, `hp`:\n  mtcars2 |&gt; \n  mutate(hp_high = case_when(\n    hp &lt;= mean(hp) ~ 0,\n    hp &gt; mean(hp) ~ 1\n  )) |&gt; \n  select(-hp)\n\nDann filtern wir die gesuchten Wahrscheinlichkeiten bzw. Anteile der AV:\n\nmtcars3_filtered &lt;-\n  mtcars3 %&gt;% \n  filter(mpg_high == 1)\n\nmtcars3_filtered\n\n               mpg_high hp_high\nMazda RX4             1       0\nMazda RX4 Wag         1       0\nDatsun 710            1       0\nHornet 4 Drive        1       0\nMerc 240D             1       0\nMerc 230              1       0\nFiat 128              1       0\nHonda Civic           1       0\nToyota Corolla        1       0\nToyota Corona         1       0\nFiat X1-9             1       0\nPorsche 914-2         1       0\nLotus Europa          1       0\nVolvo 142E            1       0\n\n\nDie Anzahl der Zeilen in mtcars3_filtered sagt uns, wie viele Autos die gesuchte Bedingung, also den “hinteren Teil” der Wahrscheinlichkeit, erfüllen.\nZur Erinnerung: Bedingte Wahrscheinlichkeit berechnen ist analog zum Filtern einer Tabelle:\nEs gibt also 14 Autos, die den oben gesuchten “hinteren Teil” der Bedingung erfüllen (mpg_high = 1).\nFiltern wir als nächstes nach dem “vorderen Teil” der gesuchten Wahrscheinlichkeit (was das gleiche ist wie ein Anteil in diesem Fall):\n\nmtcars3_filtered %&gt;% \n  filter(hp_high == 0) \n\n               mpg_high hp_high\nMazda RX4             1       0\nMazda RX4 Wag         1       0\nDatsun 710            1       0\nHornet 4 Drive        1       0\nMerc 240D             1       0\nMerc 230              1       0\nFiat 128              1       0\nHonda Civic           1       0\nToyota Corolla        1       0\nToyota Corona         1       0\nFiat X1-9             1       0\nPorsche 914-2         1       0\nLotus Europa          1       0\nVolvo 142E            1       0\n\n\nEs gibt also 14 Autos, für die gilt hp_high == 0.\nMit count kann man sich die Werte zählen lassen:\n\n\n  hp_high  n\n1       0 14\n\n\nDer gesuchte Wert beträgt also 14/14 = 1.\nVisualisieren wir noch die bedingten Wahrscheinlichkeiten, so könnte man die gesuchten Anteile einfach abzählen:\n\n\n\n\n\n\n\n\n\nSieht man in dem Diagramm nur eine Farbe (anstelle von zweien), so heißt das, dass es nur eine Gruppe gibt (und nicht zwei). Die Häufigkeit der nicht vorhandenen Gruppe ist demnach Null.\n\nCategories:\n\nprobability\nbayes\nnum"
  },
  {
    "objectID": "posts/Rethink2m6/Rethink2m6.html",
    "href": "posts/Rethink2m6/Rethink2m6.html",
    "title": "Rethink2m6",
    "section": "",
    "text": "Aufgabe\nThis question is taken from McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Ed.). Taylor and Francis, CRC Press.\n2M6. Imagine that black ink is heavy, and so cards with black sides are heavier than cards with white sides. As a result, it’s less likely that a card with black sides is pulled from the bag. So again assume there are three cards: B/B, B/W, and W/W. After experimenting a number of times, you conclude that for every way to pull the B/B card from the bag, there are 2 ways to pull the B/W card and 3 ways to pull the W/W card. Again suppose that a card is pulled and a black side appears face up. Show that the probability the other side is black is now 0.5. Use the counting method, as before.\n         \n\n\nLösung\nLet’s label the cards bb (black on both sides), bw (black on one, white on the other), and ww (both sides are white), respectively.\nWanted is the probability that the second side of the card is black (2b), given one side is already identified as black (1b): \\(Pr(2b|1b)\\).\nBayes-Box to the rescue:\nWhenever the probability of all paths (in a tree diagram) is the same, we do not need to write down the probability of the path for the likelihood. It is enough to write the number of paths.\n\n\n\n\n\n\n\n\nHyp\nPrior\nLikelihood\nunstand_post\nstd_post\n\n\n\n\nbb\n1\n2\n2\n0.5\n\n\nbw\n2\n1\n2\n0.5\n\n\nww\n3\n0\n0\n0.0\n\n\n\n\n\n\n\n\nCategories:\n\nprobability\nbayes\nbayes-grid\nrethink-chap2\nstring"
  },
  {
    "objectID": "posts/Ziele-Statistik/Ziele-Statistik.html",
    "href": "posts/Ziele-Statistik/Ziele-Statistik.html",
    "title": "Ziele-Statistik",
    "section": "",
    "text": "Welche von den folgenden Optionen gehört nicht zu den Zielen von Statistik bzw. einer Forschungsfrage mit statistischem Character?\n\n\n\nverstehen\nerklären\nvorhersagen\nbeschreiben"
  },
  {
    "objectID": "posts/Ziele-Statistik/Ziele-Statistik.html#answerlist",
    "href": "posts/Ziele-Statistik/Ziele-Statistik.html#answerlist",
    "title": "Ziele-Statistik",
    "section": "",
    "text": "verstehen\nerklären\nvorhersagen\nbeschreiben"
  },
  {
    "objectID": "posts/Ziele-Statistik/Ziele-Statistik.html#answerlist-1",
    "href": "posts/Ziele-Statistik/Ziele-Statistik.html#answerlist-1",
    "title": "Ziele-Statistik",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nbasics\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/rf-usemodels/rf-usemodels.html",
    "href": "posts/rf-usemodels/rf-usemodels.html",
    "title": "rf-usemodels",
    "section": "",
    "text": "Berechnen Sie ein prädiktives Modell mit dieser Modellgleichung:\nbody_mass_g ~ . (Datensatz: palmerpenguins::penguins).\nNutzen Sie usemodels!\nHinweise: - Tunen Sie mtry - Verwenden Sie Kreuzvalidierung - Verwenden Sie Standardwerte, wo nicht anders angegeben. - Fixieren Sie Zufallszahlen auf den Startwert 42."
  },
  {
    "objectID": "posts/rf-usemodels/rf-usemodels.html#vorbereitung",
    "href": "posts/rf-usemodels/rf-usemodels.html#vorbereitung",
    "title": "rf-usemodels",
    "section": "Vorbereitung:",
    "text": "Vorbereitung:\n\n# Setup:\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.0.8\n✔ dials        1.2.0     ✔ rsample      1.2.0\n✔ dplyr        1.1.3     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.4\n✔ lubridate 1.9.3     ✔ stringr   1.5.0\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tictoc)  # Zeitmessung\n\n\n# Data:\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\"\nd &lt;- read_csv(d_path)\n\nRows: 344 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): species, island, sex\ndbl (6): rownames, bill_length_mm, bill_depth_mm, flipper_length_mm, body_ma...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# rm NA in the dependent variable:\nd &lt;- d %&gt;% \n  drop_na(body_mass_g)\n\n\nset.seed(42)\nd_split &lt;- initial_split(d)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\n\nlibrary(usemodels)\nuse_ranger(body_mass_g ~ ., data = d_train)\n\nranger_recipe &lt;- \n  recipe(formula = body_mass_g ~ ., data = d_train) \n\nranger_spec &lt;- \n  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_engine(\"ranger\") \n\nranger_workflow &lt;- \n  workflow() %&gt;% \n  add_recipe(ranger_recipe) %&gt;% \n  add_model(ranger_spec) \n\nset.seed(32162)\nranger_tune &lt;-\n  tune_grid(ranger_workflow, resamples = stop(\"add your rsample object\"), grid = stop(\"add number of candidate points\"))\n\n\nOder die resultierende Syntax in die Zwischenablage kopieren lassen:\n\nuse_ranger(body_mass_g ~ ., data = d_train,\n           clipboard = TRUE)  # kopiert Syntax in die Zwischenablage\n\n\nCategories:\n\ntidymodels\nstatlearning\ntemplate\nstring"
  },
  {
    "objectID": "posts/euro-bayes/euro-bayes.html",
    "href": "posts/euro-bayes/euro-bayes.html",
    "title": "euro-bayes",
    "section": "",
    "text": "Exercise\nIn Information Theory, Inference, and Learning Algorithms, stellt David MacKay folgendes Problem.\n“A statistical statement appeared in The Guardian on Friday January 4, 2002:\nWhen spun on edge 250 times, a Belgian one-euro coin came up heads 140 times and tails 110. ‘It looks very suspicious to me,’ said Barry Blight, a statistics lecturer at the London School of Economics. ‘If the coin were unbiased, the chance of getting a result as extreme as that would be less than 7%.’\nBut do these data give evidence that the coin is biased rather than fair?”\nWie wahrscheinlich ist es, dass die Münze (exakt) fair ist, im Lichte dieser Daten?\nHinweise:\n\nUntersuchen Sie die Hypothesen \\(\\pi_0 = 0, \\pi_1 = 0.1, \\pi_2 = 0.2, ..., \\pi_{10} = 1\\) für die Trefferwahrscheinlichkeit (Kopf; heads).\nErstellen Sie ein Bayes-Gitter zur Lösung dieser Aufgabe.\nGehen Sie davon aus, dass Sie indifferent gegenüber der Hypothesen zu den Parameterwerten der Münze sind.\nGeben Sie Prozentzahlen immer als Anteil an und lassen Sie die führende Null weg (z.B. .42).\n\n         \n\n\nSolution\n\n\n\n\n\np_Gitter\nPriori\nLikelihood\nunstd_Post\nPost\n\n\n\n\n0.0\n1\n0.00\n0.00\n0.00\n\n\n0.1\n1\n0.00\n0.00\n0.00\n\n\n0.2\n1\n0.00\n0.00\n0.00\n\n\n0.3\n1\n0.00\n0.00\n0.00\n\n\n0.4\n1\n0.00\n0.00\n0.00\n\n\n0.5\n1\n0.01\n0.01\n0.27\n\n\n0.6\n1\n0.02\n0.02\n0.73\n\n\n0.7\n1\n0.00\n0.00\n0.00\n\n\n0.8\n1\n0.00\n0.00\n0.00\n\n\n0.9\n1\n0.00\n0.00\n0.00\n\n\n1.0\n1\n0.00\n0.00\n0.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDie Wahrscheinlichkeit \\(Pr(\\pi = 1/2 \\, | \\, X=140)\\) wenn \\(X \\sim Bin(250, 1/2)\\) beträgt ca. 27% oder .27.\nAllerdings würden viele Statistiker:innen nicht (nur) fragen, wie wahrscheinlich 140 Treffer sind. Stattdessen könnte man von folgender Überlegung ausgehen.\nZuerst: Welcher Wert wäre am wahrscheinlichsten, wenn die Münze fair wäre?\n\ndbinom(x = 0:250, size = 250, prob = 1/2) %&gt;% which.max()\n\n[1] 126\n\n\nDer 126. Wert in der Liste 0:250 ist der wahrscheinlichste (also 125 Treffer).\nWenn die Münze fair ist, dann wären doch 15 Treffer mehr als 125 genauso so unwahrscheinlich wie 15 Treffer weniger als 125 Treffer. Beide Ereignisse - 110 und 140 Treffer - sind ja gleich weit entfernt von denjenigen Wert, der am wahrscheinlichsten ist, wenn die Münze fair ist.\nEi typischi Statistiki würde also eher fragen: “Wie wahrscheinlich ist es, dass man ein Ergebnis erhält, dass mind. 15 Treffer entfernt ist von der Trefferzahl, die bei einer fairen Münze zu erwarten ist?”. Aber genug davon für diese Aufgabe :-)\n\nCategories:\n\nprobability\nbayes-grid"
  },
  {
    "objectID": "posts/Interaktionseffekt1/Interaktionseffekt1.html",
    "href": "posts/Interaktionseffekt1/Interaktionseffekt1.html",
    "title": "Interaktionseffekt1",
    "section": "",
    "text": "Wählen Sie das Diagramm, in dem kein Interaktionseffekt (in der Population) vorhanden ist (bzw. wählen Sie Diagramm, dass dies am ehesten darstellt).\n\n\n\nDiagramm A\nDiagramm B\nDiagramm C\nDiagramm D\nDiagramm E"
  },
  {
    "objectID": "posts/Interaktionseffekt1/Interaktionseffekt1.html#answerlist",
    "href": "posts/Interaktionseffekt1/Interaktionseffekt1.html#answerlist",
    "title": "Interaktionseffekt1",
    "section": "",
    "text": "Diagramm A\nDiagramm B\nDiagramm C\nDiagramm D\nDiagramm E"
  },
  {
    "objectID": "posts/Interaktionseffekt1/Interaktionseffekt1.html#answerlist-1",
    "href": "posts/Interaktionseffekt1/Interaktionseffekt1.html#answerlist-1",
    "title": "Interaktionseffekt1",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nWahr\nFalsch\n\n\nCategories:\n\ninteraction\nregression"
  },
  {
    "objectID": "posts/interpret-koeff-lm/interpret-koeff-lm.html",
    "href": "posts/interpret-koeff-lm/interpret-koeff-lm.html",
    "title": "interpret-koeff-lm",
    "section": "",
    "text": "Aufgabe\nBetrachten Sie dieses Modell, das den Zusammenhang von PS-Zahl und Spritverbrauch untersucht (Datensatz mtcars):\n\ndata(mtcars)\nlibrary(easystats)\nlm1 &lt;- lm(mpg ~ hp, data = mtcars)\nparameters(lm1)\n\nParameter   | Coefficient |   SE |         95% CI | t(30) |      p\n------------------------------------------------------------------\n(Intercept) |       30.10 | 1.63 | [26.76, 33.44] | 18.42 | &lt; .001\nhp          |       -0.07 | 0.01 | [-0.09, -0.05] | -6.74 | &lt; .001\n\n\n\nWas bedeuten die Koeffizienten?\nWie ist der Effekt von \\(\\beta_1\\) zu interpretieren?\n\n         \n\n\nLösung\n\nIntercept (\\(\\beta_0\\)): Der Achsenabschnitt gibt den geschätzten mittleren Y-Wert (Spritverbrauch) an, wenn \\(x=0\\), also für ein Auto mit 0 PS (was nicht wirklich Sinn macht). hp (\\(\\beta_1\\)) ist der Regressionskoeffizient oder Regressionsgewicht und damit die Steigung der Regressionsgeraden.\nhp (\\(\\beta_1\\)) ist der Regressionskoeffizient oder Regressionsgewicht und gibt den statistischen “Effekt” der PS-Zahl auf den Spritverbrauch an. Vorsicht: Dieser “Effekt” darf nicht vorschnell als kausaler Effekt verstanden werden. Daher muss man vorsichtig sein, wenn man von einem “Effekt” spricht. Vorsichtiger wäre zu sagen: “Ein Auto mit einem PS mehr, kommt im Mittel 0,1 Meilen weniger weit mit einer Gallone Sprit, laut diesem Modell”.\n\n\nCategories:\n\nregression\nlm\nstring"
  },
  {
    "objectID": "posts/kausal20/kausal20.html",
    "href": "posts/kausal20/kausal20.html",
    "title": "kausal20",
    "section": "",
    "text": "Gegeben sei der DAG g (s. u.). Der DAG verfügt über mehrere Variablen, die als Knoten im Graph dargestellt sind und mit \\(x_1, x_2, \\ldots x_n\\) bezeichnet sind.\nWelche minimale Variablenmenge muss kontrolliert werden, um den kausalen Effekt von der UV zur AV zu identifizieren?\nUV: x3.\nAV: x5.\nHinweise:\n\nMengen sind mittels geschweifter Klammern gekennzeichnet, z.B. {x8, x9} meint die Menge mit den zwei Elementen x8 und x9.\nDie leere Menge { } bedeutet, dass keine Variable kontrolliert werden muss, um den kausalen Effekt zu identifizieren.\nAlle Variablen werden als gemessen vorausgesetzt.\nEs ist möglich, dass es keine Lösung gibt, dass es also keine Adjustierungsmenge gibt, um den kausalen Effekt zu identifizieren. Wenn dies der Fall sein sollte, wählen Sie “keine Lösung”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n{ x2 }\n{ x5 }\n{ x2, x5 }\n{ x1, x5 }\n/"
  },
  {
    "objectID": "posts/kausal20/kausal20.html#answerlist",
    "href": "posts/kausal20/kausal20.html#answerlist",
    "title": "kausal20",
    "section": "",
    "text": "{ x2 }\n{ x5 }\n{ x2, x5 }\n{ x1, x5 }\n/"
  },
  {
    "objectID": "posts/kausal20/kausal20.html#answerlist-1",
    "href": "posts/kausal20/kausal20.html#answerlist-1",
    "title": "kausal20",
    "section": "Answerlist",
    "text": "Answerlist\n\nRichtig\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/germeval08-schimpf/germeval08-schimpf.html",
    "href": "posts/germeval08-schimpf/germeval08-schimpf.html",
    "title": "germeval08-schimpf",
    "section": "",
    "text": "Erstellen Sie ein prädiktives Modell für Textdaten. Nutzen Sie Schimpfwörter im Rahmen von Feature-Engineering.\nNutzen Sie die GermEval-2018-Daten.\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\nDie Daten sind auch über das R-Paket PradaData zu beziehen.\n\nlibrary(tidyverse)\ndata(\"germeval_train\", package = \"pradadata\")\ndata(\"germeval_test\", package = \"pradadata\")\n\nDie AV lautet c1. Die (einzige) UV lautet: text.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\nNutzen Sie Tidymodels.\nNutzen Sie das sentiws Lexikon."
  },
  {
    "objectID": "posts/germeval08-schimpf/germeval08-schimpf.html#workflow",
    "href": "posts/germeval08-schimpf/germeval08-schimpf.html#workflow",
    "title": "germeval08-schimpf",
    "section": "Workflow",
    "text": "Workflow\n\n# model:\nmod1 &lt;-\n  rand_forest(mode = \"classification\")\n\n# recipe:\nrec1 &lt;-\n  recipe(c1 ~ ., data = d_train) |&gt; \n  update_role(id, new_role = \"id\")  |&gt; \n  #update_role(c2, new_role = \"ignore\") |&gt; \n  update_role(text, new_role = \"ignore\") |&gt; \n  step_mutate(n_schimpf = get_sentiment(text,  # aus `syuzhet`\n                                    method = \"custom\",\n                                    lexicon = schimpfwoerter))  |&gt; \n  step_rm(text)  # Datensatz verschlanken\n\n\n# workflow:\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(mod1) %&gt;% \n  add_recipe(rec1)"
  },
  {
    "objectID": "posts/germeval08-schimpf/germeval08-schimpf.html#fit",
    "href": "posts/germeval08-schimpf/germeval08-schimpf.html#fit",
    "title": "germeval08-schimpf",
    "section": "Fit",
    "text": "Fit\nOhne Tuning:\n\ntic()\nfit1 &lt;-\n  fit(wf1,\n      data = d_train)\ntoc()\n\n12.185 sec elapsed\n\n#beep()\n\n\nfit1\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_mutate()\n• step_rm()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      5009 \nNumber of independent variables:  1 \nMtry:                             1 \nTarget node size:                 10 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.2137385"
  },
  {
    "objectID": "posts/germeval08-schimpf/germeval08-schimpf.html#test-set-güte",
    "href": "posts/germeval08-schimpf/germeval08-schimpf.html#test-set-güte",
    "title": "germeval08-schimpf",
    "section": "Test-Set-Güte",
    "text": "Test-Set-Güte\nVorhersagen im Test-Set:\n\ntic()\npreds &lt;-\n  predict(fit1, new_data = germeval_test)\ntoc()\n\n7.444 sec elapsed\n\n\nUnd die Vorhersagen zum Test-Set hinzufügen, damit man TRUTH und ESTIMATE vergleichen kann:\n\nd_test &lt;-\n  germeval_test |&gt; \n  bind_cols(preds) |&gt; \n  mutate(c1 = as.factor(c1))\n\n\nmy_metrics &lt;- metric_set(accuracy, f_meas)\nmy_metrics(d_test,\n           truth = c1,\n           estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.676\n2 f_meas   binary         0.336\n\n\nAls Check: Das gepreppte/bebackene Rezept:\n\ntic()\nrec1_prepped &lt;- prep(rec1)\ntoc()\n\n12.817 sec elapsed\n\n\n\ntic()\nd_train_baked &lt;- bake(rec1_prepped, new_data = NULL)\ntoc()\n\n0.006 sec elapsed\n\n\n\nd_train_baked |&gt; \n  arrange(-n_schimpf) |&gt; \n  head()\n\n# A tibble: 6 × 3\n     id c1      n_schimpf\n  &lt;int&gt; &lt;fct&gt;       &lt;dbl&gt;\n1  4493 OFFENSE         4\n2   707 OFFENSE         3\n3   771 OFFENSE         3\n4  1504 OTHER           3\n5  3145 OTHER           3\n6  3354 OFFENSE         3\n\n\n\nd_train |&gt; \n  filter(id == 707) |&gt; \n  pull(text)\n\n[1] \"@mastermikeg @machtjanix23 @AthinaMala @_macmike @Norbinator2403 @ennof_ @troll_putin @NancyPeggyMandy @petpanther0 @info2099 @lifetrend @ThomasGBauer @SchmiddieMaik @charlie_silve @NoHerrman @willjrosenblatt @feldenfrizz @nasanasal @ellibisathide @MD_Franz Der Typ hat sich mit etlichen hier angelegt, hat inhaltlich nichts zu bieten - nur Klugscheißern. Eigenes hat er auch nicht zu bieten - dafür reicht sein Intellekt nicht.\"\n\n\n\nd_train |&gt; \n  filter(id == 707) |&gt; \n  select(text) |&gt; \n  unnest_tokens(output = word, input = text) |&gt; \n  inner_join(schimpfwoerter)\n\n    word value\n1    typ     1\n2 nichts     1\n\n\n\nd_train |&gt; \n  filter(id == 4493) |&gt; \n  pull(text)\n\n[1] \"@Der_Eisenhans @focusonline Soweit mir bekannt ist das Wort Muschi ein kosenahme für eine Katze die es auch Geschlechts spezifisch gibt. Als Schwanz oder Schweif bezeichnet man das End Stück eines Pferdes oder Hund. Beides nicht Ursprung der Fortpflanzung. Ich hoffe ich konnte helfen. :-)\"\n\n\n\nd_train |&gt; \n  filter(id == 4493) |&gt; \n  select(text) |&gt; \n  unnest_tokens(output = word, input = text) |&gt; \n  inner_join(schimpfwoerter)\n\n     word value\n1   katze     1\n2 schwanz     1\n3 schweif     1\n4   stück     1\n5    hund     1\n\n\n\nCategories:\n\n2023\ntextmining\ndatawrangling\ngermeval\nprediction\ntidymodels\nstring"
  },
  {
    "objectID": "posts/n-se/n-se.html",
    "href": "posts/n-se/n-se.html",
    "title": "n-se",
    "section": "",
    "text": "Größere Stichproben sind zwar aufwändiger zu erheben, aber aussagekräftiger unter sonst gleichen Umständen, wie in der Abbildung dargestellt.\n\nWelche Aussagen sind in diesem Zusammenhang falsch?\n\n\n\nEine Möglichkeit zur Quantifizierung der Genauigkeit eines Schätzers auf Basis einer Stichprobe ist das Berechnen der Streuung bei wiederholtem Ziehen der Stichprobe.\nEine typische Kennzahl der Quantifizierung der Genauigkeit eines Schätzers ist der Standardfehler.\nJe größer die Stichprobe, desto kleiner der Standardfehler.\nAb einer Größe von \\(n=30\\) sind Stichproben robust und ausreichend präzise Schätzer für den gesuchten Populationsparameter."
  },
  {
    "objectID": "posts/n-se/n-se.html#answerlist",
    "href": "posts/n-se/n-se.html#answerlist",
    "title": "n-se",
    "section": "",
    "text": "Eine Möglichkeit zur Quantifizierung der Genauigkeit eines Schätzers auf Basis einer Stichprobe ist das Berechnen der Streuung bei wiederholtem Ziehen der Stichprobe.\nEine typische Kennzahl der Quantifizierung der Genauigkeit eines Schätzers ist der Standardfehler.\nJe größer die Stichprobe, desto kleiner der Standardfehler.\nAb einer Größe von \\(n=30\\) sind Stichproben robust und ausreichend präzise Schätzer für den gesuchten Populationsparameter."
  },
  {
    "objectID": "posts/regex-insert-char/regex-insert-char.html",
    "href": "posts/regex-insert-char/regex-insert-char.html",
    "title": "regex-insert-char",
    "section": "",
    "text": "Aufgabe\nGegeben sei ein String dieser Art (ggf. noch viel länger, aber vom gleichen Aufbau):\n\nmy_string &lt;-c(\n\"word1\",\n\"word2\",\n\"word3\"\n)\n\nwriteLines(my_string)\n\nword1\nword2\nword3\n\n\n(writeLines druckt einen String, wo wie er am Bildschirm erscheint, wenn man einfach nur my_string eingibt, werden Steuerzeichen mitangezeigt.)\nWandeln Sie diesen String (programmatisch) um in folgende Form\n\nmy_string_out &lt;-c(\n'\"word1\"',\n'\"word2\"',\n'\"word3\"'\n)\n\nwriteLines(my_string_out)\n\n\"word1\"\n\"word2\"\n\"word3\"\n\n\n         \n\n\nLösung\n\nlibrary(stringr)\n\n\nstr_replace_all(string = my_string,\n                pattern = \"(^\\\\w)(.+$)\",\n                replacement = '\\\\\"\\\\1\\\\2\\\\\"') %&gt;% \n  writeLines()\n\n\"word1\"\n\"word2\"\n\"word3\"\n\n\nErklärung:\n\n(^\\\\w) ist eine Such-Gruppe, die aus den Treffern besteht, bei denen zu Beginn des Strings ein “Wort-Zeichen” steht vgl. hier.\n(.+$) ist eine Such-Gruppe, die aus Treffern besteht, bei denen zum Ende des Strings ein beliebiges Zeichen steht.\n'\\\\\"\\\\1\\\\2\\\\\"' \\\\\" bezieht sich auf ein Anführungszeichen, \\\\1 bezieht sich auf die 1. Such-Gruppe (analog für \\\\2).\n\nHat man innerhalb eines Strings ein Anführungszeichen, so setzt man außen das einfache und innerhalb des Strings das doppelte Anführungszeichen.\n\nCategories:\n\ntextmining\nregex\nstring"
  },
  {
    "objectID": "posts/log-y-regr2/log-y-regr2.html",
    "href": "posts/log-y-regr2/log-y-regr2.html",
    "title": "log-y-regression2",
    "section": "",
    "text": "Exercise\n\nlibrary(tidyverse)\nlibrary(easystats)\n\nIn dieser Aufgabe modellieren wir den (kausalen) Effekt von Schulbildung auf das Einkommen.\nImportieren Sie zunächst den Datensatz und verschaffen Sie sich einen Überblick.\n\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Treatment.csv\"\n\nd &lt;- data_read(d_path)\n\nDokumentation und Quellenangaben zum Datensatz finden sich hier.\n\nglimpse(d)\n\nRows: 2,675\nColumns: 11\n$ rownames &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ treat    &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n$ age      &lt;int&gt; 37, 30, 27, 33, 22, 23, 32, 22, 19, 21, 18, 27, 17, 19, 27, 2…\n$ educ     &lt;int&gt; 11, 12, 11, 8, 9, 12, 11, 16, 9, 13, 8, 10, 7, 10, 13, 10, 12…\n$ ethn     &lt;chr&gt; \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\"…\n$ married  &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ re74     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ re75     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ re78     &lt;dbl&gt; 9930.05, 24909.50, 7506.15, 289.79, 4056.49, 0.00, 8472.16, 2…\n$ u74      &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n$ u75      &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n\n\nModellieren Sie den Effekt der Bildungsdauer auf das Einkommen! Gehen Sie von einem exponenziellen Zusammenhang der beiden Variablen aus. Wie verändert sich die Verteilung der abhängigen Variablen (Y) durch die Logarithmus-Transformation?\nHinweise:\n\nVerwenden Sie lm zur Modellierung.\nOperationalisieren Sie das Einkommen mit der Variable re74.\nFügen Sie keine weiteren Variablen dem Modell hinzu.\nGehen Sie von einem kausalen Effekt des Prädiktors aus.\n\n         \n\n\nSolution\n\nd2 &lt;-\n  d %&gt;% \n  filter(re74 &gt; 0) %&gt;% \n  mutate(re74_log = log(re74))\n\n\nm &lt;- lm(re74_log ~ educ, data = d2)\n\n\nggplot(d2) +\n  aes(x = re74) +\n  geom_density() +\n  labs(title = \"Income raw\")\n\n\nggplot(d2) +\n  aes(x = re74_log) +\n  geom_density() +\n  labs(title = \"Income log transformed\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBetrachten wir die deskriptiven Statistiken:\n\nd2 %&gt;% \n  select(re74, re74_log) %&gt;% \n  describe_distribution()\n\nVariable |     Mean |       SD |      IQR |             Range | Skewness | Kurtosis |    n | n_Missing\n------------------------------------------------------------------------------------------------------\nre74     | 20938.28 | 12631.52 | 15086.30 | [17.63, 1.37e+05] |     1.62 |     6.81 | 2329 |         0\nre74_log |     9.73 |     0.76 |     0.80 |     [2.87, 11.83] |    -1.67 |     6.01 | 2329 |         0\n\n\nDie Log-Transformation hat in diesem Fall nicht wirklich zu einer Normalisierung der Variablen beigetragen. Aber das war auch nicht unser Ziel.\n\nCategories:\n\nregression\nlm\nqm2\nstats-nutshell"
  },
  {
    "objectID": "posts/ppv-mtcars1/ppv-mtcars1.html",
    "href": "posts/ppv-mtcars1/ppv-mtcars1.html",
    "title": "ppv-mtcars1",
    "section": "",
    "text": "Berechnen Sie folgendes Modell (Datensatz mtcars):\nmpg ~ hp\nGeben Sie die Breite eines 50%-ETI an für eine Beobachtung mit einem z-Wert von 0 im Prädiktor!\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks."
  },
  {
    "objectID": "posts/ppv-mtcars1/ppv-mtcars1.html#setup",
    "href": "posts/ppv-mtcars1/ppv-mtcars1.html#setup",
    "title": "ppv-mtcars1",
    "section": "Setup",
    "text": "Setup\n\nlibrary(rstanarm)\nlibrary(easystats)\nlibrary(tidyverse)\n\n\nmtcars2 &lt;-\n  mtcars %&gt;% \n  mutate(hp = standardize(hp))"
  },
  {
    "objectID": "posts/ppv-mtcars1/ppv-mtcars1.html#modell",
    "href": "posts/ppv-mtcars1/ppv-mtcars1.html#modell",
    "title": "ppv-mtcars1",
    "section": "Modell",
    "text": "Modell\n\nm1 &lt;- stan_glm(mpg ~ hp, data = mtcars, seed = 42, refresh = 0)\n\nModellparameter:\n\ncoef(m1)\n\n(Intercept)          hp \n30.11668130 -0.06820988 \n\n\nModellgüte:\n\nr2(m1)\n\n# Bayesian R2 with Compatibility Interval\n\n  Conditional R2: 0.586 (95% CI [0.378, 0.746])\n\n\nOder mit z-standardisierten Werten:\n\nm2 &lt;- stan_glm(mpg ~ hp, data = mtcars2, seed = 42, refresh = 0)\ncoef(m2)\n\n(Intercept)          hp \n  20.096771   -4.676665 \n\nr2(m2)\n\n# Bayesian R2 with Compatibility Interval\n\n  Conditional R2: 0.586 (95% CI [0.378, 0.746])"
  },
  {
    "objectID": "posts/ppv-mtcars1/ppv-mtcars1.html#ppv",
    "href": "posts/ppv-mtcars1/ppv-mtcars1.html#ppv",
    "title": "ppv-mtcars1",
    "section": "PPV",
    "text": "PPV\n\nm2_ppv &lt;- estimate_prediction(m2, data = tibble(hp = 0), ci = 0.5)\nm2_ppv\n\nModel-based Prediction\n\nhp   | Predicted |   SE |         50% CI\n----------------------------------------\n0.00 |     20.14 | 4.10 | [17.46, 22.76]\n\nVariable predicted: mpg\n\n\nVisualisierung:\n\nplot(estimate_prediction(m2))\n\n\n\n\n\n\n\n\nMan beachte, dass die PPV mit mehr Ungewissheit behaftet ist, als die Post-Verteilung.\n\nplot(estimate_relation(m2))\n\n\n\n\n\n\n\n\n\nCategories:\n\nbayes\nppv\nregression\nnum"
  },
  {
    "objectID": "posts/kausal29/kausal29.html",
    "href": "posts/kausal29/kausal29.html",
    "title": "kausal29",
    "section": "",
    "text": "library(dagitty)\nlibrary(ggdag)\nlibrary(ggplot2)\n\nGegeben sei der DAG (Graph) g (s. u.). Der DAG verfügt über mehrere Variablen, die als Knoten im Graph dargestellt sind.\n\ng &lt;-\n  dagify(\n    y ~ z + m,\n    m ~ x + z,\n    exposure = \"x\",\n    outcome = \"y\"\n  )\n\nHier ist die Definition des DAGs:\n\n\ndag {\nm\nx [exposure]\ny [outcome]\nz\nm -&gt; y\nx -&gt; m\nz -&gt; m\nz -&gt; y\n}\n\n\nUnd so sieht er aus:\n\nggdag(g) + theme_dag_blank()\n\n\n\n\n\n\n\n\nWelche minimale Variablenmenge muss kontrolliert werden, um den kausalen Effekt von der UV zur AV zu identifizieren?\nUV: x\nAV: y\nHinweise:\n\nMengen sind mittels geschweifter Klammern gekennzeichnet, z.B. {x8, x9} meint die Menge mit den zwei Elementen x8 und x9.\nDie leere Menge { } bedeutet, dass keine Variable kontrolliert werden muss, um den kausalen Effekt zu identifizieren.\nAlle Variablen werden als gemessen vorausgesetzt.\nEs ist möglich, dass es keine Lösung gibt, dass es also keine Adjustierungsmenge gibt, um den kausalen Effekt zu identifizieren. Wenn dies der Fall sein sollte, wählen Sie “keine Lösung”.\n\n\n\n\n{m}\n{z}\n{m, z}\n{ }\nkeine Lösung"
  },
  {
    "objectID": "posts/kausal29/kausal29.html#answerlist",
    "href": "posts/kausal29/kausal29.html#answerlist",
    "title": "kausal29",
    "section": "",
    "text": "{m}\n{z}\n{m, z}\n{ }\nkeine Lösung"
  },
  {
    "objectID": "posts/kausal29/kausal29.html#answerlist-1",
    "href": "posts/kausal29/kausal29.html#answerlist-1",
    "title": "kausal29",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nRichtig\nFalsch\n\n\nCategories:\n\ndag\ncausal\nschoice"
  },
  {
    "objectID": "posts/summarise01/summarise01.html",
    "href": "posts/summarise01/summarise01.html",
    "title": "summarise01",
    "section": "",
    "text": "Aufgabe\nIm Datensatz mariokart:\nFassen Sie die Spalte total_pr zusammen und zwar zum maximalwert!\nGeben Sie diese Zahl als Antwort zurück!\nHinweise:\n\nRunden Sie auf die nächste ganze Zahl.\nBeachten Sie die üblichen Hinweise des Datenwerks.\n\n         \n\n\nLösung\nPakete starten:\n\nlibrary(easystats)\nlibrary(tidyverse)  # startet das Paket tidyverse\n\nDaten importieren:\n\nmariokart &lt;- data_read(\"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\")\n\nZusammenfassen:\n\nmariokart_klein &lt;- summarise(mariokart, max_preis = max(total_pr)) \nmariokart_klein\n\n  max_preis\n1    326.51\n\n\nmin analog.\nDie Lösung lautet: 327 Euro\n\nCategories:\n\ndatawrangling\neda\ntidyverse\ndplyr\nnum"
  },
  {
    "objectID": "posts/argumente/argumente.html",
    "href": "posts/argumente/argumente.html",
    "title": "argumente",
    "section": "",
    "text": "Welche der folgenden Syntax-Beispiele zeigt fehlerhaften Code?\nEs sei jeweils definiert:\n\nx &lt;- c(1, 2, 3)\n\n\n\n\nmean(x = x)\nmean(FALSE)\nmean(na.rm = FALSE, x = x)\nmean(x, 0, FALSE)"
  },
  {
    "objectID": "posts/argumente/argumente.html#answerlist",
    "href": "posts/argumente/argumente.html#answerlist",
    "title": "argumente",
    "section": "",
    "text": "mean(x = x)\nmean(FALSE)\nmean(na.rm = FALSE, x = x)\nmean(x, 0, FALSE)"
  },
  {
    "objectID": "posts/argumente/argumente.html#answerlist-1",
    "href": "posts/argumente/argumente.html#answerlist-1",
    "title": "argumente",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\nFalsch\nFalsch\n\n\nCategories:\n\nR\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/supervisedlearning/supervisedlearning.html",
    "href": "posts/supervisedlearning/supervisedlearning.html",
    "title": "supervisedlearning",
    "section": "",
    "text": "Welches der folgenden Verfahren ist ein Vertreter des geleiteten Lernens (supervised learning)?\n\n\n\nClusteranalyse\nPCA\nAnalyse der Worthäufigkeit in Textmining-Analysen\nRandom Forest"
  },
  {
    "objectID": "posts/supervisedlearning/supervisedlearning.html#answerlist",
    "href": "posts/supervisedlearning/supervisedlearning.html#answerlist",
    "title": "supervisedlearning",
    "section": "",
    "text": "Clusteranalyse\nPCA\nAnalyse der Worthäufigkeit in Textmining-Analysen\nRandom Forest"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-tune/germeval-sent-wordvec-xgb-tune.html",
    "href": "posts/germeval-sent-wordvec-xgb-tune/germeval-sent-wordvec-xgb-tune.html",
    "title": "germeval03-sent-wordvec-xgb-tune",
    "section": "",
    "text": "Erstellen Sie ein prädiktives Modell für Textdaten. Nutzen Sie Sentiments und TextFeatures im Rahmen von Feature-Engineering. Nutzen Sie außerdem deutsche Word-Vektoren für das Feature-Engineering.\nAls Lernalgorithmus verwenden Sie XGB. Tunen Sie die Lernrate und die max. Tiefe (max_depth) des Modells.\n\n\nVerwenden Sie die GermEval-2018-Daten.\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\nDie Daten sind auch über das R-Paket PradaData zu beziehen.\n\nlibrary(tidyverse)\ndata(\"germeval_train\", package = \"pradadata\")\ndata(\"germeval_test\", package = \"pradadata\")\n\n\n\n\nDie AV lautet c1. Die (einzige) UV lautet: text.\n\n\n\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\nNutzen Sie Tidymodels.\nNutzen Sie das sentiws Lexikon.\n❗ Achten Sie darauf, die Variable c2 zu entfernen bzw. nicht zu verwenden."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-tune/germeval-sent-wordvec-xgb-tune.html#daten",
    "href": "posts/germeval-sent-wordvec-xgb-tune/germeval-sent-wordvec-xgb-tune.html#daten",
    "title": "germeval03-sent-wordvec-xgb-tune",
    "section": "",
    "text": "Verwenden Sie die GermEval-2018-Daten.\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\nDie Daten sind auch über das R-Paket PradaData zu beziehen.\n\nlibrary(tidyverse)\ndata(\"germeval_train\", package = \"pradadata\")\ndata(\"germeval_test\", package = \"pradadata\")"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-tune/germeval-sent-wordvec-xgb-tune.html#av-und-uv",
    "href": "posts/germeval-sent-wordvec-xgb-tune/germeval-sent-wordvec-xgb-tune.html#av-und-uv",
    "title": "germeval03-sent-wordvec-xgb-tune",
    "section": "",
    "text": "Die AV lautet c1. Die (einzige) UV lautet: text."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-tune/germeval-sent-wordvec-xgb-tune.html#hinweise",
    "href": "posts/germeval-sent-wordvec-xgb-tune/germeval-sent-wordvec-xgb-tune.html#hinweise",
    "title": "germeval03-sent-wordvec-xgb-tune",
    "section": "",
    "text": "Orientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\nNutzen Sie Tidymodels.\nNutzen Sie das sentiws Lexikon.\n❗ Achten Sie darauf, die Variable c2 zu entfernen bzw. nicht zu verwenden."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-tune/germeval-sent-wordvec-xgb-tune.html#setup",
    "href": "posts/germeval-sent-wordvec-xgb-tune/germeval-sent-wordvec-xgb-tune.html#setup",
    "title": "germeval03-sent-wordvec-xgb-tune",
    "section": "Setup",
    "text": "Setup\n\nd_train &lt;-\n  germeval_train |&gt; \n  select(id, c1, text)\n\n\nlibrary(tictoc)\nlibrary(tidymodels)\n#library(syuzhet)\nlibrary(beepr)\nlibrary(finetune)  # anova race\nlibrary(lobstr)  # object size\nlibrary(visdat)  # footprint of csv\n#data(\"sentiws\", package = \"pradadata\")\n\nEine Vorlage für ein Tidymodels-Pipeline findet sich hier."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-tune/germeval-sent-wordvec-xgb-tune.html#learnermodell",
    "href": "posts/germeval-sent-wordvec-xgb-tune/germeval-sent-wordvec-xgb-tune.html#learnermodell",
    "title": "germeval03-sent-wordvec-xgb-tune",
    "section": "Learner/Modell",
    "text": "Learner/Modell\n\nmod &lt;-\n  boost_tree(mode = \"classification\",\n             learn_rate = tune(), \n             tree_depth = tune()\n             )"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-tune/germeval-sent-wordvec-xgb-tune.html#gebackenen-datensatz-als-neue-grundlage",
    "href": "posts/germeval-sent-wordvec-xgb-tune/germeval-sent-wordvec-xgb-tune.html#gebackenen-datensatz-als-neue-grundlage",
    "title": "germeval03-sent-wordvec-xgb-tune",
    "section": "Gebackenen Datensatz als neue Grundlage",
    "text": "Gebackenen Datensatz als neue Grundlage\nWir importieren den schon an anderer Stelle aufbereiteten Datensatz. Das hat den Vorteil (hoffentlich), das die Datenvolumina viel kleiner sind. Die Arbeit des Feature Engineering wurde uns schon abgenommen.\n\nd_train &lt;-\n  read_csv(\"https://raw.githubusercontent.com/sebastiansauer/Datenwerk2/main/data/germeval/germeval_train_recipe_wordvec_senti.csv\")\n\nRows: 5009 Columns: 121\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (1): c1\ndbl (120): id, emo_count, schimpf_count, emoji_count, textfeature_text_copy_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nvis_dat(d_train) +\n  # remove axis labels:\n  theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank() \n        )\n\n\n\n\n\n\n\n\n\nd_test_baked &lt;- read_csv(\"https://raw.githubusercontent.com/sebastiansauer/Datenwerk2/main/data/germeval/germeval_test_recipe_wordvec_senti.csv\")\n\nRows: 3532 Columns: 121\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (1): c1\ndbl (120): id, emo_count, schimpf_count, emoji_count, textfeature_text_copy_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-tune/germeval-sent-wordvec-xgb-tune.html#plain-rezept",
    "href": "posts/germeval-sent-wordvec-xgb-tune/germeval-sent-wordvec-xgb-tune.html#plain-rezept",
    "title": "germeval03-sent-wordvec-xgb-tune",
    "section": "Plain-Rezept",
    "text": "Plain-Rezept\n\nrec &lt;- \n  recipe(c1 ~ ., data = d_train)"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-tune/germeval-sent-wordvec-xgb-tune.html#neuer-workflow-mit-plainem-rezept",
    "href": "posts/germeval-sent-wordvec-xgb-tune/germeval-sent-wordvec-xgb-tune.html#neuer-workflow-mit-plainem-rezept",
    "title": "germeval03-sent-wordvec-xgb-tune",
    "section": "Neuer Workflow mit plainem Rezept",
    "text": "Neuer Workflow mit plainem Rezept\n\nwf &lt;-\n  workflow() |&gt; \n  add_recipe(rec) |&gt; \n  add_model(mod)"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-tune/germeval-sent-wordvec-xgb-tune.html#parallelisierung-über-mehrere-kerne",
    "href": "posts/germeval-sent-wordvec-xgb-tune/germeval-sent-wordvec-xgb-tune.html#parallelisierung-über-mehrere-kerne",
    "title": "germeval03-sent-wordvec-xgb-tune",
    "section": "Parallelisierung über mehrere Kerne",
    "text": "Parallelisierung über mehrere Kerne\n\nlibrary(parallel)\nall_cores &lt;- detectCores(logical = FALSE)\n\nlibrary(doFuture)\n\nLoading required package: foreach\n\n\n\nAttaching package: 'foreach'\n\n\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n\n\nLoading required package: future\n\nregisterDoFuture()\ncl &lt;- makeCluster(3)\nplan(cluster, workers = cl)\n\nAchtung: Viele Kerne brauchen auch viel Speicher."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-tune/germeval-sent-wordvec-xgb-tune.html#tuneresamplefit",
    "href": "posts/germeval-sent-wordvec-xgb-tune/germeval-sent-wordvec-xgb-tune.html#tuneresamplefit",
    "title": "germeval03-sent-wordvec-xgb-tune",
    "section": "Tune/Resample/Fit",
    "text": "Tune/Resample/Fit\n\ntic()\nfit_wordvec_senti_xgb &lt;-\n  tune_race_anova(\n    wf,\n    grid = 30,\n    resamples = vfold_cv(d_train, v = 5),\n    control = control_race(verbose_elim = TRUE))\n\nℹ Racing will maximize the roc_auc metric.\nℹ Resamples are analyzed in a random order.\nℹ Fold5: 23 eliminated; 7 candidates remain.\n\nℹ Fold4: 2 eliminated; 5 candidates remain.\n\ntoc()\n\n186.332 sec elapsed\n\nbeep()\n\nObjekt-Größe:\n\nlobstr::obj_size(fit_wordvec_senti_xgb)\n\n5.10 MB\n\n\nGroß!\nWie wir gesehen haben, ist das Rezept riesig."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-tune/germeval-sent-wordvec-xgb-tune.html#get-best-performance",
    "href": "posts/germeval-sent-wordvec-xgb-tune/germeval-sent-wordvec-xgb-tune.html#get-best-performance",
    "title": "germeval03-sent-wordvec-xgb-tune",
    "section": "Get best performance",
    "text": "Get best performance\n\nautoplot(fit_wordvec_senti_xgb)\n\n\n\n\n\n\n\n\n\nshow_best(fit_wordvec_senti_xgb)\n\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n\n\n# A tibble: 5 × 8\n  tree_depth learn_rate .metric .estimator  mean     n std_err .config          \n       &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;            \n1          7     0.255  roc_auc binary     0.765     5 0.00862 Preprocessor1_Mo…\n2         11     0.292  roc_auc binary     0.760     5 0.00859 Preprocessor1_Mo…\n3          9     0.126  roc_auc binary     0.756     5 0.00662 Preprocessor1_Mo…\n4          8     0.0796 roc_auc binary     0.755     5 0.00570 Preprocessor1_Mo…\n5         10     0.213  roc_auc binary     0.754     5 0.00628 Preprocessor1_Mo…\n\nbest_params &lt;- select_best(fit_wordvec_senti_xgb)\n\nWarning: No value of `metric` was given; metric 'roc_auc' will be used."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-tune/germeval-sent-wordvec-xgb-tune.html#finalisieren",
    "href": "posts/germeval-sent-wordvec-xgb-tune/germeval-sent-wordvec-xgb-tune.html#finalisieren",
    "title": "germeval03-sent-wordvec-xgb-tune",
    "section": "Finalisieren",
    "text": "Finalisieren\n\nbest_params &lt;- select_best(fit_wordvec_senti_xgb)\n\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n\ntic()\nwf_finalized &lt;- finalize_workflow(wf, best_params)\nlastfit_xgb &lt;- fit(wf_finalized, data = d_train)\ntoc()\n\n1.997 sec elapsed"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb-tune/germeval-sent-wordvec-xgb-tune.html#test-set-güte",
    "href": "posts/germeval-sent-wordvec-xgb-tune/germeval-sent-wordvec-xgb-tune.html#test-set-güte",
    "title": "germeval03-sent-wordvec-xgb-tune",
    "section": "Test-Set-Güte",
    "text": "Test-Set-Güte\n\ntic()\npreds &lt;-\n  predict(lastfit_xgb, new_data = d_test_baked)\ntoc()\n\n0.219 sec elapsed\n\n\n\nd_test &lt;-\n  d_test_baked |&gt; \n  bind_cols(preds) |&gt; \n  mutate(c1 = as.factor(c1))\n\n\nmy_metrics &lt;- metric_set(accuracy, f_meas)\nmy_metrics(d_test,\n           truth = c1,\n           estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.715\n2 f_meas   binary         0.479"
  },
  {
    "objectID": "posts/penguins-stan-02/penguins-stan-02.html",
    "href": "posts/penguins-stan-02/penguins-stan-02.html",
    "title": "penguins-stan-02",
    "section": "",
    "text": "Aufgabe\nWir untersuchen Einflussfaktoren bzw. Prädiktoren auf das Körpergewicht von Pinguinen. In dieser Aufgabe untersuchen wir den Zusammenhang von Schnabellänge (als UV) und Körpergewicht (als AV).\nWie groß ist der statistische Einfluss der UV auf die AV?\nGeben Sie die Breite eines 90%-HDI an (zum Effekt)!\nHinweise:\n\nNutzen Sie den Datensatz zu den Palmer Penguins.\nSie können den Datensatz z.B. hier beziehen oder über das R-Paket palmerpenguins.\nWeitere Hinweise\n\n         \n\n\nLösung\nZentrieren ist eigentlich immer nützlich, aber hier streng genommen nicht unbedingt nötig. Der Hauptgrund dafür ist, dass Stan für uns den Prior für den Intercept festlegt, und zwar auf Basis der Daten, wir uns also nicht um die komische Frage zu kümmern brauchen, welchen Prior wir für den unzentrierten Achsenabschnitt vergeben wollten: Wie schwer sind Pinguins der Schnabellänge Null? Mit zentrierten Prädiktoren ist die Frage nach dem Prior viel einfacher zu beantworten: Wie schwer ist ein Pinguin mit mittelgroßem Schnabel?\nSetup:\n\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(rstanarm)\n\ndata(\"penguins\", package = \"palmerpenguins\")\n\nEs wird in dieser Aufgabe vorausgesetzt, dass Sie den Datensatz selbständig importieren können. Tipp: Kurzes Googeln hilft ggf., den Datensatz zu finden.\nAlternativ könnten Sie den Datensatz als CSV-Datei importieren:\n\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\"\npenguins &lt;- data_read(d_path)\n\nEin Blick in die Daten zur Kontrolle, ob das Importieren richtig funktioniert hat:\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nVertrauen ist gut, aber - was Golems betrifft - ist Kontrolle eindeutig besser ;-)\n\nm1 &lt;- stan_glm(body_mass_g ~  bill_length_mm,  # Regressionsgleichung\n               data = penguins, #  Daten\n               seed = 42,  # Repro.\n               refresh = 0)  # nicht so viel Output\n\n\nparameters(m1, ci_method = \"hdi\", ci = .9, keep = \"bill_length_mm\")\n\nParameter      | Median |         90% CI |   pd |  Rhat |     ESS |                Prior\n----------------------------------------------------------------------------------------\nbill_length_mm |  87.45 | [77.17, 98.51] | 100% | 0.999 | 3931.00 | Normal (0 +- 367.22)\n\n\n\nUncertainty intervals (highest-density) and p-values (two-tailed)\n  computed using a MCMC distribution approximation.\n\n\nDie Lösung lautet also, wie aus der Ausgabe von parameters() ersichtlich, 21.34.\n\nCategories:\n\nbayes\nregression\nexam-22"
  },
  {
    "objectID": "posts/twitter06/twitter06.html",
    "href": "posts/twitter06/twitter06.html",
    "title": "twitter06",
    "section": "",
    "text": "Exercise\nLaden Sie \\(n=10^k\\) Tweets von Twitter herunter (mit \\(k=4\\)) via der Twitter API; die Tweets sollen jeweils an eine prominente Person gerichtet sein.\nBeziehen Sie sich auf folgende Personen bzw. Twitter-Accounts:\n\nMarkus_Soeder\nkarl_lauterbach.\n\nBereiten Sie die Textdaten mit grundlegenden Methoden des Textminings auf (Tokenisieren, Stopwörter entfernen, Zahlen entfernen, …).\nNutzen Sie die Daten dann, um eine Sentimentanalyse zu erstellen.\nVergleichen Sie die Ergebnisse für alle untersuchten Personen.\n         \n         \n\n\nSolution\n\nlibrary(rtweet)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()  masks stats::filter()\n✖ purrr::flatten() masks rtweet::flatten()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidytext)\nlibrary(lsa)  # Stopwörter\n\nLoading required package: SnowballC\n\nlibrary(SnowballC)  # Stemming\n\n\ndata(sentiws, package = \"pradadata\")\n\nZuerst muss man sich anmelden und die Tweets herunterladen:\n\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\n\nauth &lt;- rtweet_app(bearer_token = Bearer_Token)\n\n\ntweets_to_kl &lt;- search_tweets(\"@karl_lauterbach\", n = 1e2, include_rts = FALSE)\n#write_rds(tweets_to_kl, file = \"tweets_to_kl.rds\", compress = \"gz\")\ntweets_to_ms &lt;- search_tweets(\"@Markus_Soeder\", n = 1e4, include_rts = FALSE)\n#write_rds(tweets_to_ms, file = \"tweets_to_ms.rds\", compress = \"gz\")\n\nDie Vorverarbeitung pro Screenname packen wir in eine Funktion, das macht es hinten raus einfacher:\n\nprepare_tweets &lt;- function(tweets){\n  \n  tweets %&gt;% \n    select(full_text) %&gt;% \n    unnest_tokens(output = word, input = full_text) %&gt;% \n    anti_join(tibble(word = lsa::stopwords_de)) %&gt;% \n    mutate(word = str_replace_na(word, \"^[:digit:]+$\")) %&gt;% \n    mutate(word = str_replace_na(word, \"hptts?://\\\\w+\")) %&gt;% \n    mutate(word = str_replace_na(word, \" +\")) %&gt;% \n    drop_na()\n}\n\nTest:\n\nkl_prepped &lt;- \n  prepare_tweets(tweets_to_kl_raw)\n\nJoining with `by = join_by(word)`\n\nhead(kl_prepped)\n\n# A tibble: 6 × 1\n  word                     \n  &lt;chr&gt;                    \n1 tonline⁩                  \n2 spreche                  \n3 neuen                    \n4 pläne                    \n5 bundesgesundheitsminister\n6 karl_lauterbach⁩          \n\n\n\nms_prepped &lt;-\n  prepare_tweets(tweets_to_ms_raw)\n\nJoining with `by = join_by(word)`\n\nhead(ms_prepped)\n\n# A tibble: 6 × 1\n  word         \n  &lt;chr&gt;        \n1 markus_soeder\n2 climate      \n3 activists    \n4 are          \n5 sometimes    \n6 depicted     \n\n\nScheint zu passen.\nDie Sentimentanalyse packen wir auch in eine Funktion:\n\nget_tweets_sentiments &lt;- function(tweets){\n  \n  tweets %&gt;% \n    inner_join(sentiws) %&gt;% \n    group_by(neg_pos) %&gt;% \n    summarise(senti_avg = mean(value, na.rm = TRUE),\n              senti_sd = sd(value, na.rm = TRUE),\n              senti_n = n()) \n}\n\nTest:\n\nkl_prepped %&gt;% \n  get_tweets_sentiments()\n\nJoining with `by = join_by(word)`\n\n\nWarning in inner_join(., sentiws): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 6649 of `x` matches multiple rows in `y`.\nℹ Row 3102 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n# A tibble: 2 × 4\n  neg_pos senti_avg senti_sd senti_n\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;int&gt;\n1 neg        -0.313    0.237    3576\n2 pos         0.112    0.145    5800\n\n\nTest:\n\ntweets_to_kl_raw %&gt;% \n  prepare_tweets() %&gt;% \n  get_tweets_sentiments()\n\nJoining with `by = join_by(word)`\nJoining with `by = join_by(word)`\n\n\nWarning in inner_join(., sentiws): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 6649 of `x` matches multiple rows in `y`.\nℹ Row 3102 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n# A tibble: 2 × 4\n  neg_pos senti_avg senti_sd senti_n\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;int&gt;\n1 neg        -0.313    0.237    3576\n2 pos         0.112    0.145    5800\n\n\nScheint zu passen.\nWir könnten noch die beiden Funktionen in eine wrappen:\n\nprep_sentiments &lt;- function(tweets) {\n\n  tweets %&gt;% \n    prepare_tweets() %&gt;% \n    get_tweets_sentiments()\n}\n\n\ntweets_to_kl_raw %&gt;% \n  prep_sentiments()\n\nJoining with `by = join_by(word)`\nJoining with `by = join_by(word)`\n\n\nWarning in inner_join(., sentiws): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 6649 of `x` matches multiple rows in `y`.\nℹ Row 3102 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n# A tibble: 2 × 4\n  neg_pos senti_avg senti_sd senti_n\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;int&gt;\n1 neg        -0.313    0.237    3576\n2 pos         0.112    0.145    5800\n\n\nOkay, jetzt werden wir die Funktion auf jede Screenname bzw. die Tweets jedes Screennames an.\n\ntweets_list &lt;-\n  list(\n    kl = tweets_to_kl_raw, \n    ms = tweets_to_ms_raw)\n\n\nsentis &lt;-\n  tweets_list %&gt;% \n  map_df(prep_sentiments, .id = \"id\")\n\nJoining with `by = join_by(word)`\nJoining with `by = join_by(word)`\n\n\nWarning in inner_join(., sentiws): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 6649 of `x` matches multiple rows in `y`.\nℹ Row 3102 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\nJoining with `by = join_by(word)`\nJoining with `by = join_by(word)`\n\n\nWarning in inner_join(., sentiws): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 17223 of `x` matches multiple rows in `y`.\nℹ Row 2894 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\nCategories:\n\ntextmining\ntwitter\nprogramming"
  },
  {
    "objectID": "posts/twitter01/twitter01.html",
    "href": "posts/twitter01/twitter01.html",
    "title": "twitter01",
    "section": "",
    "text": "Exercise\nLaden Sie die neuesten Tweets an karl_lauterbach herunter!\n         \n\n\nSolution\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rtweet)\n\n\nAttaching package: 'rtweet'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\n\nEinloggen bei Twitter; zuerst die Credentials bereithalten:\n\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\n\nDann anmelden, z.B. als Bot:\n\nauth &lt;- rtweet_bot(api_key = API_Key,\n                   api_secret = API_Key_Secret,\n                   access_token = Access_Token,\n                   access_secret = Access_Token_Secret)\n\n… Oder als App, das bringt bessere Raten mit sich:\n\nauth &lt;- rtweet_app(bearer_token = Bearer_Token)\n\nTest:\n\nsesa_test &lt;- get_timeline(user = \"sauer_sebastian\", n = 3) %&gt;% \n  select(full_text)\n\nsesa_test\n\n1 RT @fuecks: By the way: Systematic destruction of life-sustaining infrastructures …\n2 RT @NoContextBrits: No shortbread for little Nazis. https://t.co/F6FUPvRz94        \n3 RT @ernst_gennat: 2 oder 3 Jahre #Tempolimit von 120 km/h. Abschließend Evaluation…\nTweets an Karl Lauterbach suchen:\n\nkarl1 &lt;- search_tweets(\"@karl_lauterbach\")\n\nIn Auszügen:\n\"@Karl_Lauterbach Ein Minister der alle paar Stunden Zeit hat einen Mist zu verbreiten....\"   \n\"@Karl_Lauterbach @focusonline Long Covid ist nichts anderes als schwere Nebenwirkungen der Gentherapie!\"  \"@Karl_Lauterbach @focusonline Wer schützt uns vor Long Lauterbach?\"\n\"@Karl_Lauterbach Also Karl, primär fordere ich und viele andere eher erstmal dein sofortigen Rücktritt.\"  \"@Karl_Lauterbach Behalt deinen Senf für dich!\"                                                            \"@Karl_Lauterbach Oh Gott 😱\"     \n\"@Karl_Lauterbach Ach nein, der Clown mit Lebensangst ….\\n\\nhttps://t.co/8cQZeHh6Ew\"                       \"@Karl_Lauterbach Ich kenne nur Leute mit Long Covid, die mehrfach geimpft sind! Das ist kein Witz! Scheinbar liegt’s wohl doch an den Spritzen???\"                                                            \"@Karl_Lauterbach @focusonline Interessiert keine Sau 😉\"                      \n\"RT @Karl_Lauterbach @focusonline „Lauterbachs Aussagen können fundamental nicht stimmen“\\nhttps://t.co/rfxnWAWiZX\"                                                                          \"@Karl_Lauterbach @focusonline 🤡😂😂😂😂😂😂\"                                         \n\"@Karl_Lauterbach Jau und sie sind kein fähiger Gesundheitsminister, sondern lediglich ein gekaufter Coronaminister\"        \nPuh, viele toxische Tweets, wie es scheint.\nUnd ohne Retweets (RT) und ohne Replies:\n\nkarl2 &lt;- search_tweets(\"@karl_lauterbach\", \n  include_rts = FALSE, `-filter` = \"replies\")\n\nTweets, die an Karl Lauterbach gerichtet sind, per API-Anweisung:\n\nkarl3 &lt;- search_tweets(\"to:karl_lauterbach\", n = 100)\n\n\"@Karl_Lauterbach Vielen Dank, dass LongCovid ein gefundenes fressen für die jenigen ist, die nicht mehr Arbeiten wollen.\"       \n \"@Karl_Lauterbach verpiss dich einfach! Immer dieser Schwachsinn\"    \n\"@Karl_Lauterbach @focusonline Das sind genau die Impfnebenwirkungen! Will man nun das wenden um die Impfnebenwirkungen zu vertuschen? \\nWofür ist die Impfung gut wenn nicht mal Long-Covid verhindert wird, die Ansteckung konnte sie noch nie verhindern!\\nWarum sind 89% Covid Patienten geimpfte in den Spitäler?\"\n\"@Karl_Lauterbach Was spielen Sie eigentlich für ein schmutziges Spiel?\\n\\nhttps://t.co/8LJIzxyF7G\"   \n \"@Karl_Lauterbach @focusonline Bessen von Covid! Ständig wird das Netz durchsucht, nach Artikeln,die instrumentalisiert werden, um für Impfung zu werben. Was hätte nur ein vernünftiger Gesundheitsminister mit so viel Zeit Vernünftiges im Gesundheitswesen auf die Beine stellen können...\"    \n\"@Karl_Lauterbach Mit Dauerschaden wegen der Impfung 💉 bin ich Arbeitslos geworden in der Pflege 🤷‍♂️ Ist das normal Herr @Karl_Lauterbach ?\"          \nOb man mit @karl_lauterbach sucht oder `to:karl_lauterbach”, scheint keinen großen Unterschied zu machen (?).\n\nCategories:\n\ntextmining\ntwitter"
  },
  {
    "objectID": "posts/kausal28/kausal28.html",
    "href": "posts/kausal28/kausal28.html",
    "title": "kausal28",
    "section": "",
    "text": "Gegeben sei der DAG g (s. u.). Der DAG verfügt über mehrere Variablen, die als Knoten im Graph dargestellt sind und mit \\(x_1, x_2, \\ldots x_n\\) bezeichnet sind.\nWelche minimale Variablenmenge muss kontrolliert werden, um den kausalen Effekt von der UV zur AV zu identifizieren?\nUV: x7.\nAV: x8.\nHinweise:\n\nMengen sind mittels geschweifter Klammern gekennzeichnet, z.B. {x8, x9} meint die Menge mit den zwei Elementen x8 und x9.\nDie leere Menge { } bedeutet, dass keine Variable kontrolliert werden muss, um den kausalen Effekt zu identifizieren.\nAlle Variablen werden als gemessen vorausgesetzt.\nEs ist möglich, dass es keine Lösung gibt, dass es also keine Adjustierungsmenge gibt, um den kausalen Effekt zu identifizieren. Wenn dies der Fall sein sollte, wählen Sie “keine Lösung”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n{ x1 , x2 , x3 , x4 , x5 , x6 }\n{ x3, x4 }\n{ x8 }\n{ x6, x7 }\n{ x2, x5 }"
  },
  {
    "objectID": "posts/kausal28/kausal28.html#answerlist",
    "href": "posts/kausal28/kausal28.html#answerlist",
    "title": "kausal28",
    "section": "",
    "text": "{ x1 , x2 , x3 , x4 , x5 , x6 }\n{ x3, x4 }\n{ x8 }\n{ x6, x7 }\n{ x2, x5 }"
  },
  {
    "objectID": "posts/kausal28/kausal28.html#answerlist-1",
    "href": "posts/kausal28/kausal28.html#answerlist-1",
    "title": "kausal28",
    "section": "Answerlist",
    "text": "Answerlist\n\nRichtig\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal\nschoice"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-glm/germeval-sent-wordvec-glm.html",
    "href": "posts/germeval-sent-wordvec-glm/germeval-sent-wordvec-glm.html",
    "title": "germeval03-sent-wordvec-glm",
    "section": "",
    "text": "Erstellen Sie ein prädiktives Modell für Textdaten. Nutzen Sie Sentiments und TextFeatures im Rahmen von Feature-Engineering. Nutzen Sie außerdem deutsche Word-Vektoren für das Feature-Engineering.\nAls Lernalgorithmus verwenden Sie eine einfache logistische Regression, d.h. ohne Tuning-Parameter.\n\n\nVerwenden Sie die GermEval-2018-Daten.\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\nDie Daten sind auch über das R-Paket PradaData zu beziehen.\n\nlibrary(tidyverse)\ndata(\"germeval_train\", package = \"pradadata\")\ndata(\"germeval_test\", package = \"pradadata\")\n\n\n\n\nDie AV lautet c1. Die (einzige) UV lautet: text.\n\n\n\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\nNutzen Sie Tidymodels.\nNutzen Sie das sentiws Lexikon.\n❗ Achten Sie darauf, die Variable c2 zu entfernen bzw. nicht zu verwenden."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-glm/germeval-sent-wordvec-glm.html#daten",
    "href": "posts/germeval-sent-wordvec-glm/germeval-sent-wordvec-glm.html#daten",
    "title": "germeval03-sent-wordvec-glm",
    "section": "",
    "text": "Verwenden Sie die GermEval-2018-Daten.\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\nDie Daten sind auch über das R-Paket PradaData zu beziehen.\n\nlibrary(tidyverse)\ndata(\"germeval_train\", package = \"pradadata\")\ndata(\"germeval_test\", package = \"pradadata\")"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-glm/germeval-sent-wordvec-glm.html#av-und-uv",
    "href": "posts/germeval-sent-wordvec-glm/germeval-sent-wordvec-glm.html#av-und-uv",
    "title": "germeval03-sent-wordvec-glm",
    "section": "",
    "text": "Die AV lautet c1. Die (einzige) UV lautet: text."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-glm/germeval-sent-wordvec-glm.html#hinweise",
    "href": "posts/germeval-sent-wordvec-glm/germeval-sent-wordvec-glm.html#hinweise",
    "title": "germeval03-sent-wordvec-glm",
    "section": "",
    "text": "Orientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\nNutzen Sie Tidymodels.\nNutzen Sie das sentiws Lexikon.\n❗ Achten Sie darauf, die Variable c2 zu entfernen bzw. nicht zu verwenden."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-glm/germeval-sent-wordvec-glm.html#setup",
    "href": "posts/germeval-sent-wordvec-glm/germeval-sent-wordvec-glm.html#setup",
    "title": "germeval03-sent-wordvec-glm",
    "section": "Setup",
    "text": "Setup\nTrain-Datensatz:\n\nd_train &lt;-\n  germeval_train |&gt; \n  select(id, c1, text)\n\nPakete:\n\nlibrary(tictoc)\nlibrary(tidymodels)\nlibrary(beepr)\nlibrary(finetune)  # anova race\n\nEine Vorlage für ein Tidymodels-Pipeline findet sich hier."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-glm/germeval-sent-wordvec-glm.html#learnermodell",
    "href": "posts/germeval-sent-wordvec-glm/germeval-sent-wordvec-glm.html#learnermodell",
    "title": "germeval03-sent-wordvec-glm",
    "section": "Learner/Modell",
    "text": "Learner/Modell\n\nmod &lt;-\n  logistic_reg(mode = \"classification\"\n  )"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-glm/germeval-sent-wordvec-glm.html#gebackenen-datensatz-als-neue-grundlage",
    "href": "posts/germeval-sent-wordvec-glm/germeval-sent-wordvec-glm.html#gebackenen-datensatz-als-neue-grundlage",
    "title": "germeval03-sent-wordvec-glm",
    "section": "Gebackenen Datensatz als neue Grundlage",
    "text": "Gebackenen Datensatz als neue Grundlage\nWir importieren den schon an anderer Stelle aufbereiteten Datensatz. Das hat den Vorteil (hoffentlich), das die Datenvolumina viel kleiner sind. Die Arbeit des Feature Engineering wurde uns schon abgenommen.\n\nd_train_raw &lt;-\n  read_csv(\"https://raw.githubusercontent.com/sebastiansauer/Datenwerk2/main/data/germeval/germeval_train_recipe_wordvec_senti.csv\")\n\nRows: 5009 Columns: 121\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (1): c1\ndbl (120): id, emo_count, schimpf_count, emoji_count, textfeature_text_copy_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nd_test_baked_raw &lt;- read_csv(\"https://raw.githubusercontent.com/sebastiansauer/Datenwerk2/main/data/germeval/germeval_test_recipe_wordvec_senti.csv\")\n\nRows: 3532 Columns: 121\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (1): c1\ndbl (120): id, emo_count, schimpf_count, emoji_count, textfeature_text_copy_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-glm/germeval-sent-wordvec-glm.html#keine-dummysierung-der-av",
    "href": "posts/germeval-sent-wordvec-glm/germeval-sent-wordvec-glm.html#keine-dummysierung-der-av",
    "title": "germeval03-sent-wordvec-glm",
    "section": "Keine Dummysierung der AV",
    "text": "Keine Dummysierung der AV\nLineare Modelle müssen dummysiert sein. Rezepte wollen das nicht so gerne für die AV besorgen.\nABER: Klassifikationsmodelle in Tidymodels (parsnip) benötigen eine Variable vom Typ factor als AV, sonst werden sie nicht als Klassifikation erkannt.\n\nd_train &lt;-\n  d_train_raw |&gt; \n  mutate(c1 = as.factor(c1)) \n\nlevels(d_train$c1)\n\n[1] \"OFFENSE\" \"OTHER\"  \n\n\nTidymodels modelliert die erste Stufe, nicht die zweite, wie Base-R glm.\n\nd_test_baked &lt;-\n  d_test_baked_raw |&gt; \n  mutate(c1 = as.factor(c1)) \n\nlevels(d_test_baked$c1)\n\n[1] \"OFFENSE\" \"OTHER\""
  },
  {
    "objectID": "posts/germeval-sent-wordvec-glm/germeval-sent-wordvec-glm.html#dummy-rezept",
    "href": "posts/germeval-sent-wordvec-glm/germeval-sent-wordvec-glm.html#dummy-rezept",
    "title": "germeval03-sent-wordvec-glm",
    "section": "Dummy-Rezept",
    "text": "Dummy-Rezept\nPlain, aber mit Dummyisierung:\n\nrec &lt;- \n  recipe(c1 ~ ., data = d_train)"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-glm/germeval-sent-wordvec-glm.html#workflow",
    "href": "posts/germeval-sent-wordvec-glm/germeval-sent-wordvec-glm.html#workflow",
    "title": "germeval03-sent-wordvec-glm",
    "section": "Workflow",
    "text": "Workflow\n\nwf &lt;-\n  workflow() |&gt; \n  add_recipe(rec) |&gt; \n  add_model(mod)"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-glm/germeval-sent-wordvec-glm.html#tuneresamplefit",
    "href": "posts/germeval-sent-wordvec-glm/germeval-sent-wordvec-glm.html#tuneresamplefit",
    "title": "germeval03-sent-wordvec-glm",
    "section": "Tune/Resample/Fit",
    "text": "Tune/Resample/Fit\n\nfit_train &lt;-\n  fit(wf,\n      data = d_train)"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-glm/germeval-sent-wordvec-glm.html#test-set-güte",
    "href": "posts/germeval-sent-wordvec-glm/germeval-sent-wordvec-glm.html#test-set-güte",
    "title": "germeval03-sent-wordvec-glm",
    "section": "Test-Set-Güte",
    "text": "Test-Set-Güte\n\ntic()\npreds &lt;-\n  predict(fit_train, new_data = d_test_baked)\ntoc()\n\n0.036 sec elapsed\n\n\n\nd_test &lt;-\n  d_test_baked |&gt; \n  bind_cols(preds) |&gt; \n  mutate(c1 = as.factor(c1))\n\n\nmy_metrics &lt;- metric_set(accuracy, f_meas)\nmy_metrics(d_test,\n           truth = c1,\n           estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.716\n2 f_meas   binary         0.509"
  },
  {
    "objectID": "posts/mtcars-simple1/mtcars-simple1.html",
    "href": "posts/mtcars-simple1/mtcars-simple1.html",
    "title": "mtcars-simple1",
    "section": "",
    "text": "Exercise\nWe will use the dataset mtcars in this exercise.\nAssume your causal model of your research dictates that fuel economy is a linear function of horse power, cylinder count and displacement of the engine.\nCompute the causal effect of horse power given the above model! Report the point estimate.\nNotes:\n\nUse can either use frequentist or bayesian modeling.\nUse R for all computations.\nThere are multiple ways to find a solution.\n\n         \n\n\nSolution\nCompute Model:\n\nlm1_freq &lt;- lm(mpg ~ hp + cyl + disp, data = mtcars)\n\nlibrary(rstanarm)\nlm1_bayes &lt;- stan_glm(mpg ~ hp + cyl + disp, data = mtcars, refresh = 0)\n\nGet parameters:\n\nlibrary(easystats)\n\n\nparameters(lm1_freq)\n\nParameter   | Coefficient |   SE |         95% CI | t(28) |      p\n------------------------------------------------------------------\n(Intercept) |       34.18 | 2.59 | [28.88, 39.49] | 13.19 | &lt; .001\nhp          |       -0.01 | 0.01 | [-0.04,  0.02] | -1.00 | 0.325 \ncyl         |       -1.23 | 0.80 | [-2.86,  0.41] | -1.54 | 0.135 \ndisp        |       -0.02 | 0.01 | [-0.04,  0.00] | -1.81 | 0.081 \n\n\n\nparameters(lm1_bayes)\n\nParameter   | Median |         95% CI |     pd |  Rhat |     ESS |                   Prior\n------------------------------------------------------------------------------------------\n(Intercept) |  34.12 | [28.88, 39.57] |   100% | 1.000 | 2289.00 | Normal (20.09 +- 15.07)\nhp          |  -0.01 | [-0.05,  0.02] | 83.30% | 1.000 | 2691.00 |   Normal (0.00 +- 0.22)\ncyl         |  -1.21 | [-2.85,  0.37] | 93.35% | 1.001 | 1836.00 |   Normal (0.00 +- 8.44)\ndisp        |  -0.02 | [-0.04,  0.00] | 95.67% | 1.000 | 2045.00 |   Normal (0.00 +- 0.12)\n\n\nThe coefficient is estimated as about -0.01\n\nCategories:\n\nregression\nen\nbayes\nfrequentist\nqm1\nstats-nutshell"
  },
  {
    "objectID": "posts/tidymodels-penguins05/tidymodels-penguins05.html",
    "href": "posts/tidymodels-penguins05/tidymodels-penguins05.html",
    "title": "tidymodels-penguins05",
    "section": "",
    "text": "Aufgabe\nBerechnen Sie ein kNN-Modell mit tidymodels und zwar anhand des penguins Datensatzes.\nModellgleichung: body_mass_g ~ bill_length_mm, data = d_train.\nGesucht ist R-Quadrat als Maß für die Modellgüte im TEST-Sample.\nHinweise:\n\nFixieren Sie die Zufallszahlen auf den Startwert 42.\nNutzen Sie eine v=5,r=2 CV.\nTunen Sie \\(K\\), setzen Sie den Tuning-Wertebereich auf 1 bis 5.\nEntfernen Sie fehlende Werte in den Variablen.\nVerzichten Sie auf weitere Schritte der Vorverarbeitung.\n\n         \n\n\nLösung\nSetup:\n\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(tictoc)  # Rechenzeit messen, optional\n# data(penguins, package = \"palmerpenguins\")\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/modeldata/penguins.csv\"\nd &lt;- read_csv(d_path)\n\nDatensatz auf NAs prüfen:\n\nd2 &lt;-\n  d %&gt;% \n  drop_na() \n\nDatensatz aufteilen:\n\nset.seed(42)\nd_split &lt;- initial_split(d2)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\nWorkflow:\n\nrec1 &lt;-\n  recipe(body_mass_g ~ bill_length_mm, data = d_train) %&gt;% \n  step_naomit(all_numeric())\n\nknn_model &lt;-\n  nearest_neighbor(\n    mode = \"regression\",\n    neighbors = tune()\n  ) \n\nwflow &lt;-\n  workflow() %&gt;%\n  add_recipe(rec1) %&gt;%\n  add_model(knn_model)\n\nwflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_naomit()\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-Nearest Neighbor Model Specification (regression)\n\nMain Arguments:\n  neighbors = tune()\n\nComputational engine: kknn \n\n\nBacken:\n\nd_baked &lt;- prep(rec1) %&gt;% bake(new_data = NULL)\nd_baked %&gt;% head()\n\n# A tibble: 6 × 2\n  bill_length_mm body_mass_g\n           &lt;dbl&gt;       &lt;dbl&gt;\n1           34.5        2900\n2           52.2        3450\n3           45.4        4800\n4           42.1        4000\n5           50          5350\n6           41.5        4000\n\n\nAuf NA prüfen:\n\nsum(is.na(d_baked))\n\n[1] 0\n\n\nCV:\n\nset.seed(42)\nfolds &lt;- vfold_cv(d_train, v = 5, repeats = 2)\nfolds\n\n#  5-fold cross-validation repeated 2 times \n# A tibble: 10 × 3\n   splits           id      id2  \n   &lt;list&gt;           &lt;chr&gt;   &lt;chr&gt;\n 1 &lt;split [199/50]&gt; Repeat1 Fold1\n 2 &lt;split [199/50]&gt; Repeat1 Fold2\n 3 &lt;split [199/50]&gt; Repeat1 Fold3\n 4 &lt;split [199/50]&gt; Repeat1 Fold4\n 5 &lt;split [200/49]&gt; Repeat1 Fold5\n 6 &lt;split [199/50]&gt; Repeat2 Fold1\n 7 &lt;split [199/50]&gt; Repeat2 Fold2\n 8 &lt;split [199/50]&gt; Repeat2 Fold3\n 9 &lt;split [199/50]&gt; Repeat2 Fold4\n10 &lt;split [200/49]&gt; Repeat2 Fold5\n\n\nTunen:\n\nd_resamples &lt;-\n  tune_grid(\n    wflow,\n    resamples = folds,\n    control = control_grid(save_workflow = TRUE),\n    grid = grid_regular(\n      neighbors(range = c(1, 5))\n    )\n  )\n\nd_resamples\n\n# Tuning results\n# 5-fold cross-validation repeated 2 times \n# A tibble: 10 × 5\n   splits           id      id2   .metrics         .notes          \n   &lt;list&gt;           &lt;chr&gt;   &lt;chr&gt; &lt;list&gt;           &lt;list&gt;          \n 1 &lt;split [199/50]&gt; Repeat1 Fold1 &lt;tibble [6 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [199/50]&gt; Repeat1 Fold2 &lt;tibble [6 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [199/50]&gt; Repeat1 Fold3 &lt;tibble [6 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [199/50]&gt; Repeat1 Fold4 &lt;tibble [6 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [200/49]&gt; Repeat1 Fold5 &lt;tibble [6 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [199/50]&gt; Repeat2 Fold1 &lt;tibble [6 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [199/50]&gt; Repeat2 Fold2 &lt;tibble [6 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [199/50]&gt; Repeat2 Fold3 &lt;tibble [6 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [199/50]&gt; Repeat2 Fold4 &lt;tibble [6 × 5]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [200/49]&gt; Repeat2 Fold5 &lt;tibble [6 × 5]&gt; &lt;tibble [0 × 3]&gt;\n\n\nBester Kandidat:\n\nshow_best(d_resamples)\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\n\n# A tibble: 3 × 7\n  neighbors .metric .estimator  mean     n std_err .config             \n      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1         5 rmse    standard    733.    10    19.3 Preprocessor1_Model3\n2         3 rmse    standard    777.    10    23.8 Preprocessor1_Model2\n3         1 rmse    standard    945.    10    28.0 Preprocessor1_Model1\n\n\n\nfitbest &lt;- fit_best(d_resamples)\nfitbest\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_naomit()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nkknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(5L,     data, 5))\n\nType of response variable: continuous\nminimal mean absolute error: 497.0257\nMinimal mean squared error: 407926.4\nBest kernel: optimal\nBest k: 5\n\n\nLast Fit:\n\nfit_last &lt;- last_fit(fitbest, d_split)\nfit_last\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits           id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;           &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [249/84]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n\nModellgüte im Test-Sample:\n\nfit_last %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard     654.    Preprocessor1_Model1\n2 rsq     standard       0.294 Preprocessor1_Model1\n\n\nR-Quadrat:\n\nsol &lt;- collect_metrics(fit_last)[[\".estimate\"]][2]\nsol\n\n[1] 0.2935091\n\n\n\nCategories:\n\nds1\ntidymodels\nprediction\nyacsda\nstatlearning\nnum"
  },
  {
    "objectID": "posts/Urne2/Urne2.html",
    "href": "posts/Urne2/Urne2.html",
    "title": "Urne2",
    "section": "",
    "text": "Aufgabe\nIn einer Urne befinden sich fünf Kugeln, von denen 4 rot sind und 1 weiß.\nAufgabe: Wie groß ist die Wahrscheinlichkeit, dass bei 2 Ziehungen mit Zurücklegen (ZmZ) genau 2 rote Kugeln gezogen werden?\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n         \n\n\nLösung\nSei \\(R1\\) “rote Kugel im 1. Zug gezogen”.\nSei \\(R2\\) “rote Kugel im 2. Zug gezogen”.\nGesucht ist die gemeinsame Wahrscheinlichkeit für R1 und R2: \\(Pr(R1 \\cap R2)\\), die Wahrscheinlichkeit also, dass beide Ereignisse (R1 und R2) eintreten: \\(Pr(R1 \\cap R2)\\).\nFür R1 gilt: \\(Pr(R1) = 4/5\\).\nFür R2 gilt: \\(Pr(R2|R1) = 4/5\\).\nMan beachte, dass R1 und R2 unabhängig sind: \\(R1 \\perp \\!\\!\\! \\perp R2\\), d.h. sie sind nicht abhängig (voneinander).\n\nPr_R1 &lt;- 4/5\nPr_R2_geg_R1 &lt;- 4/5\nPr_R1_R2 &lt;- Pr_R1 * Pr_R2_geg_R1\nPr_R1_R2\n\n[1] 0.64\n\n\nDie Lösung lautet 0.64.\n\nCategories:\n\nR\nprobability\nnum"
  },
  {
    "objectID": "posts/kausal21/kausal21.html",
    "href": "posts/kausal21/kausal21.html",
    "title": "kausal21",
    "section": "",
    "text": "Gegeben sei der DAG g (s. u.). Der DAG verfügt über mehrere Variablen, die als Knoten im Graph dargestellt sind und mit \\(x_1, x_2, \\ldots x_n\\) bezeichnet sind.\nWelche minimale Variablenmenge muss kontrolliert werden, um den kausalen Effekt von der UV zur AV zu identifizieren?\nUV: x3.\nAV: x5.\nHinweise:\n\nMengen sind mittels geschweifter Klammern gekennzeichnet, z.B. {x8, x9} meint die Menge mit den zwei Elementen x8 und x9.\nDie leere Menge { } bedeutet, dass keine Variable kontrolliert werden muss, um den kausalen Effekt zu identifizieren.\nAlle Variablen werden als gemessen vorausgesetzt.\nEs ist möglich, dass es keine Lösung gibt, dass es also keine Adjustierungsmenge gibt, um den kausalen Effekt zu identifizieren. Wenn dies der Fall sein sollte, wählen Sie “keine Lösung”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n{ x3, x4 }\n{ x2, x4 }\n{ x5, x7 }\n{ x1, x7 }\n{ }"
  },
  {
    "objectID": "posts/kausal21/kausal21.html#answerlist",
    "href": "posts/kausal21/kausal21.html#answerlist",
    "title": "kausal21",
    "section": "",
    "text": "{ x3, x4 }\n{ x2, x4 }\n{ x5, x7 }\n{ x1, x7 }\n{ }"
  },
  {
    "objectID": "posts/kausal21/kausal21.html#answerlist-1",
    "href": "posts/kausal21/kausal21.html#answerlist-1",
    "title": "kausal21",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nFalsch\nRichtig\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/tidymodels-penguins02/tidymodels-penguins02.html",
    "href": "posts/tidymodels-penguins02/tidymodels-penguins02.html",
    "title": "tidymodels-penguins02",
    "section": "",
    "text": "Aufgabe\nBerechnen Sie ein kNN-Modell mit tidymodels und zwar anhand des penguins Datensatzes.\nModellgleichung: body_mass_g ~ bill_length_mm, data = d_train.\nGesucht ist R-Quadrat als Maß für die Modellgüte im TEST-Sample.\nHinweise:\n\nFixieren Sie die Zufallszahlen auf den Startwert 42.\nNutzen Sie eine v=5,r=1 CV.\nTunen Sie nicht.\nEntfernen Sie fehlende Werte in den Variablen.\nVerzichten Sie auf weitere Schritte der Vorverarbeitung.\n\n         \n\n\nLösung\nSetup:\n\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(tictoc)  # Rechenzeit messen, optional\n# data(penguins, package = \"palmerpenguins\")\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/modeldata/penguins.csv\"\nd &lt;- read_csv(d_path)\n\nDatensatz auf NAs prüfen:\n\nd2 &lt;-\n  d %&gt;% \n  drop_na() \n\nDatensatz aufteilen:\n\nset.seed(42)\nd_split &lt;- initial_split(d2)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\nWorkflow:\n\nrec1 &lt;-\n  recipe(body_mass_g ~ bill_length_mm, data = d_train) %&gt;% \n  step_naomit(all_numeric())\n\nknn_model &lt;-\n  nearest_neighbor(\n    mode = \"regression\"\n  ) \n\nwflow &lt;-\n  workflow() %&gt;%\n  add_recipe(rec1) %&gt;%\n  add_model(knn_model)\n\nwflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_naomit()\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-Nearest Neighbor Model Specification (regression)\n\nComputational engine: kknn \n\n\nBacken:\n\nd_baked &lt;- prep(rec1) %&gt;% bake(new_data = NULL)\nd_baked %&gt;% head()\n\n# A tibble: 6 × 2\n  bill_length_mm body_mass_g\n           &lt;dbl&gt;       &lt;dbl&gt;\n1           34.5        2900\n2           52.2        3450\n3           45.4        4800\n4           42.1        4000\n5           50          5350\n6           41.5        4000\n\n\nAuf NA prüfen:\n\nsum(is.na(d_baked))\n\n[1] 0\n\n\nCV:\n\nset.seed(42)\nfolds &lt;- vfold_cv(d_train, v = 5)\nfolds\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits           id   \n  &lt;list&gt;           &lt;chr&gt;\n1 &lt;split [199/50]&gt; Fold1\n2 &lt;split [199/50]&gt; Fold2\n3 &lt;split [199/50]&gt; Fold3\n4 &lt;split [199/50]&gt; Fold4\n5 &lt;split [200/49]&gt; Fold5\n\n\nResampling:\n\nd_resamples &lt;-\n  fit_resamples(\n    wflow,\n    resamples = folds\n  )\n\nd_resamples\n\n# Resampling results\n# 5-fold cross-validation \n# A tibble: 5 × 4\n  splits           id    .metrics         .notes          \n  &lt;list&gt;           &lt;chr&gt; &lt;list&gt;           &lt;list&gt;          \n1 &lt;split [199/50]&gt; Fold1 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n2 &lt;split [199/50]&gt; Fold2 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n3 &lt;split [199/50]&gt; Fold3 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n4 &lt;split [199/50]&gt; Fold4 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n5 &lt;split [200/49]&gt; Fold5 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n\n\nLast Fit:\n\nfit_last &lt;- last_fit(wflow, d_split)\n\nModellgüte im Test-Sample:\n\nfit_last %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard     654.    Preprocessor1_Model1\n2 rsq     standard       0.294 Preprocessor1_Model1\n\n\nR-Quadrat:\n\nsol &lt;- collect_metrics(fit_last)[[\".estimate\"]][2]\nsol\n\n[1] 0.2935091\n\n\n\nCategories:\n\nds1\ntidymodels\nprediction\nyacsda\nstatlearning\nnum"
  },
  {
    "objectID": "posts/corona-blutgruppe/corona-blutgruppe.html",
    "href": "posts/corona-blutgruppe/corona-blutgruppe.html",
    "title": "corona-blutgruppe",
    "section": "",
    "text": "Aufgabe\nBetrachten wir das Ereignis “Schwerer Coronaverlauf” (\\(S\\)); ferner betrachten wir das Ereignis “Blutgruppe ist A” (\\(A\\)) und das Gegenereignis von \\(A\\): “Blutgruppe ist nicht A”. Ein Gegenereignis wird auch als Komplementärereignis oder Komplement (complement) mit dem Operator \\(\\bar{A}\\) oder \\(A^C\\) bezeichnet.\nSei \\(Pr(S|A) = 0.01\\) und sei \\(Pr(S|A^C) = 0.01\\).\nWas kann man auf dieser Basis zur Abhängigkeit der Ereignisse \\(S\\) und \\(A\\) sagen?\nGeben Sie ein Adjektiv an, dass diesen Sachverhalt kennzeichnet!\n         \n\n\nLösung\nDie Lösung lautet: unabhängig.\n\\(S\\) und \\(A\\) sind unabhängig: Offenbar ist die Wahrscheinlichkeit eines schweren Verlaufs gleich groß unabhängig davon, ob die Blutgruppe A ist oder nicht. In diesem Fall spricht man von stochastischer Unabhängigkeit.\n\\(Pr(S|A) = Pr(S|A^C) = Pr(S)\\)\nHinweis: \\(A^C\\) meint das Komplement von \\(A\\), auch als \\(\\neg A\\) bezeichnet.\n\nCategories:\n\nprobability\ndependent\nstring"
  },
  {
    "objectID": "posts/mariokart-mean1/mariokart-mean1.html",
    "href": "posts/mariokart-mean1/mariokart-mean1.html",
    "title": "mariokart-mean1",
    "section": "",
    "text": "Aufgabe\nImportieren Sie den Datensatz mariokart in R. Berechnen Sie den mittleren Verkaufspreis (total_pr)!\nHinweise:\n\nRunden Sie auf 1 Dezimalstelle.\n\n         \n\n\nLösung\nPakete starten:\n\nlibrary(tidyverse)\nlibrary(easystats)\n\nDaten importieren:\n\nd_url &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\"\nd &lt;- data_read(d_url)\n\n\nd  %&gt;% \n  summarise(pr_mean = mean(total_pr))\n\n   pr_mean\n1 49.88049\n\n\nLösung: 49.88.\n\nCategories:\n\ndatawrangling\ndplyr\neda\nnum"
  },
  {
    "objectID": "posts/ReThink4e3/ReThink4e3.html",
    "href": "posts/ReThink4e3/ReThink4e3.html",
    "title": "ReThink4e3",
    "section": "",
    "text": "Exercise\nGegeben dem folgenden Modell, schreiben Sie die passende Form des Bayes-Theorem auf.\nLikelihood: \\(h_i \\sim \\mathcal{N}(\\mu, \\sigma)\\)\nPrior für \\(\\mu\\): \\(\\mu \\sim \\mathcal{N}(178, 20)\\)\nPrior für \\(\\sigma\\): \\(\\sigma \\sim \\mathcal{U}(0, 50)\\)\nQuelle: McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Aufl.). Taylor and Francis, CRC Press.\n         \n\n\nSolution\nDie allgemeine Form des Bayes-Theorem hatten wir so kennen gelernt:\n\\[Pr(H|D) = \\frac{Pr(D|H)\\cdot Pr(H)}{Pr(D)}\\]\n\\(Pr(\\mu, \\sigma|h)\\) gibt die Posteriori-Wahrscheinlichkeit für ein bestimmte Hypothese \\(H\\) an, z.B. für die Hypothese \\(\\mu=0\\).\n\\(Pr(D|H)\\) ist der Likelihood unserer Daten \\(D\\) gegeben der gerade untersuchten Hypothese \\(H\\).\n\\(Pr(H)\\) ist die Apriori-Wahrscheinlichkeit (das “Apriori-Gewicht”) der gerade untersuchten Hypothese.\nDer Zähler gibt die unstandardisierte Posteriori-Wahrscheinlichkeit der gerade untersuchten Hypothese an.\nDer Nenner ist nur ein Normalisierungsfaktor, der dafür sorgt, dass der ganze Bruch die standardisierte Posteriori-Wahrscheinlichkeit angibt.\nIn diesem konkreten Fall untersuchen wir Hypothesen zu einem “Parameter-Pärchen”, \\(\\mu\\sigma\\). Wir fragen also, wie wahrscheinlich es ist, einen gewissen Mittelwert \\(\\mu\\) und (gleichzeitig) eine gewisse Streuung \\(\\sigma\\) aufzufinden.\nZum Beispiel könnten wir fragen: “Wie wahrscheinlich ist es, dass \\(\\mu=194\\) und \\(\\sigma=12\\)?”. Bayes’ Theorem gibt uns die Wahrscheinlichkeit für diese Hypothese.\nZur Erinnerung, Bayes’ Theorem:\n\\[Pr(\\mu \\cap \\sigma|D) = \\frac{Pr(D|\\mu \\cap \\sigma)\\cdot Pr(\\mu) \\cdot Pr(\\sigma)}{Pr(H)}\\]\nHier ist zu beachten, dass die Apriori-Wahrscheinlichkeit auf zwei Termen besteht, \\(Pr(\\mu)\\) und \\(Pr(\\sigma)\\). Sind diese unabhängig, so kann man ihre Wahrscheinlichkeiten multiplizieren, um die gemeinsame Wahrscheinlichkeit zu erhalten, also die Wahrscheinlichkeit für ein bestimmten “Mu-Sigma-Pärchen”, etwa \\(\\mu=194,\\sigma=12\\).\n\nCategories:\n\nbayes\nprobability"
  },
  {
    "objectID": "posts/Priori-Streuung/Priori-Streuung.html",
    "href": "posts/Priori-Streuung/Priori-Streuung.html",
    "title": "Priori-Streuung",
    "section": "",
    "text": "Welche Verteilung ist (am besten) geeignet, um Streuung (\\(\\sigma\\)) zu modellieren?\n\n\n\nN(0,1)\nN(1,1)\nExp(1)\nExp(0)\nExp(-1)"
  },
  {
    "objectID": "posts/Priori-Streuung/Priori-Streuung.html#answerlist",
    "href": "posts/Priori-Streuung/Priori-Streuung.html#answerlist",
    "title": "Priori-Streuung",
    "section": "",
    "text": "N(0,1)\nN(1,1)\nExp(1)\nExp(0)\nExp(-1)"
  },
  {
    "objectID": "posts/Priori-Streuung/Priori-Streuung.html#answerlist-1",
    "href": "posts/Priori-Streuung/Priori-Streuung.html#answerlist-1",
    "title": "Priori-Streuung",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nWahr\nFalsch\nFalsch\n\nDa Streuung \\(\\sigma\\) per Definition positiv ist, kommt eine Verteilung, die negative Werte erlaubt, nicht in Frage. Die Normalverteilung scheidet also aus.\nDie Rate der Exponentialverteilung regelt gleichzeitig Streuung und Mittelwert. Allerdings hat \\(Exp(0)\\) eine unendliche Streuung, was nicht wünschenswert ist. Eine negative Rate ist für die Exponentialverteilung nicht definiert.\nNormalverteilungen:\n\n\n\n\n\\(N(0,1)\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(N(1,1)\\):\n\n\n\n\n\n\n\n\n\nExponentialverteilungen:\n\n\n\n\\(Exp(1)\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\\(Exp(0)\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\\(Exp(-1)\\):\n\n\nWarning in fun(x_trans, rate = -1): NaNs produced\n\n\nWarning: Removed 101 rows containing missing values (`geom_function()`).\n\n\n\n\n\n\n\n\n\n\nCategories:\n\nprobability\nsimulation\ndistributions\nbayes"
  },
  {
    "objectID": "posts/repro1-sessioninfo/repro1-sessioninfo.html",
    "href": "posts/repro1-sessioninfo/repro1-sessioninfo.html",
    "title": "repro1-sessioninfo",
    "section": "",
    "text": "Aufgabe\nSie analysieren fröhlich, bestens gelaunt geradezu, in Ruhe einige Daten. Läuft. Plötzlich: Oh nein! Eine Fehlermeldung! Das darf nicht wahr sein!\nSicherlich ein Bug in R oder in RStudio oder in einem R-Paket…\nSie fragen jemanden um Hilfe und jemand antwortet mit der Frage, ob Sie denn die aktuelle Version von R/RStudio/einem R-Paket hätten.\nTja, gute Frage … Woher weiß man das eigentlich?\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n         \n\n\nLösung\nMit dem Befehl sessionInfo bekommen Sie einen Überblick über die Versionen der aktuell gestarteten R-Pakete sowie von R. Die Version von RStudio ist zumeist nicht wichtig, da RStudio keinen R-Code ausführt; das macht nur R.\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.2.1    fastmap_1.1.1     cli_3.6.2        \n [5] tools_4.2.1       htmltools_0.5.7   rstudioapi_0.15.0 yaml_2.3.8       \n [9] rmarkdown_2.26    knitr_1.45        jsonlite_1.8.8    xfun_0.41        \n[13] digest_0.6.33     rlang_1.1.3       evaluate_0.23    \n\n\n\nCategories:\n\nR\nrepro\nstring"
  },
  {
    "objectID": "posts/tmdb08/tmdb08.html",
    "href": "posts/tmdb08/tmdb08.html",
    "title": "tmdb08",
    "section": "",
    "text": "Wir bearbeiten hier die Fallstudie TMDB Box Office Prediction - Can you predict a movie’s worldwide box office revenue?, ein Kaggle-Prognosewettbewerb.\nZiel ist es, genaue Vorhersagen zu machen, in diesem Fall für Filme.\nDie Daten können Sie von der Kaggle-Projektseite beziehen oder so:\n\nd_train_path &lt;- \"https://raw.githubusercontent.com/sebastiansauer/Lehre/main/data/tmdb-box-office-prediction/train.csv\"\nd_test_path &lt;- \"https://raw.githubusercontent.com/sebastiansauer/Lehre/main/data/tmdb-box-office-prediction/test.csv\""
  },
  {
    "objectID": "posts/tmdb08/tmdb08.html#train-set-verschlanken",
    "href": "posts/tmdb08/tmdb08.html#train-set-verschlanken",
    "title": "tmdb08",
    "section": "Train-Set verschlanken",
    "text": "Train-Set verschlanken\n\nd_train &lt;-\n  d_train_raw %&gt;% \n  select(id, popularity, runtime, revenue, budget)"
  },
  {
    "objectID": "posts/tmdb08/tmdb08.html#test-set-verschlanken",
    "href": "posts/tmdb08/tmdb08.html#test-set-verschlanken",
    "title": "tmdb08",
    "section": "Test-Set verschlanken",
    "text": "Test-Set verschlanken\n\nd_test &lt;-\n  d_test_raw %&gt;% \n  select(id,popularity, runtime, budget)"
  },
  {
    "objectID": "posts/tmdb08/tmdb08.html#rezept-definieren",
    "href": "posts/tmdb08/tmdb08.html#rezept-definieren",
    "title": "tmdb08",
    "section": "Rezept definieren",
    "text": "Rezept definieren\n\nrec2 &lt;-\n  recipe(revenue ~ ., data = d_train) %&gt;% \n  step_mutate(budget = ifelse(budget == 0, 1, budget)) %&gt;%  # log mag keine 0\n  step_log(budget) %&gt;% \n  step_impute_knn(all_predictors()) %&gt;% \n  step_dummy(all_nominal_predictors())  %&gt;% \n  update_role(id, new_role = \"id\")\n\nrec2"
  },
  {
    "objectID": "posts/tmdb08/tmdb08.html#lm-regularisiert",
    "href": "posts/tmdb08/tmdb08.html#lm-regularisiert",
    "title": "tmdb08",
    "section": "LM regularisiert",
    "text": "LM regularisiert\nMit mixture = 1 definieren wir ein Lasso.\n\nmod_lm &lt;-\n  linear_reg(penalty = tune(), mixture = 1) %&gt;% \n  set_engine(\"glmnet\")\n\nCheck:\n\nmod_lm\n\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet"
  },
  {
    "objectID": "posts/tmdb08/tmdb08.html#finalisieren-1",
    "href": "posts/tmdb08/tmdb08.html#finalisieren-1",
    "title": "tmdb08",
    "section": "Finalisieren",
    "text": "Finalisieren\nFinalisieren bedeutet:\n\nBesten Workflow identifizieren (zur Erinnerung: Workflow = Rezept + Modell)\nDen besten Workflow mit den optimalen Modell-Parametern ausstatten\nDamit dann den ganzen Train-Datensatz fitten\nAuf dieser Basis das Test-Sample vorhersagen\n\n\nbest_wf2 &lt;- \nall_workflows2 %&gt;% \n  extract_workflow(\"rec1_lm1\")\n\nbest_wf2\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_mutate()\n• step_log()\n• step_impute_knn()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n\n\nbest_wf_finalized2 &lt;- \n  best_wf2 %&gt;% \n  finalize_workflow(best_model_params2)\n\nbest_wf_finalized2\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_mutate()\n• step_log()\n• step_impute_knn()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 1.12138579835732e-10\n  mixture = 1\n\nComputational engine: glmnet"
  },
  {
    "objectID": "posts/tmdb08/tmdb08.html#final-fit",
    "href": "posts/tmdb08/tmdb08.html#final-fit",
    "title": "tmdb08",
    "section": "Final Fit",
    "text": "Final Fit\n\nfit_final2 &lt;-\n  best_wf_finalized2 %&gt;% \n  fit(d_train)\n\nfit_final2\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_mutate()\n• step_log()\n• step_impute_knn()\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~1) \n\n   Df  %Dev   Lambda\n1   0  0.00 63460000\n2   1  3.62 57820000\n3   1  6.62 52680000\n4   1  9.11 48000000\n5   1 11.18 43740000\n6   1 12.90 39850000\n7   2 15.24 36310000\n8   2 17.19 33090000\n9   2 18.81 30150000\n10  2 20.16 27470000\n11  2 21.28 25030000\n12  2 22.21 22800000\n13  3 23.10 20780000\n14  3 23.95 18930000\n15  3 24.66 17250000\n16  3 25.25 15720000\n17  3 25.74 14320000\n18  3 26.15 13050000\n19  3 26.49 11890000\n20  3 26.77 10830000\n21  3 27.00  9872000\n22  3 27.20  8995000\n23  3 27.36  8196000\n24  3 27.49  7467000\n25  3 27.60  6804000\n26  3 27.69  6200000\n27  3 27.77  5649000\n28  3 27.83  5147000\n29  3 27.88  4690000\n30  3 27.93  4273000\n31  3 27.96  3894000\n32  3 27.99  3548000\n33  3 28.02  3232000\n34  3 28.04  2945000\n35  3 28.06  2684000\n36  3 28.07  2445000\n37  3 28.08  2228000\n38  3 28.09  2030000\n39  3 28.10  1850000\n40  3 28.11  1685000\n41  3 28.11  1536000\n42  3 28.12  1399000\n43  3 28.12  1275000\n44  3 28.13  1162000\n45  3 28.13  1058000\n46  3 28.13   964500\n\n...\nand 12 more lines.\n\n\n\npreds &lt;- \nfit_final2 %&gt;% \n  predict(new_data = d_test)\n\nhead(preds)\n\n# A tibble: 6 × 1\n       .pred\n       &lt;dbl&gt;\n1 -14840891.\n2  10804710.\n3  11698900.\n4  99190531.\n5  41798496.\n6  29974421."
  },
  {
    "objectID": "posts/tmdb08/tmdb08.html#submission-df",
    "href": "posts/tmdb08/tmdb08.html#submission-df",
    "title": "tmdb08",
    "section": "Submission df",
    "text": "Submission df\nWir brauchen die ID-Spalte und die Vorhersagen für die Einreichung:\n\nsubmission_df &lt;-\n  d_test %&gt;% \n  select(id) %&gt;% \n  bind_cols(preds) %&gt;% \n  rename(revenue = .pred)\n\nhead(submission_df)\n\n# A tibble: 6 × 2\n     id    revenue\n  &lt;dbl&gt;      &lt;dbl&gt;\n1  3001 -14840891.\n2  3002  10804710.\n3  3003  11698900.\n4  3004  99190531.\n5  3005  41798496.\n6  3006  29974421.\n\n\nAbspeichern und einreichen:\n\nwrite_csv(submission_df, file = \"submission_regul_lm.csv\")\n\nLeider ein schlechter Score: 5.77945.\n\nCategories:\n\nds1\ntidymodels\nstatlearning\ntmdb\nrandom-forest\nnum"
  },
  {
    "objectID": "posts/tmdb01/tmdb01.html",
    "href": "posts/tmdb01/tmdb01.html",
    "title": "tmdb01",
    "section": "",
    "text": "Aufgabe\nMelden Sie sich an für die Kaggle Competition TMDB Box Office Prediction - Can you predict a movie’s worldwide box office revenue?.\nSie benötigen dazu ein Konto; es ist auch möglich, sich mit seinem Google-Konto anzumelden.\nBei diesem Prognosewettbewerb geht es darum, vorherzusagen, wieviel Umsatz wohl einige Filme machen werden. Als Prädiktoren stehen einige Infos wie Budget, Genre, Titel etc. zur Verfügung. Eine klassische “predictive Competition” also :-) Allerdings können immer ein paar Schwierigkeiten auftreten ;-)\nAufgabe\nErstellen Sie ein Random-Forest-Modell mit Tidymodels! Reichen Sie es bei Kaggle ein un berichten Sie den Score!\nHinweise\n\n\nVerzichten Sie auf Vorverarbeitung.\nTunen Sie die typischen Parameter.\nBegrenzen Sie sich auf folgende Prädiktoren.\n\n\npreds_chosen &lt;- \n  c(\"id\", \"budget\", \"popularity\", \"runtime\")\n\n\nAusnahme: Log-transformieren Sie budget.\nTunen Sie die typischen Parameter.\nReichen Sie das Modell ein und berichten Sie Ihren Score.\n\n\npreds_chosen &lt;- \n  c(\"id\", \"budget\", \"popularity\", \"runtime\", \"status\", \"revenue\")\n\n         \n\n\nLösung\n\nPakete starten\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(tictoc)\nlibrary(doParallel)\n\n\n\nDaten importieren\n\nd_train_path &lt;- \"https://raw.githubusercontent.com/sebastiansauer/Lehre/main/data/tmdb-box-office-prediction/train.csv\"\nd_test_path &lt;- \"https://raw.githubusercontent.com/sebastiansauer/Lehre/main/data/tmdb-box-office-prediction/test.csv\"\n\nd_train &lt;- read_csv(d_train_path)\nd_test &lt;- read_csv(d_test_path)\n\nWerfen wir einen Blick in die Daten:\n\nglimpse(d_train)\n\nRows: 3,000\nColumns: 23\n$ id                    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ belongs_to_collection &lt;chr&gt; \"[{'id': 313576, 'name': 'Hot Tub Time Machine C…\n$ budget                &lt;dbl&gt; 1.40e+07, 4.00e+07, 3.30e+06, 1.20e+06, 0.00e+00…\n$ genres                &lt;chr&gt; \"[{'id': 35, 'name': 'Comedy'}]\", \"[{'id': 35, '…\n$ homepage              &lt;chr&gt; NA, NA, \"http://sonyclassics.com/whiplash/\", \"ht…\n$ imdb_id               &lt;chr&gt; \"tt2637294\", \"tt0368933\", \"tt2582802\", \"tt182148…\n$ original_language     &lt;chr&gt; \"en\", \"en\", \"en\", \"hi\", \"ko\", \"en\", \"en\", \"en\", …\n$ original_title        &lt;chr&gt; \"Hot Tub Time Machine 2\", \"The Princess Diaries …\n$ overview              &lt;chr&gt; \"When Lou, who has become the \\\"father of the In…\n$ popularity            &lt;dbl&gt; 6.575393, 8.248895, 64.299990, 3.174936, 1.14807…\n$ poster_path           &lt;chr&gt; \"/tQtWuwvMf0hCc2QR2tkolwl7c3c.jpg\", \"/w9Z7A0GHEh…\n$ production_companies  &lt;chr&gt; \"[{'name': 'Paramount Pictures', 'id': 4}, {'nam…\n$ production_countries  &lt;chr&gt; \"[{'iso_3166_1': 'US', 'name': 'United States of…\n$ release_date          &lt;chr&gt; \"2/20/15\", \"8/6/04\", \"10/10/14\", \"3/9/12\", \"2/5/…\n$ runtime               &lt;dbl&gt; 93, 113, 105, 122, 118, 83, 92, 84, 100, 91, 119…\n$ spoken_languages      &lt;chr&gt; \"[{'iso_639_1': 'en', 'name': 'English'}]\", \"[{'…\n$ status                &lt;chr&gt; \"Released\", \"Released\", \"Released\", \"Released\", …\n$ tagline               &lt;chr&gt; \"The Laws of Space and Time are About to be Viol…\n$ title                 &lt;chr&gt; \"Hot Tub Time Machine 2\", \"The Princess Diaries …\n$ Keywords              &lt;chr&gt; \"[{'id': 4379, 'name': 'time travel'}, {'id': 96…\n$ cast                  &lt;chr&gt; \"[{'cast_id': 4, 'character': 'Lou', 'credit_id'…\n$ crew                  &lt;chr&gt; \"[{'credit_id': '59ac067c92514107af02c8c8', 'dep…\n$ revenue               &lt;dbl&gt; 12314651, 95149435, 13092000, 16000000, 3923970,…\n\nglimpse(d_test)\n\nRows: 4,398\nColumns: 22\n$ id                    &lt;dbl&gt; 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, …\n$ belongs_to_collection &lt;chr&gt; \"[{'id': 34055, 'name': 'Pokémon Collection', 'p…\n$ budget                &lt;dbl&gt; 0.00e+00, 8.80e+04, 0.00e+00, 6.80e+06, 2.00e+06…\n$ genres                &lt;chr&gt; \"[{'id': 12, 'name': 'Adventure'}, {'id': 16, 'n…\n$ homepage              &lt;chr&gt; \"http://www.pokemon.com/us/movies/movie-pokemon-…\n$ imdb_id               &lt;chr&gt; \"tt1226251\", \"tt0051380\", \"tt0118556\", \"tt125595…\n$ original_language     &lt;chr&gt; \"ja\", \"en\", \"en\", \"fr\", \"en\", \"en\", \"de\", \"en\", …\n$ original_title        &lt;chr&gt; \"ディアルガVSパルキアVSダークライ\", \"Attack of t…\n$ overview              &lt;chr&gt; \"Ash and friends (this time accompanied by newco…\n$ popularity            &lt;dbl&gt; 3.851534, 3.559789, 8.085194, 8.596012, 3.217680…\n$ poster_path           &lt;chr&gt; \"/tnftmLMemPLduW6MRyZE0ZUD19z.jpg\", \"/9MgBNBqlH1…\n$ production_companies  &lt;chr&gt; NA, \"[{'name': 'Woolner Brothers Pictures Inc.',…\n$ production_countries  &lt;chr&gt; \"[{'iso_3166_1': 'JP', 'name': 'Japan'}, {'iso_3…\n$ release_date          &lt;chr&gt; \"7/14/07\", \"5/19/58\", \"5/23/97\", \"9/4/10\", \"2/11…\n$ runtime               &lt;dbl&gt; 90, 65, 100, 130, 92, 121, 119, 77, 120, 92, 88,…\n$ spoken_languages      &lt;chr&gt; \"[{'iso_639_1': 'en', 'name': 'English'}, {'iso_…\n$ status                &lt;chr&gt; \"Released\", \"Released\", \"Released\", \"Released\", …\n$ tagline               &lt;chr&gt; \"Somewhere Between Time & Space... A Legend Is B…\n$ title                 &lt;chr&gt; \"Pokémon: The Rise of Darkrai\", \"Attack of the 5…\n$ Keywords              &lt;chr&gt; \"[{'id': 11451, 'name': 'pok√©mon'}, {'id': 1155…\n$ cast                  &lt;chr&gt; \"[{'cast_id': 3, 'character': 'Tonio', 'credit_i…\n$ crew                  &lt;chr&gt; \"[{'credit_id': '52fe44e7c3a368484e03d683', 'dep…\n\n\npreds_chosen sind alle Prädiktoren im Datensatz, oder nicht? Das prüfen wir mal kurz:\n\npreds_chosen %in% names(d_train) %&gt;% \n  all()\n\n[1] TRUE\n\n\nJa, alle Elemente von preds_chosen sind Prädiktoren im (Train-)Datensatz.\n\nCV\nNur um Zeit zu sparen, setzen wir die Anzahl der Folds auf \\(v=4\\). Besser wäre z.B. \\(v=10\\).\n\ncv_scheme &lt;- vfold_cv(d_train, v = 4)\n\n\n\n\nRezept 1\n\nrec1 &lt;- \n  recipe(revenue ~ budget + popularity + runtime, data = d_train) %&gt;% \n  step_impute_bag(all_predictors()) %&gt;% \n  step_naomit(all_predictors()) \n\nMan beachte, dass noch 21 Prädiktoren angezeigt werden, da das Rezept noch nicht auf den Datensatz angewandt (“gebacken”) wurde.\n\ntidy(rec1)\n\n# A tibble: 2 × 6\n  number operation type       trained skip  id              \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;      &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;           \n1      1 step      impute_bag FALSE   FALSE impute_bag_3h7s4\n2      2 step      naomit     FALSE   TRUE  naomit_5oazu    \n\n\nRezept checken:\n\nprep(rec1)\n\n\nd_train_baked &lt;-\n  rec1 %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL)\n\nglimpse(d_train_baked)\n\nRows: 3,000\nColumns: 4\n$ budget     &lt;dbl&gt; 1.40e+07, 4.00e+07, 3.30e+06, 1.20e+06, 0.00e+00, 8.00e+06,…\n$ popularity &lt;dbl&gt; 6.575393, 8.248895, 64.299990, 3.174936, 1.148070, 0.743274…\n$ runtime    &lt;dbl&gt; 93, 113, 105, 122, 118, 83, 92, 84, 100, 91, 119, 98, 122, …\n$ revenue    &lt;dbl&gt; 12314651, 95149435, 13092000, 16000000, 3923970, 3261638, 8…\n\n\nFehlende Werte noch übrig?\n\nlibrary(easystats)\ndescribe_distribution(d_train_baked) %&gt;% \n  select(Variable, n_Missing)\n\nVariable   | n_Missing\n----------------------\nbudget     |         0\npopularity |         0\nruntime    |         0\nrevenue    |         0\n\n\n\n\nModell 1: RF\n\nmodel1 &lt;- rand_forest(mtry = tune(),\n                        trees = tune(),\n                        min_n = tune()) %&gt;% \n            set_engine('ranger') %&gt;% \n            set_mode('regression')\n\n\n\nWorkflow 1\n\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(model1) %&gt;% \n  add_recipe(rec1)\n\n\n\nModell fitten (und tunen)\nParallele Verarbeitung starten:\n\ncl &lt;- makePSOCKcluster(4)  # Create 4 clusters\nregisterDoParallel(cl)\n\n\ntic()\nrf_fit1 &lt;-\n  wf1 %&gt;% \n  tune_grid(resamples = cv_scheme)\ntoc()\n\n27.47 sec elapsed\n\n\nIrgendwelche Probleme oder Hinweise?\n\nrf_fit1[[\".notes\"]][1]\n\n[[1]]\n# A tibble: 0 × 3\n# ℹ 3 variables: location &lt;chr&gt;, type &lt;chr&gt;, note &lt;chr&gt;\n\n\nNein; bei mir nicht jedenfalls.\n\n\nBester Kandidat\n\nselect_best(rf_fit1)\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\n\n# A tibble: 1 × 4\n   mtry trees min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     1  1851    25 Preprocessor1_Model04\n\n\n\n\nWorkflow finalisieren\n\nwf_best &lt;-\n  wf1 %&gt;% \n  finalize_workflow(parameters = select_best(rf_fit1))\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\n\n\n\nFinal Fit\n\nfit1_final &lt;-\n  wf_best %&gt;% \n  fit(d_train)\n\nfit1_final\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_impute_bag()\n• step_naomit()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~1L,      x), num.trees = ~1851L, min.node.size = min_rows(~25L, x),      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1)) \n\nType:                             Regression \nNumber of trees:                  1851 \nSample size:                      3000 \nNumber of independent variables:  3 \nMtry:                             1 \nTarget node size:                 25 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       6.709961e+15 \nR squared (OOB):                  0.6452598 \n\n\n\npreds &lt;-\n  fit1_final %&gt;% \n  predict(d_test)\n\n\n\nSubmission df\n\nsubmission_df &lt;-\n  d_test %&gt;% \n  select(id) %&gt;% \n  bind_cols(preds) %&gt;% \n  rename(revenue = .pred)\n\nhead(submission_df)\n\n# A tibble: 6 × 2\n     id   revenue\n  &lt;dbl&gt;     &lt;dbl&gt;\n1  3001  4975575.\n2  3002  6349295.\n3  3003 15825986.\n4  3004 38573272.\n5  3005  4449452.\n6  3006 26780034.\n\n\nAbspeichern und einreichen:\n\n#write_csv(submission_df, file = \"submission.csv\")\n\n\n\nKaggle Score\nDiese Submission erzielte einen Score von Score: 2.76961 (RMSLE).\n\nsol &lt;-  2.76961\n\n\nCategories:\n\nds1\ntidymodels\nstatlearning\ntmdb\nrandom-forest\nnum"
  },
  {
    "objectID": "posts/tidymodels_workflowset01/tidymodels_workflowset01.html",
    "href": "posts/tidymodels_workflowset01/tidymodels_workflowset01.html",
    "title": "tidymodels_workflowset01",
    "section": "",
    "text": "Aufgabe\nBerechnen Sie\n         \n\n\nLösung\n\nCategories:\nnum"
  },
  {
    "objectID": "posts/purrr-map06/purrr-map06.html",
    "href": "posts/purrr-map06/purrr-map06.html",
    "title": "purrr-map06",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nExercise\nErstellen Sie eine Tabelle mit mit folgenden Spalten:\n\nID-Spalte: \\(1,2,..., 10\\)\nEine Spalte mit Namem ds (ds wie Plural von Datensatz), die als geschachtelt (nested) pro Element jeweils einen der folgenden Datensätze enthält: mtcars, iris, chickweight, ToothGrowth (alle in R enthalten)\n\nBerechnen Sie eine Spalte, die die Anzahl der Spalten von ds zählt!\n         \n\n\nSolution\nHier sind einige Datensätze, in einer Liste zusammengefasst:\n\nds &lt;- list(mtcars = mtcars, iris = iris, chickweight =  ChickWeight, toothgrowth = ToothGrowth)\n\nDaraus erstellen wir eine Tabelle mit Listenspalte für die Daten:\n\nd &lt;- \n  tibble(id = 1:length(ds),\n         ds = ds)\n\nJetzt führen wir die Funktion ncol aus, und zwar für jedes Element von ds. Wir brauchen also eine Art Schleife, das besorgt map für uns. Viele Funktionen in R sind “auomatisch verschleift” - das nennt man vektorisiert. Vektorisierte Funktionen werden für jedes Element eines Vektors ausgeführt.\nEin Beispiel für eine vektorisierte Funktion ist die Funktion +:\n\nx &lt;- c(1,2,3)\ny &lt;- c(10, 20, 30)\nx + y\n\n[1] 11 22 33\n\n\nMan könnte übrigens auch schreiben:\n\n`+`(x, y)\n\n[1] 11 22 33\n\n\nWas zeigt, dass + eine normale Funktion ist.\nZurück zur eigentlichen Aufgabe. Aber ncol ist eben nicht vektorisiert, darum müssen wir da noch eine Schleife dazu bauen, das macht map.\n\nd2 &lt;- \n  d %&gt;% \n  mutate(n_col = map(ds, ncol)) \n\nhead(d2)\n\n# A tibble: 4 × 3\n     id ds                   n_col       \n  &lt;int&gt; &lt;named list&gt;         &lt;named list&gt;\n1     1 &lt;df [32 × 11]&gt;       &lt;int [1]&gt;   \n2     2 &lt;df [150 × 5]&gt;       &lt;int [1]&gt;   \n3     3 &lt;nfnGrpdD [578 × 4]&gt; &lt;int [1]&gt;   \n4     4 &lt;df [60 × 3]&gt;        &lt;int [1]&gt;   \n\n\nEntnesten wir noch n_col:\n\nd2 %&gt;% \n  unnest(n_col)\n\n# A tibble: 4 × 3\n     id ds                   n_col\n  &lt;int&gt; &lt;named list&gt;         &lt;int&gt;\n1     1 &lt;df [32 × 11]&gt;          11\n2     2 &lt;df [150 × 5]&gt;           5\n3     3 &lt;nfnGrpdD [578 × 4]&gt;     4\n4     4 &lt;df [60 × 3]&gt;            3\n\n\nWir können auch gleich map anweisen, keine Liste, sondern eine Zahl (double, reelle ) Zahl zurückzuliefern, dann sparen wir uns das entschachteln:\n\nd %&gt;% \n  mutate(n_col = map_dbl(ds, ncol)) \n\n# A tibble: 4 × 3\n     id ds                   n_col\n  &lt;int&gt; &lt;named list&gt;         &lt;dbl&gt;\n1     1 &lt;df [32 × 11]&gt;          11\n2     2 &lt;df [150 × 5]&gt;           5\n3     3 &lt;nfnGrpdD [578 × 4]&gt;     4\n4     4 &lt;df [60 × 3]&gt;            3\n\n\n\nCategories:\n\nprogramming\nloop"
  },
  {
    "objectID": "posts/emojis1/emojis1.html",
    "href": "posts/emojis1/emojis1.html",
    "title": "emojis1",
    "section": "",
    "text": "Extrahieren Sie die Anzahl der Emojis aus einem Text.\nNutzen Sie die GermEval-2018-Daten.\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\nDie Daten sind auch über das R-Paket PradaData zu beziehen.\n\nlibrary(tidyverse)\nlibrary(easystats)\ndata(\"germeval_train\", package = \"pradadata\")\n\nNutzen Sie diesen Text-Datensatz, bevor Sie den größeren germeval-Datensatz verwenden:"
  },
  {
    "objectID": "posts/emojis1/emojis1.html#setup",
    "href": "posts/emojis1/emojis1.html#setup",
    "title": "emojis1",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(tictoc)\nlibrary(emoji)  # emoji_xxx\nlibrary(tidyEmoji)"
  },
  {
    "objectID": "posts/emojis1/emojis1.html#test-1",
    "href": "posts/emojis1/emojis1.html#test-1",
    "title": "emojis1",
    "section": "Test 1",
    "text": "Test 1\n\ntest_text |&gt; \n  mutate(n_emojis = emoji_count(text))\n\n  id                     text n_emo n_emojis\n1  1 Abbau, Abbruch ist jetzt     2        0\n2  2   Test   🧑‍🎓 😄 heute!!     0        3\n [ reached 'max' / getOption(\"max.print\") -- omitted 6 rows ]\n\n\nDas Paket emoji beinhaltet eine Menge Emojis:\n\nemoji_name |&gt; length()\n\n[1] 4538"
  },
  {
    "objectID": "posts/emojis1/emojis1.html#test2",
    "href": "posts/emojis1/emojis1.html#test2",
    "title": "emojis1",
    "section": "Test2",
    "text": "Test2\n\ntest_text$text |&gt; \n  emoji_subset()\n\n[1] \"Test   🧑‍🎓 😄 heute!!\""
  },
  {
    "objectID": "posts/emojis1/emojis1.html#tidyemoji---emojis-kategorisieren",
    "href": "posts/emojis1/emojis1.html#tidyemoji---emojis-kategorisieren",
    "title": "emojis1",
    "section": "TidyEmoji - Emojis kategorisieren",
    "text": "TidyEmoji - Emojis kategorisieren\n\ndata.frame(tweets = c(\"I love tidyverse \\U0001f600\\U0001f603\\U0001f603\",\n\"R is my language! \\U0001f601\\U0001f606\\U0001f605\",\n\"This Tweet does not have Emoji!\",\n\"Wearing a mask\\U0001f637\\U0001f637\\U0001f637.\",\n\"Emoji does not appear in all Tweets\",\n\"A flag \\U0001f600\\U0001f3c1\")) %&gt;%\nemoji_categorize(tweets)\n\n# A tibble: 4 × 2\n  tweets                   .emoji_category        \n  &lt;chr&gt;                    &lt;chr&gt;                  \n1 I love tidyverse 😀😃😃  Smileys & Emotion      \n2 R is my language! 😁😆😅 Smileys & Emotion      \n3 Wearing a mask😷😷😷.    Smileys & Emotion      \n4 A flag 😀🏁              Smileys & Emotion|Flags\n\n\n\ntest_text |&gt; \n  emoji_categorize(text)\n\n# A tibble: 1 × 4\n     id text                 n_emo .emoji_category                        \n  &lt;int&gt; &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;                                  \n1     2 Test   🧑‍🎓 😄 heute!!     0 Smileys & Emotion|People & Body|Objects\n\n\n\ndata(wild_emojis, package = \"pradadata\")\n\n\nwild_emojis |&gt; \n  emoji_categorize(emoji)\n\n# A tibble: 28 × 2\n   emoji .emoji_category                           \n   &lt;chr&gt; &lt;chr&gt;                                     \n 1 💣    Smileys & Emotion|NULL|NULL|NULL|NULL|NULL\n 2 💀    Smileys & Emotion|NULL|NULL|NULL|NULL|NULL\n 3 ☠️     Smileys & Emotion|NULL|NULL|NULL|NULL|NULL\n 4 😠    Smileys & Emotion|NULL|NULL|NULL|NULL|NULL\n 5 👹    Smileys & Emotion|NULL|NULL|NULL|NULL|NULL\n 6 💩    Smileys & Emotion|NULL|NULL|NULL|NULL|NULL\n 7 😡    Smileys & Emotion|NULL|NULL|NULL|NULL|NULL\n 8 🤢    Smileys & Emotion|NULL|NULL|NULL|NULL|NULL\n 9 🤮    Smileys & Emotion|NULL|NULL|NULL|NULL|NULL\n10 😖    Smileys & Emotion|NULL|NULL|NULL|NULL|NULL\n# ℹ 18 more rows\n\n\nAlternativ kann man auch via Regex und Unicode Regex ansprechen… emoji_pattern &lt;- \"\\\\p{So}\".\nDas ist vermutlich cleverer 🤓.\n\nCategories:\n\nemoji\ntextmining\nstring"
  },
  {
    "objectID": "posts/filter-na1/filter-na1.html",
    "href": "posts/filter-na1/filter-na1.html",
    "title": "filter-na1",
    "section": "",
    "text": "Filtern Sie alle Zeilen ohne fehlende Werte im Datensatz penguins!"
  },
  {
    "objectID": "posts/filter-na1/filter-na1.html#setup",
    "href": "posts/filter-na1/filter-na1.html#setup",
    "title": "filter-na1",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\"\nd &lt;- read_csv(d_path)\nnrow(d)\n\n[1] 344"
  },
  {
    "objectID": "posts/filter-na1/filter-na1.html#weg-1",
    "href": "posts/filter-na1/filter-na1.html#weg-1",
    "title": "filter-na1",
    "section": "Weg 1",
    "text": "Weg 1\n\nd_nona &lt;-\n  d %&gt;% \n  filter(complete.cases(.))\n\nnrow(d_nona)\n\n[1] 333"
  },
  {
    "objectID": "posts/filter-na1/filter-na1.html#weg-2",
    "href": "posts/filter-na1/filter-na1.html#weg-2",
    "title": "filter-na1",
    "section": "Weg 2",
    "text": "Weg 2\n\nd %&gt;% \n  filter(if_all(everything(), ~ !is.na(.))) %&gt;% \n  nrow()\n\n[1] 333"
  },
  {
    "objectID": "posts/filter-na1/filter-na1.html#weg-3",
    "href": "posts/filter-na1/filter-na1.html#weg-3",
    "title": "filter-na1",
    "section": "Weg 3",
    "text": "Weg 3\n\nd |&gt; \n  drop_na() |&gt; \n  nrow()\n\n[1] 333\n\n\n\nCategories:\n\n2023\neda\nna\nstring"
  },
  {
    "objectID": "posts/penguins-stan-04/penguins-stan-04.html",
    "href": "posts/penguins-stan-04/penguins-stan-04.html",
    "title": "penguins-stan-04",
    "section": "",
    "text": "Aufgabe\nWir untersuchen Einflussfaktoren bzw. Prädiktoren auf das Körpergewicht von Pinguinen. In dieser Aufgabe untersuchen wir den Zusammenhang von Schnabellänge (als UV) und Körpergewicht (als AV).\nWie groß ist die Wahrscheinlichkeit, dass der Effekt vorhanden ist (also größer als Null ist), die “Effektwahrscheinlichkeit”? Geben Sie die Wahrscheinlichkeit an.\nHinweise:\n\nNutzen Sie den Datensatz zu den Palmer Penguins.\nVerwenden Sie Methoden der Bayes-Statistik und die Software Stan.\nSie können den Datensatz z.B. hier beziehen oder über das R-Paket palmerpenguins.\nWeitere Hinweise\n\n         \n\n\nLösung\nZentrieren ist eigentlich immer nützlich, aber hier streng genommen nicht unbedingt nötig. Der Hauptgrund dafür ist, dass Stan für uns den Prior für den Intercept festlegt, und zwar auf Basis der Daten, wir uns also nicht um die komische Frage zu kümmern brauchen, welchen Prior wir für den unzentrierten Achsenabschnitt vergeben wollten: Wie schwer sind Pinguins der Schnabellänge Null? Mit zentrierten Prädiktoren ist die Frage nach dem Prior viel einfacher zu beantworten: Wie schwer ist ein Pinguin mit mittelgroßem Schnabel?\nSetup:\n\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(rstanarm)\n\ndata(\"penguins\", package = \"palmerpenguins\")\n\nEs wird in dieser Aufgabe vorausgesetzt, dass Sie den Datensatz selbständig importieren können. Tipp: Kurzes Googeln hilft ggf., den Datensatz zu finden.\nAlternativ könnten Sie den Datensatz als CSV-Datei importieren:\n\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\"\npenguins &lt;- data_read(d_path)\n\nEin Blick in die Daten zur Kontrolle, ob das Importieren richtig funktioniert hat:\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nVertrauen ist gut, aber - was Golems betrifft - ist Kontrolle eindeutig besser ;-)\n\nm1 &lt;- stan_glm(body_mass_g ~  bill_length_mm,  # Regressionsgleichung\n               data = penguins, #  Daten\n               seed = 42,  # Repro.\n               refresh = 0)  # nicht so viel Output\n\nMit pd() kann man sich die Effektwahrscheinlichkeit (“probability of direction”) ausgeben lassen:\n\npd(m1)\n\nProbability of Direction\n\nParameter      |     pd\n-----------------------\n(Intercept)    | 89.55%\nbill_length_mm |   100%\n\n\nMehr Informationen zu dieser Statistik findet sich hier oder hier.\nAlternativ bekommt man die Statistik auch mit parameters().\nDie Lösung lautet also 1.\n\nCategories:\n\nbayes\nregression\nexam-22"
  },
  {
    "objectID": "posts/Bed-Wskt3/Bed-Wskt3.html",
    "href": "posts/Bed-Wskt3/Bed-Wskt3.html",
    "title": "Bed-Wskt3",
    "section": "",
    "text": "Aufgabe\nAls Bildungsforscher(in) untersuchen Sie den Lernerfolg in einem Statistikkurs.\nEine Gruppe von Studierenden absolviert einen Statistikkurs. Ein Teil lernt gut mit (Ereignis \\(A\\)), ein Teil nicht (Ereignis \\(A^C\\)). Ein Teil besteht die Prüfung (Ereignis \\(B\\)); ein Teil nicht (\\(B^C\\)).\n(Eselsbrücke: Das Ereignis “A” steht für “Ah, hat Aufgepasst.)\nWir ziehen zufällig eine/n Studierende/n: Siehe da – Die Person hat bestanden. Yeah!\nDie Anteile der Gruppen (bzw. Wahrscheinlichkeit des Ereignisses) lassen sich unten stehender Tabelle entnehmen.\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(c(B, Bneg), round, digits = 2)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n\n\n\n\n\nrow_ids\nB\nBneg\nSumme\n\n\n\n\nA\n0.29\n0.25\n0.54\n\n\nA_neg\n0.42\n0.04\n0.46\n\n\nSumme\n0.70\n0.29\n0.99\n\n\n\n\n\n\nAufgabe: Gesucht ist die (bedingte) Wahrscheinlichkeit, dass diese Person gut mitgelernt hat, gegeben der Tatsache, dass sie bestanden hat. Geben Sie die Wahrscheinlichkeit des gesuchten Ereignisses an!\nHinweise:\n\nRunden Sie auf 2 Dezimalstellen.\nGeben Sie Anteile stets in der Form 0.42 an (mit führender Null und Dezimalpunkt).\n“A_neg” bezieht sich auf das Komplementärereignis zu A.\n\n         \n\n\nLösung\nDer gesuchte Wert lautet: 0.41.\n\nCategories:\n\nprobability\nbayes\nnum"
  },
  {
    "objectID": "posts/rope1/rope1.html",
    "href": "posts/rope1/rope1.html",
    "title": "rope1",
    "section": "",
    "text": "Question\nDas Testen von Nullhypothesen wird u.a. deswegen kritisiert, weil die Nullhypothese zumeist apriori als falsch bekannt ist, weswegen es keinen Sinne mache, so die Kritiker, sie zu testen.\nNennen Sie ein Verfahren von John Kruschke, das einen Äquivalenzbereich testet und insofern eine Alternative zum Testen von Nullhypothesen anbietet.\nHinweise:\n\nGeben Sie nur Kleinbuchstaben ein.\nGeben Sie nur ein einziges Wort ein.\n\n\n\nSolution\nrope"
  },
  {
    "objectID": "posts/Weinhaendler/Weinhaendler.html",
    "href": "posts/Weinhaendler/Weinhaendler.html",
    "title": "Weinhaendler",
    "section": "",
    "text": "Sie sind kürzlich in ein Startup-Unternehmen eingestiegen. Das Unternehmen versucht, einen Online-Weinhandel aufzubauen. Kern des Unternehmens ist eine künstliche Intelligenz, die versucht, den Kundis den best möglich passenden Wein anzudreh… zu verkaufen.\nSie haben sich bei Ihrem Bewerbungsgespräch persönlich von der Qualität der Produkte eingehend überzeugt und sind daher hoch motiviert, sich zum Wohle des Unternehmens einzusetzen.\nKürzlich hat eine Beratungsfirma, die Ihre Kunden im Rahmen einer qualitativen Studie untersucht hat, herausgefunden, dass doch ein beachtlicher Teil von einem Menschen, nicht von einem Roboter (bzw. der KI) beim Wein aussuchen beraten werden möchte. Diesen Anteil von Kunden (die nicht von der KI beraten werden möchten) möchten Sie jetzt genauer bestimmen.\nDazu haben Sie \\(N=42\\) Kundis befragt. Gut die Hälfte (\\(n=23\\)) hat sich zugunsten der KI ausgesprochen; der Rest der Kundis möchte lieber von einem Menschen beraten werden.\nGehen Sie im Folgenden davon aus, dass die Studie bzw. die erhaltenen Daten von guter Qualität ist (man also keine Probleme wie mangelnde Repräsentativität erwarten muss).\nVerwenden Sie die Gittermethode und gleichverteilte Priori-Werte.\n\nWie groß ist die Wahrscheinlichkeit, dass die KI-freundlichen Kundis bei Ihnen überwiegen?\nWie groß ist die Wahrscheinlichkeit (laut Modell), dass künftig eine Mehrheit an KI-freundlichen Kundis zu beobachten sein wird?\nWenn Sie nur eine Zahl angeben dürften: Was ist Ihr Schätzwert zum Anteil der KI-Freunde (in dieser Studie)?"
  },
  {
    "objectID": "posts/Weinhaendler/Weinhaendler.html#a",
    "href": "posts/Weinhaendler/Weinhaendler.html#a",
    "title": "Weinhaendler",
    "section": "A)",
    "text": "A)\n\nWie groß ist die Wahrscheinlichkeit (laut Modell), dass die KI-freundlichen Kundis bei Ihnen überwiegen?\n\nDas ist eine Frage nach der kumulative Verteilungsfuntion (cumulative distribution function, cdf).\n\np_grid &lt;- seq(from=0, \n              to=1, \n              length.out=1000)  # Gitterwerte\n\nprior &lt;- rep(1, 1000)  # Priori-Gewichte\n\nset.seed(42)  # Zufallszahlen festlegen\nlikelihood &lt;- dbinom(23, size = 42, prob=p_grid ) \n\nunstandardisierte_posterior &lt;- likelihood * prior \n\nposterior &lt;- unstandardisierte_posterior / sum(unstandardisierte_posterior)\n\nZiehen wir daraus Stichproben:\n\nset.seed(42)  # Zufallszahlen festlegen\nsamples &lt;- \n  tibble(\n    p = sample(p_grid , \n               prob = posterior, \n               size=1e4, \n               replace=TRUE))  \nsamples &lt;-\n  samples %&gt;% \n  mutate(id = 1:nrow(samples))\n\n\nsamples %&gt;% \n  filter(p &gt; 0.5) %&gt;% \n  summarise(wskt_mehrheit_will_ki = n()/nrow(samples))\n\n# A tibble: 1 × 1\n  wskt_mehrheit_will_ki\n                  &lt;dbl&gt;\n1                 0.731\n\n\nVisualisieren:\nMit {ggpubr}:\n\nlibrary(ggpubr)\n\ngghistogram(samples, x = \"p\") +\n  geom_vline(xintercept = 0.5, color = \"red\", linewidth=2) \n\n\n\n\n\n\n\n\nMit {ggplot2}:\n\nsamples %&gt;% \n  ggplot() +\n  aes(x = p) +\n  geom_histogram() +\n  geom_vline(xintercept = 0.5) +\n  labs(title = \"Post-Verteilung\")"
  },
  {
    "objectID": "posts/Weinhaendler/Weinhaendler.html#b",
    "href": "posts/Weinhaendler/Weinhaendler.html#b",
    "title": "Weinhaendler",
    "section": "b)",
    "text": "b)\n\nWie groß ist die Wahrscheinlichkeit (laut Modell), dass künftig eine Mehrheit an KI-freundlichen Kunfis zu beobachten sein wird?\n\n\nPPV &lt;-\n  samples %&gt;% \n  mutate(Anzahl_will_KI = rbinom(n = 1e4, size = 42, prob = p))\n\n\nPPV %&gt;% \n  ggplot() +\n  aes(x = Anzahl_will_KI) +\n  geom_histogram() +\n  labs(title = \"PPV\")\n\n\n\n\n\n\n\n\nEine Mehrheit entspricht mind. 22 von 42 Personen.\n\nPPV %&gt;% \n  filter(Anzahl_will_KI &gt;= 22) %&gt;% \n  summarise(prob_mehrheit_will_ki = n()/nrow(PPV))\n\n# A tibble: 1 × 1\n  prob_mehrheit_will_ki\n                  &lt;dbl&gt;\n1                 0.623"
  },
  {
    "objectID": "posts/Weinhaendler/Weinhaendler.html#c",
    "href": "posts/Weinhaendler/Weinhaendler.html#c",
    "title": "Weinhaendler",
    "section": "C)",
    "text": "C)\n\nWenn Sie nur eine Zahl angeben dürften: Was ist Ihr Schätzwert zum Anteil der KI-Freunde (in dieser Studie)?\n\nMan könnte den Mittelwert oder den Median angeben:\n\nlibrary(easystats)\ndescribe_distribution(samples)\n\nVariable |    Mean |      SD |     IQR |            Range | Skewness | Kurtosis |     n | n_Missing\n---------------------------------------------------------------------------------------------------\np        |    0.54 |    0.07 |    0.10 |     [0.27, 0.79] |    -0.09 |    -0.06 | 10000 |         0\nid       | 5000.50 | 2886.90 | 5000.50 | [1.00, 10000.00] |     0.00 |    -1.20 | 10000 |         0\n\n\n\nCategories:\n\nprobability\nbayes-box\nbayes\nstring"
  },
  {
    "objectID": "posts/Regression3/Regression3.html",
    "href": "posts/Regression3/Regression3.html",
    "title": "Regression3",
    "section": "",
    "text": "dabei wird noch die Gruppierungsvariable \\(g\\) (mit den Stufen 0 und 1) berücksichtigt (vgl. Farbe und Form der Punkte). Zur besseren Orientierung ist die Regressionsgerade pro Gruppe eingezeichnet.\n\n\nWarning: Using the `size` aesthetic with geom_line was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\n\n\n\n\n\n\n\nWählen Sie das (für die Population) am besten passende Modell aus der Liste aus!\nHinweis: Ein Interaktionseffekt der Variablen \\(x\\) und \\(g\\) ist mit \\(xg\\) gekennzeichnet.\n\n\n\n\\(y = 40 + -10\\cdot x + 0 \\cdot g + -10 \\cdot xg + \\epsilon\\)\n\\(y = -40 + -10\\cdot x + 0 \\cdot g + -10 \\cdot xg + \\epsilon\\)\n\\(y = 40 + 10\\cdot x + -40 \\cdot g + -10 \\cdot xg + \\epsilon\\)\n\\(y = -40 + 10\\cdot x + 0 \\cdot g + -10 \\cdot xg + \\epsilon\\)"
  },
  {
    "objectID": "posts/Regression3/Regression3.html#answerlist",
    "href": "posts/Regression3/Regression3.html#answerlist",
    "title": "Regression3",
    "section": "",
    "text": "\\(y = 40 + -10\\cdot x + 0 \\cdot g + -10 \\cdot xg + \\epsilon\\)\n\\(y = -40 + -10\\cdot x + 0 \\cdot g + -10 \\cdot xg + \\epsilon\\)\n\\(y = 40 + 10\\cdot x + -40 \\cdot g + -10 \\cdot xg + \\epsilon\\)\n\\(y = -40 + 10\\cdot x + 0 \\cdot g + -10 \\cdot xg + \\epsilon\\)"
  },
  {
    "objectID": "posts/Regression3/Regression3.html#answerlist-1",
    "href": "posts/Regression3/Regression3.html#answerlist-1",
    "title": "Regression3",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nRichtig\nFalsch\nFalsch\n\n\nCategories:\n\ndyn\nregression\nlm\nschoice"
  },
  {
    "objectID": "posts/ausreisser1/ausreisser1.html",
    "href": "posts/ausreisser1/ausreisser1.html",
    "title": "ausreisser1",
    "section": "",
    "text": "Entfernen Sie alle Ausreißer im Datensatz mariokart!\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\nDefinieren Sie “Ausreißer” als Werte, die mehr als 3SD vom Mittelwert entfernt sind."
  },
  {
    "objectID": "posts/ausreisser1/ausreisser1.html#setup",
    "href": "posts/ausreisser1/ausreisser1.html#setup",
    "title": "ausreisser1",
    "section": "Setup",
    "text": "Setup"
  },
  {
    "objectID": "posts/ausreisser1/ausreisser1.html#überblick",
    "href": "posts/ausreisser1/ausreisser1.html#überblick",
    "title": "ausreisser1",
    "section": "Überblick",
    "text": "Überblick\nWir verschaffen uns einen Überblick über die Verteilungen:\n\nplot_histogram(mariokart)  # aus Paket `DataExplorer`\n\n\n\n\n\n\n\n\nWie man sieht gibt es einige Ausreißer, z.B. bei ship_pr und total_pr."
  },
  {
    "objectID": "posts/ausreisser1/ausreisser1.html#daten-aufbereiten-mit-z-transformation",
    "href": "posts/ausreisser1/ausreisser1.html#daten-aufbereiten-mit-z-transformation",
    "title": "ausreisser1",
    "section": "Daten aufbereiten mit z-Transformation",
    "text": "Daten aufbereiten mit z-Transformation\n\nmariokart2 &lt;-\n  mariokart %&gt;% \n  select(-id) %&gt;% \n  mutate(across(  # \"across\" wiederholt die Funktionen \".fns\" über alle Spalten \".cols\"\n    .cols = where(is.numeric),\n    .fns = ~ as.numeric(standardize(.x))))\n\nLeider gibt standardize kein vernünftiges numerisches Objekt zurück, so dass wir mit as.numeric die Daten noch zur Räson rufen müssen.\nWie man sieht, ändert sich die Verteilungsform nicht durch die z-Transformation (oder durch irgendeine lineare Transformation):\n\nplot_histogram(mariokart2)"
  },
  {
    "objectID": "posts/ausreisser1/ausreisser1.html#extremwerte-durch-mw-ersetzen",
    "href": "posts/ausreisser1/ausreisser1.html#extremwerte-durch-mw-ersetzen",
    "title": "ausreisser1",
    "section": "Extremwerte durch MW ersetzen",
    "text": "Extremwerte durch MW ersetzen\n\nmariokart3 &lt;-\n  mariokart2 %&gt;% \n  mutate(across(\n    .cols = where(is.numeric),\n    .fns = ~ case_when(abs(.x) &lt;= 3 ~ .x,\n                       abs(.x) &gt; 3 ~ mean(.x))\n  ))\n\n\nplot_histogram(mariokart3)\n\n\n\n\n\n\n\n\nJetzt sind die Daten deutlich weniger extrem.\n\nCategories:\n\neda\ndatawrangling\ntidyverse\nausreisser\nstring"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-16/Verteilungen-Quiz-16.html",
    "href": "posts/Verteilungen-Quiz-16/Verteilungen-Quiz-16.html",
    "title": "Verteilungen-Quiz-16",
    "section": "",
    "text": "Für die mittlere Körpergröße des deutschen Mannes \\(X\\) gelte \\(X \\sim N(180,06)\\) (in Zentimetern).\nQuelle Mittelwert Quelle SD geschätzt\nÄhnliche Daten finden sich bei Our World in Data.\nAufgabe: Ist folgende Aussage wahr?\nDas 50%-Quantil von \\(X\\) beträgt 180.\n\n  Ja    Nein \n\n\nAntworten"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-16/Verteilungen-Quiz-16.html#answerlist",
    "href": "posts/Verteilungen-Quiz-16/Verteilungen-Quiz-16.html#answerlist",
    "title": "Verteilungen-Quiz-16",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/wskt-quiz04/wskt-quiz04.html",
    "href": "posts/wskt-quiz04/wskt-quiz04.html",
    "title": "wskt-quiz04",
    "section": "",
    "text": "Sei \\(X \\sim Bin(10, 1/2)\\). Dann ist die zugehörige Verteilung (von \\(X\\)) symmetrisch.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n\n\n\nFalsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz04/wskt-quiz04.html#answerlist",
    "href": "posts/wskt-quiz04/wskt-quiz04.html#answerlist",
    "title": "wskt-quiz04",
    "section": "",
    "text": "Falsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz04/wskt-quiz04.html#answerlist-1",
    "href": "posts/wskt-quiz04/wskt-quiz04.html#answerlist-1",
    "title": "wskt-quiz04",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\nquiz\nprobability\ndistribution\nquiz1-qm2-ws23\nschoice"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-11/Verteilungen-Quiz-11.html",
    "href": "posts/Verteilungen-Quiz-11/Verteilungen-Quiz-11.html",
    "title": "Verteilungen-Quiz-11",
    "section": "",
    "text": "Ist folgende Aussage wahr?\nBei einer Verteilung gilt: \\(\\bar{x} = Md = \\text{Modus}\\).\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-11/Verteilungen-Quiz-11.html#answerlist",
    "href": "posts/Verteilungen-Quiz-11/Verteilungen-Quiz-11.html#answerlist",
    "title": "Verteilungen-Quiz-11",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-11/Verteilungen-Quiz-11.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-11/Verteilungen-Quiz-11.html#answerlist-1",
    "title": "Verteilungen-Quiz-11",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/adjustieren2a/adjustieren2a.html",
    "href": "posts/adjustieren2a/adjustieren2a.html",
    "title": "adjustieren2a",
    "section": "",
    "text": "Aufgabe\nBetrachten Sie folgendes Modell, das den Zusammenhang des Preises (price) und dem Gewicht (carat) von Diamanten untersucht (Datensatz diamonds).\n\nlibrary(tidyverse)\nlibrary(easystats)\ndiamonds &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/ggplot2/diamonds.csv\")\n\nRows: 53940 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): cut, color, clarity\ndbl (8): rownames, carat, depth, table, price, x, y, z\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAber zuerst zentrieren wir den metrischen Prädiktor carat, um den Achsenabschnitt besser interpretieren zu können.\n\ndiamonds &lt;-\n  diamonds %&gt;% \n  mutate(carat_z = carat - mean(carat, na.rm = TRUE))\n\nDann berechnen wir ein (nicht-bayesianisches, sondern frequentistisches) Regressionsmodell:\n\nlm1 &lt;- lm(price ~ carat_z, data = diamonds)\nparameters(lm1)\n\nParameter   | Coefficient |    SE |             95% CI | t(53938) |      p\n--------------------------------------------------------------------------\n(Intercept) |     3932.80 |  6.67 | [3919.73, 3945.87] |   589.83 | &lt; .001\ncarat z     |     7756.43 | 14.07 | [7728.86, 7784.00] |   551.41 | &lt; .001\n\n\nZur Verdeutlichung ein Diagramm zum Modell:\n\nestimate_relation(lm1) |&gt; plot()\n\n\n\n\n\n\n\n\nAufgaben:\n\nWas kostet in Diamant mittlerer Größe laut Modell lm1? Runden Sie auf eine Dezimale. Geben Sie nur eine Zahl ein.\nGeben Sie eine Regressionsformel an, die lm1 ergänzt, so dass die Schliffart (cut) des Diamanten kontrolliert (adjustiert) wird. Anders gesagt: Das Modell soll die mittleren Preise für jede der fünf Schliffarten angeben. Geben Sie nur die Regressionsformel an. Lassen Sie zwischen Termen jeweils ein Leerzeichen Abstand.\n\nHinweis: Es gibt (laut Datensatz) folgende Schliffarten (und zwar in der folgenden Reihenfolge):\n\ndiamonds %&gt;% \n  distinct(cut)\n\n# A tibble: 5 × 1\n  cut      \n  &lt;chr&gt;    \n1 Ideal    \n2 Premium  \n3 Good     \n4 Very Good\n5 Fair     \n\n\n         \n\n\nLösung\n\nUnser Modell lm1 schätzt den Preis eines Diamanten mittlerer Größe auf etwa 3932.5 (was immer auch die Einheiten sind, Dollar vermutlich). Da der Prädiktor carat_z zentriert ist, entspricht ein Wert von 0 dem Mittelwert der ursprünglichen Verteilung, carat. Der Y-Wert, wenn X=0, wird vom Intercept angegeben.\n\n\nparameters(lm1)\n\nParameter   | Coefficient |    SE |             95% CI | t(53938) |      p\n--------------------------------------------------------------------------\n(Intercept) |     3932.80 |  6.67 | [3919.73, 3945.87] |   589.83 | &lt; .001\ncarat z     |     7756.43 | 14.07 | [7728.86, 7784.00] |   551.41 | &lt; .001\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\nOder so:\n\ncoef(lm1)\n\n(Intercept)     carat_z \n   3932.800    7756.426 \n\n\nAlternativ können wir uns mit predict für jeden beliebigen Wert des Prädiktors die Vorhersage des Modells ausgeben lassen.\nWir definieren eine (hier sehr kurze) Tabelle mit Prädiktorwerten, für die wir die Vorhersage laut lm1 wissen möchten:\n\nneue_daten &lt;-\n  tibble(carat_z = 0)\n\nDann weisen wir unseren Lieblingsroboter an, auf Basis von lm1 eine Vorhersage (prediction) für neue_daten zu erstellen.\n\npredict(lm1, newdata = neue_daten)\n\n     1 \n3932.8 \n\n\n\nprice ~ carat_z + cut\n\nDieses zweite Modell könnten wir so berechnen:\n\nlm2 &lt;- lm(price ~ carat_z + cut, data = diamonds)\nparameters(lm2)\n\nParameter       | Coefficient |    SE |             95% CI | t(53934) |      p\n------------------------------------------------------------------------------\n(Intercept)     |     2405.18 | 37.83 | [2331.04, 2479.32] |    63.58 | &lt; .001\ncarat z         |     7871.08 | 13.98 | [7843.68, 7898.48] |   563.04 | &lt; .001\ncut [Good]      |     1120.33 | 43.50 | [1035.07, 1205.59] |    25.76 | &lt; .001\ncut [Ideal]     |     1800.92 | 39.34 | [1723.81, 1878.04] |    45.77 | &lt; .001\ncut [Premium]   |     1439.08 | 39.87 | [1360.94, 1517.21] |    36.10 | &lt; .001\ncut [Very Good] |     1510.14 | 40.24 | [1431.26, 1589.01] |    37.53 | &lt; .001\n\n\nMan könnte hier noch einen Interaktionseffekt ergänzen, wenn man Grund zur Annahme hat, dass es einen gibt.\n\nCategories:\n\nregression\n‘2023’\nstring"
  },
  {
    "objectID": "posts/movies-vis2/movies-vis2.html",
    "href": "posts/movies-vis2/movies-vis2.html",
    "title": "movies-vis2",
    "section": "",
    "text": "Aufgabe\nImportieren Sie bitte für diese Aufgabe den Datensatz movies (aus dem R-Paket ggplot2movies). Ein Data-Dictionary findet sich hier.\nErstellen Sie folgende Visualisierung:\n\nGruppenvergleich des Budgets pro Jahr\nBerücksichtigen Sie nur Actionfilme ab 2000\nVerzichten Sie auf Filme mit einer unterdurchschnittlichen Zahl an Bewertungen (votes; gemessen an allen Filmen, gerundet zur nächsten ganzen Zahl)\n\n         \n\n\nLösung\nPakete starten:\n\nlibrary(tidyverse)\nlibrary(DataExplorer)\n\nDaten importieren:\n\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/ggplot2movies/movies.csv\"\nd &lt;- read.csv(d_path)\n\nDurchschnittliche Zahl an Bewertungen:\n\nd %&gt;% \n  summarise(votes_mean = mean(votes))\n\n  votes_mean\n1   632.1304\n\n\nDie durchschnittliche Zahl an Bewertungen beträgt also 632.\n\nd %&gt;% \n  select(budget, rating, year, votes, Action) %&gt;% \n  filter(year &gt;= 2000) %&gt;% \n  filter(Action == 1) %&gt;% \n  filter(votes &gt;= 632) %&gt;% \n  select(-Action) %&gt;% \n  mutate(year = factor(year)) %&gt;% \n  select(budget, year) %&gt;% \n  plot_boxplot(by = \"year\")\n\nWarning: Removed 66 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nHinweis: Die Zahl “5.0e+07” ist eine Zahl in der Exponenzial-Schreibweise, nämlich \\(5\\cdot10^7\\), also \\(5 \\cdot 1000000\\).\n\nCategories:\n\nvis\neda\nstring"
  },
  {
    "objectID": "posts/mtcars-abhaengig_var2/mtcars-abhaengig_var2.html",
    "href": "posts/mtcars-abhaengig_var2/mtcars-abhaengig_var2.html",
    "title": "mtcars-abhaengig_var2",
    "section": "",
    "text": "Aufgabe\nIm Folgenden ist der Datensatz mtcars zu analysieren.\nDer Datensatz ist z.B. als CSV-Datei von dieser Webseite abrufbar.\nHilfe zum Datensatz ist via help(\"name_des_datensatzes\") oder auf dieser Webseite abrufbar.\nOb die Variable hp (UV; Ereignis \\(A\\)) und Spritverbrauch (mpg; AV; Ereignis \\(B\\)) wohl voneinander abhängig sind? Was meinen Sie? Was ist Ihre Einschätzung dazu? Vermutlich haben Sie ein (wenn vielleicht auch implizites) Vorab-Wissen zu dieser Frage. Lassen wir dieses Vorab-Wissen aber einmal außen vor und schauen uns rein Daten dazu an. Vereinfachen wir die Frage etwas, indem wir beide Variablen am Mittelwert aufteilen: Wenn eine Beobachtung (d.h. ein Auto) einen Wert in der jeweiligen Variablen höchstens so groß wie der Mittelwert der Variable aufweist, geben wir der Beobachtung der Wert 0, ansonsten den Wert 1. Wir nehmen die Anteile der gesuchten Größen als Schätzwert für deren Wahrscheinlichkeit.\nDie resultierenden binären Variablen nennen wir av_high bzw. uv_high (im schönsten Denglisch).\nBerechnen Sie: \\(Pr(\\neg \\text{uvhigh} \\, | \\,  \\text{avhigh})\\)\nHinweise:\n\nDas “Ellbogen-Zeichen” \\(\\neg\\) kennzeichnet eine logische Negierung (das Gegenteil).\nDie angegebene Wahrscheinlichkeit ist eine bedingte Wahrscheinlichkeit.\nWeitere Hinweise\n\n         \n\n\nLösung\nSchauen wir zuerst mal in den Datensatz:\n\nmtcars %&gt;% \n  select(mpg, hp) %&gt;% \n  slice_head(n = 5)\n\n                   mpg  hp\nMazda RX4         21.0 110\nMazda RX4 Wag     21.0 110\nDatsun 710        22.8  93\nHornet 4 Drive    21.4 110\nHornet Sportabout 18.7 175\n\n\nDann berechnen wir die binären Variablen.\nZuerst av_high:\n\n# split by mean:\nd2 &lt;-\n  mtcars %&gt;% \n  select(mpg, hp) %&gt;% \n  mutate(av_high = case_when(\n    mpg &lt;= mean(mpg) ~ 0,\n    mpg &gt; mean(mpg) ~ 1\n  )) %&gt;% \n  select(-mpg) \n\nglimpse(d2)\n\nRows: 32\nColumns: 2\n$ hp      &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, …\n$ av_high &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,…\n\n\nav_high = 1 zeigt hohe Werte in mpg an, und av_high = 0 zeigt geringe Werte (im Verhältnis zum Mittelwert).\nAchtung! Wenn es fehlende Werte im Datensatz gäbe, müssten wir diese in geeigneter Manier vorab entfernen. Also z.B.:\n\n# split by mean:\nd2 &lt;-\n  mtcars %&gt;% \n  select(mpg, hp) %&gt;% \n  drop_na() |&gt; \n  mutate(av_high = case_when(\n    mpg &lt;= mean(mpg) ~ 0,\n    mpg &gt; mean(mpg) ~ 1\n  )) %&gt;% \n  select(-mpg) \n\nglimpse(d2)\n\nRows: 32\nColumns: 2\n$ hp      &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, …\n$ av_high &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,…\n\n\nOder so:\n\n# split by mean:\nd2 &lt;-\n  mtcars %&gt;% \n  select(mpg, hp) %&gt;% \n  mutate(av_high = case_when(\n    mpg &lt;= mean(mpg, na.rm = TRUE) ~ 0,\n    mpg &gt; mean(mpg, na.rm = TRUE) ~ 1\n  )) %&gt;% \n  select(-mpg) \n\nglimpse(d2)\n\nRows: 32\nColumns: 2\n$ hp      &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, …\n$ av_high &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,…\n\n\nMan beachte, dass hohe Werte in MPG einen geringen Spritverbrauch bedeuten (also eine hohe Sparsamkeit im Verbrauch).\nDann berechnen wir uv_high:\n\nd3 &lt;-\n  d2 %&gt;% \n  select(av_high, hp) %&gt;% \n  mutate(uv_high = case_when(\n    hp &lt;= mean(hp) ~ 0,\n    hp &gt; mean(hp) ~ 1\n  )) %&gt;% \n  select(-hp) \n\nglimpse(d3)\n\nRows: 32\nColumns: 2\n$ av_high &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,…\n$ uv_high &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,…\n\n\nAuch hier gilt, dass die Funktionen der deskriptiven Statistik alle Beine von sich strecken und den Dienst “verweigern”, wenn es fehlende Werte im Vektor geben sollte. Sie meinen es natürlich nur gut mit Ihnen. 🤷‍♂️\nDann zählen wir die gesuchten Wahrscheinlichkeiten bzw. Anteile der AV:\n\nd3 %&gt;% \n  count(av_high)\n\n  av_high  n\n1       0 18\n2       1 14\n\n\nEs gibt also 14 Autos, die den oben gesuchten “hinteren Teil” der Bedingung erfüllen (av_high = 1).\nFiltern wir als nächstes nur in diesen 14 Autos nach dem “vorderen Teil” der gesuchten Wahrscheinlichkeit, also uv_high = 0.\n\nd3 %&gt;% \n  filter(av_high == 1) %&gt;% \n  count(uv_high) %&gt;% \n  mutate(prop = n/sum(n))\n\n  uv_high  n prop\n1       0 14    1\n\n\nEs gibt also 14 von 14 Autos, die diese Bedingung, uv_high = 0 erfüllen. Das sind 100%.\nIn Worten: Von den Autos mit hoher Sparsamkeit haben alle eine geringe PS-Zahl. Das macht intuitiv Sinn.\nDer gesuchte Wert beträgt also 1.\n\nCategories:\n\ndyn\nprobability"
  },
  {
    "objectID": "posts/Typ-Fehler-R-06a/Typ-Fehler-R-06a.html",
    "href": "posts/Typ-Fehler-R-06a/Typ-Fehler-R-06a.html",
    "title": "Typ-Fehler-R-06a",
    "section": "",
    "text": "Aufgabe\nBetrachten Sie folgende R-Syntax, für die R eine Fehlermeldung ausgibt:\n\nx &lt;- c(1, 2, 3)\nsum(abs(mean(x) - x)))\n\nError: &lt;text&gt;:2:22: unexpected ')'\n1: x &lt;- c(1, 2, 3)\n2: sum(abs(mean(x) - x)))\n                        ^\n\n\nGeben Sie die korrekte Syntax an! Ändern Sie nur die notwendigen Zeichen an der Syntax oben. Gehen Sie davon aus, dass die aufgerufenen Funktionen existieren.\nGeben Sie keine Leerzeichen ein.\n         \n\n\nLösung\nHinten ist eine (schließende) Klammer zu viel, die muss weg:\n\nsum(abs(mean(x)-x))  # so geht's\n\nError in mean(x): object 'x' not found\n\n\nDie Antwort lautet: sum(abs(mean(x)-x)).\n\nCategories:\n\nR\n‘2023’\nstring"
  },
  {
    "objectID": "posts/wrangle1/wrangle1.html",
    "href": "posts/wrangle1/wrangle1.html",
    "title": "wrangle1",
    "section": "",
    "text": "Welche der folgenden Spalte ist nicht Teil des Datensatzes flights aus dem R-Paket nycflights13?\nAlternativ können Sie den Datensatz hier beziehen. Hilfe zum Datensatz (Codebook) finden Sie hier.\n\n\n\nyear\nmonth\n\nday\ndep_time\nsched_dep_time\nestimated_dep_time\narr_time\nsched_arr_time"
  },
  {
    "objectID": "posts/wrangle1/wrangle1.html#answerlist",
    "href": "posts/wrangle1/wrangle1.html#answerlist",
    "title": "wrangle1",
    "section": "",
    "text": "year\nmonth\n\nday\ndep_time\nsched_dep_time\nestimated_dep_time\narr_time\nsched_arr_time"
  },
  {
    "objectID": "posts/wrangle1/wrangle1.html#answerlist-1",
    "href": "posts/wrangle1/wrangle1.html#answerlist-1",
    "title": "wrangle1",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\n\nFalsch\nFalsch\nFalsch\nWahr\nFalsch\nFalsch\n\n\nCategories:\n\neda\ndatawrangling\ntidyverse\ndplyr\nschoice"
  },
  {
    "objectID": "posts/targets-multiple-data-files/targets-multiple-data-files.html",
    "href": "posts/targets-multiple-data-files/targets-multiple-data-files.html",
    "title": "targets-multiple-data-files",
    "section": "",
    "text": "Aufgabe\nSchreiben Sie eine targets Pipeline, die einen Ordner mit Datendateien beobachtet und sich aktualisiert, wenn neue Daten dazukommt. Die Pipeline soll die Datendateien importieren und zu einer Tabelle zusammenfügen und schließlich die Zeilen zählen.\n         \n\n\nLösung\nDie folgende Lösung ist stark inspiriert von diesem SO-Post.\nWir scheiben eine _targets.R Datei mit folgendem Inhalt.\nZuerst das Setup:\n\nlibrary(targets)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tarchetypes)  # für tar_files()\n\nDann definieren wir Konstanten; hier den Pfad:\n\npath &lt;- list()\npath$data &lt;- \"data/\"\n\nAus Gründen der Ordnungsfreude haben wir eine Liste erstellt, in der dann alle möglichen Pfade abgelegt werden können.\nSchließlich definieren wir die Pipeline. Hier spielt die Musik:\n\nlist(\n  tar_files(data_paths, path$data %&gt;% list.files(full.names = TRUE, pattern = \"csv\")),  # Liste der Daten-Dateien\n  tar_target(data_proc, data_paths %&gt;% read_csv(),  # Einlesen\n             pattern = map(data_paths)),  # Über alle Elemente von data_paths iterieren, also über alle Datendateien\n  tar_target(n_row, nrow(data_proc))  # Zeilen zählen\n)\n\n[[1]]\n[[1]]$data_paths_files\n&lt;tar_stem&gt; \n  name: data_paths_files \n  command:\n    path$data %&gt;% list.files(full.names = TRUE, pattern = \"csv\") \n  format: rds \n  repository: local \n  iteration method: vector \n  error mode: stop \n  memory mode: persistent \n  storage mode: main \n  retrieval mode: main \n  deployment mode: worker \n  priority: 0 \n  resources:\n    list() \n  cue:\n    mode: always\n    command: TRUE\n    depend: TRUE\n    format: TRUE\n    repository: TRUE\n    iteration: TRUE\n    file: TRUE\n    seed: TRUE \n  packages:\n    tarchetypes\n    lubridate\n    forcats\n    stringr\n    dplyr\n    purrr\n    readr\n    tidyr\n    tibble\n    ggplot2\n    tidyverse\n    targets\n    stats\n    graphics\n    grDevices\n    utils\n    datasets\n    colorout\n    methods\n    base \n  library:\n    NULL\n[[1]]$data_paths\n&lt;tar_pattern&gt; \n  name: data_paths \n  command:\n    data_paths_files \n  pattern:\n    map(data_paths_files) \n  format: file \n  repository: local \n  iteration method: vector \n  error mode: stop \n  memory mode: persistent \n  storage mode: main \n  retrieval mode: main \n  deployment mode: main \n  priority: 0 \n  resources:\n    list() \n  cue:\n    mode: thorough\n    command: TRUE\n    depend: TRUE\n    format: TRUE\n    repository: TRUE\n    iteration: TRUE\n    file: TRUE\n    seed: TRUE \n  packages:\n    character(0) \n  library:\n    NULL\n\n[[2]]\n&lt;tar_pattern&gt; \n  name: data_proc \n  command:\n    data_paths %&gt;% read_csv() \n  pattern:\n    map(data_paths) \n  format: rds \n  repository: local \n  iteration method: vector \n  error mode: stop \n  memory mode: persistent \n  storage mode: main \n  retrieval mode: main \n  deployment mode: worker \n  priority: 0 \n  resources:\n    list() \n  cue:\n    mode: thorough\n    command: TRUE\n    depend: TRUE\n    format: TRUE\n    repository: TRUE\n    iteration: TRUE\n    file: TRUE\n    seed: TRUE \n  packages:\n    tarchetypes\n    lubridate\n    forcats\n    stringr\n    dplyr\n    purrr\n    readr\n    tidyr\n    tibble\n    ggplot2\n    tidyverse\n    targets\n    stats\n    graphics\n    grDevices\n    utils\n    datasets\n    colorout\n    methods\n    base \n  library:\n    NULL\n[[3]]\n&lt;tar_stem&gt; \n  name: n_row \n  command:\n    nrow(data_proc) \n  format: rds \n  repository: local \n  iteration method: vector \n  error mode: stop \n  memory mode: persistent \n  storage mode: main \n  retrieval mode: main \n  deployment mode: worker \n  priority: 0 \n  resources:\n    list() \n  cue:\n    mode: thorough\n    command: TRUE\n    depend: TRUE\n    format: TRUE\n    repository: TRUE\n    iteration: TRUE\n    file: TRUE\n    seed: TRUE \n  packages:\n    tarchetypes\n    lubridate\n    forcats\n    stringr\n    dplyr\n    purrr\n    readr\n    tidyr\n    tibble\n    ggplot2\n    tidyverse\n    targets\n    stats\n    graphics\n    grDevices\n    utils\n    datasets\n    colorout\n    methods\n    base \n  library:\n    NULL\n\n\nMit pattern = map(data_paths) iterieren wir nicht nur über alle Elemente von data_path, sondern fügen die Elemente auch zu einer Tabelle zusammen.\nHier ist die ganze Syntax noch einmal:\n\n# _targets.R file\n\nlibrary(targets)\nlibrary(tidyverse)\nlibrary(tarchetypes)\n\n\npath &lt;- list()\npath$data &lt;- \"data/\"\n\n\nlist(\n  tar_files(data_paths, path$data %&gt;% list.files(full.names = TRUE, pattern = \"csv\")),\n  tar_target(data_proc, data_paths %&gt;% read_csv(),\n             pattern = map(data_paths)),\n  tar_target(n_row, nrow(data_proc))\n)\n\n[[1]]\n[[1]]$data_paths_files\n&lt;tar_stem&gt; \n  name: data_paths_files \n  command:\n    path$data %&gt;% list.files(full.names = TRUE, pattern = \"csv\") \n  format: rds \n  repository: local \n  iteration method: vector \n  error mode: stop \n  memory mode: persistent \n  storage mode: main \n  retrieval mode: main \n  deployment mode: worker \n  priority: 0 \n  resources:\n    list() \n  cue:\n    mode: always\n    command: TRUE\n    depend: TRUE\n    format: TRUE\n    repository: TRUE\n    iteration: TRUE\n    file: TRUE\n    seed: TRUE \n  packages:\n    tarchetypes\n    lubridate\n    forcats\n    stringr\n    dplyr\n    purrr\n    readr\n    tidyr\n    tibble\n    ggplot2\n    tidyverse\n    targets\n    stats\n    graphics\n    grDevices\n    utils\n    datasets\n    colorout\n    methods\n    base \n  library:\n    NULL\n[[1]]$data_paths\n&lt;tar_pattern&gt; \n  name: data_paths \n  command:\n    data_paths_files \n  pattern:\n    map(data_paths_files) \n  format: file \n  repository: local \n  iteration method: vector \n  error mode: stop \n  memory mode: persistent \n  storage mode: main \n  retrieval mode: main \n  deployment mode: main \n  priority: 0 \n  resources:\n    list() \n  cue:\n    mode: thorough\n    command: TRUE\n    depend: TRUE\n    format: TRUE\n    repository: TRUE\n    iteration: TRUE\n    file: TRUE\n    seed: TRUE \n  packages:\n    character(0) \n  library:\n    NULL\n\n[[2]]\n&lt;tar_pattern&gt; \n  name: data_proc \n  command:\n    data_paths %&gt;% read_csv() \n  pattern:\n    map(data_paths) \n  format: rds \n  repository: local \n  iteration method: vector \n  error mode: stop \n  memory mode: persistent \n  storage mode: main \n  retrieval mode: main \n  deployment mode: worker \n  priority: 0 \n  resources:\n    list() \n  cue:\n    mode: thorough\n    command: TRUE\n    depend: TRUE\n    format: TRUE\n    repository: TRUE\n    iteration: TRUE\n    file: TRUE\n    seed: TRUE \n  packages:\n    tarchetypes\n    lubridate\n    forcats\n    stringr\n    dplyr\n    purrr\n    readr\n    tidyr\n    tibble\n    ggplot2\n    tidyverse\n    targets\n    stats\n    graphics\n    grDevices\n    utils\n    datasets\n    colorout\n    methods\n    base \n  library:\n    NULL\n[[3]]\n&lt;tar_stem&gt; \n  name: n_row \n  command:\n    nrow(data_proc) \n  format: rds \n  repository: local \n  iteration method: vector \n  error mode: stop \n  memory mode: persistent \n  storage mode: main \n  retrieval mode: main \n  deployment mode: worker \n  priority: 0 \n  resources:\n    list() \n  cue:\n    mode: thorough\n    command: TRUE\n    depend: TRUE\n    format: TRUE\n    repository: TRUE\n    iteration: TRUE\n    file: TRUE\n    seed: TRUE \n  packages:\n    tarchetypes\n    lubridate\n    forcats\n    stringr\n    dplyr\n    purrr\n    readr\n    tidyr\n    tibble\n    ggplot2\n    tidyverse\n    targets\n    stats\n    graphics\n    grDevices\n    utils\n    datasets\n    colorout\n    methods\n    base \n  library:\n    NULL\n\n\n\nCategories:\n\nprojectmgt\ntargets\nrepro\nstring"
  },
  {
    "objectID": "posts/filter01/filter01.html",
    "href": "posts/filter01/filter01.html",
    "title": "filter01",
    "section": "",
    "text": "Aufgabe\nIm Datensatz mariokart:\nFiltern Sie alle Spiele, die mehr als 50 Euro kosten (total_pr) erzielt haben und die Versandkosten erheben (ship_pr)!\nGeben Sie die Antwort der Zeilen zurück, die nach dem Filtern im Datensatz verbleiben!\n         \n\n\nLösung\nPakete starten:\n\nlibrary(easystats)\nlibrary(tidyverse)  # startet das Paket tidyverse\n\nDaten importieren:\n\nmariokart &lt;- data_read(\"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\")\n\nFiltern:\n\nmariokart2 &lt;- filter(mariokart, total_pr &gt; 50.00 & ship_pr &gt; 0)  # R bentzt Dezimalpunkt\n\nDie Lösung lautet: 32 Zeilen verbleiben im Datensatz nach dem Filtern.\n\nCategories:\n\ndatawrangling\neda\ntidyverse\ndplyr\nnum"
  },
  {
    "objectID": "posts/mtcars-post2/mtcars-post2.html",
    "href": "posts/mtcars-post2/mtcars-post2.html",
    "title": "mtcars-post2",
    "section": "",
    "text": "Im Datensatz mtcars: Wie groß ist der Effekt der UV vs auf die AV mpg? Geben Sie die Breite des 95% PI an (im Bezug zur gesuchten Größe). Berechnen Sie das dazu passende Modell mit Methoden der Bayes-Statistik.\nHinweise\nWählen Sie die am besten passende Option:\n\n\n\n0.7\n2.7\n4.7\n6.7\n8.7"
  },
  {
    "objectID": "posts/mtcars-post2/mtcars-post2.html#answerlist",
    "href": "posts/mtcars-post2/mtcars-post2.html#answerlist",
    "title": "mtcars-post2",
    "section": "",
    "text": "0.7\n2.7\n4.7\n6.7\n8.7"
  },
  {
    "objectID": "posts/mtcars-post2/mtcars-post2.html#answerlist-1",
    "href": "posts/mtcars-post2/mtcars-post2.html#answerlist-1",
    "title": "mtcars-post2",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nWahr\nFalsch\n\n\nCategories:\n\nbayes\nregression\npost\nexam-22"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-10/Verteilungen-Quiz-10.html",
    "href": "posts/Verteilungen-Quiz-10/Verteilungen-Quiz-10.html",
    "title": "Verteilungen-Quiz-10",
    "section": "",
    "text": "Ist folgende Aussage wahr?\nSei \\(X \\sim N(100,15)\\), dann ist \\(Pr(X = \\bar{x}) = 1/2\\).\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-10/Verteilungen-Quiz-10.html#answerlist",
    "href": "posts/Verteilungen-Quiz-10/Verteilungen-Quiz-10.html#answerlist",
    "title": "Verteilungen-Quiz-10",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-10/Verteilungen-Quiz-10.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-10/Verteilungen-Quiz-10.html#answerlist-1",
    "title": "Verteilungen-Quiz-10",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/boxplots-de1a/boxplots-de1a.html",
    "href": "posts/boxplots-de1a/boxplots-de1a.html",
    "title": "boxplots-de1a",
    "section": "",
    "text": "laut zwei Stichproben (A und B) mithilfe zweier Boxplots dargestellt. Welche der folgenden Aussagen ist korrekt?\nHinweis: Die Aussagen sind entweder eindeutig richtig oder eindeutig falsch.\n\n\n\n\n\n\n\n\n\n\n\n\nDie zentrale Tendenz der Verteilungen ist (etwa) identisch.\nBeide Verteilungen haben keine Ausreißer.\nDie Streuung in Stichprobe A ist deutlich größer als in Stichprobe B.\nDie Schiefe der beiden Stichproben ist ähnlich.\nVerteilung A ist rechtsschief."
  },
  {
    "objectID": "posts/boxplots-de1a/boxplots-de1a.html#answerlist",
    "href": "posts/boxplots-de1a/boxplots-de1a.html#answerlist",
    "title": "boxplots-de1a",
    "section": "",
    "text": "Die zentrale Tendenz der Verteilungen ist (etwa) identisch.\nBeide Verteilungen haben keine Ausreißer.\nDie Streuung in Stichprobe A ist deutlich größer als in Stichprobe B.\nDie Schiefe der beiden Stichproben ist ähnlich.\nVerteilung A ist rechtsschief."
  },
  {
    "objectID": "posts/boxplots-de1a/boxplots-de1a.html#answerlist-1",
    "href": "posts/boxplots-de1a/boxplots-de1a.html#answerlist-1",
    "title": "boxplots-de1a",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch. Verteilung B hat im Durchschnitt höhere Werte als Verteilung A.\nWahr. Beide Verteilungen haben keine Beobachtungen, die mehr als das 1.5-fache der Interquartilsabstands von der Box entfernt sind\nFalsch. Der Interquartilsabstand in Stichprobe A ist nicht deutlich größer als in B.\nFalsch. Die Schiefe der beiden Verteilungen ist unterschiedlich. Stichprobe A ist etwa symmtrisch. Stichprobe B ist rechtsschief.\nFalsch. Verteilung A ist etwa symmetrisch.\n\n\nCategories:\n\nvis\neda\nschoice"
  },
  {
    "objectID": "posts/wskt-quiz02/wskt-quiz02.html",
    "href": "posts/wskt-quiz02/wskt-quiz02.html",
    "title": "wskt-quiz02",
    "section": "",
    "text": "Gilt \\(Pr(AB) = Pr(A\\cap B) = Pr(A) \\cdot Pr(B)\\), so sind die Ereignisse \\(A\\) und \\(B\\) abhängig.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n\n\n\nFalsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz02/wskt-quiz02.html#answerlist",
    "href": "posts/wskt-quiz02/wskt-quiz02.html#answerlist",
    "title": "wskt-quiz02",
    "section": "",
    "text": "Falsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz02/wskt-quiz02.html#answerlist-1",
    "href": "posts/wskt-quiz02/wskt-quiz02.html#answerlist-1",
    "title": "wskt-quiz02",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch. Ja, denn es ist die Definition für stochastische Unabhängigkeit angegeben.\nWahr. Nein, denn es war nicht nach Abhängigkeit, sondern nach Unabhängigkeit gefragt.\n\n\nCategories:\n\nquiz\nprobability\nquiz1-qm2-ws23\nschoice"
  },
  {
    "objectID": "posts/vis-mariokart-variab/vis-mariokart-variab.html",
    "href": "posts/vis-mariokart-variab/vis-mariokart-variab.html",
    "title": "vis-mariokart-variab",
    "section": "",
    "text": "Aufgabe\nIm Datensatz mariokart:\nVisualisieren Sie die Streuung der Variablen total_pr.\n         \n\n\nLösung\nPakete starten:\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.6.0 (red = needs update)\n✔ bayestestR  0.13.1   ✔ correlation 0.8.4 \n✔ datawizard  0.9.0    ✔ effectsize  0.8.6 \n✔ insight     0.19.6   ✔ modelbased  0.8.6 \n✔ performance 0.10.8   ✔ parameters  0.21.3\n✔ report      0.5.7    ✖ see         0.8.0 \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`.\n\nlibrary(tidyverse)  # startet das Paket tidyverse\nlibrary(DataExplorer)  # Data-Vis\nlibrary(ggpubr)  # Data-Vis\n\n\nAttaching package: 'ggpubr'\n\n\nThe following objects are masked from 'package:datawizard':\n\n    mean_sd, median_mad\n\n\nDaten importieren:\n\nmariokart &lt;- data_read(\"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\")\n\nOder so:\n\ndata(mariokart, package = \"openintro\")  # aus dem Paket \"openintro\"\n\nDazu muss das Paket openintro auf Ihrem Computer installiert sein.\nVisualisieren:\nMit dataExplorer:\n\nmariokart %&gt;% \n  select(total_pr) %&gt;% \n  plot_density()  # oder \"plot_histogram()\"\n\n\n\n\n\n\n\n\nMit ggpubr:\n\ngghistogram(mariokart, x = \"total_pr\")\n\nWarning: Using `bins = 30` by default. Pick better value with the argument\n`bins`.\n\n\n\n\n\n\n\n\n\nMit ggplot:\n\nmariokart %&gt;% \n  ggplot(aes(x = total_pr)) +\n  geom_density()  # oder \"geom_histogram()\"\n\n\n\n\n\n\n\n\nFalls Sie Teile der R-Syntax nicht kennen: Im Zweifel einfach ignorieren :-)\n\nCategories:\n\ndatawrangling\neda\ntidyverse\nvis\nvariability\nstring"
  },
  {
    "objectID": "posts/Regression5/Regression5.html",
    "href": "posts/Regression5/Regression5.html",
    "title": "Regression5",
    "section": "",
    "text": "Aufgabe\nGegeben sei ein Datensatz mit fünf Prädiktoren, wobei Studierende die Beobachtungseinheit darstellen:\n\n\\(X_1\\): Quereinsteiger (0: nein, 1: ja)\n\\(X_2\\): Abitur-Durchschnitt (z-Wert)\n\\(X_3\\): Alter (z-Wert)\n\\(X_4\\): Interaktion von \\(X1\\) und \\(X2\\)\n\nDie vorherzusagende Variable (\\(Y\\); Kriterium) ist Gehalt nach Studienabschluss.\nWie lautet das Kriterium \\(y\\) für eine Person mit folgenden Werten:\n\n\\(x_1\\): 1\n\\(x_2\\): -0.37\n\\(x_3\\): 0.01\n\nBerechnen Sie dazu ein Regressionsmodell (Least Squares) anhand folgender Modellparameter:\n\n\\(\\beta_0: 60\\)\n\\(\\beta_1: 10\\)\n\\(\\beta_2: 3\\)\n\\(\\beta_3: 10\\)\n\\(\\beta_4: 1\\)\n\nGeben Sie als Antwort den vorhergesagten \\(Y\\)-Wert an!\nHinweis: Runden Sie auf zwei Dezimalstellen.\n         \n\n\nLösung\nDie Antwort lautet 68.62.\n\nCategories:\n\ndyn\nregression\nlm\nnum"
  },
  {
    "objectID": "posts/kausal-einfach/kausal-einfach.html",
    "href": "posts/kausal-einfach/kausal-einfach.html",
    "title": "kausal-einfach",
    "section": "",
    "text": "Eine Forscher:in aus Kalifornien entdeckt, dass Haiangriffe mit Eisverkauf korreliert sind: Haiangriffe treten gehäuft dann auf, wenn am Strand viel Eis verkauft wird. Dieser Zusammenhang ist zwar nicht perfekt, aber die Forscher:in findet in ihren Daten einen starken, sogar “signifikanten” Zusammenhang.\nWelche Schlüsse sind aus diesen Daten zu ziehen? Wählen Sie die Antwort, die am besten passt!\n\n\n\nDa Eisverkauf die UV und Haiangriff die AV ist, sind die Daten im Sinne eines Kausalschlusses “Eisverkauf führt (tendenziell) zu Haiangriffen” zu interpretieren. Natürlich gilt dies nur für linearen Zusammenhänge, da Korrelationen nur linearen Zusammenhänge identifizieren können.\nEs ist kein Kausalschluss möglich; eine Drittvariable könnte den Zusammenhang der beobachteten Variablen konfundieren.\nDie Daten (soweit bekannt bzw. oben aufgeführt sind) machen deutlich, dass es einen Zusammenhang zwischen den beiden Variablen gibt; folglich ist die eine Variable Ursache und die andere Wirkung. Die Daten lassen aber keine Aussage zu, welche der beiden Variablen Ursache und welche Wirkung ist.\nEs ist davon auszugehen, dass Haiangriff die Ursache ist und Eisverkauf die Wirkung.\nDa es sich nur um Beobachtungsdaten, nicht um Experimentaldaten handelt, ist keine Aussage möglich."
  },
  {
    "objectID": "posts/kausal-einfach/kausal-einfach.html#answerlist",
    "href": "posts/kausal-einfach/kausal-einfach.html#answerlist",
    "title": "kausal-einfach",
    "section": "",
    "text": "Da Eisverkauf die UV und Haiangriff die AV ist, sind die Daten im Sinne eines Kausalschlusses “Eisverkauf führt (tendenziell) zu Haiangriffen” zu interpretieren. Natürlich gilt dies nur für linearen Zusammenhänge, da Korrelationen nur linearen Zusammenhänge identifizieren können.\nEs ist kein Kausalschluss möglich; eine Drittvariable könnte den Zusammenhang der beobachteten Variablen konfundieren.\nDie Daten (soweit bekannt bzw. oben aufgeführt sind) machen deutlich, dass es einen Zusammenhang zwischen den beiden Variablen gibt; folglich ist die eine Variable Ursache und die andere Wirkung. Die Daten lassen aber keine Aussage zu, welche der beiden Variablen Ursache und welche Wirkung ist.\nEs ist davon auszugehen, dass Haiangriff die Ursache ist und Eisverkauf die Wirkung.\nDa es sich nur um Beobachtungsdaten, nicht um Experimentaldaten handelt, ist keine Aussage möglich."
  },
  {
    "objectID": "posts/kausal-einfach/kausal-einfach.html#answerlist-1",
    "href": "posts/kausal-einfach/kausal-einfach.html#answerlist-1",
    "title": "kausal-einfach",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/korr02/korr02.html",
    "href": "posts/korr02/korr02.html",
    "title": "korr02",
    "section": "",
    "text": "Aufgabe\nWelcher Korrelationswert (Pearson) beschreibt die Korrelation in den Daten am besten?\n\n.9\n.4\n0\n-0.4\n-0.9\n\n\n\n\n\n\n\n\n\n\n         \n\n\nLösung\nDie Korrelation in der zugehörigen (bivariaten) Population beträgt -0.9.\nIn der Stichprobe kann der zugehörige Wert (etwas abweichen).\nDas ist genauso, wie wenn man sagt, dass der “mittlere deutsche Mann” 1,80m groß sei, aber wenn Sie eine Stichprobe ziehen, muss der Mittelwert ja auch nicht (notwendigerweise) exakt bei 1,80m liegen.\n\n\n\n\n\n\n\n\n\n\nCategories:\n\ndyn\neda\nassociation\nnum"
  },
  {
    "objectID": "posts/tidymodels-ames-04/tidymodels-ames-04.html",
    "href": "posts/tidymodels-ames-04/tidymodels-ames-04.html",
    "title": "tidymodels-ames-04",
    "section": "",
    "text": "Aufgabe\nBerechnen Sie ein lineares Modell mit tidymodels und zwar anhand des ames Datensatzes.\nModellgleichung: Sale_Price ~ Gr_Liv_Area, data = ames.\nBerechnen Sie ein multiplikatives (exponenzielles) Modell.\nGesucht ist R-Quadrat als Maß für die Modellgüte im TEST-Sample.\nHinweise:\n\nFixieren Sie die Zufallszahlen auf den Startwert 42.\nVerwenden Sie die Funktion last_fit.\n\n         \n\n\nLösung\n\nlibrary(tidymodels)\ndata(ames)\n\nMultiplikatives Modell:\n\names &lt;- \n  ames %&gt;% \n  mutate(Sale_Price = log10(Sale_Price)) %&gt;% \n  select(Sale_Price, Gr_Liv_Area)\n\nNicht vergessen: AV-Transformation in beiden Samples!\nDatensatz aufteilen:\n\nset.seed(42)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\nModell definieren:\n\nm1 &lt;-\n  linear_reg() # engine ist \"lm\" im Default\n\nRezept definieren:\n\nrec1 &lt;- \n  recipe(Sale_Price ~ Gr_Liv_Area, data = ames) \n\nVorhersagen mit last_fit:\n\nfit1_last &lt;- last_fit(object = m1, preprocessor = rec1, split = ames_split)  \nfit1_last\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits             id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;             &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [2342/588]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n\nWir bekommen ein Objekt, in dem Fit, Modellgüte, Vorhersagen und Hinweise enthalten sind.\nOhne Rezept lässt sich last_fit nicht anwenden.\nVorhersagen:\n\nfit1_last %&gt;% collect_predictions() %&gt;% \n  head()\n\n# A tibble: 6 × 5\n  id               .pred  .row Sale_Price .config             \n  &lt;chr&gt;            &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;               \n1 train/test split  5.07     2       5.02 Preprocessor1_Model1\n2 train/test split  5.18     3       5.24 Preprocessor1_Model1\n3 train/test split  5.31    18       5.60 Preprocessor1_Model1\n4 train/test split  5.11    26       5.15 Preprocessor1_Model1\n5 train/test split  5.18    29       5.26 Preprocessor1_Model1\n6 train/test split  5.10    30       4.98 Preprocessor1_Model1\n\n\nModellgüte im Test-Sample:\n\nfit1_last %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.118 Preprocessor1_Model1\n2 rsq     standard       0.517 Preprocessor1_Model1\n\n\nR-Quadrat:\n\nsol &lt;- 0.517\nsol\n\n[1] 0.517\n\n\n\nCategories:\n\nds1\ntidymodels\nprediction\nyacsda\nstatlearning\nnum"
  },
  {
    "objectID": "posts/adjustieren1a/adjustieren1a.html",
    "href": "posts/adjustieren1a/adjustieren1a.html",
    "title": "adjustieren1a",
    "section": "",
    "text": "Aufgabe\nBetrachten Sie folgendes Modell, das den Zusammenhang von PS-Zahl und Spritverbrauch untersucht (Datensatz mtcars).\nAber zuerst zentrieren wir den metrischen Prädiktor hp, um den Achsenabschnitt besser interpretieren zu können.\n\nlibrary(tidyverse)\nlibrary(easystats)\ndata(mtcars)\n\nmtcars &lt;-\n  mtcars %&gt;% \n  mutate(hp_z = hp - mean(hp))\n\n\nlm1 &lt;- lm(mpg ~ hp_z, data = mtcars)\nparameters(lm1)\n\nParameter   | Coefficient |   SE |         95% CI | t(30) |      p\n------------------------------------------------------------------\n(Intercept) |       20.09 | 0.68 | [18.70, 21.49] | 29.42 | &lt; .001\nhp z        |       -0.07 | 0.01 | [-0.09, -0.05] | -6.74 | &lt; .001\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\nJetzt können wir aus dem Achsenabschnitt (Intercept) herauslesen, dass ein Auto mit hp_z = 0 - also mit mittlerer PS-Zahl - vielleicht gut 20 Meilen weit mit einer Gallone Sprit kommt.\nZur Verdeutlichung ein Diagramm zum Modell:\n\nestimate_relation(lm1) |&gt; plot()\n\n\n\n\n\n\n\n\nAdjustieren Sie im Modell die PS-Zahl um die Art des Schaltgetriebes (am), so dass das neue Modell den statistischen Effekt (nicht notwendig auch kausal) der PS-Zahl bereinigt bzw. unabhängig von der Art des Schaltgetriebes widerspiegelt!\nGeben Sie den Punktschätzer für den Effekt von am in diesem Modell an!\nHinweise:\n\nam=0 ist ein Auto mit Automatikgetriebe.\nWir gehen davon aus, dass der Regressionseffekt gleich stark ist auf allen (beiden) Stufen von am. M.a.W.: Es liegt kein Interaktionseffekt vor.\nBeachten Sie die üblichen Hinweise des Datenwerks.\nNutzen Sie lm, um das Modell zu berechnen.\n\n         \n\n\nLösung\n\nlm2 &lt;- lm(mpg ~ hp_z + am, data = mtcars)\nparameters(lm2)\n\nParameter   | Coefficient |       SE |         95% CI | t(29) |      p\n----------------------------------------------------------------------\n(Intercept) |       17.95 |     0.68 | [16.56, 19.33] | 26.55 | &lt; .001\nhp z        |       -0.06 | 7.86e-03 | [-0.07, -0.04] | -7.50 | &lt; .001\nam          |        5.28 |     1.08 | [ 3.07,  7.48] |  4.89 | &lt; .001\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\nDie Spalte Coefficient gibt den mittleren geschätzten Wert für den jeweiligen Koeffizienten an, also den Schätzwert zum Koeffizienten.\nDie Koeffizienten zeigen, dass der Achsenabschnitt für Autos mit Automatikgetriebe um etwa 5 Meilen geringer ist als für Autos mit manueller Schaltung: Ein durchschnittliches Auto mit manueller Schaltung kommt also etwa 5 Meilen weiter als ein Auto mit Automatikschaltung, glaubt unser Modell.\n\nestimate_relation(lm2) |&gt; plot()\n\n\n\n\n\n\n\n\nam wird als numerische Variable erkannt.\nDas können wir so ändern:\n\nmtcars &lt;- \n  mtcars |&gt; \n  mutate(am = factor(am))\n\n\nlm3 &lt;- lm(mpg ~ hp_z + am, data = mtcars)\nparameters(lm3)\n\nParameter   | Coefficient |       SE |         95% CI | t(29) |      p\n----------------------------------------------------------------------\n(Intercept) |       17.95 |     0.68 | [16.56, 19.33] | 26.55 | &lt; .001\nhp z        |       -0.06 | 7.86e-03 | [-0.07, -0.04] | -7.50 | &lt; .001\nam [1]      |        5.28 |     1.08 | [ 3.07,  7.48] |  4.89 | &lt; .001\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\nDie Koeffizienten bleiben gleich.\nLösung: 5.28.\nAber im Diagramm wird am jetzt als Faktor-Variable erkannt, was Sinn macht:\n\nestimate_relation(lm3) |&gt; plot()\n\n\n\n\n\n\n\n\nMan könnte hier noch einen Interaktionseffekt ergänzen.\n\nCategories:\n\nregression\n‘2023’\nstring"
  },
  {
    "objectID": "posts/within-design-analysis1/within-design-analysis1.html",
    "href": "posts/within-design-analysis1/within-design-analysis1.html",
    "title": "within-design-analysis1",
    "section": "",
    "text": "Analysieren Sie die Veränderung in einem längsschnittlichen Experiment (Within-Design).\nIm Zuge des Experiments durchliefen alle \\(n\\) Versuchspersonen 3 Bedingungen. Entsprechend liegen für jede Versuchsperson 3 Messungen vor (y1, y2, y3). Anders gesagt gab es drei Messzeitpunkte (t1, t2, t3), zu denen die abhängige Variable (y) jeweils gemessen wurde Die Messung bestand bei jeder Bedingung aus 10 Items, wobei die Wahrscheinlichkeit, ein Item zu lösen zwischen den Bedingungen unterschiedlich war.\nPrüfen Sie die folgende Hypothesen:\n\n\\(y_{t2} - y_{t1} &gt; 0\\)\n\\(y_{t3} - y_{t2} &gt; 0\\)\n\nGehen Sie von folgenden (hier einfach simulierten) Daten aus:\n\nn &lt;- 40  # Anzahl Versuchspersonen\nn_items &lt;- 10  # Anzahl Items pro Messung von y\nprob &lt;- c(.5, .7, .9)  # Lösungswahrscheinlichkeit pro Messzeitpunkt (t1, t2, t3)\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nset.seed(42)\nd &lt;-\n  tibble(id = 1:n,\n         y1 = rbinom(n = n, size = n_items, prob = prob[1]),\n         y2 = rbinom(n = n, size = n_items, prob = prob[2]),\n         y3 = rbinom(n = n, size = n_items, prob = prob[3]),\n         g = c(rep(times = n/2, x = \"A\"), rep(times = n/2, x = \"B\"))\n         )\nhead(d)\n\n# A tibble: 6 × 5\n     id    y1    y2    y3 g    \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;\n1     1     7     8     9 A    \n2     2     7     7    10 A    \n3     3     4     9     9 A    \n4     4     7     4     9 A    \n5     5     6     7     8 A    \n6     6     5     4     9 A    \n\n\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks."
  },
  {
    "objectID": "posts/within-design-analysis1/within-design-analysis1.html#pakete-starten",
    "href": "posts/within-design-analysis1/within-design-analysis1.html#pakete-starten",
    "title": "within-design-analysis1",
    "section": "Pakete starten",
    "text": "Pakete starten\n\nlibrary(tidyverse)\nlibrary(rstanarm)\nlibrary(easystats)"
  },
  {
    "objectID": "posts/within-design-analysis1/within-design-analysis1.html#daten-aufbereiten",
    "href": "posts/within-design-analysis1/within-design-analysis1.html#daten-aufbereiten",
    "title": "within-design-analysis1",
    "section": "Daten aufbereiten",
    "text": "Daten aufbereiten\nUm die Daten (besser) analysieren zu können, formen wir sie ins “lange Format” um.\n\nd_long &lt;-\n  d %&gt;% \n  pivot_longer(cols = c(y1, y2, y3), names_to = \"time\", values_to = \"y\")"
  },
  {
    "objectID": "posts/within-design-analysis1/within-design-analysis1.html#daten-zusammenfassen",
    "href": "posts/within-design-analysis1/within-design-analysis1.html#daten-zusammenfassen",
    "title": "within-design-analysis1",
    "section": "Daten zusammenfassen",
    "text": "Daten zusammenfassen\n\nd_long %&gt;% \n  group_by(time) %&gt;% \n  summarise(y_mean = mean(y),\n            y_sd = sd(y)) %&gt;% \n  mutate(delta = y_mean - lag(y_mean))\n\n# A tibble: 3 × 4\n  time  y_mean  y_sd delta\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 y1      5.45 1.74  NA   \n2 y2      7.05 1.55   1.6 \n3 y3      8.98 0.891  1.92"
  },
  {
    "objectID": "posts/within-design-analysis1/within-design-analysis1.html#daten-visualisieren",
    "href": "posts/within-design-analysis1/within-design-analysis1.html#daten-visualisieren",
    "title": "within-design-analysis1",
    "section": "Daten visualisieren",
    "text": "Daten visualisieren\n\nd_long %&gt;% \n  ggplot(aes(x = time, y = y)) +\n  geom_jitter(width = .1) +\n  stat_summary(fun.y = mean, geom = \"point\", color = \"red\", size = 3) +\n  stat_summary(fun.y = mean, geom = \"line\", color = \"red\", linewidth = 1, group = 1) \n\nWarning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0.\nℹ Please use the `fun` argument instead.\n\n\n\n\n\n\n\n\n\nMan sieht, dass der Wert von Y steigt von t1 zu t2 und genauso von t2 zu t3."
  },
  {
    "objectID": "posts/within-design-analysis1/within-design-analysis1.html#daten-transformieren",
    "href": "posts/within-design-analysis1/within-design-analysis1.html#daten-transformieren",
    "title": "within-design-analysis1",
    "section": "Daten transformieren",
    "text": "Daten transformieren\nMan kann auch die Veränderung (das “delta”) zwischen den Messzeitpunkten berechnen, um dann zu prüfen, ob dieses delta dann positiv ist.\n\nd2 &lt;-\n  d %&gt;% \n  mutate(t2mt1 = y2 - y1,  # t2 *m*inus t1\n         t3mt2 = y3 - y2,  # t3 minus t2\n         t3mt1 = y3 - y1)  # t3 mind t1, die Gesamtveränderung von \"Anfang\" zu \"Ende\"\n\n\nd2_long &lt;- \n  d2 %&gt;% \n  pivot_longer(cols = c(t2mt1, t3mt2, t3mt1), names_to = \"time\", values_to = \"delta\")"
  },
  {
    "objectID": "posts/within-design-analysis1/within-design-analysis1.html#daten-zusammenfassen-1",
    "href": "posts/within-design-analysis1/within-design-analysis1.html#daten-zusammenfassen-1",
    "title": "within-design-analysis1",
    "section": "Daten zusammenfassen",
    "text": "Daten zusammenfassen\n\nd2_long %&gt;% \n  group_by(time) %&gt;% \n  summarise(delta_mean = mean(delta),\n            delta_sd = sd(delta)) %&gt;% \n  mutate(delta2 = delta_mean - lag(delta_mean))\n\n# A tibble: 3 × 4\n  time  delta_mean delta_sd delta2\n  &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 t2mt1       1.6      2.63  NA   \n2 t3mt1       3.52     1.78   1.92\n3 t3mt2       1.92     1.97  -1.6 \n\n\nWie man sieht, ist das Delta t2mt1 positiv, im Mittelwert steigt also y. Gleiches gilt für t3mt2 und t3mt1.\n\nd2_long %&gt;% \n  filter(time != \"t3mt1\") %&gt;% \n  ggplot(aes(x = time, y = delta)) +\n  geom_jitter(width = .1) +\n  stat_summary(fun.y = mean, geom = \"point\", color = \"red\", size = 3) +\n  stat_summary(fun.y = mean, geom = \"line\", color = \"red\", linewidth = 1, group = 1) \n\n\n\n\n\n\n\n\nDie Veränderungen von t1 zu t2 (t2mt1) sind ähnlich zu denen von t2 zu t3 (t3mt2)."
  },
  {
    "objectID": "posts/within-design-analysis1/within-design-analysis1.html#modell-t2mt1",
    "href": "posts/within-design-analysis1/within-design-analysis1.html#modell-t2mt1",
    "title": "within-design-analysis1",
    "section": "Modell t2mt1",
    "text": "Modell t2mt1\nDas entsprechende Regressionsmodell für t2mt2 liefert einfach den Mittelwert des Deltas.\n\nm1_t2mt1 &lt;- lm(t2mt1 ~ 1, data = d2)\ncoef(m1_t2mt1)\n\n(Intercept) \n        1.6 \n\n\nEin Bayes-Modell hat den Vorteil, dass es uns einfach zu interpretierende Inferenzstatistik gibt.\n\nm1_bayes &lt;- stan_glm(t2mt1 ~ 1, data = d2, refresh = 0)\ncoef(m1_bayes)\n\n(Intercept) \n   1.602565 \n\n\n\nparameters(m1_bayes)\n\nParameter   | Median |       95% CI |     pd |  Rhat |     ESS |                 Prior\n--------------------------------------------------------------------------------------\n(Intercept) |   1.60 | [0.77, 2.47] | 99.95% | 1.000 | 2449.00 | Normal (1.60 +- 6.57)\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a MCMC distribution approximation.\n\n\nMit einer Wahrscheinlichkeit von 100% ist das Delta positiv (laut m1_bayes). Das kann man aus dem Koeffizienten pd ablesen (probability of direction)."
  },
  {
    "objectID": "posts/within-design-analysis1/within-design-analysis1.html#m_t3mt2",
    "href": "posts/within-design-analysis1/within-design-analysis1.html#m_t3mt2",
    "title": "within-design-analysis1",
    "section": "m_t3mt2",
    "text": "m_t3mt2\n\nm_t3mt2_bayes &lt;- stan_glm(t3mt2 ~ 1, data = d2, refresh = 0)\nparameters(m_t3mt2_bayes)\n\nParameter   | Median |       95% CI |   pd |  Rhat |     ESS |                 Prior\n------------------------------------------------------------------------------------\n(Intercept) |   1.93 | [1.32, 2.53] | 100% | 1.001 | 2753.00 | Normal (1.93 +- 4.92)\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a MCMC distribution approximation.\n\n\nAuch hier ist das Modell sehr meinungsstark: Mit einer Wahrscheinlichkeit von 100% ist der Koeffizient (Veränderung von t2 zu t3) positiv.\n\nCategories:\n\nregression\nwithin-design\nresearchdesign\nfopro\nstring"
  },
  {
    "objectID": "posts/tidymodels-ames-03/tidymodels-ames-03.html",
    "href": "posts/tidymodels-ames-03/tidymodels-ames-03.html",
    "title": "tidymodels-ames-03",
    "section": "",
    "text": "Aufgabe\nBerechnen Sie ein lineares Modell mit tidymodels und zwar anhand des ames Datensatzes.\nModellgleichung: Sale_Price ~ Gr_Liv_Area, data = ames.\nBerechnen Sie ein multiplikatives (exponenzielles) Modell.\nRücktransformieren Sie die Log-Werte in “Roh-Dollar”.\nGeben Sie den mittleren Vorhersagewert an als Lösung.\nHinweise:\n\nFixieren Sie die Zufallszahlen auf den Startwert 42.\n\n         \n\n\nLösung\n\nlibrary(tidymodels)\ndata(ames)\n\nMultiplikatives Modell:\n\names &lt;- \n  ames %&gt;% \n  mutate(Sale_Price = log10(Sale_Price)) %&gt;% \n  select(Sale_Price, Gr_Liv_Area)\n\nNicht vergessen: AV-Transformation in beiden Samples!\nDatensatz aufteilen:\n\nset.seed(42)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\nModell definieren:\n\nm1 &lt;-\n  linear_reg() # engine ist \"lm\" im Default\n\nModell fitten:\n\nfit1 &lt;-\n  m1 %&gt;% \n  fit(Sale_Price ~ Gr_Liv_Area, data = ames)\n\n\nfit1 %&gt;% pluck(\"fit\") \n\n\nCall:\nstats::lm(formula = Sale_Price ~ Gr_Liv_Area, data = data)\n\nCoefficients:\n(Intercept)  Gr_Liv_Area  \n  4.8552133    0.0002437  \n\n\nModellgüte im Train-Sample:\n\nfit1_performance &lt;-\n  fit1 %&gt;% \n  extract_fit_engine()  # identisch zu pluck(\"fit\")\n\nModellgüte im Train-Sample:\n\nfit1_performance %&gt;% summary()\n\n\nCall:\nstats::lm(formula = Sale_Price ~ Gr_Liv_Area, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.02587 -0.06577  0.01342  0.07202  0.39231 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 4.855e+00  7.355e-03  660.12   &lt;2e-16 ***\nGr_Liv_Area 2.437e-04  4.648e-06   52.43   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1271 on 2928 degrees of freedom\nMultiple R-squared:  0.4842,    Adjusted R-squared:  0.484 \nF-statistic:  2749 on 1 and 2928 DF,  p-value: &lt; 2.2e-16\n\n\nR-Quadrat via easystats:\n\nlibrary(easystats)\nfit1_performance %&gt;% r2()  # rmse()\n\n# R2 for Linear Regression\n       R2: 0.484\n  adj. R2: 0.484\n\n\n\ntidy(fit1_performance)  # ähnlich zu parameters()\n\n# A tibble: 2 × 5\n  term        estimate  std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) 4.86     0.00736        660.        0\n2 Gr_Liv_Area 0.000244 0.00000465      52.4       0\n\n\nVorhersagen im Test-Sample:\n\npreds &lt;- predict(fit1, new_data = ames_test)  # liefert TABELLE (tibble) zurück\nhead(preds)\n\n# A tibble: 6 × 1\n  .pred\n  &lt;dbl&gt;\n1  5.07\n2  5.18\n3  5.31\n4  5.11\n5  5.18\n6  5.10\n\n\npreds ist ein Tibble, also müssen wir noch die Spalte .pred. herausziehen, z.B. mit pluck(preds, \".pred\"):\n\npreds_vec &lt;- preds$.pred\n\n\names_test2 &lt;-\n  ames_test %&gt;% \n  mutate(preds = pluck(preds, \".pred\"),  # pluck aus der Tabelle rausziehen\n         .pred = preds_vec)  # oder  mit dem Dollar-Operator\n\nhead(ames_test2)\n\n# A tibble: 6 × 4\n  Sale_Price Gr_Liv_Area preds .pred\n       &lt;dbl&gt;       &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1       5.02         896  5.07  5.07\n2       5.24        1329  5.18  5.18\n3       5.60        1856  5.31  5.31\n4       5.15        1056  5.11  5.11\n5       5.26        1337  5.18  5.18\n6       4.98         987  5.10  5.10\n\n\nOder mit unnest:\n\names_test2 &lt;-\n  ames_test %&gt;% \n  mutate(preds = preds) %&gt;% \n  unnest(preds) # Listenspalte \"entschachteln\"\n\nhead(ames_test2)\n\n# A tibble: 6 × 3\n  Sale_Price Gr_Liv_Area .pred\n       &lt;dbl&gt;       &lt;int&gt; &lt;dbl&gt;\n1       5.02         896  5.07\n2       5.24        1329  5.18\n3       5.60        1856  5.31\n4       5.15        1056  5.11\n5       5.26        1337  5.18\n6       4.98         987  5.10\n\n\nOder wir binden einfach die Spalte an den Tibble:\n\names_test2 &lt;-\n  ames_test %&gt;% \n  bind_cols(preds = preds)  # nimmt Tabelle und bindet die Spalten dieser Tabelle an eine Tabelle\n\nhead(ames_test2)\n\n# A tibble: 6 × 3\n  Sale_Price Gr_Liv_Area .pred\n       &lt;dbl&gt;       &lt;int&gt; &lt;dbl&gt;\n1       5.02         896  5.07\n2       5.24        1329  5.18\n3       5.60        1856  5.31\n4       5.15        1056  5.11\n5       5.26        1337  5.18\n6       4.98         987  5.10\n\n\nModellgüte im Test-Sample:\n\nrsq(ames_test2,\n    truth = Sale_Price,\n    estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.517\n\n\n\nsol &lt;- 0.51679\n\nZur Interpretation von Log10-Werten\n\n5e5\n\n[1] 5e+05\n\n5*10^5 - 500000\n\n[1] 0\n\n\nRücktransformation (ohne Bias-Korrektur):\n\names_test2 &lt;-\n  ames_test2 %&gt;% \n  mutate(pred_raw = 10^(.pred))\n\nMittelwert der Vorhersagen:\n\nsol &lt;- mean(ames_test2$pred_raw)\nsol\n\n[1] 175973.8\n\n\n\nCategories:\n\nds1\ntidymodels\nprediction\nyacsda\nstatlearning\nnum"
  },
  {
    "objectID": "posts/wrangle9/wrangle9.html",
    "href": "posts/wrangle9/wrangle9.html",
    "title": "wrangle9",
    "section": "",
    "text": "Aufgabe\nUm Variablen in einem Datensatz anzulegen oder zu verändern, nutzt man mutate() im tidyverse. mutate() ist nützlich für vektorielles Rechnen. In diesem Zusammenhang: Was ist das Ergebnis folgenden Ausdrucks?\n\nsum(1:3 + 1:3)\n\n         \n\n\nLösung\n12\n\nsum(1:3 + 1:3)\n\n[1] 12\n\n\n\nCategories:\n\neda\n‘2023’\nnum"
  },
  {
    "objectID": "posts/ReThink3m1/ReThink3m1.html",
    "href": "posts/ReThink3m1/ReThink3m1.html",
    "title": "ReThink3m1",
    "section": "",
    "text": "Aufgabe\nNehmen wir an, wir haben 8 (Wasser-)“Treffer” (\\(W=8\\)) bei 15 Würfen (\\(N=15\\)) erhalten (wieder im Globusversuch). Gehen Sie wieder von einer “flachen”, also gleichverteilten, Priori-Verteilung aus.\n👉 Aufgabe: Berechnen Sie die Posteriori-Verteilung und visualisieren Sie sie. Nutzen Sie die Gittermethode.\nQuelle: McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Aufl.). Taylor and Francis, CRC Press.\n         \n\n\nLösung\n\np_grid &lt;- seq(from = 0, to = 1, length.out = 100)\nprior &lt;- rep(1, 100)\nlikelihood &lt;- dbinom(8, size = 15, prob = p_grid)\npost_unstand &lt;- likelihood * prior\nposterior &lt;- post_unstand / sum(post_unstand)\n\nd &lt;- tibble(p = p_grid, posterior = posterior)\n\nJetzt visualisieren; mit ggplot2:\n\nlibrary(tidyverse)\n d %&gt;%\n  ggplot(aes(x = p, y = posterior)) +\n # geom_point() +\n  geom_line() +\n  labs(x = \"Anteil Wasser (p)\", y = \"Posterior Density\")\n\n\n\n\n\n\n\n\nOder mit ggpubr:\n\nlibrary(ggpubr)\n\nggline(d, x = \"p\", y = \"posterior\",\n       plot_type = \"l\")\n\n\n\n\n\n\n\n\nQuelle\n\nCategories:\n\nbayes\nppv\nprobability\nstring"
  },
  {
    "objectID": "posts/Shrinkage1/Shrinkage1.html",
    "href": "posts/Shrinkage1/Shrinkage1.html",
    "title": "Shrinkage1",
    "section": "",
    "text": "Shrinkage (Penalisierung) ist eine Erweiterung der klassischen Linearen Modelle. Welche Aussage dazu ist richtig?\n\n\n\nDie Modellkoeffizienten von penalisierten linearen Modelle können wie normale lineare Modelle interpretiert werden.\nDie L2-Norm der Penalisierung kann zur Auswahl von Prädiktoren herangezogen werden.\nDie L1-Norm der Penalisierung wird auch als Ridge-Regression bezeichnet.\nDie Ridge-Regression ist ein Algorithmus, der eine Größe minimiert, in der ein Strafterm zum üblichen Least-Square-Termin hinzuaddiert wird, wobei dieser Strafterm die (mit \\(\\lambda\\)) gewichtete Summe der Absolutwerte der \\(\\beta\\)-Koeffizienten beschreibt.\nDie Lasso-Regression liefert im Vergleich zur Ridge-Regression tendenziell bessere Ergebnisse, wenn das Kriterium eine Funktion von vielen Prädiktoren ist, deren Koeffizienten jeweils etwa gleich stark sind."
  },
  {
    "objectID": "posts/Shrinkage1/Shrinkage1.html#answerlist",
    "href": "posts/Shrinkage1/Shrinkage1.html#answerlist",
    "title": "Shrinkage1",
    "section": "",
    "text": "Die Modellkoeffizienten von penalisierten linearen Modelle können wie normale lineare Modelle interpretiert werden.\nDie L2-Norm der Penalisierung kann zur Auswahl von Prädiktoren herangezogen werden.\nDie L1-Norm der Penalisierung wird auch als Ridge-Regression bezeichnet.\nDie Ridge-Regression ist ein Algorithmus, der eine Größe minimiert, in der ein Strafterm zum üblichen Least-Square-Termin hinzuaddiert wird, wobei dieser Strafterm die (mit \\(\\lambda\\)) gewichtete Summe der Absolutwerte der \\(\\beta\\)-Koeffizienten beschreibt.\nDie Lasso-Regression liefert im Vergleich zur Ridge-Regression tendenziell bessere Ergebnisse, wenn das Kriterium eine Funktion von vielen Prädiktoren ist, deren Koeffizienten jeweils etwa gleich stark sind."
  },
  {
    "objectID": "posts/Shrinkage1/Shrinkage1.html#answerlist-1",
    "href": "posts/Shrinkage1/Shrinkage1.html#answerlist-1",
    "title": "Shrinkage1",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\nschoice"
  },
  {
    "objectID": "posts/mtcars-abhaengig_var3/mtcars-abhaengig_var3.html",
    "href": "posts/mtcars-abhaengig_var3/mtcars-abhaengig_var3.html",
    "title": "mtcars-abhaengig_var3",
    "section": "",
    "text": "Aufgabe\nIm Folgenden ist der Datensatz mtcars zu analysieren. Er ist unter dieser Quelle erhältlich.\nDer Datensatz ist z.B. als CSV-Datei von dieser Webseite abrufbar.\nHilfe zum Datensatz ist via help(\"name_des_datensatzes\") oder auf dieser Webseite abrufbar.\nOb die Variable hp (UV; Ereignis \\(A\\)) und Spritverbrauch (mpg; AV; Ereignis \\(B\\)) wohl voneinander abhängig sind? Was meinen Sie? Was ist Ihre Einschätzung dazu? Vermutlich haben Sie ein (wenn vielleicht auch implizites) Vorab-Wissen zu dieser Frage. Lassen wir dieses Vorab-Wissen aber einmal außen vor und schauen uns rein Daten dazu an. Vereinfachen wir die Frage etwas, indem wir beide Variablen am Mittelwert aufteilen: Wenn eine Beobachtung (d.h. ein Auto) einen Wert in der jeweiligen Variablen höchstens so groß wie der Mittelwert der Variable aufweist, geben wir der Beobachtung der Wert 0, ansonsten den Wert 1.\nDie resultierenden binären Variablen nennen wir av_high bzw. uv_high (im schönsten Denglisch).\nBerechnen Sie: \\(Pr(\\neg \\text{uvhigh} \\, | \\,  \\text{avhigh})\\)\nHinweise:\n\nDas “Ellbogen-Zeichen” \\(\\neg\\) kennzeichnet eine logische Negierung (das Gegenteil).\nDie angegebene Wahrscheinlichkeit ist eine bedingte Wahrscheinlichkeit.\nWeitere Hinweise\n\n         \n\n\nLösung\nDieser Prädiktor wurde als UV bestimmt:\n\n\n[1] \"hp\"\n\n\nSchauen wir zuerst mal in den Datensatz:\n\nd %&gt;% \n  select(mpg, one_of(pred_chosen)) %&gt;% \n  slice_head(n = 5)\n\n# A tibble: 5 × 2\n    mpg    hp\n  &lt;dbl&gt; &lt;dbl&gt;\n1  21     110\n2  21     110\n3  22.8    93\n4  21.4   110\n5  18.7   175\n\n\nDann berechnen wir die binären Variablen:\n\n# split by mean:\nd2 &lt;-\n  d %&gt;% \n  select(mpg, all_of(pred_chosen)) %&gt;% \n  mutate(av_high = case_when(\n    mpg &lt;= mean(mpg) ~ 0,\n    mpg &gt; mean(mpg) ~ 1\n  )) %&gt;% \n  select(-mpg) \n\n\n# split at mean:\nd2[1] &lt;- ifelse(d[[pred_chosen]] &lt; mean(d[[pred_chosen]]), 0, 1)\nnames(d2)[1] &lt;- \"uv_high\"\n\n# abstracted variables names:\npred_binary_name &lt;- names(d2)[1]\nav_binary_name &lt;- names(d2)[2]\n\nDann filtern wir die gesuchten Wahrscheinlichkeiten bzw. Anteile der AV:\n\nd3 &lt;-\n  d2 %&gt;% \n  filter(av_high == 1)\n\nav_high_sum &lt;- nrow(d3)\nav_high_sum\n\n[1] 14\n\n\nEs gibt also 14 Autos, die den oben gesuchten “hinteren Teil” der Bedingung erfüllen (av_high = TRUE).\nFiltern wir als nächstes nach dem “vorderen Teil” der gesuchten Wahrscheinlichkeit (was das gleiche ist wie ein Anteil in diesem Fall):\n\nd3a &lt;- \n  d3 %&gt;% \n  count(uv_high) \n\nBetrachten wir die nach av_high = TRUE gefilterte Häufigkeitstabelle:\n\n\n\n\n\n\n\n\nuv_high\nn\n\n\n\n\n0\n14\n\n\n1\n1\n\n\n\n\n\n\n\nUnd dann zählen wir die relativen Häufigkeiten der UV, und zwar für uv_high == FALSE:\n\nprop_not_uv_high_cond_av_high &lt;- \n  d3a %&gt;% \n  mutate(prop = n / av_high_sum) %&gt;% \n  filter(uv_high == 0) %&gt;% \n  pull(prop)\n\n\n\nsol &lt;- prop_not_uv_high_cond_av_high\nsol\n\n[1] 1\n\n\nDer gesuchte Wert beträgt also 1.\nVisualisieren wir noch die bedingten Wahrscheinlichkeiten, so könnte man die gesuchten Anteile einfach abzählen:\n\nd2 %&gt;% \n  mutate(across(everything(), factor)) %&gt;%  # factor() brauchn wir nur für die Farbfüllung\n  ggplot() +\n  aes_string(x = av_binary_name, fill = pred_binary_name) +\n  geom_bar() +\n  scale_y_continuous(breaks = 1:100) +\n  scale_x_discrete(drop = FALSE)  # zeigt Kategorien ohne Daten in der Legende an.\n\n\n\n\n\n\n\n\nSieht man in dem Diagramm nur eine Farbe (anstelle von zweien), so heißt das, dass es nur eine Gruppe gibt (und nicht zwei). Die Häufigkeit der nicht vorhandenen Gruppe ist demnach Null.\nAm besten, Sie führen den letzten Code Schritt für Schritt aus und schauen sich jeweils das Ergebnis an, das hilft beim Verstehen.\nAlternativ kann man sich die Häufigkeiten auch so ausgeben lassen:\n\n\n       av_high\nuv_high         0         1\n      0 0.1666667 1.0000000\n      1 0.8333333 0.0000000\n\n\n\nCategories:\n\ndyn\nprobability\nnum"
  },
  {
    "objectID": "posts/Kausale-Verben/Kausale-Verben.html",
    "href": "posts/Kausale-Verben/Kausale-Verben.html",
    "title": "Kausale-Verben",
    "section": "",
    "text": "Question"
  },
  {
    "objectID": "posts/Kausale-Verben/Kausale-Verben.html#answerlist",
    "href": "posts/Kausale-Verben/Kausale-Verben.html#answerlist",
    "title": "Kausale-Verben",
    "section": "Answerlist",
    "text": "Answerlist\n\nX hat einen Effekt auf Y\nX steht mit Y in Zusammenhang\nHohe Werte in X geht mit hohen Werten in Y einher (und umgekehrt)\nEs wird ein statistischer Effekt von X auf Y erwartet\nX reallokiert die Ressourcen in Y"
  },
  {
    "objectID": "posts/Kausale-Verben/Kausale-Verben.html#answerlist-1",
    "href": "posts/Kausale-Verben/Kausale-Verben.html#answerlist-1",
    "title": "Kausale-Verben",
    "section": "Answerlist",
    "text": "Answerlist\n\nRichtig\nFalsch\nFalsch\nFalsch\nRichtig\n\n\nCategories:\n\ncausal\nresearch-question\nmchoice"
  },
  {
    "objectID": "posts/Sim-Prior/Sim-Prior.html",
    "href": "posts/Sim-Prior/Sim-Prior.html",
    "title": "Sim-Prior",
    "section": "",
    "text": "Exercise\nGegeben dem folgenden Modell, simulieren Sie Daten aus der Prior-Verteilung (Priori-Prädiktiv-Verteilung).\nLikelihood: \\(h_i \\sim \\mathcal{N}(\\mu, \\sigma)\\)\nPrior für \\(\\mu\\): \\(\\mu \\sim \\mathcal{N}(0, 1)\\)\nPrior für \\(\\sigma\\): \\(\\sigma \\sim \\mathcal{U}(0, 10)\\)\n         \n\n\nSolution\n\nlibrary(tidyverse)\n\nn &lt;- 1e4\n\n\nsim &lt;- tibble(\n  mu = rnorm(n = n),  # Default-Werte sind mean=0, sd = 1\n  sigma = runif(n = n, 0, 10)) %&gt;%\n  mutate(\n    y = rnorm(n = n, mean = mu, sd = sigma))\n\nggplot(sim, aes(x = y)) +\n  geom_density() +\n  labs(x = \"y\", y = \"Dichte\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCategories:\n~"
  },
  {
    "objectID": "posts/smape/smape.html",
    "href": "posts/smape/smape.html",
    "title": "smape",
    "section": "",
    "text": "Zur Bemessung der (prädiktiven) Güte eines Modells existieren verschiedene Kennzahlen, auch abhängig davon, ob es sich um eine Regression oder Klassifikation handelt. Eine Kennzahl heißt SMAPE (Symmetric Mean Absolute Percentage). Welche Aussage zu dieser Kennzahl ist falsch?\n\n\n\nDie SMAPE-Werte von Variablen verschiedener Skalierung sind kaum zu vergleichen.\nSMAPE hat einen Wertebereich von 0 bis 1, d.h. SMAPE \\(\\in [0,1]\\).\nGrößere Werte zeigen schlechtere Vorhersagegüte an.\nDer SMAPE kann nicht negativ werden."
  },
  {
    "objectID": "posts/smape/smape.html#answerlist",
    "href": "posts/smape/smape.html#answerlist",
    "title": "smape",
    "section": "",
    "text": "Die SMAPE-Werte von Variablen verschiedener Skalierung sind kaum zu vergleichen.\nSMAPE hat einen Wertebereich von 0 bis 1, d.h. SMAPE \\(\\in [0,1]\\).\nGrößere Werte zeigen schlechtere Vorhersagegüte an.\nDer SMAPE kann nicht negativ werden."
  },
  {
    "objectID": "posts/Flex-vs-nichtflex-Methode2/Flex-vs-nichtflex-Methode2.html",
    "href": "posts/Flex-vs-nichtflex-Methode2/Flex-vs-nichtflex-Methode2.html",
    "title": "Flex-vs-nichtflex-Methode2",
    "section": "",
    "text": "Algorithmen des statistischen Lernens lassen sich unterteilen hinsichtlich ihrer Flexibilität; es gibt mehr bzw. weniger flexible Algorithmen.\nWelche der folgenden Aussagen ist in diesem Zusammenhang korrekt?\n\n\n\nBei kleiner Stichprobe und großer Zahl an Prädiktoren schneidet eine flexible Methode tendenziell schlechter ab als eine weniger flexible Methode aufgrund von höherem Overfitting.\nBei kleiner Stichprobe und großer Zahl an Prädiktoren schneidet eine flexible Methode tendenziell besser ab als eine weniger flexible Methode aufgrund von geringerem Overfitting.\nBei kleiner Stichprobe und großer Zahl an Prädiktoren schneidet eine flexible Methode tendenziell besser ab als eine weniger flexible Methode aufgrund von geringerer Verzerrung.\nBei kleiner Stichprobe und großer Zahl an Prädiktoren schneidet eine flexible Methode tendenziell schlechter ab als eine weniger flexible Methode aufgrund von höherer Verzerrung.\nBei kleiner Stichprobe und großer Zahl an Prädiktoren schneidet eine flexible Methode tendenziell schlechter ab als eine weniger flexible Methode aufgrund von höherer Verzerrung und von höherem Overfitting."
  },
  {
    "objectID": "posts/Flex-vs-nichtflex-Methode2/Flex-vs-nichtflex-Methode2.html#answerlist",
    "href": "posts/Flex-vs-nichtflex-Methode2/Flex-vs-nichtflex-Methode2.html#answerlist",
    "title": "Flex-vs-nichtflex-Methode2",
    "section": "",
    "text": "Bei kleiner Stichprobe und großer Zahl an Prädiktoren schneidet eine flexible Methode tendenziell schlechter ab als eine weniger flexible Methode aufgrund von höherem Overfitting.\nBei kleiner Stichprobe und großer Zahl an Prädiktoren schneidet eine flexible Methode tendenziell besser ab als eine weniger flexible Methode aufgrund von geringerem Overfitting.\nBei kleiner Stichprobe und großer Zahl an Prädiktoren schneidet eine flexible Methode tendenziell besser ab als eine weniger flexible Methode aufgrund von geringerer Verzerrung.\nBei kleiner Stichprobe und großer Zahl an Prädiktoren schneidet eine flexible Methode tendenziell schlechter ab als eine weniger flexible Methode aufgrund von höherer Verzerrung.\nBei kleiner Stichprobe und großer Zahl an Prädiktoren schneidet eine flexible Methode tendenziell schlechter ab als eine weniger flexible Methode aufgrund von höherer Verzerrung und von höherem Overfitting."
  },
  {
    "objectID": "posts/Flex-vs-nichtflex-Methode2/Flex-vs-nichtflex-Methode2.html#answerlist-1",
    "href": "posts/Flex-vs-nichtflex-Methode2/Flex-vs-nichtflex-Methode2.html#answerlist-1",
    "title": "Flex-vs-nichtflex-Methode2",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nstatlearning\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/iq02/iq02.html",
    "href": "posts/iq02/iq02.html",
    "title": "iq02",
    "section": "",
    "text": "Aufgabe\nIntelligenz wird häufig mittels einem IQ-Test ermittelt.\nWie groß ist die Wahrscheinlichkeit, dass die nächste Person, die Sie treffen, mindestens zwei Streuungseinheiten über dem Mittelwert liegt?\nHinweise:\n\nNutzen Sie Simulationsmethoden.\nGehen Sie von folgender IQ-Verteilung aus: \\(IQ \\sim N(100,15)\\).\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\nSimulieren Sie \\(n=10^3\\) Stichproben.\nNutzen Sie die Zahl 42 als Startwert für Ihre Zufallszahlen (um die Reproduzierbarkeit zu gewährleisten).\nWir wollen hier keine Post-Verteilung berechnen, sondern lediglich Werte simulieren.\n\n         \n\n\nLösung\n\nlibrary(tidyverse)\n\nWir simulieren die Daten:\n\nset.seed(42)  # Reproduzierbarkeit\nd &lt;- tibble(\n  id = 1:10^3,  # Der Doppelpunkt heißt \"bis\", also \"von 1 bis 10 hoch 3\". Diese Spalte ist nicht so wichtig.\n  iq = rnorm(n = 10^3, mean = 100, sd = 15))\n\nhead(d)  # Die ersten paar Zeilen\n\n# A tibble: 6 × 2\n     id    iq\n  &lt;int&gt; &lt;dbl&gt;\n1     1 121. \n2     2  91.5\n3     3 105. \n4     4 109. \n5     5 106. \n6     6  98.4\n\n\nDa \\(\\sigma=15\\), filtern wir ab 130, da 130 genau 2 SD-Einheiten über dem Mittelwert liegt: 130 - 2*15 = 100.\n\nd %&gt;% \n  count(iq &gt;= 130)\n\n# A tibble: 2 × 2\n  `iq &gt;= 130`     n\n  &lt;lgl&gt;       &lt;int&gt;\n1 FALSE         979\n2 TRUE           21\n\n\n21/1000 sind ca. 0.02.\nDie Wahrscheinlichkeit beträgt ca. 2%.\nJa, diese Aufgaben ist faktisch identische zur Aufgabe iq01. Darum ging es: Sie sollen erkennen, dass ein IQ-Wert von 130 das gleiche ist wie MW+2sd.\nÜbrigens: “Wie viele SD-Einheiten liegt der Wert von Beobachtung \\(i\\) über dem Mittelwert, \\(\\bar{X}\\) ?” ist die Frage, die der z-Wert beantwortet:\n\\(z_i = \\frac{x_i - \\bar{X}}{sd(x)}\\)\n\nCategories:\n\nprobability\nsimulation\nnormal-distribution\nnum"
  },
  {
    "objectID": "posts/iq05/iq05.html",
    "href": "posts/iq05/iq05.html",
    "title": "iq05",
    "section": "",
    "text": "Aufgabe\nIntelligenz wird häufig mittels einem IQ-Test ermittelt.\nWie intelligent muss man sein, um zu den schlauesten Promill der Bevölkerung zu gehören?\nHinweise:\n\nNutzen Sie Simulationsmethoden.\nGehen Sie von folgender IQ-Verteilung aus: \\(IQ \\sim N(100,15)\\)\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\nSimulieren Sie \\(n=10^5\\) Stichproben.\nNutzen Sie die Zahl 42 als Startwert für Ihre Zufallszahlen (um die Reproduzierbarkeit zu gewährleisten)\n\n         \n\n\nLösung\n\nlibrary(tidyverse)\n\nWir simulieren die Daten:\n\nset.seed(42)\nd &lt;- tibble(\n  id = 1:10^5,\n  iq = rnorm(n = 10^5, mean = 100, sd= 15))\n\nWir filtern die schlauesten 0,1 Prozent:\n\nd %&gt;% \n  summarise(iq_top_0komma1_prozent = quantile(iq, prob = .999))\n\n# A tibble: 1 × 1\n  iq_top_0komma1_prozent\n                   &lt;dbl&gt;\n1                   146.\n\n\nMan muss mindestens über einen IQ von ca. 145 verfügen.\nAchtung: Das sind immer Zahlen als der “kleinen Welt” des Modells. Sollten unsere Annahmen nicht stimmen (normalverteilt mit MW 100 und SD 15), dann stimmt natürlich unser Ergebnis auch nicht.\nOb unsere Annahmen stimmen, kann der Computer nicht sagen. Das ist weiterhin Menschenjob.\n\nCategories:\n\nprobability\nsimulation\nnormal-distribution\nnum"
  },
  {
    "objectID": "posts/wskt-quiz11/wskt-quiz11.html",
    "href": "posts/wskt-quiz11/wskt-quiz11.html",
    "title": "wskt-quiz11",
    "section": "",
    "text": "Sei \\(X \\sim N(100, 15)\\).\nBehauptung: Es gilt: \\(Pr(X \\ge 115) &lt; .2\\).\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n\n\n\nFalsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz11/wskt-quiz11.html#answerlist",
    "href": "posts/wskt-quiz11/wskt-quiz11.html#answerlist",
    "title": "wskt-quiz11",
    "section": "",
    "text": "Falsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz11/wskt-quiz11.html#answerlist-1",
    "href": "posts/wskt-quiz11/wskt-quiz11.html#answerlist-1",
    "title": "wskt-quiz11",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\nquiz\nprobability\nbayes\ndistribution\nquiz1-qm2-ws23\nschoice"
  },
  {
    "objectID": "posts/Gem-Wskt3/Gem-Wskt3.html",
    "href": "posts/Gem-Wskt3/Gem-Wskt3.html",
    "title": "Gem-Wskt3",
    "section": "",
    "text": "Aufgabe\nEin renommiertes Unternehmen sucht ei Kandidati (m/w/d) für eine (hoch dotierte) Führungsposition. Das könnten Sie sein. Ein Managementberatungsunternehmung führt ein Assessmentcenter durch, welches pro Kandidati eine positive bzw. negative Empfehlung ergibt. Aus früheren Erfahrungen heraus wissen die Berater, dass die tatsächlich geeigneten Kandidaten (Ereignis \\(E\\) wie eligible) mit \\(62\\%\\) eine positive Empfehlung für die Stelle ausgesprochen bekommen (Ereignis \\(R\\) wie recommendation). Weiterhin bekommen von den nicht geeigneten Kandidaten \\(73\\%\\) eine negative Empfehlung. Insgesamt wissen die Berater, dass \\(8\\%\\) der Bewerber/innen tatsächlich geeignet sind.\nGeben Sie den Wert folgender Kenngröße aus der entsprechenden Kontingenztabelle an: \\(E \\cap R\\)!\nHinweise:\n\n\\(\\overline{R}=R^C= \\neg R\\) (logische Verneinung).\n\\(\\cap\\) meint das logische “Und”.\nGeben Sie Wahrscheinlichkeiten nicht als Prozentzahlen, sondern als Anteile an.\nRunden Sie auf zwei Dezimalstellen.\nAchten Sie darauf, das richtige Dezimaltrennzeichen Ihres Systems zu verwenden.\n\n         \n\n\nLösung\nEinige Wahrscheinlichkeiten lassen sich direkt aus dem Text errechnen:\n\\(P(E \\cap R) = P(R | E) \\cdot P(E) = 0.62 \\cdot 0.08 = 0.0496 = 4.96\\%\\)\n\\(P(\\overline{E} \\cap \\overline{R}) =\n    P(\\overline{R} | \\overline{E}) \\cdot P(\\overline{E}) = 0.73 \\cdot 0.92 = 0.6716 = 67.16\\%\\)\nDie restlichen gemeinsamen Wahrscheinlichkeiten lassen sich durch Addieren und Subtrahieren in der Kontingenztabelle errechnen:\n\n\n\n\n\n\n\n\n\n\n\\(R\\)\n\\(\\overline{R}\\)\nSumme\n\n\n\n\n\\(E\\)\n4.96\n3.04\n8.00\n\n\n\\(\\overline{E}\\)\n24.84\n67.16\n92.00\n\n\nSumme\n29.80\n70.20\n100.00\n\n\n\nLösung: Der gesuchte Wert lautet: 0.05.\n\nCategories:\n\nprobability\ndyn\nbayes\nnum"
  },
  {
    "objectID": "posts/stan_glm_parameterzahl_simple/stan_glm_parameterzahl_simple.html",
    "href": "posts/stan_glm_parameterzahl_simple/stan_glm_parameterzahl_simple.html",
    "title": "stan_glm_parameterzahl_simple",
    "section": "",
    "text": "Exercise\nBetrachten Sie dazu dieses Modell:\nstan_glm(price ~ cut, data = diamonds)\nWie viele Parameter gibt es in diesem Modell?\nHinweise:\n\nGeben Sie nur eine (ganze) Zahl ein.\n\n         \n\n\nSolution\nGrundsätzlich hat ein Regressionsmodell die folgenden Parameter:\n\neinen Parameter für den Intercept, \\(\\beta_0\\)\npro UV ein weiterer Parameter, \\(\\beta_1, \\beta_2, \\ldots\\)\nfür sigma (\\(\\sigma\\)) noch ein zusätzlicher Parameter\n\nZu beachten ist aber, dass bei einer nominalen Variablen mit zwei Stufen nur ein Regressionsgewicht (\\(\\beta_1\\)) berechnet wird. Allgemein gilt bei nominalen also, dass bei \\(k\\) Stufen nur \\(k-1\\) Regressionsgewichte berechnet werden.\nIm vorliegenden Fall hat die Variable cut 5 Stufen, also werden 4 Regressiongewiche berechnet.\nDie Anzahl der Parameter in diesem Modell ist also: 6"
  },
  {
    "objectID": "posts/wskt-quiz16/wskt-quiz16.html",
    "href": "posts/wskt-quiz16/wskt-quiz16.html",
    "title": "wskt-quiz16",
    "section": "",
    "text": "Behauptung:\n\\(Pr(A|B) = Pr(AB) / Pr(B)\\).\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n\n\n\nFalsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz16/wskt-quiz16.html#answerlist",
    "href": "posts/wskt-quiz16/wskt-quiz16.html#answerlist",
    "title": "wskt-quiz16",
    "section": "",
    "text": "Falsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz16/wskt-quiz16.html#answerlist-1",
    "href": "posts/wskt-quiz16/wskt-quiz16.html#answerlist-1",
    "title": "wskt-quiz16",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\nquiz\nprobability\nbayes\nquiz1-qm2-ws23\nschoice"
  },
  {
    "objectID": "posts/wskt-quiz20/wskt-quiz20.html",
    "href": "posts/wskt-quiz20/wskt-quiz20.html",
    "title": "wskt-quiz20",
    "section": "",
    "text": "Behauptung:\nMann (Frau auch) kann jede Kennzahl der Deskriptivstatistik mit Methoden der Inferenzstatistik untersuchen.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n\n\n\nFalsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz20/wskt-quiz20.html#answerlist",
    "href": "posts/wskt-quiz20/wskt-quiz20.html#answerlist",
    "title": "wskt-quiz20",
    "section": "",
    "text": "Falsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz20/wskt-quiz20.html#answerlist-1",
    "href": "posts/wskt-quiz20/wskt-quiz20.html#answerlist-1",
    "title": "wskt-quiz20",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\nquiz\nprobability\ninference\nquiz1-qm2-ws23\nschoice"
  },
  {
    "objectID": "posts/germeval05/germeval05.html",
    "href": "posts/germeval05/germeval05.html",
    "title": "germeval05",
    "section": "",
    "text": "Erstellen Sie ein prädiktives Modell für Textdaten. Nutzen Sie Word-Vektoren für das Feature-Engineering.\nNutzen Sie die GermEval-2018-Daten.\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\nDie Daten sind auch über das R-Paket PradaData zu beziehen.\n\nlibrary(tidyverse)\ndata(\"germeval_train\", package = \"pradadata\")\ndata(\"germeval_test\", package = \"pradadata\")\n\nDie AV lautet c1. Die (einzige) UV lautet: text.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\nNutzen Sie Tidymodels."
  },
  {
    "objectID": "posts/germeval05/germeval05.html#textvektoren-importieren",
    "href": "posts/germeval05/germeval05.html#textvektoren-importieren",
    "title": "germeval05",
    "section": "Textvektoren importieren",
    "text": "Textvektoren importieren\n\nlibrary(textdata)\n\nglove_embedding &lt;- embedding_glove6b(\n  dir = \"/Users/sebastiansaueruser/datasets\",\n  return_path = TRUE,\n  manual_download = TRUE\n)\n\nhead(glove_embedding)\n\n# A tibble: 6 × 51\n  token     d1      d2     d3      d4    d5      d6     d7     d8        d9\n  &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1 the   0.418   0.250  -0.412  0.122  0.345 -0.0445 -0.497 -0.179 -0.000660\n2 ,     0.0134  0.237  -0.169  0.410  0.638  0.477  -0.429 -0.556 -0.364   \n3 .     0.152   0.302  -0.168  0.177  0.317  0.340  -0.435 -0.311 -0.450   \n4 of    0.709   0.571  -0.472  0.180  0.544  0.726   0.182 -0.524  0.104   \n5 to    0.680  -0.0393  0.302 -0.178  0.430  0.0322 -0.414  0.132 -0.298   \n6 and   0.268   0.143  -0.279  0.0163 0.114  0.699  -0.513 -0.474 -0.331   \n# ℹ 41 more variables: d10 &lt;dbl&gt;, d11 &lt;dbl&gt;, d12 &lt;dbl&gt;, d13 &lt;dbl&gt;, d14 &lt;dbl&gt;,\n#   d15 &lt;dbl&gt;, d16 &lt;dbl&gt;, d17 &lt;dbl&gt;, d18 &lt;dbl&gt;, d19 &lt;dbl&gt;, d20 &lt;dbl&gt;,\n#   d21 &lt;dbl&gt;, d22 &lt;dbl&gt;, d23 &lt;dbl&gt;, d24 &lt;dbl&gt;, d25 &lt;dbl&gt;, d26 &lt;dbl&gt;,\n#   d27 &lt;dbl&gt;, d28 &lt;dbl&gt;, d29 &lt;dbl&gt;, d30 &lt;dbl&gt;, d31 &lt;dbl&gt;, d32 &lt;dbl&gt;,\n#   d33 &lt;dbl&gt;, d34 &lt;dbl&gt;, d35 &lt;dbl&gt;, d36 &lt;dbl&gt;, d37 &lt;dbl&gt;, d38 &lt;dbl&gt;,\n#   d39 &lt;dbl&gt;, d40 &lt;dbl&gt;, d41 &lt;dbl&gt;, d42 &lt;dbl&gt;, d43 &lt;dbl&gt;, d44 &lt;dbl&gt;,\n#   d45 &lt;dbl&gt;, d46 &lt;dbl&gt;, d47 &lt;dbl&gt;, d48 &lt;dbl&gt;, d49 &lt;dbl&gt;, d50 &lt;dbl&gt;"
  },
  {
    "objectID": "posts/germeval05/germeval05.html#workflow",
    "href": "posts/germeval05/germeval05.html#workflow",
    "title": "germeval05",
    "section": "Workflow",
    "text": "Workflow\n\n# model:\nmod1 &lt;-\n  logistic_reg()\n\n\n# cv:\nset.seed(42)\nrsmpl &lt;- vfold_cv(d_train, v = 5)\n\n\n# recipe:\nrec1 &lt;-\n  recipe(c1 ~ ., data = d_train) |&gt; \n  update_role(id, new_role = \"id\")  |&gt; \n  #update_role(c2, new_role = \"ignore\") |&gt; \n  step_tokenize(text) %&gt;%\n  step_stopwords(text, keep = FALSE) %&gt;%\n  step_word_embeddings(text,\n                       embeddings = glove_embedding,\n                       aggregation = \"mean\") |&gt; \n  step_normalize(all_numeric_predictors()) \n\n\n# workflow:\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(mod1) %&gt;% \n  add_recipe(rec1)"
  },
  {
    "objectID": "posts/germeval05/germeval05.html#tuiningfitting",
    "href": "posts/germeval05/germeval05.html#tuiningfitting",
    "title": "germeval05",
    "section": "Tuining/Fitting",
    "text": "Tuining/Fitting\n\ntic()\nwf1_fit &lt;-\n  wf1 %&gt;% \n  fit_resamples(\n    resamples = rsmpl,\n    metrics = metric_set(accuracy, f_meas, roc_auc),\n    control = control_grid(verbose = TRUE))\ntoc()\n\n26.374 sec elapsed\n\nbeep()\n\n\nwf1_fit |&gt; collect_metrics()\n\n# A tibble: 3 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.656     5 0.00726 Preprocessor1_Model1\n2 f_meas   binary     0.129     5 0.0156  Preprocessor1_Model1\n3 roc_auc  binary     0.593     5 0.00947 Preprocessor1_Model1\n\n\nBester Fold:\n\nshow_best(wf1_fit)\n\n# A tibble: 1 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.656     5 0.00726 Preprocessor1_Model1"
  },
  {
    "objectID": "posts/germeval05/germeval05.html#fit",
    "href": "posts/germeval05/germeval05.html#fit",
    "title": "germeval05",
    "section": "Fit",
    "text": "Fit\n\nfit1 &lt;- \n  wf1 |&gt; \n  fit(data = d_train)"
  },
  {
    "objectID": "posts/germeval05/germeval05.html#test-set-güte",
    "href": "posts/germeval05/germeval05.html#test-set-güte",
    "title": "germeval05",
    "section": "Test-Set-Güte",
    "text": "Test-Set-Güte\nVorhersagen im Test-Set:\n\ntic()\npreds &lt;-\n  predict(fit1, new_data = germeval_test)\ntoc()\n\n2.229 sec elapsed\n\n\nUnd die Vorhersagen zum Test-Set hinzufügen, damit man TRUTH und ESTIMATE vergleichen kann:\n\nd_test &lt;-\n  germeval_test |&gt; \n  bind_cols(preds) |&gt; \n  mutate(c1 = as.factor(c1))\n\n\nmy_metrics &lt;- metric_set(accuracy, f_meas)\nmy_metrics(d_test,\n           truth = c1,\n           estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.652\n2 f_meas   binary         0.138"
  },
  {
    "objectID": "posts/germeval05/germeval05.html#fazit",
    "href": "posts/germeval05/germeval05.html#fazit",
    "title": "germeval05",
    "section": "Fazit",
    "text": "Fazit\nglove6b ist für die englische Sprache vorgekocht. Das macht wenig Sinn für einen deutschsprachigen Corpus.\n\nCategories:\n\ntextmining\ndatawrangling\ngermeval\nprediction\ntidymodels\nwordvec\nstring"
  },
  {
    "objectID": "posts/wuerfel01/wuerfel01.html",
    "href": "posts/wuerfel01/wuerfel01.html",
    "title": "wuerfel01",
    "section": "",
    "text": "Wie hoch ist die Wahrscheinlichkeit, mit zwei fairen Würfeln genau 10 Augen zu werfen?\nHinweise:\n\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\nRunden Sie auf zwei Dezimalstellen.\nFixieren Sie die Zufallszahlen auf den Startwert 42.\nMit expand_grid können Sie komfortabel alle 36 Ereignisse dieses Zufallsexperiments in einen Dataframe bringen.\n\nWählen Sie die am besten passende Option:\n\n\n\n.04\n.08\n.12\n.16\n.20"
  },
  {
    "objectID": "posts/wuerfel01/wuerfel01.html#answerlist",
    "href": "posts/wuerfel01/wuerfel01.html#answerlist",
    "title": "wuerfel01",
    "section": "",
    "text": ".04\n.08\n.12\n.16\n.20"
  },
  {
    "objectID": "posts/wuerfel01/wuerfel01.html#answerlist-1",
    "href": "posts/wuerfel01/wuerfel01.html#answerlist-1",
    "title": "wuerfel01",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch.\nWahr.\nFalsch.\nFalsch.\nFalsch.\n\n\nCategories:\n\nprobability\ndice\nexam-22"
  },
  {
    "objectID": "posts/mariokart-korr3/mariokart-korr3.html",
    "href": "posts/mariokart-korr3/mariokart-korr3.html",
    "title": "mariokart-korr3",
    "section": "",
    "text": "Aufgabe\nImportieren Sie den Datensatz mariokart in R. Berechnen Sie die Korrelation von mittlerem Verkaufspreis (total_pr) und Startgebot (start_pr) für Spiele, die sowohl neu sind und über Lenkräder (wheels) verfügen.\nHinweise:\n\nRunden Sie auf 1 Dezimalstelle.\nBeachten Sie die Hinweise des Datenwerk.\n\n         \n\n\nLösung\nPakete starten:\n\nlibrary(tidyverse)\nlibrary(easystats)\n\nDaten importieren:\n\nd_url &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\"\nd &lt;- data_read(d_url)\n\n\nsolution &lt;-\nd  %&gt;% \n  filter(cond == \"new\" & wheels &gt; 0) %&gt;% \n  summarise(pr_corr = cor(total_pr, start_pr))\n\nsolution\n\n    pr_corr\n1 0.4315485\n\n\nAlternativ kann man (komfortabel) die Korrelation z.B. so berechnen:\n\nd %&gt;% \n  select(start_pr, total_pr, cond, wheels) %&gt;% \n  filter(cond == \"new\" & wheels &gt; 0) %&gt;%  # logisches UND\n  correlation()\n\n# Correlation Matrix (pearson-method)\n\nParameter1 | Parameter2 |    r |        95% CI | t(53) |         p\n------------------------------------------------------------------\nstart_pr   |   total_pr | 0.43 | [ 0.19, 0.63] |  3.48 | 0.002**  \nstart_pr   |     wheels | 0.12 | [-0.15, 0.37] |  0.86 | 0.393    \ntotal_pr   |     wheels | 0.77 | [ 0.64, 0.86] |  8.82 | &lt; .001***\n\np-value adjustment method: Holm (1979)\nObservations: 55\n\n\nLösung: 0.4.\n\nCategories:\n\ndatawrangling\ndplyr\neda\nassociation\nnum"
  },
  {
    "objectID": "posts/ppv-dyn1/ppv-dyn1.html",
    "href": "posts/ppv-dyn1/ppv-dyn1.html",
    "title": "ppv-dyn1",
    "section": "",
    "text": "Berechnen Sie folgendes Modell (Datensatz mtcars):\nmpg ~ hp\nGeben Sie die Breite eines 50%-ETI an für eine Beobachtung mit einem z-Wert von 0 im Prädiktor!\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks."
  },
  {
    "objectID": "posts/ppv-dyn1/ppv-dyn1.html#setup",
    "href": "posts/ppv-dyn1/ppv-dyn1.html#setup",
    "title": "ppv-dyn1",
    "section": "Setup",
    "text": "Setup\n\nlibrary(rstanarm)\nlibrary(easystats)\nlibrary(tidyverse)\n\n\nmtcars2 &lt;-\n  mtcars %&gt;% \n  mutate(hp = standardize(hp))"
  },
  {
    "objectID": "posts/ppv-dyn1/ppv-dyn1.html#modell",
    "href": "posts/ppv-dyn1/ppv-dyn1.html#modell",
    "title": "ppv-dyn1",
    "section": "Modell",
    "text": "Modell\n\nm1 &lt;- stan_glm(mpg ~ hp, data = mtcars, seed = 42, refresh = 0)\ncoef(m1)\n\n(Intercept)          hp \n30.11668130 -0.06820988 \n\nr2(m1)\n\n# Bayesian R2 with Compatibility Interval\n\n  Conditional R2: 0.586 (95% CI [0.378, 0.746])\n\n\nOder mit z-standardisierten Werten:\n\nm2 &lt;- stan_glm(mpg ~ hp, data = mtcars2, seed = 42, refresh = 0)\ncoef(m2)\n\n(Intercept)          hp \n  20.096771   -4.676665 \n\nr2(m2)\n\n# Bayesian R2 with Compatibility Interval\n\n  Conditional R2: 0.586 (95% CI [0.378, 0.746])"
  },
  {
    "objectID": "posts/ppv-dyn1/ppv-dyn1.html#ppv",
    "href": "posts/ppv-dyn1/ppv-dyn1.html#ppv",
    "title": "ppv-dyn1",
    "section": "PPV",
    "text": "PPV\n\nm2_ppv &lt;- estimate_prediction(m2, data = tibble(hp = 0), ci = 0.5)\nm2_ppv\n\nModel-based Prediction\n\nhp   | Predicted |   SE |         50% CI\n----------------------------------------\n0.00 |     20.10 | 3.99 | [17.45, 22.72]\n\nVariable predicted: mpg\n\n\nVisualisierung:\n\nplot(estimate_prediction(m2))\n\n\n\n\n\n\n\n\nMan beachte, dass die PPV mit mehr Ungewissheit behaftet ist, als die Post-Verteilung.\n\nplot(estimate_relation(m2))\n\n\n\n\n\n\n\n\n\nCategories:\n\nbayes\nppv\nregression\nnum"
  },
  {
    "objectID": "posts/wskt-quiz18/wskt-quiz18.html",
    "href": "posts/wskt-quiz18/wskt-quiz18.html",
    "title": "wskt-quiz18",
    "section": "",
    "text": "Behauptung:\nHat eine Hypothese die Priori-Wahrscheinlichkeit von 1, so wird die Post-Wahrscheinlichkeit dieser Hypothese 1 sein.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n\n\n\nFalsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz18/wskt-quiz18.html#answerlist",
    "href": "posts/wskt-quiz18/wskt-quiz18.html#answerlist",
    "title": "wskt-quiz18",
    "section": "",
    "text": "Falsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz18/wskt-quiz18.html#answerlist-1",
    "href": "posts/wskt-quiz18/wskt-quiz18.html#answerlist-1",
    "title": "wskt-quiz18",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\nquiz\nprobability\nbayes\nquiz1-qm2-ws23\nschoice"
  },
  {
    "objectID": "posts/abh-ereignisse2/abh-ereignisse2.html",
    "href": "posts/abh-ereignisse2/abh-ereignisse2.html",
    "title": "abh-ereignisse2",
    "section": "",
    "text": "Aufgabe\n\n\n\n\n\n\n\n\n\nBerechnen Sie folgende Wahrscheinlichkeiten:\n\n\\(Pr(BA)\\)\n\\(Pr(AB)\\)\n\nHinweise:\n\nDas Ereignis “B tritt ein” ist mit “B+” im Diagramm eingezeichnet (entsprechend für A). Analog ist das Ereignis “B tritt nicht ein” mit “B-” eingezeichnet (entsprechend für A).\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n         \n\n\nLösung\n\n\\(Pr(AB) = Pr(A|B) \\cdot Pr(B) = 3/4 \\cdot 1/2 = 3/8\\)\n\\(Pr(BA) = Pr(B|A) \\cdot Pr(A) = 3/5 \\cdot 5/8 = 3/8\\)\n\n\nCategories:\n\nR\nprobability\nnum"
  },
  {
    "objectID": "posts/Warum-Bayes/Warum-Bayes.html",
    "href": "posts/Warum-Bayes/Warum-Bayes.html",
    "title": "Warum-Bayes",
    "section": "",
    "text": "Exercise\nNennen Sie einen (fachlichen) Grund, warum Sie eine Bayes-Analyse machen würden (und nicht etwa ein Analyse auf Basis der frequentistischen Statistik).\n         \n\n\nSolution\nEs existieren mehrere Gründe, einige wichtige sind:\n\nBayes-Analysen erlauben es, Vorwissen in die Analyse einfließen zu lassen.\nBayes-Analysen geben die Wahrscheinlichkeit einer Hypothese bzw. eines Parameterwerts zurück.\nBayes-Analysen erlauben es, Modelle exakt und flexibel zu spezifizieren.\nBayes-Analysen sind bei kleineren Stichproben genauer.\n\n“Quantifizierung” ist keine ausreichende Begründung für die Verwendung der Bayes-Statistik, da auch z.B. eine Frequentistische Analyse Quantifizierung bietet. Hingegen ist “Quantifizierung der Wahrscheinlichkeit der Forschungshypothese” ein valider Grund, denn der Frequentismus erlaubt nicht die Wahrscheinlichkeit einer Hypothese zu quantifizieren.\n“Wahrscheinlichkeitsaussagen” ist ebenfalls keine ausreichende Begründung für Bayes, denn auch im Frequentismus gibt es Wahrscheinlichkeitsaussagen, auch wenn diese weniger stark in die Wahrscheinlichkeitstheorie geknüpft sind als die Bayes-Inferenz (vgl. Jaynes, 2003).\nEs ist als Begründung nicht ausreichend, z.B. von “Erwartungen ans die Auswertung” zu sprechen, wenn man auf die Priori-Verteilung als (valider) Vorteil der Bayes-Inferenz abzielen möchte.\nEbenso ist es nicht ausreichend, allgemein auf eine “höhere Zuverlässigkeit” o.Ä. der Bayes-Inferenz hinzuweisen.\nDas ROPE ist eine praktische, sinnvolle Methode, allerdings gibt es mittlerweile vergleichbare Verfahren im Frequentismus, sog. Äquivalenztests.\nDer Grund, warum Bayes-Analysen bei kleineren Stichproben zu genaueren Ergebnissen kommen, liegt im Priori-Wissen. Spezifiziert man z.B. eine Normalverteilung mit Sigma=1 und findet in den Daten einen Wert von zB. Sigma=6, also einen extremen Ausreißer, so wird die Priori-Verteilung dafür sorgen, den Extremwert “zurechtzustutzen” auf einen Wert näher der Mittelwert der Verteilung. Sofern dies sinnvoll/korrekt ist, wird man mit diesem Vorgehen zu genaueren Ergebnissen kommen. Die Hoffnung ist, dass einzelne Extremwerte eher Messfehler sind.\n\nCategories:\n\nqm2\nbayes\nprobability"
  },
  {
    "objectID": "posts/iq03/iq03.html",
    "href": "posts/iq03/iq03.html",
    "title": "iq03",
    "section": "",
    "text": "Aufgabe\nIntelligenz wird häufig mittels einem IQ-Test ermittelt.\nPersonen mit einem Testwert von höchstens 100 Punkten kann man als “nicht überdurchschnittlich intelligent” bezeichnen.\nDefinieren wir also das Ereignis “nicht überdurchschnittlich intelligent” als “die nächste Person, die Sie treffen, hat einen IQ von höchstens 100 Punkten”.\nWie groß ist die Wahrscheinlichkeit, dass die nächste Person, die Sie treffen, nicht überdurchschnittlich intelligent ist?\nHinweise:\n\nNutzen Sie Simulationsmethoden.\nGehen Sie von folgender IQ-Verteilung aus: \\(IQ \\sim N(100,15)\\).\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\nSimulieren Sie \\(n=10^3\\) Stichproben.\nNutzen Sie die Zahl 42 als Startwert für Ihre Zufallszahlen (um die Reproduzierbarkeit zu gewährleisten).\nGeben Sie keine Prozentzahlen, sondern stets Anteile an.\n\n         \n\n\nLösung\n\nlibrary(tidyverse)\n\nWir simulieren die Daten:\n\nset.seed(42)\nd &lt;- tibble(\n  id = 1:10^3,\n  iq = rnorm(n = 10^3, mean = 100, sd= 15))\n\nDa \\(\\sigma=15\\), filtern wir bis höchstens 100:\n\nsolution_d &lt;- \n  d %&gt;% \n  count(iq &lt;= 100) %&gt;% \n  mutate(prop = n / sum(n))\n\nsolution_d\n\n# A tibble: 2 × 3\n  `iq &lt;= 100`     n  prop\n  &lt;lgl&gt;       &lt;int&gt; &lt;dbl&gt;\n1 FALSE         485 0.485\n2 TRUE          515 0.515\n\n\nLösung: Die Wahrscheinlichkeit für “nicht überdurchschnittlich intelligent” beträgt ca. 0.52.\nDas Ereignis “nicht überdurchschnittlich intelligent” kann man vielleicht einfacher - und auf jeden Fall präziser benennen mit \\(iq \\le 100\\).\n\nCategories:\n\nprobability\nsimulation\nnormal-distribution\nnum"
  },
  {
    "objectID": "posts/sentiws2/sentiws2.html",
    "href": "posts/sentiws2/sentiws2.html",
    "title": "sentiws2",
    "section": "",
    "text": "Aufgabe\nImportieren Sie das sentiws Lexikon:\nDie Spalte inflections birgt eine Reihe von Word-Varianten. Es scheint sinnvoll zu sein, diese Wörter zu nutzen. Aber um sie zu nutzen, muss man sie tokenisieren.\nAufgabe: Tokenisieren Sie die Tabelle sentiws, Spalte inflections.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n         \n\n\nLösung\n\n\n# A tibble: 28,620 × 3\n   neg_pos word           value\n   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n 1 neg     abbaus       -0.058 \n 2 neg     abbaues      -0.058 \n 3 neg     abbauen      -0.058 \n 4 neg     abbaue       -0.058 \n 5 neg     abbruches    -0.0048\n 6 neg     abbrüche     -0.0048\n 7 neg     abbruchs     -0.0048\n 8 neg     abbrüchen    -0.0048\n 9 neg     abdankungen  -0.0048\n10 neg     abdämpfungen -0.0048\n# ℹ 28,610 more rows\n\n\nDas ging einfach!\nNur die NAs sollten wir vielleicht noch entfernen.\n\nCategories:\n\ntextmining\ntokenizer\nstring"
  },
  {
    "objectID": "posts/bootstrap/bootstrap.html",
    "href": "posts/bootstrap/bootstrap.html",
    "title": "bootstrap",
    "section": "",
    "text": "In einer Analyse ist ein Team von Analysten interessiert, den Spritverbrauch von Fahrzeugen (gemessen in Meilen per Gallone mpg) in einem bestimmten Marksegment zu modellieren auf Basis der PS-Zahl (horse power, hp).\nDas Team analysiert die vorliegenden Daten des Trainings-Datensatzes und stellt folgendes Modell auf:\n\ndata(mtcars)\nlm1 &lt;- lm(mpg ~ hp, data = mtcars)\ncoef(lm1)\n\n(Intercept)          hp \n30.09886054 -0.06822828 \n\n\nDas Einflussgewicht des Prädiktors wird auf 0 geschätzt.\nIm Testdatensatz wird nun der mittlere Verbrauch mittels Bootstrapping-Methode bestimmt. Es ergibt sich folgendes Diagramm:\n\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\n\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\n\n\nAttaching package: 'mosaic'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\n\n\n\n\n\n\n\n\nWelche Aussage lässt sich aus diesem Diagramm ableiten?\n\n\n\nDas 95%-Konfidenzintervall für den Einfluss von hp liegt ca. zwischen -0.10 und -0.05.\nDas 95%-Konfidenzintervall für den Einfluss von hp liegt bei ca. -0.07.\nDie resultierende Verteilung ist normalverteilt.\nEine Entscheidung zur statistischen Signifikanz des Prädiktors hp kann nicht abgeleitet werden."
  },
  {
    "objectID": "posts/bootstrap/bootstrap.html#answerlist",
    "href": "posts/bootstrap/bootstrap.html#answerlist",
    "title": "bootstrap",
    "section": "",
    "text": "Das 95%-Konfidenzintervall für den Einfluss von hp liegt ca. zwischen -0.10 und -0.05.\nDas 95%-Konfidenzintervall für den Einfluss von hp liegt bei ca. -0.07.\nDie resultierende Verteilung ist normalverteilt.\nEine Entscheidung zur statistischen Signifikanz des Prädiktors hp kann nicht abgeleitet werden."
  },
  {
    "objectID": "posts/tidymodels-tree1/tidymodels-tree1.html",
    "href": "posts/tidymodels-tree1/tidymodels-tree1.html",
    "title": "tidymodels-tree1",
    "section": "",
    "text": "library(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.0.8\n✔ dials        1.2.0     ✔ rsample      1.2.0\n✔ dplyr        1.1.3     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org"
  },
  {
    "objectID": "posts/tidymodels-tree1/tidymodels-tree1.html#setup",
    "href": "posts/tidymodels-tree1/tidymodels-tree1.html#setup",
    "title": "tidymodels-tree1",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidymodels)\ndata(mtcars)\nlibrary(tictoc)  # Zeitmessung\nlibrary(baguette)\n\nFür Klassifikation verlangt Tidymodels eine nominale AV, keine numerische:\n\nmtcars &lt;-\n  mtcars %&gt;% \n  mutate(am = factor(am))"
  },
  {
    "objectID": "posts/tidymodels-tree1/tidymodels-tree1.html#daten-teilen",
    "href": "posts/tidymodels-tree1/tidymodels-tree1.html#daten-teilen",
    "title": "tidymodels-tree1",
    "section": "Daten teilen",
    "text": "Daten teilen\n\nd_split &lt;- initial_split(mtcars)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)"
  },
  {
    "objectID": "posts/tidymodels-tree1/tidymodels-tree1.html#modelle",
    "href": "posts/tidymodels-tree1/tidymodels-tree1.html#modelle",
    "title": "tidymodels-tree1",
    "section": "Modell(e)",
    "text": "Modell(e)\n\nmod_tree &lt;-\n  decision_tree(mode = \"classification\",\n                cost_complexity = tune(),\n                tree_depth = tune(),\n                min_n = tune())\n\nmod_bag &lt;-\n  bag_tree(mode = \"classification\",\n           cost_complexity = tune(),\n           tree_depth = tune(),\n           min_n = tune())"
  },
  {
    "objectID": "posts/tidymodels-tree1/tidymodels-tree1.html#rezepte",
    "href": "posts/tidymodels-tree1/tidymodels-tree1.html#rezepte",
    "title": "tidymodels-tree1",
    "section": "Rezept(e)",
    "text": "Rezept(e)\n\nrec_plain &lt;- \n  recipe(am ~ ., data = d_train)"
  },
  {
    "objectID": "posts/tidymodels-tree1/tidymodels-tree1.html#resampling",
    "href": "posts/tidymodels-tree1/tidymodels-tree1.html#resampling",
    "title": "tidymodels-tree1",
    "section": "Resampling",
    "text": "Resampling\n\nrsmpl &lt;- vfold_cv(d_train, v = 2)"
  },
  {
    "objectID": "posts/tidymodels-tree1/tidymodels-tree1.html#workflows",
    "href": "posts/tidymodels-tree1/tidymodels-tree1.html#workflows",
    "title": "tidymodels-tree1",
    "section": "Workflows",
    "text": "Workflows\n\nwf_tree &lt;-\n  workflow() %&gt;%  \n  add_recipe(rec_plain) %&gt;% \n  add_model(mod_tree)\n\n\nwf_bag &lt;-\n  workflow() %&gt;%  \n  add_recipe(rec_plain) %&gt;% \n  add_model(mod_bag)"
  },
  {
    "objectID": "posts/tidymodels-tree1/tidymodels-tree1.html#tuningfitting",
    "href": "posts/tidymodels-tree1/tidymodels-tree1.html#tuningfitting",
    "title": "tidymodels-tree1",
    "section": "Tuning/Fitting",
    "text": "Tuning/Fitting\nTuninggrid:\n\ntune_grid &lt;- grid_regular(extract_parameter_set_dials(mod_tree), levels = 5)\ntune_grid\n\n# A tibble: 125 × 3\n   cost_complexity tree_depth min_n\n             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt;\n 1    0.0000000001          1     2\n 2    0.0000000178          1     2\n 3    0.00000316            1     2\n 4    0.000562              1     2\n 5    0.1                   1     2\n 6    0.0000000001          4     2\n 7    0.0000000178          4     2\n 8    0.00000316            4     2\n 9    0.000562              4     2\n10    0.1                   4     2\n# ℹ 115 more rows\n\n\nDa beide Modelle die gleichen Tuningparameter aufweisen, brauchen wir nur ein Grid zu erstellen.\n\ntic()\nfit_tree &lt;-\n  tune_grid(object = wf_tree,\n            grid = tune_grid,\n            metrics = metric_set(roc_auc),\n            resamples = rsmpl)\n\n→ A | warning: 21 samples were requested but there were 12 rows in the data. 12 will be used.\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x11\n\n\n→ B | warning: 30 samples were requested but there were 12 rows in the data. 12 will be used.\n\n\nThere were issues with some computations   A: x11\nThere were issues with some computations   A: x25   B: x16\n→ C | warning: 40 samples were requested but there were 12 rows in the data. 12 will be used.\nThere were issues with some computations   A: x25   B: x16\nThere were issues with some computations   A: x25   B: x25   C: x24\nThere were issues with some computations   A: x26   B: x25   C: x25\nThere were issues with some computations   A: x50   B: x31   C: x25\nThere were issues with some computations   A: x50   B: x50   C: x38\nThere were issues with some computations   A: x50   B: x50   C: x50\n\ntoc()\n\n20.49 sec elapsed\n\nfit_tree\n\n# Tuning results\n# 2-fold cross-validation \n# A tibble: 2 × 4\n  splits          id    .metrics           .notes           \n  &lt;list&gt;          &lt;chr&gt; &lt;list&gt;             &lt;list&gt;           \n1 &lt;split [12/12]&gt; Fold1 &lt;tibble [125 × 7]&gt; &lt;tibble [75 × 3]&gt;\n2 &lt;split [12/12]&gt; Fold2 &lt;tibble [125 × 7]&gt; &lt;tibble [75 × 3]&gt;\n\nThere were issues with some computations:\n\n  - Warning(s) x50: 21 samples were requested but there were 12 rows in the data. 12 ...\n  - Warning(s) x50: 30 samples were requested but there were 12 rows in the data. 12 ...\n  - Warning(s) x50: 40 samples were requested but there were 12 rows in the data. 12 ...\n\nRun `show_notes(.Last.tune.result)` for more information.\n\n\n\ntic()\nfit_bag &lt;-\n  tune_grid(object = wf_bag,\n            grid = tune_grid,\n            metrics = metric_set(roc_auc),\n            resamples = rsmpl)\n\n→ A | warning: There were 11 warnings in `dplyr::mutate()`.\n               The first warning was:\n               ℹ In argument: `model = iter(...)`.\n               Caused by warning:\n               ! 21 samples were requested but there were 12 rows in the data. 12 will be used.\n               ℹ Run `dplyr::last_dplyr_warnings()` to see the 10 remaining warnings.\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x7\n\n\nThere were issues with some computations   A: x13\n\n\nThere were issues with some computations   A: x19\n\n\nThere were issues with some computations   A: x25\n\n\n→ B | warning: There were 11 warnings in `dplyr::mutate()`.\n               The first warning was:\n               ℹ In argument: `model = iter(...)`.\n               Caused by warning:\n               ! 30 samples were requested but there were 12 rows in the data. 12 will be used.\n               ℹ Run `dplyr::last_dplyr_warnings()` to see the 10 remaining warnings.\n\n\nThere were issues with some computations   A: x25\nThere were issues with some computations   A: x25   B: x5\nThere were issues with some computations   A: x25   B: x11\nThere were issues with some computations   A: x25   B: x16\nThere were issues with some computations   A: x25   B: x22\n→ C | warning: There were 11 warnings in `dplyr::mutate()`.\n               The first warning was:\n               ℹ In argument: `model = iter(...)`.\n               Caused by warning:\n               ! 40 samples were requested but there were 12 rows in the data. 12 will be used.\n               ℹ Run `dplyr::last_dplyr_warnings()` to see the 10 remaining warnings.\nThere were issues with some computations   A: x25   B: x22\nThere were issues with some computations   A: x25   B: x25   C: x3\nThere were issues with some computations   A: x25   B: x25   C: x9\nThere were issues with some computations   A: x25   B: x25   C: x14\nThere were issues with some computations   A: x25   B: x25   C: x19\nThere were issues with some computations   A: x25   B: x25   C: x25\nThere were issues with some computations   A: x26   B: x25   C: x25\nThere were issues with some computations   A: x30   B: x25   C: x25\nThere were issues with some computations   A: x35   B: x25   C: x25\nThere were issues with some computations   A: x40   B: x25   C: x25\nThere were issues with some computations   A: x47   B: x25   C: x25\nThere were issues with some computations   A: x50   B: x27   C: x25\nThere were issues with some computations   A: x50   B: x33   C: x25\nThere were issues with some computations   A: x50   B: x39   C: x25\nThere were issues with some computations   A: x50   B: x45   C: x25\nThere were issues with some computations   A: x50   B: x50   C: x27\nThere were issues with some computations   A: x50   B: x50   C: x33\nThere were issues with some computations   A: x50   B: x50   C: x38\nThere were issues with some computations   A: x50   B: x50   C: x44\nThere were issues with some computations   A: x50   B: x50   C: x50\nThere were issues with some computations   A: x50   B: x50   C: x50\n\ntoc()\n\n112.989 sec elapsed"
  },
  {
    "objectID": "posts/tidymodels-tree1/tidymodels-tree1.html#bester-kandidat",
    "href": "posts/tidymodels-tree1/tidymodels-tree1.html#bester-kandidat",
    "title": "tidymodels-tree1",
    "section": "Bester Kandidat",
    "text": "Bester Kandidat\n\nshow_best(fit_tree)\n\n# A tibble: 5 × 9\n  cost_complexity tree_depth min_n .metric .estimator  mean     n std_err\n            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1    0.0000000001          1     2 roc_auc binary     0.847     2  0.0694\n2    0.0000000178          1     2 roc_auc binary     0.847     2  0.0694\n3    0.00000316            1     2 roc_auc binary     0.847     2  0.0694\n4    0.000562              1     2 roc_auc binary     0.847     2  0.0694\n5    0.1                   1     2 roc_auc binary     0.847     2  0.0694\n# ℹ 1 more variable: .config &lt;chr&gt;\n\n\n\nshow_best(fit_bag)\n\n# A tibble: 5 × 9\n  cost_complexity tree_depth min_n .metric .estimator  mean     n std_err\n            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1    0.000562              8     2 roc_auc binary     0.889     2  0.111 \n2    0.0000000178          4    40 roc_auc binary     0.889     2  0.111 \n3    0.1                  15    11 roc_auc binary     0.884     2  0.0880\n4    0.00000316            8    11 roc_auc binary     0.875     2  0.0972\n5    0.000562              4    30 roc_auc binary     0.875     2  0.0972\n# ℹ 1 more variable: .config &lt;chr&gt;\n\n\nBagging erzielte eine klar bessere Modellgüte (in den Validierungssamples) als das Entscheidungsbaum-Modell."
  },
  {
    "objectID": "posts/tidymodels-tree1/tidymodels-tree1.html#finalisieren",
    "href": "posts/tidymodels-tree1/tidymodels-tree1.html#finalisieren",
    "title": "tidymodels-tree1",
    "section": "Finalisieren",
    "text": "Finalisieren\n\nwf_best_finalized &lt;-\n  wf_bag %&gt;% \n  finalize_workflow(select_best(fit_bag))"
  },
  {
    "objectID": "posts/tidymodels-tree1/tidymodels-tree1.html#last-fit",
    "href": "posts/tidymodels-tree1/tidymodels-tree1.html#last-fit",
    "title": "tidymodels-tree1",
    "section": "Last Fit",
    "text": "Last Fit\n\nfinal_fit &lt;- \n  last_fit(object = wf_best_finalized, d_split)\n\ncollect_metrics(final_fit)\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.875 Preprocessor1_Model1\n2 roc_auc  binary         0.906 Preprocessor1_Model1\n\n\nWie man sieht, ist die Modellgüte im Test-Sample schlechter als in den Train- bzw. Validierungssamples; ein typischer Befund.\n\nCategories:\n\nstatlearning\ntrees\ntidymodels\nstring"
  },
  {
    "objectID": "posts/Flex-vs-nichtflex-Methode3/Flex-vs-nichtflex-Methode3.html",
    "href": "posts/Flex-vs-nichtflex-Methode3/Flex-vs-nichtflex-Methode3.html",
    "title": "Flex-vs-nichtflex-Methode3",
    "section": "",
    "text": "Algorithmen des statistischen Lernens lassen sich unterteilen in ihrer Flexibilität; es gibt mehr bzw. weniger flexible Algorithmen.\nWelche der folgenden Aussagen ist in diesem Zusammenhang korrekt?\n\n\n\nWeisen die Fehler eine hohe Streuung auf (\\(\\sigma^2 = Var(\\epsilon)\\)), so schneidet eine flexiblere Methode tendenziell schlechter ab als eine weniger flexible Methode aufgrund der hohen Varianz in der Test-\\(MSE\\).\nWeisen die Fehler eine hohe Streuung auf (\\(\\sigma^2 = Var(\\epsilon)\\)), so schneidet eine flexiblere Methode tendenziell besser ab als eine weniger flexible Methode aufgrund der hohen Varianz in der Test-\\(MSE\\).\nWeisen die Fehler eine hohe Streuung auf (\\(\\sigma^2 = Var(\\epsilon)\\)), so schneidet eine flexiblere Methode tendenziell besser ab als eine weniger flexible Methode aufgrund der geringen Varianz in der Test-\\(MSE\\).\nWeisen die Fehler eine hohe Streuung auf (\\(\\sigma^2 = Var(\\epsilon)\\)), so schneidet eine flexiblere Methode tendenziell schlechter ab als eine weniger flexible Methode aufgrund der geringen Varianz in der Test-\\(MSE\\).\nWeisen die Fehler eine hohe Streuung auf (\\(\\sigma^2 = Var(\\epsilon)\\)), so schneidet eine flexiblere Methode tendenziell schlechter ab als eine weniger flexible Methode aufgrund der hohen Verzerrung in der Test-\\(MSE\\)."
  },
  {
    "objectID": "posts/Flex-vs-nichtflex-Methode3/Flex-vs-nichtflex-Methode3.html#answerlist",
    "href": "posts/Flex-vs-nichtflex-Methode3/Flex-vs-nichtflex-Methode3.html#answerlist",
    "title": "Flex-vs-nichtflex-Methode3",
    "section": "",
    "text": "Weisen die Fehler eine hohe Streuung auf (\\(\\sigma^2 = Var(\\epsilon)\\)), so schneidet eine flexiblere Methode tendenziell schlechter ab als eine weniger flexible Methode aufgrund der hohen Varianz in der Test-\\(MSE\\).\nWeisen die Fehler eine hohe Streuung auf (\\(\\sigma^2 = Var(\\epsilon)\\)), so schneidet eine flexiblere Methode tendenziell besser ab als eine weniger flexible Methode aufgrund der hohen Varianz in der Test-\\(MSE\\).\nWeisen die Fehler eine hohe Streuung auf (\\(\\sigma^2 = Var(\\epsilon)\\)), so schneidet eine flexiblere Methode tendenziell besser ab als eine weniger flexible Methode aufgrund der geringen Varianz in der Test-\\(MSE\\).\nWeisen die Fehler eine hohe Streuung auf (\\(\\sigma^2 = Var(\\epsilon)\\)), so schneidet eine flexiblere Methode tendenziell schlechter ab als eine weniger flexible Methode aufgrund der geringen Varianz in der Test-\\(MSE\\).\nWeisen die Fehler eine hohe Streuung auf (\\(\\sigma^2 = Var(\\epsilon)\\)), so schneidet eine flexiblere Methode tendenziell schlechter ab als eine weniger flexible Methode aufgrund der hohen Verzerrung in der Test-\\(MSE\\)."
  },
  {
    "objectID": "posts/Flex-vs-nichtflex-Methode3/Flex-vs-nichtflex-Methode3.html#answerlist-1",
    "href": "posts/Flex-vs-nichtflex-Methode3/Flex-vs-nichtflex-Methode3.html#answerlist-1",
    "title": "Flex-vs-nichtflex-Methode3",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nstatlearning\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/there-is-no-package/there-is-no-package.html",
    "href": "posts/there-is-no-package/there-is-no-package.html",
    "title": "there-is-no-package",
    "section": "",
    "text": "Sie führen folgende R-Syntax aus:\nlibrary(tidyverse)\nUnd bekommen als Antwort eine Fehlermeldung quittiert:\nthere is no package called 'tidyverse'.\nWas ist die Ursache bzw. zu tun?\n\n\n\nEs existiert kein Paket namens tidyverse.\nEs existiert kein Paket namens tidyverse auf Ihrem Rechner.\nDas Paket tidyverse ist nicht gestartet.\nDas Paket tidyverse ist kaputt.\nR ist in Sie verliebt und versucht auf ungelenke Weise Kontakt aufzunehmen."
  },
  {
    "objectID": "posts/there-is-no-package/there-is-no-package.html#answerlist",
    "href": "posts/there-is-no-package/there-is-no-package.html#answerlist",
    "title": "there-is-no-package",
    "section": "",
    "text": "Es existiert kein Paket namens tidyverse.\nEs existiert kein Paket namens tidyverse auf Ihrem Rechner.\nDas Paket tidyverse ist nicht gestartet.\nDas Paket tidyverse ist kaputt.\nR ist in Sie verliebt und versucht auf ungelenke Weise Kontakt aufzunehmen."
  },
  {
    "objectID": "posts/there-is-no-package/there-is-no-package.html#answerlist-1",
    "href": "posts/there-is-no-package/there-is-no-package.html#answerlist-1",
    "title": "there-is-no-package",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nR\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/Inferenz-fuer-alle/Inferenz-fuer-alle.html",
    "href": "posts/Inferenz-fuer-alle/Inferenz-fuer-alle.html",
    "title": "Inferenz-fuer-alle",
    "section": "",
    "text": "Exercise\nDie Inferenzstatistik ist eine Sammlung an Verfahren zur Bemessung von Unsicherheit in statistischen Schlüssen.\n\nFür welche Statistiken - also Kennzahlen der Deskriptivstatistik wie etwa \\(\\bar{X}, sd, r\\) - kann man die Inferenzstatistik verwenden?\nFür welche Forschungsfragen oder -bereiche kann man die Inferenzstatistik verwenden?\nGibt es besondere Fälle, in denen man nicht die Inferenzstatistik verwenden möchte? Wenn ja, welche?\n\n         \n\n\nSolution\n\nFür (grundsätzlich) alle: Für jede Statistik kann man prinzipiell von der jeweiligen Stichprobe (auf Basis derer die Statistik berechnet wurde) auf eine zugehörige Grundgesamtheit schließen.\nFür (grundsätzlich) alle: Die Methoden der Inferenzstatistik sind prinzipiell unabhängig von den Spezifika bestimmter Forschungsfragen oder -bereiche. In den meisten Forschungsfragen ist man daran interessiert allgemeingültige Aussagen zu treffen. Da Statistiken sich nur auf eine Stichprobe - also einen zumeist nur kleinen Teil einer Grundgesamtheit beziehen - wird man sich kaum mit einer Statistik zufrieden geben, sondern nach Inferenzstatistik verlangen.\nIn einigen Ausnahmefällen wird man auf eine Inferenzstatistik verzichten. Etwa wenn man bereits eine Vollerhebung durchgeführt hat, z.B. alle Mitarbeitis eines Unternehmens befragt hat, dann kennt man ja bereits den wahren Populationswert. Ein anderer Fall ist, wenn man nicht an Verallgemeinerungen interessiert ist: Kennt man etwa die Überlebenschance \\(p\\) des Titanic-Unglücks, so ist es fraglich auf welche Grundgesamtheit man die Statistik \\(p\\) bzw. zu welchem Parameter \\(\\pi\\) (kleines Pi) man generalisieren möchte.\n\n\nCategories:\n\nqm2\ninference"
  },
  {
    "objectID": "posts/alphafehler-inflation4/alphafehler-inflation4.html",
    "href": "posts/alphafehler-inflation4/alphafehler-inflation4.html",
    "title": "alphafehler-inflation4",
    "section": "",
    "text": "Aufgabe\nEine Klettererin verwendet ein Seil, dass eine Sicherheit von \\(r=.99\\) hat: mit einer Wahrscheinlichkeit von 1% reißt das Seil. Jetzt knüpft sie mehrere dieser Seile (hintereinander, Seil an Seil) zusammen zu einem “Gesamtseil”. Wie groß ist die Gefahr, dass das „Gesamtseil“ reist?\nHinweise:\n\nEtwaige (physikalisch plausible) Verringerung der Zugfestigkeit durch (Seilbiegung aufgrund der) Knoten ist zu vernachlässigen.\nUnterstellen Sie Unabhängkeit der einzelnen Ereignisse.\nWie immer, beachten sie die übrigen Hinweise des Datenwerks.\n\nBetrachten wir mehrere Fälle:\n\nSie knüpft 2 zusammen.\nSie knüpft 5 zusammen.\nSie knüpft 10 zusammen.\nSie knüpft 20 zusammen.\n\n         \n\n\nLösung\nSei \\(R\\) die Wahrscheinlichkeit, dass das Gesamtseil hält (nicht reißt). \\(1-R\\) ist dann die Wahrscheinlichkeit des Gegenereignisses: das Gesamtseil reißt.\nAllgemein ist \\(R\\) bei k Tests gleich r hoch k: \\(R=r^k\\). (Das Aufaddieren der Fehlalarm-Wahrscheinlichkeit bezeichnet man als Alphafehler-Inflation.)\n\nlibrary(tidyverse)\nr &lt;- .99\nR2 &lt;- r^2 %&gt;% round(2)  # Auf 2 Dezimalen runden\nR5  &lt;- r^5  %&gt;% round(2)\nR10 &lt;- r^10  %&gt;% round(2)\nR20 &lt;- r^20  %&gt;% round(2)\n\nDie Gesamtsicherheiten lauten also:\n\nR2\n\n[1] 0.98\n\nR5\n\n[1] 0.95\n\nR10\n\n[1] 0.9\n\nR20\n\n[1] 0.82\n\n\nDie Seilriss-Gefahr ist dann:\n\n1 - R2\n\n[1] 0.02\n\n1 - R5\n\n[1] 0.05\n\n1 - R10\n\n[1] 0.1\n\n1 - R20\n\n[1] 0.18\n\n\n\n\nVertiefung\nBetrachten wir abschließend aus Neugier die Wahrscheinlichkeit, dass die Klettererin abstürzt (\\(1-R\\)) als Funktion der Anzahl der Seie.\nDiese Überlegung ist etwas weiterführender und nicht ganz so zentral, aber ziemlich interessant.\nDefinieren wir die Parameter:\n\nanz_seile &lt;- 1:20  # von 1 bis max 20 Seile\nr &lt;- c(.9, .95, .99, .999)  # verschiedene Seil-Sicherheiten\n\nJetzt erstellen wir einen Tabelle, die alle anz_seile * r Werte kombiniert:\n\nd &lt;- \n  expand_grid(anz_seile, r)\n\nhead(d)\n\n# A tibble: 6 × 2\n  anz_seile     r\n      &lt;int&gt; &lt;dbl&gt;\n1         1 0.9  \n2         1 0.95 \n3         1 0.99 \n4         1 0.999\n5         2 0.9  \n6         2 0.95 \n\n\nJetzt berechnen wir für jede Kombination die Gesamtsicherheit R sowie die Wahrscheinlichkeit, dass das Seil reißt, \\(1-R\\):\n\nd &lt;-\n  d %&gt;% \n  mutate(R = r^anz_seile,\n         seil_reisst_prob = 1 - R)\n\nUnd plotten das Ganze mit dem Paket ggpubr:\n\nlibrary(ggpubr)\nd &lt;-\n  d |&gt; \n  mutate(r_fctr = factor(r))  # um \"r\" zum Gruppieren zu verwenden, sollte es eine nominale Variable sein, daher wandeln wir mit \"factor\" in eine nominale Variable um.\n\nggline(d,\n       x = \"anz_seile\",\n       y = \"seil_reisst_prob\",\n       color = \"r_fctr\",\n       linetype = \"r_fctr\",\n       group = \"r_fctr\") +\n  labs(color = \"Reißfestigkeit\",\n       linetype = \"Reißfestigkeit\")\n\n\n\n\n\n\n\n\nOder mit ggplot plotten:\n\nd %&gt;% \n  ggplot(aes(x = anz_seile,\n             y = seil_reisst_prob,\n             color = factor(r))) +\n  geom_line() +\n  labs(color = \"Reißfestigkeit\")\n\n\n\n\n\n\n\n\nHat ein Seil eine Sicherheit von 90%, dann will man nicht dranhängen, wenn 20 Seile zusammengeknotet sind!\nAntworten:\n\n\\(R_2 = r\\cdot r = r^2 = 0.98\\)\n\\(R_5 =  r^5 = 0.95\\)\n\\(R_{10}= r^{10} = 0.9\\)\n\\(R_{20}= r^{20} = 0.82\\)\n\n\nCategories:\n\nprobability\nR\ninference\nstring"
  },
  {
    "objectID": "posts/tidydata1/tidydata1.html",
    "href": "posts/tidydata1/tidydata1.html",
    "title": "tidydata1",
    "section": "",
    "text": "Laden Sie die folgende Tabellen mit folgendem Befehl aus dem Paket tidyverse:\n\ntable1_path &lt;- \"https://raw.githubusercontent.com/sebastiansauer/Lehre/main/data/tidy-table1.csv\"\ntable1 &lt;- read_csv(table1_path)\n\nInsgesamt sollten Sie als folgende Tabellen in Ihrem environment verfügbar haben:\n\ntable1\ntable2\ntable3\ntable4\ntable5\n\nWelche der Tabellen ist in der Normalform?\n\n\n\ntable1\ntable2\ntable3\ntable4\ntable5"
  },
  {
    "objectID": "posts/tidydata1/tidydata1.html#answerlist",
    "href": "posts/tidydata1/tidydata1.html#answerlist",
    "title": "tidydata1",
    "section": "",
    "text": "table1\ntable2\ntable3\ntable4\ntable5"
  },
  {
    "objectID": "posts/tidydata1/tidydata1.html#answerlist-1",
    "href": "posts/tidydata1/tidydata1.html#answerlist-1",
    "title": "tidydata1",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndatawrangling\ntidy\nschoice"
  },
  {
    "objectID": "posts/Typ-Fehler-R-01/Typ-Fehler-R-01.html",
    "href": "posts/Typ-Fehler-R-01/Typ-Fehler-R-01.html",
    "title": "Typ-Fehler-R-01",
    "section": "",
    "text": "Aufgabe\nKorrigieren Sie den Fehler in der Syntax:\n\nmean(x = c(1, 5, 10, 52)\n\nÄndern Sie nur diejenigen Teile der Syntax, die zwingend geändert werden müssen, damit der Fehler korrigiert wird.\nGeben Sie in der Lösung keine Leerzeichen ein.\n         \n\n\nLösung\n\nmean(x=c(1,5,10,52))\n\n[1] 17\n\n\nDie Antwort lautet: mean(x=c(1,5,10,52)).\n\nCategories:\n\nR\n‘2023’\nstring"
  },
  {
    "objectID": "posts/korr-als-regr/korr-als-regr.html",
    "href": "posts/korr-als-regr/korr-als-regr.html",
    "title": "korr-als-regr",
    "section": "",
    "text": "options(digits=2)\noptions(width = 80)\n\n\nAufgabe\nDie Korrelation prüft, ob bzw. inwieweit zwei Merkmale linear zusammenhängen.\nWie viele andere Verfahren kann die Korrelation als ein Spezialfall der Regression bzw. des linearen Modells \\(y = \\beta_0 + \\beta_1 + \\ldots \\beta_n + \\epsilon\\) betrachtet werden.\nAls ein spezielles Beispiel betrachten wir die Frage, ob das Gewicht eines Diamanten (carat) mit dem Preis (price) zusammenhängt (Datensatz diamonds).\nDen Datensatz können Sie so laden:\n\nlibrary(tidyverse)\ndata(diamonds)\n\n\nGeben Sie das Skalenniveau beider Variablen an!\nBetrachten Sie die Ausgabe von R:\n\n\nlm1 &lt;- lm(price ~ carat, data = diamonds)\nsummary(lm1)\n\n\nCall:\nlm(formula = price ~ carat, data = diamonds)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-18585   -805    -19    537  12732 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -2256.4       13.1    -173   &lt;2e-16 ***\ncarat         7756.4       14.1     551   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1550 on 53938 degrees of freedom\nMultiple R-squared:  0.849, Adjusted R-squared:  0.849 \nF-statistic: 3.04e+05 on 1 and 53938 DF,  p-value: &lt;2e-16\n\n\nWie (bzw. wo) ist aus dieser Ausgabe die Korrelation herauszulesen?\n\nMacht es einen Unterschied, ob man Preis mit Karat bzw. Karat mit Preis korreliert?\nIn der klassischen Inferenzstatistik ist der \\(p\\)-Wert eine zentrale Größe; ist er klein (\\(p&lt;.05\\)) so nennt man die zugehörige Statistik signifikant und verwirft die getestete Hypothese.\nIm Folgenden sehen Sie einen Korrelationstest auf statistische Signifikanz, mit R durchgeführt. Zeigt der Test ein (statistisch) signifikantes Ergebnis? Wie groß ist der “Unsicherheitskorridor”, um den Korrelationswert (zugleich Punktschätzer für den Populationswert)?\n\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.7.0 (red = needs update)\n✔ bayestestR  0.13.1   ✔ correlation 0.8.4 \n✖ datawizard  0.9.0    ✔ effectsize  0.8.6 \n✔ insight     0.19.7   ✔ modelbased  0.8.6 \n✔ performance 0.10.8   ✔ parameters  0.21.3\n✔ report      0.5.8    ✔ see         0.8.1 \n\nRestart the R-Session and update packages with `easystats::easystats_update()`.\n\ndiamonds %&gt;% \n  sample_n(30) %&gt;% \n  select(price, carat) %&gt;% \n  correlation()\n\n# Correlation Matrix (pearson-method)\n\nParameter1 | Parameter2 |    r |       95% CI | t(28) |         p\n-----------------------------------------------------------------\nprice      |      carat | 0.92 | [0.84, 0.96] | 12.74 | &lt; .001***\n\np-value adjustment method: Holm (1979)\nObservations: 30\n\n\n         \n\n\nLösung\n\ncarat ist metrisch (verhältnisskaliert) und price ist metrisch (verhältnisskaliert)\n\\(R^2\\) kann bei einer einfachen (univariaten) Regression als das Quadrat von \\(r\\) berechnet werden. Daher \\(r = \\sqrt{R^2}\\).\n\n\nsqrt(0.8493)\n\n[1] 0.92\n\n\nZum Vergleich\n\ndiamonds %&gt;% \n  summarise(r = cor(price, carat))\n\n# A tibble: 1 × 1\n      r\n  &lt;dbl&gt;\n1 0.922\n\n\nMan kann den Wert der Korrelation auch noch anderweitig berechnen (\\(\\beta\\) umrechnen in \\(\\rho\\)).\n\nNein. Die Korrelation ist eine symmetrische Relation.\nJa; die Zahl “3.81e-14” bezeichnet eine positive Zahl kleiner eins mit 13 Nullern vor der ersten Ziffer, die nicht Null ist (3.81 in diesem Fall). Der “Unsicherheitskorridor” reicht von etwa 0.87 bis 0.97.\n\n\nCategories:\n\ncorrelation\nlm\nregression\nstring"
  },
  {
    "objectID": "posts/germeval04/germeval04.html",
    "href": "posts/germeval04/germeval04.html",
    "title": "germeval04",
    "section": "",
    "text": "Erstellen Sie ein prädiktives Modell für Textdaten. Nutzen Sie Sentiments und TextFeatures im Rahmen von Feature-Engineering.\nNutzen Sie die GermEval-2018-Daten.\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\nDie Daten sind auch über das R-Paket PradaData zu beziehen.\n\nlibrary(tidyverse)\ndata(\"germeval_train\", package = \"pradadata\")\ndata(\"germeval_test\", package = \"pradadata\")\n\nDie AV lautet c1. Die (einzige) UV lautet: text.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\nNutzen Sie Tidymodels.\nNutzen Sie das sentiws Lexikon."
  },
  {
    "objectID": "posts/germeval04/germeval04.html#finalisieren",
    "href": "posts/germeval04/germeval04.html#finalisieren",
    "title": "germeval04",
    "section": "Finalisieren",
    "text": "Finalisieren\n\nfit1_best &lt;- select_best(wf1_fit)\n\n\nwf1_final &lt;- finalize_workflow(wf1, fit1_best)\nwf1_final_fit &lt;- fit(wf1_final, data = d_train)\n\nVorhersagen:\n\npreds &lt;- predict(wf1_final_fit, germeval_test)"
  },
  {
    "objectID": "posts/germeval04/germeval04.html#test-set-güte",
    "href": "posts/germeval04/germeval04.html#test-set-güte",
    "title": "germeval04",
    "section": "Test-Set-Güte",
    "text": "Test-Set-Güte\nUnd die Vorhersagen zum Test-Set hinzufügen, damit man TRUTH und ESTIMATE vergleichen kann:\n\nd_test &lt;-\n  germeval_test |&gt; \n  bind_cols(preds) |&gt; \n  mutate(c1 = as.factor(c1))\n\n\nmy_metrics &lt;- metric_set(accuracy, f_meas)\nmy_metrics(d_test,\n           truth = c1,\n           estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.639\n2 f_meas   binary         0.286"
  },
  {
    "objectID": "posts/germeval04/germeval04.html#fazit",
    "href": "posts/germeval04/germeval04.html#fazit",
    "title": "germeval04",
    "section": "Fazit",
    "text": "Fazit\nEine Reihe der Text-Features passen nicht gut auf nicht-englische Texte.\n\nCategories:\n\n2023\ntextmining\ndatawrangling\ngermeval\nprediction\ntidymodels\nsentiment\nstring"
  },
  {
    "objectID": "posts/mariokart-korr2/mariokart-korr2.html",
    "href": "posts/mariokart-korr2/mariokart-korr2.html",
    "title": "mariokart-korr2",
    "section": "",
    "text": "Aufgabe\nImportieren Sie den Datensatz mariokart in R. Filtern Sie die neuen Spiele. Berechnen Sie die Korrelation von Verkaufspreis (total_pr) und Startgebot (start_pr)!\nHinweise:\n\nRunden Sie auf 2 Dezimalstellen.\n\n         \n\n\nLösung\nPakete starten:\n\nlibrary(tidyverse)\nlibrary(easystats)\n\nDaten importieren:\n\nd_url &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\"\nd &lt;- data_read(d_url)\n\nOder so:\n\ndata(mariokart, package = \"openintro\")\n\n\nsolution &lt;- \nd  %&gt;% \n  filter(cond == \"new\") %&gt;% \n  summarise(pr_cor = cor(total_pr, start_pr))\nsolution\n\n    pr_cor\n1 0.405102\n\n\nAlternativ kann man (komfortabel) die Korrelation z.B. so berechnen:\n\nd %&gt;% \n  select(start_pr, total_pr, cond) %&gt;% \n  filter(cond == \"new\") %&gt;% \n  correlation()\n\n# Correlation Matrix (pearson-method)\n\nParameter1 | Parameter2 |    r |       95% CI | t(57) |       p\n---------------------------------------------------------------\nstart_pr   |   total_pr | 0.41 | [0.17, 0.60] |  3.35 | 0.001**\n\np-value adjustment method: Holm (1979)\nObservations: 59\n\n\nLösung: 0.41.\n\nCategories:\n\ndatawrangling\ndplyr\neda\nassociation\nnum"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-02/Verteilungen-Quiz-02.html",
    "href": "posts/Verteilungen-Quiz-02/Verteilungen-Quiz-02.html",
    "title": "Verteilungen-Quiz-02",
    "section": "",
    "text": "Beziehen Sie sich auf den Standard-Globusversuch mit \\(N=9\\) Würfen und \\(W=6\\) Wassertreffern (binomialverteilt).\nAufgabe: Ist es (auf dieser Basis) plausibler von einem 50%-PI [.6,.8] auszugehen als von einem 50%-PI [.05,.95]?\n\n\n  Ja    Nein    Keine Antwort möglich \n\nAntworten"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-02/Verteilungen-Quiz-02.html#answerlist",
    "href": "posts/Verteilungen-Quiz-02/Verteilungen-Quiz-02.html#answerlist",
    "title": "Verteilungen-Quiz-02",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/wskt-quiz10/wskt-quiz10.html",
    "href": "posts/wskt-quiz10/wskt-quiz10.html",
    "title": "wskt-quiz10",
    "section": "",
    "text": "Sei \\(X \\sim U(0, 2)\\).\nBehauptung: Es gilt: \\(f(X=1) = .5\\).\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n\n\n\nFalsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz10/wskt-quiz10.html#answerlist",
    "href": "posts/wskt-quiz10/wskt-quiz10.html#answerlist",
    "title": "wskt-quiz10",
    "section": "",
    "text": "Falsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz10/wskt-quiz10.html#answerlist-1",
    "href": "posts/wskt-quiz10/wskt-quiz10.html#answerlist-1",
    "title": "wskt-quiz10",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\nquiz\nprobability\nbayes\ndistribution\nquiz1-qm2-ws23\nschoice"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-05/Verteilungen-Quiz-05.html",
    "href": "posts/Verteilungen-Quiz-05/Verteilungen-Quiz-05.html",
    "title": "Verteilungen-Quiz-05",
    "section": "",
    "text": "Ist folgende Aussage wahr?\nIst eine stetige Verteilung symmetrisch, gilt dann\n\\(Pr(X \\ge \\bar{x} + 1) = Pr(X \\le \\bar{x} - 1)\\).\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-05/Verteilungen-Quiz-05.html#answerlist",
    "href": "posts/Verteilungen-Quiz-05/Verteilungen-Quiz-05.html#answerlist",
    "title": "Verteilungen-Quiz-05",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-05/Verteilungen-Quiz-05.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-05/Verteilungen-Quiz-05.html#answerlist-1",
    "title": "Verteilungen-Quiz-05",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/titanic_casestudy/titanic_casestudy.html",
    "href": "posts/titanic_casestudy/titanic_casestudy.html",
    "title": "titanic_casestudy",
    "section": "",
    "text": "Aufgabe\nFallstudie\nEine Analystin untersucht die Daten zum Titanic-Unglück.\n\nlibrary(tidyverse)\nlibrary(mosaic)\ndata(titanic_train, package = \"titanic\")\n\nZunächst berechnet Sie die Gesamt-Überlebensrate:\n\ntally(Survived ~ 1, data = titanic_train, format = \"percent\")\n\n        1\nSurvived        1\n       0 61.61616\n       1 38.38384\n\n\nDanach überprüft sie, ob sich die Geschlechter hinsichtlich der Überlebensrate unterscheiden.\n\nmosaicplot(Sex ~ Survived, data = titanic_train)\n\n\n\n\n\n\n\n\nAls dritten Schritt versucht Sie, die Überlebensrate auf Basis mehrerer Variablen vorherzusagen, dazu verwendet Sie ein lineares (Logit-)Modell.\n\nlm_titanic1 &lt;- glm(Survived ~ Sex + Age + Fare, \n                   data = titanic_train, family = \"binomial\")\n\nsummary(lm_titanic1)\n\n\nCall:\nglm(formula = Survived ~ Sex + Age + Fare, family = \"binomial\", \n    data = titanic_train)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.4107  -0.6376  -0.5875   0.7900   2.0342  \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.934841   0.239101   3.910 9.24e-05 ***\nSexmale     -2.347599   0.189956 -12.359  &lt; 2e-16 ***\nAge         -0.010570   0.006498  -1.627    0.104    \nFare         0.012773   0.002696   4.738 2.16e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 964.52  on 713  degrees of freedom\nResidual deviance: 716.07  on 710  degrees of freedom\n  (177 observations deleted due to missingness)\nAIC: 724.07\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nInterpretieren Sie das Ergebnis des Mosaicplots!\nKann man (fundiert) auf Basis dieses Modells sagen, dass das Geschlecht eine Ursache des Überlebens ist? Begründen Sie!\nWelche Variablen eignen sich (laut diesem Modell), um Überleben vorherzusagen?\nWelche Variable ist die wichtigste (laut diesem Modell)?\n\n         \n\n\nLösung\nInterpretieren Sie das Ergebnis des Mosaicplots!\n\nFrauen haben eine deutlich höhere Überlebensrate als Männer.\nEs gibt deutlich mehr Männer als Frauen.\n\nKann man (fundiert) auf Basis dieses Modells sagen, dass das Geschlecht eine Ursache des Überlebens ist? Begründen Sie!\n\nNein.\nZwar ist Geschlecht mit Überlebens korreliert (bzw. die beiden Variablen sind abhängig), aber das heißt noch nicht (zwingend), dass es eine kausale Beziehung ist. So wie “Störche” und “Babies” nur “scheinkorreliert” sind, könnte hier ebenfalls eine Scheinkorrelation vorliegen.\n\nWelche Variablen eignen sich (laut diesem Modell), um Überleben vorherzusagen?\n\nZu diesem Zweck wird mitunter die Signifikanz der Regressiongewichte \\(\\beta\\) herangezogen.\nHier sind sex und fare signifikant.\n\nWelche Variable ist die wichtigste (laut diesem Modell)?\n\nZu diesem Zweck kann der t-Wert herangezogen werden.\nFür sexMale ist dieser Wert (im Modell) am größten.\n\n\nsol &lt;- \"s. text\"\n\n\nCategories:\nstring"
  },
  {
    "objectID": "posts/groesse01/groesse01.html",
    "href": "posts/groesse01/groesse01.html",
    "title": "groesse01",
    "section": "",
    "text": "Aufgabe\nWir interessieren uns für die typische Körpergröße deutscher Studentis. Hier findet sich dazu ein Datensatz.\nAusgehend von der Annahme, dass sich die Körpergröße normalverteilt (innerhalb eines Geschlechts) suchen wir die Parameter der Normalverteilung, also Mittelwert und Streuung.\nGehen wir von folgenden Apriori-Wahrscheinlichkeiten für die Parameter der Normalverteilung aus:\n\nMittelwert: 150cm bis 200 cm, jeder Wert gleich plausibel, alle anderen Werte unmöglich\nSD: 1cm bis 20cm, jeder Wert gleich plausibel, alle anderen Werte unmöglich\n\nJa, das sind ziemlich einfältige Annahmen, aber gut, fangen wir damit an.\nErstellen Sie eine Bayes-Box!\nHinweise:\n\nUntersuchen Sie den angegebenen Parameterbereich in 1cm-Schritten.\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n         \n\n\nLösung\n\nlibrary(pradadata)  # für den Datensatz `wo_men`\nlibrary(prada)  # für bayesbox, alternativ mit `source`\nlibrary(tidyverse)\nlibrary(ggpubr)\n\nDaten importieren:\n\ndata(wo_men)\n\nMittelwert in der Stichprobe:\n\nwo_men |&gt; \n  group_by(sex) |&gt; \n  summarise(height_avg = mean(height, na.rm = TRUE),\n            height_sd = sd(height, na.rm = TRUE))\n\n# A tibble: 3 × 3\n  sex   height_avg height_sd\n  &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n1 man         183.      9.96\n2 woman       161.     42.8 \n3 &lt;NA&gt;        NaN      NA   \n\n\nZur Berechnung der Likelihoods diskretisieren wir die stetige Variable height in Stufen von jeweils 1cm, der Einfachheit halber.\nDie Wahrscheinlichkeit für das 1cm-Intervall um unserem Stichprobenergebnis herem (182.5cm bis 183.5cm), bei z.B. einem Mittelwert von 180cm und einer SD von 10cm, entspricht dann dieser Differenz:\n\nobere_grenze &lt;- pnorm(q = 183 + 0.5, mean = 180, sd = 10)\nuntere_grenze &lt;- pnorm(q = 183 - 0.5, mean = 180, sd = 10)\n\nobere_grenze\n\n[1] 0.6368307\n\nuntere_grenze\n\n[1] 0.5987063\n\nobere_grenze - untere_grenze\n\n[1] 0.03812433\n\n\nVisualisieren wir uns kurz dieses Intervall.\n\nlibrary(mosaic)\nxpnorm(q = c(182.5, 183.5), mean = 180, sd = 10)\n\n[1] 0.5987063 0.6368307\n\n\n\n\n\n\n\n\n\nAls nächstes legen wir die Werte für unsere Bayes-Box fest.\n\nnorm_mean &lt;- seq(from = 150, to = 200, by = 1)\nnorm_sd &lt;- seq(from = 1, to = 20, by = 1)\n\nJetzt bauen wir unsere Bayes-Box.\nWenn wir die Wahrscheinlichkeiten der Parameter für alle Kombinationen aus 51 Mittelwerten und 20 SD-Werten prüfen wollen, wird die Tabelle ganz schön lang:\n\nanzahl_kombinationen &lt;- length(norm_mean) * length(norm_sd)\nanzahl_kombinationen\n\n[1] 1020\n\n\nMit expand_grid kann man sich eine Tabelle erstellen lassen, die alle Kombinationen zweier Variablen aufschreibt:\n\nbayes_box &lt;-\n  expand_grid(norm_mean, norm_sd)\n\nhead(bayes_box)\n\n# A tibble: 6 × 2\n  norm_mean norm_sd\n      &lt;dbl&gt;   &lt;dbl&gt;\n1       150       1\n2       150       2\n3       150       3\n4       150       4\n5       150       5\n6       150       6\n\n\nDas sind unsere Parameterwerte: Jede Kombination eines Mittelwerts und einer Streuung ist eine Hypothese. Insgesamt haben wir also 1020 Parameterwerte.\nSo, bauen wir die Bayes-Box weiter:\n\nL &lt;- pnorm(183.5, mean = bayes_box$norm_mean, sd = bayes_box$norm_sd)\n\nbayes_box2 &lt;-\n  bayes_box |&gt; \n  mutate(hyp = 1:anzahl_kombinationen,\n         lik = L,\n         post_unstand = hyp * lik,\n         post_std = post_unstand / sum(post_unstand))\n\nSchauen wir uns die Post-Verteilung einmal an:\n\nggline(bayes_box2,\n       x = \"hyp\",\n       y = \"post_std\")\n\n\n\n\n\n\n\n\nOhje! Da stimmt was nicht! Warum sieht die Post-Verteilung so komisch aus?\nDie Antwort ist, dass für einen bestimmten Mittelwert jeweils 10 verschiedene SD-Werte zugeordnet sind. Und jeder SD-Wert (für einen MW-Wert) hat eine andere Post-Wahrscheinlichkeit!\nWas wir machen können, ist die beiden Parameter MW und SD einzeln aufzuschlüsseln, aber gemeinsam zu betrachten:\n\nbayes_box2 |&gt; \n  ggplot() +\n  aes(x = norm_sd,\n      y = norm_mean,\n      fill = post_std) +\n  geom_tile() +\n  scale_fill_viridis_c()\n\n\n\n\n\n\n\n\n\nCategories:\n\n2023\nbayes\nbayesbox\nstring"
  },
  {
    "objectID": "posts/IQ-Studentis/IQ-Studentis.html",
    "href": "posts/IQ-Studentis/IQ-Studentis.html",
    "title": "IQ-Studentis",
    "section": "",
    "text": "Exercise\nIntelligenz von Studentis\nEine Psychologin möchte die Intelligenz von Studentis bestimmen: Was ist wohl der Mittelwert? Wie schlau sind die schlausten 10%? Von wo bis wo geht das mittlere 90%-Intervall von IQ-Werten? Natürlich ist ihr klar, dass es nicht reicht, einen Mittelwert zu schätzen. Nein, sie will alles, sprich: die Posteriori-Verteilung.\nZuerst überlegt sie sich die Prioris: “Was ist meine Einschätzung zur Intelligenz von Studentis?”. Dazu liest sie alle verfügbare Literatur, beurteilt die methodische Qualität jeder einzelnen Studie und spricht mit den Expertis. Auf dieser Basis kommt sie zu folgenden Prioris:\n\\[\\mu \\sim \\mathcal{N}(115, 5)\\] Ein paar Überlegungen, die unsere Psychologin dazu hatte: Die Studentis sind im Mittel schlauer als die Normalbevölkerung. Um ein Gefühl für die Verteilungsfunktion vom IQ zu bekommen, nutzt sie folgenden R-Befehl:\n\npnorm(q = 115, mean = 100, sd = 15)\n\n[1] 0.8413447\n\n\nDieser Befehl gibt ihr an, welcher Prozentsatz der allgemeinen Bevölkerung (die Wahrscheinlichkeitsmasse) nicht schlauer ist als 115.\nDann versucht sie ein Gefühl für die Streuung (\\(\\sigma\\)) zu bekommen, folgender R-Befehl hilft ihr:\n\nq_iq &lt;- 50\nrate_lambda &lt;- 0.1\npexp(q = q_iq, rate = rate_lambda)\n\n[1] 0.9932621\n\n\nAh! Nimmt man an, dass Sigma exponentialverteilt ist mit einer Rate von 0.1, dass sind etwa 99 Prozent der Leute nicht mehr als q_iq IQ-Punkte vom Mittelwert \\(\\mu\\) entfernt. Das deckt sich mit ihren Informationen aus der Literatur.\nDamit sind die Priors spezifiziert.\nAugaben:\n\nGeben Sie die Priors an.\nSimulieren Sie die Prior-Prädiktiv-Verteilung dazu.\nBefragen Sie die Prior-Prädiktiv-Verteilung mit geeigneten Fragen Ihrer Wahl.\n\n         \nHinweise\n\n\nSolution\n\nGeben Sie die Priors an.\n\n\\[\\mu \\sim \\mathcal{N}(115, 5)\\]\n\\[\\sigma \\sim \\mathcal{E}(0.1)\\]\n\nSimulieren Sie die Prior-Prädiktiv-Verteilung dazu.\n\nZiehen wir Zufallszahlen entsprechend der Priori-Werte:\n\nlibrary(tidyverse)\nn &lt;- 1e4\n\nsim &lt;-\n  tibble(\n    sample_mu = rnorm(n,\n      mean = 115,\n      sd   = 10\n    ),\n    sample_sigma = rexp(n,\n      rate = 0.1\n    ),\n    iq = rnorm(n,\n      mean = sample_mu,\n      sd   = sample_sigma\n    )\n  )\n\nWas ist wohl der Mittelwert und die SD dieser Priori-Prädiktiv-Verteilung?\n\nheight_sim_sd &lt;-\n  sd(sim$iq) %&gt;% round()\nheight_sim_sd\n\n[1] 17\n\n\n\nheight_sim_mean &lt;-\n  mean(sim$iq) %&gt;% round()\nheight_sim_mean\n\n[1] 115\n\n\nUnd jetzt plotten wir diese Verteilung:\n\nsim %&gt;%\n  ggplot() +\n  aes(x = iq) +\n  geom_histogram() +\n  geom_point(\n    y = 0, x = height_sim_mean, size = 5,\n    color = \"blue\", alpha = .5\n  ) +\n  geom_vline(\n    xintercept = c(\n      height_sim_mean + height_sim_sd,\n      height_sim_mean - height_sim_sd\n    ),\n    linetype = \"dotted\"\n  ) +\n  labs(caption = \"Der blaue Punkt zeigt den Mittelwert; die gepunkteten Linien MD±SD\") +\n  scale_x_continuous(\n    limits = c(70, 145),\n    breaks = seq(70, 145, by = 5)\n  )\n\n\n\n\n\n\n\n\nOder vielleicht besser als Dichte-Diagramm, das zeigt das “Big Picture” vielleicht besser:\n\nsim %&gt;%\n  ggplot() +\n  aes(x = iq) +\n  geom_density()\n\n\n\n\n\n\n\n\nHm, etwas randlastig die Verteilung.\nZoomen wir etwas mehr rein:\n\nsim %&gt;%\n  ggplot() +\n  aes(x = iq) +\n  geom_density() +\n  scale_x_continuous(limits = c(65, 165))\n\n\n\n\n\n\n\n\n\nBefragen Sie die Prior-Prädiktiv-Verteilung mit geeigneten Fragen Ihrer Wahl.\n\nWas ist der Mittelwert und die SD und die üblichen deskriptiven Kennwerte?\n\nlibrary(easystats)\n\n\nsim %&gt;%\n  select(iq) %&gt;%\n  describe_distribution()\n\nVariable |   Mean |    SD |   IQR |            Range | Skewness | Kurtosis |     n | n_Missing\n----------------------------------------------------------------------------------------------\niq       | 114.82 | 17.06 | 18.16 | [-13.16, 276.43] |    -0.04 |     5.66 | 10000 |         0\n\n\nIn welchem Bereich liegen die mittleren 95% der IQ-Werte?\n\nsim %&gt;%\n  eti()\n\nEqual-Tailed Interval\n\nParameter    |         95% ETI\n------------------------------\nsample_mu    | [94.92, 134.59]\nsample_sigma | [ 0.26,  36.55]\niq           | [81.07, 148.30]\n\n\nAlternativ könnten wir in z-transformierten Daten denken:\n\nsim2 &lt;-\n  tibble(\n    sample_mu =\n      rnorm(n,\n        mean = 0,\n        sd   = 1\n      ),\n    sample_sigma =\n      rexp(n,\n        rate = 1\n      )\n  ) %&gt;%\n  mutate(\n    iq =\n      rnorm(n,\n        mean = sample_mu,\n        sd   = sample_sigma\n      )\n  )\n\n\nsim2 %&gt;%\n  ggplot() +\n  aes(x = iq) +\n  geom_density()\n\n\n\n\n\n\n\n\n\nCategories:\n\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/Rethink2m4/Rethink2m4.html",
    "href": "posts/Rethink2m4/Rethink2m4.html",
    "title": "Rethink2m4",
    "section": "",
    "text": "Aufgabe\nThis question is taken from McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Ed.). Taylor and Francis, CRC Press.\n2M4. Suppose you have a deck with only three cards. Each card has only two sides, and each side is either black or white. One card has two black sides. The second card has one black and one white side. The third card has two white sides. Now suppose all three cards are placed in a bag and shuffled. Someone reaches into the bag and pulls out a card and places it flat on a table. A black side is shown facing up, but you don’t know the color of the side facing down. Show that the probability that the other side is also black is 2/3. Use the counting method (Section 2 of the chapter) to approach this problem. This means counting up the ways that each card could produce the observed data (a black side faceing up on the table).\n         \n\n\nLösung\nLet’s firmly remember that our data is a black side is facing up.\nWe have three cards in the deck, giving us three hypothesises. Let’s label the cards bb (black on both sides), bw (black on one, white on the other), and ww (both sides are white), respectively.\nWanted is the probability that both sides are black (bb), given one side is black (1b): \\(Pr(bb|1b)\\).\nLet’s count the ways how the data - one black side - can come up in each conjecture (hypothesis), bb, bw, ww. Let’s denote “first side black” as 1b” and “second side black” as 2b (and similarly for white).\nHypothesis bb has 2 valid paths:\n\n\n\n\n\n\nThat is, if the card is black on both sides, there are two ways to get a black side.\nHypothesis bw has 1 valid path:\n\n\n\n\n\n\nGiven that we have observed a black side already, the other side must be white – assuming the card is bw.\nHypothesis ww has 0 valid path:\n\n\n\n\n\n\nThe Bayes-Box nicely summarizes these data:\n\n\n\n\n\n\n\n\nHyp\nPrior\nLikelihood\nunstand_post\nstd_post\n\n\n\n\nbb\n1\n2\n2\n0.67\n\n\nbw\n1\n1\n1\n0.33\n\n\nww\n1\n0\n0\n0.00\n\n\n\n\n\n\n\nThe important piece is that there are two ways that a all-black card (bb) can show a black side, since ist has two black sides.\n\nCategories:\n\nprobability\nbayes\nbayes-grid\nrethink-chap2\nstring"
  },
  {
    "objectID": "posts/mw-berechnen/mw-berechnen.html",
    "href": "posts/mw-berechnen/mw-berechnen.html",
    "title": "mw-berechnen",
    "section": "",
    "text": "Question\n\nAufgabe\nBerechnen Sie den Mittelwert folgender zahlenreihe; ignorieren sie etwaige fehlende Werte. Runden Sie auf zwei Dezimalstellen.\n\n\n[1] 0.97 0.54 0.34 0.00 0.30\n\n\n         \n\n\nLösung\nDer Mittelwert liegt bei 0.43.\nDie Antwort lautet 0.43.\nIn R kann man den Mittelwert z.B. so berechnen:\n\nmean(zahlenreihe, na.rm = TRUE)\n\n[1] 0.43\n\n\nDas Argument na.rm = TRUE sorgt dafür, dass R auch bei Vorhandensein fehlender Werte ein Ergebnis ausgibt. Ohne dieses Argument würde R ein sprödes NA zurückgeben, falls fehlende Werte vorliegen. Dieses Verhalten von R ist recht defensiv, getreu dem Motto: Wenn es ein Problem gibt, sollte man so früh wie möglich darüber deutlich informiert werden (und nicht erst, wenn die Marsrakete gestartet ist…).\n\nCategories:\n\neda\ndatawrangling\ndyn\nnum"
  },
  {
    "objectID": "posts/bike02/bike02.html",
    "href": "posts/bike02/bike02.html",
    "title": "bike02",
    "section": "",
    "text": "Kann man die Anzahl gerade verliehener Fahrräder eines entsprechenden Anbieters anhand der Temperatur vorhersagen?\nIn dieser Übung untersuchen wir diese Frage.\nSie können die Daten von der Webseite der UCI herunterladen.\nWir beziehen uns auf den Datensatz day.\nBerechnen Sie einen Entscheidungsbaum mit der Anzahl der aktuell vermieteten Räder als AV und der aktuellen Temperatur als UV!\nTunen Sie den Cp-Parameter des Baumes.\nGeben Sie den MSE an!\nHinweise"
  },
  {
    "objectID": "posts/bike02/bike02.html#data-split",
    "href": "posts/bike02/bike02.html#data-split",
    "title": "bike02",
    "section": "Data split",
    "text": "Data split\n\nset.seed(42)\nd_split &lt;- initial_split(d, strata = cnt)\n\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)"
  },
  {
    "objectID": "posts/bike02/bike02.html#define-recipe",
    "href": "posts/bike02/bike02.html#define-recipe",
    "title": "bike02",
    "section": "Define recipe",
    "text": "Define recipe\n\nrec1 &lt;- \n  recipe(cnt ~ temp, data = d)"
  },
  {
    "objectID": "posts/bike02/bike02.html#define-model",
    "href": "posts/bike02/bike02.html#define-model",
    "title": "bike02",
    "section": "Define model",
    "text": "Define model\n\nm1 &lt;-\n  decision_tree(cost_complexity = tune(),\n                mode = \"regression\")"
  },
  {
    "objectID": "posts/bike02/bike02.html#define-resamples",
    "href": "posts/bike02/bike02.html#define-resamples",
    "title": "bike02",
    "section": "Define Resamples",
    "text": "Define Resamples\n\nrsmpl &lt;- vfold_cv(d_train)"
  },
  {
    "objectID": "posts/bike02/bike02.html#workflow",
    "href": "posts/bike02/bike02.html#workflow",
    "title": "bike02",
    "section": "Workflow",
    "text": "Workflow\n\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(m1) %&gt;% \n  add_recipe(rec1)"
  },
  {
    "objectID": "posts/bike02/bike02.html#fit",
    "href": "posts/bike02/bike02.html#fit",
    "title": "bike02",
    "section": "Fit",
    "text": "Fit\n\ntic()\nfit1 &lt;- tune_grid(\n  object = wf1, \n  resamples = rsmpl)\ntoc()\n\n7.197 sec elapsed\n\nfit1\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits           id     .metrics          .notes          \n   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n 1 &lt;split [492/55]&gt; Fold01 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [492/55]&gt; Fold02 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [492/55]&gt; Fold03 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [492/55]&gt; Fold04 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [492/55]&gt; Fold05 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [492/55]&gt; Fold06 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [492/55]&gt; Fold07 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [493/54]&gt; Fold08 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [493/54]&gt; Fold09 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [493/54]&gt; Fold10 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;"
  },
  {
    "objectID": "posts/bike02/bike02.html#bester-kandidat",
    "href": "posts/bike02/bike02.html#bester-kandidat",
    "title": "bike02",
    "section": "Bester Kandidat",
    "text": "Bester Kandidat\n\nshow_best(fit1)\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\n\n# A tibble: 5 × 7\n  cost_complexity .metric .estimator  mean     n std_err .config              \n            &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1      0.0208     rmse    standard   1478.    10    34.7 Preprocessor1_Model09\n2      0.00220    rmse    standard   1538.    10    36.4 Preprocessor1_Model01\n3      0.000306   rmse    standard   1556.    10    40.3 Preprocessor1_Model07\n4      0.00000175 rmse    standard   1558.    10    39.7 Preprocessor1_Model02\n5      0.0000194  rmse    standard   1558.    10    39.7 Preprocessor1_Model03\n\n\n\nwf1_best &lt;-\n  wf1 %&gt;% \n  finalize_workflow(parameters = select_best(fit1))\n\nWarning: No value of `metric` was given; metric 'rmse' will be used."
  },
  {
    "objectID": "posts/bike02/bike02.html#last-fit",
    "href": "posts/bike02/bike02.html#last-fit",
    "title": "bike02",
    "section": "Last Fit",
    "text": "Last Fit\n\nfit_testsample &lt;- last_fit(wf1_best, d_split)"
  },
  {
    "objectID": "posts/bike02/bike02.html#model-performance-metrics-in-test-set",
    "href": "posts/bike02/bike02.html#model-performance-metrics-in-test-set",
    "title": "bike02",
    "section": "Model performance (metrics) in test set",
    "text": "Model performance (metrics) in test set\n\nfit_testsample %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    1430.    Preprocessor1_Model1\n2 rsq     standard       0.473 Preprocessor1_Model1\n\n\n\nMSE &lt;- fit_testsample %&gt;% collect_metrics() %&gt;% pluck(3, 1)\nMSE\n\n[1] 1430.304\n\n\nSolution: 1430.3042213\n\nCategories:\n\nstatlearning\ntidymodels\nnum"
  },
  {
    "objectID": "posts/kausal-bedrooms1/kausal-bedrooms1.html",
    "href": "posts/kausal-bedrooms1/kausal-bedrooms1.html",
    "title": "kausal-bedrooms1",
    "section": "",
    "text": "Exercise\nBetrachten wir den Datensatz SaratogaHouses, den Sie hier herunterladen können. Ein Codebook findet sich hier.\nSie kommen auch so an die Daten ran:\n\nlibrary(mosaicData)\ndata(\"SaratogaHouses\")\n\nGegeben sei in diesem Zusammenhang folgender DAG:\n\ndag1 &lt;- \"\ndag{\na -&gt; p\na -&gt; b -&gt; p\n}\n\"\n\nWobei a für (living) area steht, also der Wohnfläche eines Hauses, b für bedrooms, der Anzahl der Schlafzimmer und p für prize, den Preis, den das Haus beim Verkauf erzielt hat.\nSo sieht das dann aus:\n\nggdag(dag1) + theme_dag()\n\n\n\n\n\n\n\n\nUV sei a; AV sei p.\n\nBerechnen Sie den direkten Effekt der Wohnfläche auf den Preis!\nBerechnen Sie den totalen Effekt der Wohnfläche auf den Preis!\n\nMit direkter Effekt ist der kausale Effekt von UV auf AV - ohne Zwischenglieder (Mediatoren) - gemeint. Mit indirekter Effekt ist der kausale Effekt von UV über einen (oder ggf. mehrere) Mediator(en) auf die AV gemeint. Mit totaler Effekt ist die Summe des direkten plus des oder der indirekten Effekte gemeint.\nDas folgende Diagramm verdeutlicht diese drei Arten von Kausal-Effekten.\n\n(CC-BY-SA, 3275Sartell, Wikipedia)\nHinweise:\n\nGeben Sie jeweils den Punktschätzer eines linearen Regressionsmodells an!\nGehen Sie vom oben genannten DAG aus.\nRunden Sie ohne Dezimalstellen.\n\n         \n\n\nSolution\n\nd &lt;-\n  SaratogaHouses %&gt;% \n  select(price, bedrooms, livingArea) %&gt;% \n  drop_na()\n\n\ndirekter Effekt:\n\n\ndirekter_eff_lm &lt;-\n  stan_glm(price ~ bedrooms + livingArea, \n           data = d,\n           refresh = 0)\ncoef(direkter_eff_lm)\n\n(Intercept)    bedrooms  livingArea \n 36657.2918 -14205.7482    125.3559 \n\n\nUm einen direkten Effekt zu berechnen, müssen wir den spezifischen, uniquen Effekt der UV berechnen. Das erreichen wir durch eine multiple Regression, in der also die übrigen Prädiktoren aufgenommen sind. Das Resultat ist ein Koeffizient für die Assoziation der UV mit der AV, bereinigt um die Zusammenhänge der übrigen Prädiktoren.\nZur Erinnerung: Die multiple Regression liefert Koeffizienten pro Prädiktor, die bereinigt sind um den (statistischen) Einfluss der anderen Prädiktoren, mit anderne Worten: die Koeffizienten der multiplen Regression zeigen den Effekt von “nur diesem Prädiktor”.\nDer Punktschätzer für den direkten Effekt (von Wohnfläche) ist:\n\ndirekter_eff &lt;-\n  coef(direkter_eff_lm)[3] %&gt;% \n  round(0)\n\ndirekter_eff\n\nlivingArea \n       125 \n\n\n\ntotaler Effekt:\n\n\n\n(Intercept)  livingArea \n 13504.9835    113.0787 \n\n\nDer totale Effekt lässt sich berechnen, in dem man keine weiteren Prädiktoren neben der UV in die Regression mitaufnimmt. Die einfache (univariate) Regression zeigt den totalen Effekt der UV auf die AV.\nDer Punktschätzer für den totalen Effekt beträgt:\n\n\nlivingArea \n       113 \n\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/max-corr1/max-corr1.html",
    "href": "posts/max-corr1/max-corr1.html",
    "title": "max-corr1",
    "section": "",
    "text": "Aufgabe\nWelches Diagramm zeigt den stärksten (absoluten) linearen Zusammenhang (Korrelation)?\nGeben Sie die Nummer ein, die in der Kopfzeile jedes Teildiagramms angezeigt wird.\n         \n\n\nLösung\n\n\n\n\n\n\n\n\n\n\nCategories:\n\nvis\n‘2023’\nnum"
  },
  {
    "objectID": "posts/anim01/anim01.html",
    "href": "posts/anim01/anim01.html",
    "title": "anim01",
    "section": "",
    "text": "Visualisieren Sie in animierter Form den Zusammenhang von Lebenserwartung und Bruttosozialprodukt im Verlauf der Jahre (Datensatz gapminder); der Kontinent soll in der Visualisierung berücksichtigt sein.\nHinweise:\n\nNutzen Sie gganimate zur Visualisierung."
  },
  {
    "objectID": "posts/anim01/anim01.html#setup",
    "href": "posts/anim01/anim01.html#setup",
    "title": "anim01",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(gapminder)\nlibrary(gganimate)\ndata(gapminder)"
  },
  {
    "objectID": "posts/anim01/anim01.html#statisches-diagramm",
    "href": "posts/anim01/anim01.html#statisches-diagramm",
    "title": "anim01",
    "section": "Statisches Diagramm",
    "text": "Statisches Diagramm\n\np &lt;- gapminder %&gt;% \n  ggplot(aes(x = gdpPercap, y = lifeExp, color = continent, frame = year)) +\n  geom_point()+\n  scale_x_log10()\np"
  },
  {
    "objectID": "posts/anim01/anim01.html#animation",
    "href": "posts/anim01/anim01.html#animation",
    "title": "anim01",
    "section": "Animation",
    "text": "Animation\n\np + transition_time(year) +\n  labs(title = \"Year: {frame_time}\")\n\n\n\n\n\n\n\n\nDieser Post orientiert sich an dieser Quelle; dort finden sich auch mehr Beispiele.\n\nCategories:\n\n2023\nvis\nanimation\nstring"
  },
  {
    "objectID": "posts/mutate02/mutate02.html",
    "href": "posts/mutate02/mutate02.html",
    "title": "mutate02",
    "section": "",
    "text": "Aufgabe\nIm Datensatz mariokart:\nErzeugen Sie eine Spalte zu_teuer, die folgende Prüfung durchführt: total_pr &gt; 100.\nBerechnen Sie dann den Mittelwert der “zu teueren” Spiele.\nHinweise:\n\nRunden Sie auf die nächste ganze Zahl.\nBeachten Sie die üblichen Hinweise des Datenwerks.\n\n         \n\n\nLösung\nPakete starten:\n\nlibrary(tidyverse)  # startet das Paket tidyverse\n\nDaten importieren:\n\nmariokart &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\")\n\n\nmariokart |&gt; \n  mutate(zu_teuer = total_pr &gt; 100) |&gt; \n  filter(zu_teuer == TRUE) |&gt; \n  summarise(mw_zu_teuere_spiele = mean(total_pr))\n\n  mw_zu_teuere_spiele\n1             222.505\n\n\nDie Antwort lautet: 223.\n\nCategories:\n\ndatawrangling\neda\ntidyverse\ndplyr\nnum"
  },
  {
    "objectID": "posts/kollision-eignung/kollision-eignung.html",
    "href": "posts/kollision-eignung/kollision-eignung.html",
    "title": "kollision-eignung",
    "section": "",
    "text": "Sagen wir, über die Eignung, e, für ein Studium würden nur (die individuellen Ausprägungen) von Intelligenz (iq) und Fleiss (fleiss) entscheiden, s. den DAG in Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: Kollisionsstruktur im Dag zur Studiumseignung, mit s für Studium, f für fleiss und iq für Intelligenz\n\n\n\n\n\nBei positiver Eignung wird ein Studium aufgenommen (studium = 1) ansonsten nicht (studium = 0).\nQuelle\nEignung (fürs Studium) sei definiert als die Summe von iq und fleiss, plus etwas Glück, s. Listing 1.\n\n\n\n\nListing 1: Eignung ist die Summe von Fleiss und Intelligenz, plus ein Quentchen Glück\n\n\nset.seed(42)  # Reproduzierbarkeit\nN &lt;- 1e03  \n\nd_eignung &lt;-\ntibble(\n  iq = rnorm(N),  # normalverteilt mit MW=0, sd=1\n  fleiss = rnorm(N),\n  glueck = rnorm(N, mean = 0, sd = .1),\n  eignung = 1/2 * iq + 1/2 * fleiss + glueck,\n  # nur wer geeignet ist, studiert (in unserem Modell):\n  studium = ifelse(eignung &gt; 0, 1, 0) \n  )\n\n\n\n\nLaut unserem Modell setzt sich Eignung zur Hälfte aus Intelligenz und zur Hälfte aus Fleiss zusammen, plus etwas Glück.\nAufgabe: Zeigen Sie, dass eine Scheinkorrelation entsteht zwischen fleiss und iq, wenn man studium kontrolliert. Zeigen Sie außerdem, dass die Scheinkorrelation verschwindet, wenn man studium nicht kontrolliert.\nHinweise:\n\nBeachten Sie die Standardhinweise des Datenwerks."
  },
  {
    "objectID": "posts/kollision-eignung/kollision-eignung.html#setup",
    "href": "posts/kollision-eignung/kollision-eignung.html#setup",
    "title": "kollision-eignung",
    "section": "Setup",
    "text": "Setup\n\nlibrary(rstanarm)\nlibrary(easystats)"
  },
  {
    "objectID": "posts/kollision-eignung/kollision-eignung.html#modell-nur-studis",
    "href": "posts/kollision-eignung/kollision-eignung.html#modell-nur-studis",
    "title": "kollision-eignung",
    "section": "Modell Nur-Studis",
    "text": "Modell Nur-Studis\nHier ist das Modell, in dem wir nur Studenten betrachten, also studium == 1.\n\nm_eignung &lt;-\n  stan_glm(iq ~ fleiss, \n           data = d_eignung %&gt;%  filter(studium == 1), \n           refresh = 0)\n\nhdi(m_eignung)\n\nHighest Density Interval\n\nParameter   |        95% HDI\n----------------------------\n(Intercept) | [ 0.70,  0.86]\nfleiss      | [-0.53, -0.36]\n\nplot(estimate_relation(m_eignung))\n\n\n\n\n\n\n\n\nWie man sieht, gibt es einen Zusammenhang zwischen Fleiss und Intelligenz."
  },
  {
    "objectID": "posts/kollision-eignung/kollision-eignung.html#modell-alle-menschen",
    "href": "posts/kollision-eignung/kollision-eignung.html#modell-alle-menschen",
    "title": "kollision-eignung",
    "section": "Modell Alle-Menschen",
    "text": "Modell Alle-Menschen\n\nm_eignung_gesamtpop &lt;-\n  stan_glm(iq ~ fleiss, \n           data = d_eignung , \n           refresh = 0)\n\nplot(estimate_relation(m_eignung_gesamtpop))\n\nhdi(m_eignung_gesamtpop)\n\nHighest Density Interval\n\nParameter   |       95% HDI\n---------------------------\n(Intercept) | [-0.09, 0.03]\nfleiss      | [-0.05, 0.07]\n\n\n\n\n\n\n\n\n\nWie man sieht, löst sich der Zusammenhang zwischen Fleiss und Intelligenz auf, wenn man studium nicht kontrolliert."
  },
  {
    "objectID": "posts/Test-MSE2/Test-MSE2.html",
    "href": "posts/Test-MSE2/Test-MSE2.html",
    "title": "Test-MSE2",
    "section": "",
    "text": "Anhand der folgenden Abbildung sollen Aussagen zur Test-MSE (Mean Squared Error) untersucht werden.\n\nIn der linken Teil-Abbildung zeigt die Gerade die wahre Funktion; die übrigen Kurven sind verschiedene geschätzte Funktionen. In der rechten Teil-Abbildungen sind verschiedene Fehlerarten der geschätzten Funktionen dargestellt (in Bezug zur linken Teil-Abbildung).\nWelche Aussage zur rechten Teil-Abbildung ist richtig?\n\n\n\nDie obere (konvexe, nach rechts ansteigende) Kurve zeigt den Test-Fehler.\nDie obere (konvexe, nach rechts ansteigende) Kurve zeigt den Train-Fehler.\nKurven, die den Testfehler in Abhängigkeit der Modellflexibilität zeigen, sind nicht U-förmig.\nKurven, die den Trainfehler in Abhängigkeit der Modellflexibilität zeigen, gehen nicht gegen Null.\nBei sehr hoher Modellflexibilität nähern sich die Kurven für Train- und Testfehler zunehmend an."
  },
  {
    "objectID": "posts/Test-MSE2/Test-MSE2.html#answerlist",
    "href": "posts/Test-MSE2/Test-MSE2.html#answerlist",
    "title": "Test-MSE2",
    "section": "",
    "text": "Die obere (konvexe, nach rechts ansteigende) Kurve zeigt den Test-Fehler.\nDie obere (konvexe, nach rechts ansteigende) Kurve zeigt den Train-Fehler.\nKurven, die den Testfehler in Abhängigkeit der Modellflexibilität zeigen, sind nicht U-förmig.\nKurven, die den Trainfehler in Abhängigkeit der Modellflexibilität zeigen, gehen nicht gegen Null.\nBei sehr hoher Modellflexibilität nähern sich die Kurven für Train- und Testfehler zunehmend an."
  },
  {
    "objectID": "posts/Test-MSE2/Test-MSE2.html#answerlist-1",
    "href": "posts/Test-MSE2/Test-MSE2.html#answerlist-1",
    "title": "Test-MSE2",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nds1\ntidymodels\nstatlearning\nschoice"
  },
  {
    "objectID": "posts/Rethink2m2/Rethink2m2.html",
    "href": "posts/Rethink2m2/Rethink2m2.html",
    "title": "Rethink2m2",
    "section": "",
    "text": "Aufgabe\nThis question is taken from McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Ed.). Taylor and Francis, CRC Press.\nRecall the globe tossing model from the chapter. Compute and plot the grid approximate posterior distribution for each of the following sets of observations. In each case, assume a uniform prior for p.\nData:\n\nWWW\nWWWL\nLWWLWWW\n\nNow assume a prior for p that is equal to zero when p &lt; 0.5 and is a positive constant when p ≥ 0.5. Again compute and plot the grid approximate posterior distribution for each of the sets of observations in the problem just above.\nNB:\n\nConsider 21 different values for p such that \\(p = (0, .05, 1., .15, \\ldots, 1)\\).\nRound to 2 decimal places.\n\n         \n\n\nLösung\nThe solution is taken from this source.\n\nlibrary(tidyverse)\n\ndist &lt;- \n  tibble(\n    # Gridwerte bestimmen:\n    p_grid = seq(from = 0, to = 1, length.out = 21),\n    # Priori-Wskt bestimmen:\n    prior  = case_when(\n      p_grid &lt; 0.5 ~ 0,\n      p_grid &gt;= 0.5 ~ 1)) %&gt;%\n  mutate(\n    # Likelihood berechnen:\n    likelihood_1 = dbinom(3, size = 3, prob = p_grid),\n    likelihood_2 = dbinom(3, size = 4, prob = p_grid),\n    likelihood_3 = dbinom(5, size = 7, prob = p_grid),\n    # unstand. Posterior-Wskt:\n    unstand_post_1 = likelihood_1 * prior,\n    unstand_post_2 = likelihood_2 * prior,\n    unstand_post_3 = likelihood_3 * prior,\n    # stand. Post-Wskt:\n    std_post_1 = unstand_post_1 / sum(unstand_post_1),\n    std_post_2 = unstand_post_2 / sum(unstand_post_2),\n    std_post_3 = unstand_post_3 / sum(unstand_post_3)\n    ) \n\nHere is the Bayes Box:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np_grid\nprior\nlikelihood_1\nlikelihood_2\nlikelihood_3\nunstand_post_1\nunstand_post_2\nunstand_post_3\nstd_post_1\nstd_post_2\nstd_post_3\n\n\n\n\n0.00\n0\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n0.05\n0\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n0.10\n0\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n0.15\n0\n0.00\n0.01\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n0.20\n0\n0.01\n0.03\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n0.25\n0\n0.02\n0.05\n0.01\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n0.30\n0\n0.03\n0.08\n0.03\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n0.35\n0\n0.04\n0.11\n0.05\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n0.40\n0\n0.06\n0.15\n0.08\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n0.45\n0\n0.09\n0.20\n0.12\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n0.50\n1\n0.13\n0.25\n0.16\n0.13\n0.25\n0.16\n0.02\n0.07\n0.07\n\n\n0.55\n1\n0.17\n0.30\n0.21\n0.17\n0.30\n0.21\n0.03\n0.09\n0.10\n\n\n0.60\n1\n0.22\n0.35\n0.26\n0.22\n0.35\n0.26\n0.04\n0.10\n0.12\n\n\n0.65\n1\n0.27\n0.38\n0.30\n0.27\n0.38\n0.30\n0.05\n0.11\n0.13\n\n\n0.70\n1\n0.34\n0.41\n0.32\n0.34\n0.41\n0.32\n0.07\n0.12\n0.14\n\n\n0.75\n1\n0.42\n0.42\n0.31\n0.42\n0.42\n0.31\n0.08\n0.13\n0.14\n\n\n0.80\n1\n0.51\n0.41\n0.28\n0.51\n0.41\n0.28\n0.10\n0.12\n0.12\n\n\n0.85\n1\n0.61\n0.37\n0.21\n0.61\n0.37\n0.21\n0.12\n0.11\n0.09\n\n\n0.90\n1\n0.73\n0.29\n0.12\n0.73\n0.29\n0.12\n0.14\n0.09\n0.06\n\n\n0.95\n1\n0.86\n0.17\n0.04\n0.86\n0.17\n0.04\n0.16\n0.05\n0.02\n\n\n1.00\n1\n1.00\n0.00\n0.00\n1.00\n0.00\n0.00\n0.19\n0.00\n0.00\n\n\n\n\n\nJetzt können wir das Diagramm zeichnen.\nMit ggpubr:\n\nlibrary(ggpubr)\nggline(dist, \n       x = \"p_grid\", \n       y = \"std_post_1\")\n\n\n\n\n\n\n\n\nOder mit ggplot2:\n\nggplot(dist) +\n  aes(x = p_grid, y= std_post_1) +\n  geom_line()+\n  geom_point() +\n  labs(x = \"p(W)\",\n       y = \"Posteriori-Wahrscheinlichkeit\",\n       title = \"Daten: WWW\")\n\n\n\n\n\n\n\n\n\nggplot(dist) +\n  aes(x = p_grid, y= std_post_2) +\n  geom_line()+\n  geom_point() +\n  labs(x = \"p(W)\",\n       y = \"Posteriori-Wahrscheinlichkeit\",\n       title = \"Daten: WWWL\")\n\n\n\n\n\n\n\n\n\nggplot(dist) +\n  aes(x = p_grid, y= std_post_3) +\n  geom_line()+\n  geom_point() +\n  labs(x = \"p(W)\",\n       y = \"Posteriori-Wahrscheinlichkeit\",\n       title = \"Daten: LWWLWWW\")\n\n\n\n\n\n\n\n\n\nCategories:\n\nprobability\nbayes-grid\nbayes\nrethink-chap2\nstring"
  },
  {
    "objectID": "posts/bath42/bath42.html",
    "href": "posts/bath42/bath42.html",
    "title": "bath42",
    "section": "",
    "text": "Mehrere Proben werden zu einem unbekannten Planeten geschossen. Die Forschungsfrage lautet: Ist es die Erde (70% Wasseranteil) oder der Planet “Bath42” mit 90% Wasseranteil?\nWir sind indifferent (apriori) zu den Parameterwerten.\nDaten: 6 Treffer (Wasser) von 9 Versuchen (Proben).\nBehauptung: “Das ist fast sicher Bath42!”.\nAufgabe: Wie groß ist die Wahrscheinlichkeit, dass es sich um Bath42 handelt (und nicht um die Erde)?\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks."
  },
  {
    "objectID": "posts/bath42/bath42.html#answerlist",
    "href": "posts/bath42/bath42.html#answerlist",
    "title": "bath42",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\nquiz\nprobability\nbayes\nnum"
  },
  {
    "objectID": "posts/bike04/bike04.html",
    "href": "posts/bike04/bike04.html",
    "title": "bike04",
    "section": "",
    "text": "Kann man die Anzahl gerade verliehener Fahrräder eines entsprechenden Anbieters anhand der Temperatur vorhersagen?\nIn dieser Übung untersuchen wir diese Frage.\nSie können die Daten von der Webseite der UCI herunterladen.\nWir beziehen uns auf den Datensatz day.\nBerechnen Sie einen Entscheidungsbaum mit der Anzahl der aktuell vermieteten Räder als AV und der aktuellen Temperatur als UV!\nTunen Sie alle Paramter; lassen Sie sich 20 Tuningparameter vorschlagen.\nGeben Sie den MSE an!\nHinweise"
  },
  {
    "objectID": "posts/bike04/bike04.html#data-split",
    "href": "posts/bike04/bike04.html#data-split",
    "title": "bike04",
    "section": "Data split",
    "text": "Data split\n\nset.seed(42)\nd_split &lt;- initial_split(d, strata = cnt)\n\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)"
  },
  {
    "objectID": "posts/bike04/bike04.html#define-recipe",
    "href": "posts/bike04/bike04.html#define-recipe",
    "title": "bike04",
    "section": "Define recipe",
    "text": "Define recipe\n\nrec1 &lt;- \n  recipe(cnt ~ temp, data = d)"
  },
  {
    "objectID": "posts/bike04/bike04.html#define-model",
    "href": "posts/bike04/bike04.html#define-model",
    "title": "bike04",
    "section": "Define model",
    "text": "Define model\n\nm1 &lt;-\n  decision_tree(cost_complexity = tune(),\n                tree_depth = tune(),\n                min_n = tune(),\n                mode = \"regression\")"
  },
  {
    "objectID": "posts/bike04/bike04.html#tuning-grid",
    "href": "posts/bike04/bike04.html#tuning-grid",
    "title": "bike04",
    "section": "Tuning grid",
    "text": "Tuning grid\n\ngrid &lt;-\n  grid_latin_hypercube(cost_complexity(), \n               tree_depth(),\n               min_n(),\n               size = 20)\ngrid\n\n# A tibble: 20 × 3\n   cost_complexity tree_depth min_n\n             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt;\n 1        1.09e- 7          8    13\n 2        9.98e- 9         14    32\n 3        1.72e- 5          8    38\n 4        6.73e- 5         11     9\n 5        5.01e- 6         13    20\n 6        1.60e- 2          5    18\n 7        4.08e- 9         12     4\n 8        3.49e- 3          2     8\n 9        3.72e-10          9    27\n10        3.14e- 7         11    21\n11        3.92e- 2          3    30\n12        8.08e- 5          6    26\n13        1.04e- 6         14    33\n14        1.17e-10          1    36\n15        9.35e-10          4    16\n16        3.05e- 4          7    15\n17        1.80e- 6          6    23\n18        8.38e- 4          3     5\n19        8.01e- 3         13    11\n20        3.46e- 8         10    39\n\n\nAlternativ:\n\ngrid &lt;-\n  grid_latin_hypercube(extract_parameter_set_dials(m1), size = 50)\ngrid\n\n# A tibble: 50 × 3\n   cost_complexity tree_depth min_n\n             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt;\n 1   0.000390               6    21\n 2   0.0000000863           8    15\n 3   0.000576              12    37\n 4   0.0000000469           2    31\n 5   0.0000000283           5    19\n 6   0.00000000207          4     5\n 7   0.000000614            2    23\n 8   0.00000000952         14    13\n 9   0.00000413            11     7\n10   0.0000472              7    12\n# ℹ 40 more rows"
  },
  {
    "objectID": "posts/bike04/bike04.html#define-resamples",
    "href": "posts/bike04/bike04.html#define-resamples",
    "title": "bike04",
    "section": "Define Resamples",
    "text": "Define Resamples\n\nrsmpl &lt;- vfold_cv(d_train)"
  },
  {
    "objectID": "posts/bike04/bike04.html#workflow",
    "href": "posts/bike04/bike04.html#workflow",
    "title": "bike04",
    "section": "Workflow",
    "text": "Workflow\n\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(m1) %&gt;% \n  add_recipe(rec1)"
  },
  {
    "objectID": "posts/bike04/bike04.html#fit",
    "href": "posts/bike04/bike04.html#fit",
    "title": "bike04",
    "section": "Fit",
    "text": "Fit\n\ntic()\nfit1 &lt;- tune_grid(\n  object = wf1, \n  resamples = rsmpl)\ntoc()\n\n6.762 sec elapsed\n\nfit1\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits           id     .metrics          .notes          \n   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n 1 &lt;split [492/55]&gt; Fold01 &lt;tibble [20 × 7]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [492/55]&gt; Fold02 &lt;tibble [20 × 7]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [492/55]&gt; Fold03 &lt;tibble [20 × 7]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [492/55]&gt; Fold04 &lt;tibble [20 × 7]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [492/55]&gt; Fold05 &lt;tibble [20 × 7]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [492/55]&gt; Fold06 &lt;tibble [20 × 7]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [492/55]&gt; Fold07 &lt;tibble [20 × 7]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [493/54]&gt; Fold08 &lt;tibble [20 × 7]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [493/54]&gt; Fold09 &lt;tibble [20 × 7]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [493/54]&gt; Fold10 &lt;tibble [20 × 7]&gt; &lt;tibble [0 × 3]&gt;"
  },
  {
    "objectID": "posts/bike04/bike04.html#bester-kandidat",
    "href": "posts/bike04/bike04.html#bester-kandidat",
    "title": "bike04",
    "section": "Bester Kandidat",
    "text": "Bester Kandidat\n\nshow_best(fit1)\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\n\n# A tibble: 5 × 9\n  cost_complexity tree_depth min_n .metric .estimator  mean     n std_err\n            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1        3.92e- 4          2    18 rmse    standard   1443.    10    29.9\n2        1.46e- 2         11    38 rmse    standard   1453.    10    33.5\n3        1.23e- 2         14    10 rmse    standard   1458.    10    32.5\n4        1.17e- 9          3    29 rmse    standard   1459.    10    29.2\n5        4.46e-10          5    36 rmse    standard   1460.    10    29.9\n# ℹ 1 more variable: .config &lt;chr&gt;\n\n\n\nwf1_best &lt;-\n  wf1 %&gt;% \n  finalize_workflow(parameters = select_best(fit1))\n\nWarning: No value of `metric` was given; metric 'rmse' will be used."
  },
  {
    "objectID": "posts/bike04/bike04.html#last-fit",
    "href": "posts/bike04/bike04.html#last-fit",
    "title": "bike04",
    "section": "Last Fit",
    "text": "Last Fit\n\nfit_testsample &lt;- last_fit(wf1_best, d_split)"
  },
  {
    "objectID": "posts/bike04/bike04.html#model-performance-metrics-in-test-set",
    "href": "posts/bike04/bike04.html#model-performance-metrics-in-test-set",
    "title": "bike04",
    "section": "Model performance (metrics) in test set",
    "text": "Model performance (metrics) in test set\n\nfit_testsample %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    1399.    Preprocessor1_Model1\n2 rsq     standard       0.497 Preprocessor1_Model1\n\n\n\nMSE &lt;- fit_testsample %&gt;% collect_metrics() %&gt;% pluck(3, 1)\nMSE\n\n[1] 1398.675\n\n\nSolution: 1398.6748691\n\nCategories:\n\nstatlearning\ntidymodels\nnum"
  },
  {
    "objectID": "posts/Post-befragen1/Post-befragen1.html",
    "href": "posts/Post-befragen1/Post-befragen1.html",
    "title": "Post-befragen1",
    "section": "",
    "text": "Welcher R-Code passt am besten, um folgende Frage aus der Post-Verteilung herauszulesen:\n\nWie wahrscheinlich ist es, dass die mittlere Größe bei mind. 155 cm liegt?\n\nHinweise:\n\na ist der Achsenabschnitt, b ist das Regressionsgewicht.\npost_tab_df ist eine Tabelle (in Form eines R-Dataframe), die die Stichproben aus der Post-Verteilung enthält.\nEs handelt sich um Regressionsmodell, das mit der Bayes-Methode berechnet wurde.\nDer bzw. die Prädiktoren sind zentriert.\nEs handelt sich um den Datensatz aus McElreath’ Lehrbuch (Statistical Rethinking).\n\nCode A\n\npost_tab_df %&gt;% \n  count(gross = a == 155) %&gt;% \n  mutate(prop = n / sum(n))\n\nCode B\n\npost_tab_df %&gt;% \n\n  count(gross = a &gt; 155) %&gt;% \n  mutate(prop = n / sum(n))\n\nCode C\n\npost_tab_df %&gt;% \n  count(gross = a &lt;= 155) %&gt;% \n  mutate(prop = n / sum(n))\n\nCode D\n\npost_tab_df %&gt;% \n  count(gross = a &gt;= 155) %&gt;% \n  mutate(prop = n / sum(n))\n\nCode E\n\npost_tab_df %&gt;% \n  count(gross = a &lt; 155) %&gt;% \n  mutate(prop = n / sum(n))\n\n\n\n\nCode A\nCode B\nCode C\nCode D\nCode E"
  },
  {
    "objectID": "posts/Post-befragen1/Post-befragen1.html#answerlist",
    "href": "posts/Post-befragen1/Post-befragen1.html#answerlist",
    "title": "Post-befragen1",
    "section": "",
    "text": "Code A\nCode B\nCode C\nCode D\nCode E"
  },
  {
    "objectID": "posts/Post-befragen1/Post-befragen1.html#answerlist-1",
    "href": "posts/Post-befragen1/Post-befragen1.html#answerlist-1",
    "title": "Post-befragen1",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nWahr\nFalsch\n\n\nCategories:\n\nregression\nbayes\npost"
  },
  {
    "objectID": "posts/tidymodels1/tidymodels1.html",
    "href": "posts/tidymodels1/tidymodels1.html",
    "title": "tidymodels1",
    "section": "",
    "text": "Prof. Salzig übt sich im statistischen Lernen. Dazu will er das Überleben im Titanic-Unglück Vorhersagen; es handelt sich um eine klassische Aufgabe im statistischen Lernen. Betrachten Sie dazu den folgenden R-Code sowie die Kommentare dazu. Wählen Sie die am besten passende Aussage.\nZuerst lädt er die nötigen R-Pakete:\n\nlibrary(tidyverse)  # data wrangling\nlibrary(tidymodels)  # modelling\nlibrary(broom)  # tidy model output\nlibrary(parallel)  # multiple cores -- *nix only, d.h. Mac und Linux\nlibrary(finetune)  # tune race anova\n\nDann initialisiert er die Anzahl der Prozessoren auf seinem Computer:\n\ncores &lt;- parallel::detectCores(logical = FALSE)\ncores\n\n[1] 4\n\n\nDaten importieren:\n\ndata_path &lt;- \"https://raw.githubusercontent.com/sebastiansauer/Lehre\"\ntraindata_path_url  &lt;- \"/main/data/titanic/titanic_train.csv\"\ntestdata_path_url &lt;- \"/main/data/titanic/titanic_test.csv\"\n\ntraindata_url &lt;- paste0(data_path, traindata_path_url)\ntestdata_url &lt;- paste0(data_path, testdata_path_url)\n\n\n# import the data:\ntrain_raw &lt;- read_csv(traindata_url)\ntest &lt;- read_csv(testdata_url)\n\nUnd aufbereiten:\n\n# drop unused variables:\ntrain &lt;-\n  train_raw %&gt;% \n  select(-c(Name, Cabin, Ticket))\n\n# convert string to factors:\ntrain2 &lt;- \n  train %&gt;% \n  mutate(across(where(is.character), as.factor))\n  \n# convert numeric outcome to nominal, to indicate classification:\ntrain2 &lt;- \n  train2 %&gt;% \n  mutate(Survived = as.factor(Survived))\n\nGibt es fehlende Werte in der AV?\n\nsum(is.na(train2$Survived))\n\n[1] 0\n\n\nVorverarbeitung des Datensatzes macht er via ein recipe aus tidymodels:\n\ntitanic_recipe &lt;- \n  \n  # define model formula:\n  recipe(Survived ~ ., data = train2) %&gt;%\n  \n  # Use \"ID\" etc as ID, not as predictor:\n  update_role(PassengerId, new_role = \"ID\") %&gt;% \n  \n   # impute missing values:\n  step_impute_bag(all_predictors()) %&gt;% \n  \n  # convert to dummy variables:\n  step_dummy(all_nominal_predictors())\n\nCheck no missings:\n\ntitanic_train_baked &lt;- titanic_recipe %&gt;% prep() %&gt;% bake(new_data = NULL)\n\nsum(is.na(titanic_train_baked))\n\n[1] 0\n\n\nDann definiert ein ein Modell:\n\nrf_mod2 &lt;- \n  rand_forest(mtry = tune(), # tune mtry\n              min_n = tune(), # tune minimal n per node\n              trees = 1000) %&gt;%  # set number of trees to 1000\n  set_engine(\"ranger\", \n             num.threads = cores) %&gt;% \n  set_mode(\"classification\")\n\n… und ein Kreuzvalidierungsschema:\n\ntrain_cv &lt;- vfold_cv(train2, \n                     v = 10,\n                     repeats = 1, \n                     strata = \"Survived\")\n\nAus der Hilfe zu vfold_cv:\n\nV-Fold Cross-Validation\nDescription\nV-fold cross-validation randomly splits the data into V groups of roughly equal size (called “folds”). A resample of the analysis data consisted of V-1 of the folds while the assessment set contains the final fold. In basic V-fold cross-validation (i.e. no repeats), the number of resamples is equal to V.\nUsage\nvfold_cv(data, v = 10, repeats = 1, strata = NULL, breaks = 4, ...)\nArguments\ndata A data frame.\nv The number of partitions of the data set.\nrepeats The number of times to repeat the V-fold partitioning.\nstrata A variable that is used to conduct stratified sampling to create the folds. This could be a single character value or a variable name that corresponds to a variable that exists in the data frame.\nbreaks A single number giving the number of bins desired to stratify a numeric stratification variable.\n... Not currently used.\nDetails\nThe strata argument causes the random sampling to be conducted within the stratification variable. This can help ensure that the number of data points in the analysis data is equivalent to the proportions in the original data set. (Strata below 10% of the total are pooled together.) When more than one repeat is requested, the basic V-fold cross-validation is conducted each time. For example, if three repeats are used with v = 10, there are a total of 30 splits which as three groups of 10 that are generated separately.\n\nSo entsteht dieser Workflow:\n\ntitanic_rf_wf2 &lt;-\n  workflow() %&gt;% \n  add_model(rf_mod2) %&gt;% \n  add_recipe(titanic_recipe)\n\nJetzt: Fit the grid!\n\nset.seed(42)\n\nn_candidates &lt;- 2\n\nrf_res2 &lt;- \n  titanic_rf_wf2 %&gt;% \n  tune_race_anova(\n    resamples = train_cv,\n    grid = n_candidates,  # test 25 different tuning parameter values\n    #control = control_grid(save_pred = TRUE),\n    metrics = metric_set(roc_auc))\n\nMit dem Parameter grid kann man die Anzahl der zu berechnenden Kandidaten-Modelle festlegen.\nFür gute Vorhersagen bieten sich hohe Werte an; das kostet aber Rechenzeit.\nAus den Resampling-Kandidaten wählt er nun das beste aus:\n\nrf_best2 &lt;- \n  rf_res2 %&gt;% \n  select_best(metric = \"roc_auc\")\nrf_best2\n\n# A tibble: 1 × 3\n   mtry min_n .config             \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;               \n1     3    12 Preprocessor1_Model1\n\n\nDas beste Kandidatenmodell nutzt er nun, um den ganzen Train-Datensatz zu “fitten”:\n\n# write best parameter values to the workflow:\nrf_final_wf2 &lt;- \n  titanic_rf_wf2 %&gt;% \n  finalize_workflow(rf_best2)\n\n# fit the model:\nrf_final_model2 &lt;- \nrf_final_wf2 %&gt;% \n  fit(train2)\n\nZum Abschluss speichert er die Vorhersagen, die er dann bei Kaggle einreichen will:\n\nrf2_preds &lt;- \n  predict(rf_final_model2, new_data = test)  # compute prediction on test set\n\nEin letzter Blick auf die Verteilung der vorhergesagten Werte:\n\ncount(rf2_preds, .pred_class)\n\n# A tibble: 2 × 2\n  .pred_class     n\n  &lt;fct&gt;       &lt;int&gt;\n1 0             284\n2 1             134\n\n\nAuf Basis dieser Analyse: Wählen Sie am besten passende Aussage!\n\n\n\nEs wurden 2 Kandidaten von Tuningparameterwerten in die Analyse einbezogen.\nEs wurde kein Parameter-Tuning durchgeführt.\nDie Metrik \\(AUC\\) sollte nicht für Klassifikationsmodelle verwendet werden.\nEs wurde eine 10-fache Kreuzvalidierung (ohne Wiederholungen) verwendet.\nDie Anzahl der Bäume im Random Forest wurde hier nicht ins Parametertuning einbezogen; allerdings wäre es sinnvoll (und üblich), dies zu tun.\nder Parameter mtry wurde hier nicht ins Parametertuning einbezogen."
  },
  {
    "objectID": "posts/tidymodels1/tidymodels1.html#answerlist",
    "href": "posts/tidymodels1/tidymodels1.html#answerlist",
    "title": "tidymodels1",
    "section": "",
    "text": "Es wurden 2 Kandidaten von Tuningparameterwerten in die Analyse einbezogen.\nEs wurde kein Parameter-Tuning durchgeführt.\nDie Metrik \\(AUC\\) sollte nicht für Klassifikationsmodelle verwendet werden.\nEs wurde eine 10-fache Kreuzvalidierung (ohne Wiederholungen) verwendet.\nDie Anzahl der Bäume im Random Forest wurde hier nicht ins Parametertuning einbezogen; allerdings wäre es sinnvoll (und üblich), dies zu tun.\nder Parameter mtry wurde hier nicht ins Parametertuning einbezogen."
  },
  {
    "objectID": "posts/tidymodels1/tidymodels1.html#answerlist-1",
    "href": "posts/tidymodels1/tidymodels1.html#answerlist-1",
    "title": "tidymodels1",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nds1\ntidymodels\nprediction\nyacsda\nstatlearning\ndyn\nschoice"
  },
  {
    "objectID": "posts/Bayesmod-bestimmen01/Bayesmod-bestimmen01.html",
    "href": "posts/Bayesmod-bestimmen01/Bayesmod-bestimmen01.html",
    "title": "Bayesmod-bestimmen01",
    "section": "",
    "text": "Exercise\nSie möchten, im Rahmen einer Studie, ein einfaches lineare Modell spezifizieren, d.h. den Likelihood und die Priori-Verteilungen benennen.\nFolgende Informationen sind gegeben:\n\nAV: einnahmen\nUV: werbebudget\nAlle empirischen Variablen sind z-standardisiert.\nAlle Variablen sollen als normalverteilt angegeben werden mit Ausnahme der Streuung der AV, diese ist exponenzialverteilt mit Rate 1 zu modellieren.\nStreuungen der Normalverteilung sind mit 2.5 SD anzugeben.\n\nSchreiben Sie in mathematischer Notation folgende Notation auf:\nDie Priori-Verteilung des Regressionsgewichts\nHinweise:\n\nVerzichten Sie auf Leerstellen in Ihrer Antwort. \nBenennen Sie \\(\\beta\\) mit b, \\(\\alpha\\) mit a und \\(\\sigma\\) mit s.\nNutzen Sie die Tilde ~ um stochastische Relationen (Verteilungen) anzuzeigen.\nGeben Sie Normalverteilungen als Normal(x;y) und Exponentialverteilung als Exp(x) an (jeweils mit den korrekten Argumenten in der allgemein üblichen Form).\n\n         \n\n\nSolution\nb~Normal(0, 2.5)\n\nCategories:\n\nregression\nbayes\nprior"
  },
  {
    "objectID": "posts/Kennwert-robust/Kennwert-robust.html",
    "href": "posts/Kennwert-robust/Kennwert-robust.html",
    "title": "Kennwert-robust",
    "section": "",
    "text": "Welcher der folgenden Kennwerte ist robust (im statistischen Sinn)?\n\n\n\nMedian\nMittelwert\nKorrelation\nStandardabweichung\nVarianz\nMaximalwert\nMinimalwert"
  },
  {
    "objectID": "posts/Kennwert-robust/Kennwert-robust.html#answerlist",
    "href": "posts/Kennwert-robust/Kennwert-robust.html#answerlist",
    "title": "Kennwert-robust",
    "section": "",
    "text": "Median\nMittelwert\nKorrelation\nStandardabweichung\nVarianz\nMaximalwert\nMinimalwert"
  },
  {
    "objectID": "posts/Kennwert-robust/Kennwert-robust.html#answerlist-1",
    "href": "posts/Kennwert-robust/Kennwert-robust.html#answerlist-1",
    "title": "Kennwert-robust",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\neda\nlagemaße\nvariability\nschoice"
  },
  {
    "objectID": "posts/Likelihood2/Likelihood2.html",
    "href": "posts/Likelihood2/Likelihood2.html",
    "title": "Likelihood2",
    "section": "",
    "text": "Der Likelihood eines Datensatzes ist definiert als das Produkt der Likelihoods aller Beobachtungen:\n\\[\\mathcal{L} = \\prod_{i=1}^n \\mathcal{L_i}\\]\nwobei die Beobachtungen bzw. ihre Likelihood als unabhängig angenommen werden: \\(\\mathcal{L_i} \\perp \\mathcal{L_j}, \\quad i \\ne j\\).\nJe größer \\(n\\), desto …….. \\(\\mathcal{L}\\)!\nFüllen Sie die Lücke!\n\n\n\ngrößer\nkleiner\nunabhängig voneinander\nkeine Aussage möglich\nkommt auf weitere, hier nicht benannte Bedingungen an"
  },
  {
    "objectID": "posts/Likelihood2/Likelihood2.html#answerlist",
    "href": "posts/Likelihood2/Likelihood2.html#answerlist",
    "title": "Likelihood2",
    "section": "",
    "text": "größer\nkleiner\nunabhängig voneinander\nkeine Aussage möglich\nkommt auf weitere, hier nicht benannte Bedingungen an"
  },
  {
    "objectID": "posts/Likelihood2/Likelihood2.html#answerlist-1",
    "href": "posts/Likelihood2/Likelihood2.html#answerlist-1",
    "title": "Likelihood2",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nRichtig\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nregression\nbayes\nlikelihood"
  },
  {
    "objectID": "posts/kausal05/kausal05.html",
    "href": "posts/kausal05/kausal05.html",
    "title": "kausal05",
    "section": "",
    "text": "Im Rahmen einer Studie soll untersucht werden, ob eine Influenza-Infektion einen (kausalen) Einfluss auf eine Covid19-Infektion hat.\nIn Wahrheit (aber unbekannt) sei der DAG wie folgt (s.u.).\n\n\n\n\n\n\n\n\n\nIst es sinnvoll, das Auftreten von Fieber (Fever) zu kontrollieren?\n\n\n\nNein, da durch eine Kontrolle von Fever eine Verzerrung erzeugt wird (Kollisionsverzerrung)\nJa, durch eine Kontrolle von Fever ist ein kausaler Effekt identifizierbar\nJa, eine Kontrolle von Fever ist zwar nicht nötig, aber wird zu exakteren Ergebnissen führen\nNein, da eine Kontrolle von Fever eine Verzerrung erzeugt wird (Konfundierung)\nNein, da eine Kontrolle von Fever nicht nötig ist (aber auch nicht schädlich)"
  },
  {
    "objectID": "posts/kausal05/kausal05.html#answerlist",
    "href": "posts/kausal05/kausal05.html#answerlist",
    "title": "kausal05",
    "section": "",
    "text": "Nein, da durch eine Kontrolle von Fever eine Verzerrung erzeugt wird (Kollisionsverzerrung)\nJa, durch eine Kontrolle von Fever ist ein kausaler Effekt identifizierbar\nJa, eine Kontrolle von Fever ist zwar nicht nötig, aber wird zu exakteren Ergebnissen führen\nNein, da eine Kontrolle von Fever eine Verzerrung erzeugt wird (Konfundierung)\nNein, da eine Kontrolle von Fever nicht nötig ist (aber auch nicht schädlich)"
  },
  {
    "objectID": "posts/kausal05/kausal05.html#answerlist-1",
    "href": "posts/kausal05/kausal05.html#answerlist-1",
    "title": "kausal05",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/postvert-vis-zwielicht/postvert-vis-zwielicht.html",
    "href": "posts/postvert-vis-zwielicht/postvert-vis-zwielicht.html",
    "title": "postvert-vis-zwielicht",
    "section": "",
    "text": "Aufgabe\nLesen Sie diesen Abschnitt.\nBauen Sie daraufhin die dort gezeigte Abbildung der Post-Verteilung nach.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n         \n\n\nLösung\nPakete starten:\nPost-Verteilung erstellen:\nStichproben ziehen aus der Posteriori-Verteilung:\nVerteilung visualisieren, z.B. mit ggpubr:\n\n\n\n\n\n\n\n\n\nOder mit purem ggplot:\n\n\n\n\n\n\n\n\n\nEs reicht i.d.R. vollkommen, wenn Sie eine der beiden Möglichkeiten beherrschen.\nTipp: Fragen Sie ChatGPT and Friends nach dem R-Code.\n\nCategories:\n\n2023\nvis\nbayes\npost\nstring"
  },
  {
    "objectID": "posts/mutate03/mutate03.html",
    "href": "posts/mutate03/mutate03.html",
    "title": "mutate03",
    "section": "",
    "text": "Aufgabe\nIm Datensatz mariokart:\nGrupieren Sie die den Datensatz in zwei Gruppen:\n\nkeinem oder einem Lenkrad\n2 oder mehr Lenkräder\n\nBerechnen Sie dann den Mittelwert zum Verkaufspreis der Spiele der 1. Gruppe.\nHinweise:\n\nRunden Sie auf die nächste ganze Zahl.\nBeachten Sie die üblichen Hinweise des Datenwerks.\n\n         \n\n\nLösung\nPakete starten:\n\nlibrary(tidyverse)  # startet das Paket tidyverse\n\nDaten importieren:\n\nmariokart &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\")\n\nWie ist die Verteilung von wheels?\n\nmariokart |&gt; \n  mutate(Anz_wheels_gruppe = \n           case_when(wheels &lt;= 1 ~ \"0-1\",\n                     wheels &gt;= 1 ~ \"2 oder mehr\")) |&gt; \n  group_by(Anz_wheels_gruppe) |&gt; \n  summarise(total_pr = mean(total_pr))\n\n# A tibble: 2 × 2\n  Anz_wheels_gruppe total_pr\n  &lt;chr&gt;                &lt;dbl&gt;\n1 0-1                   42.9\n2 2 oder mehr           61.4\n\n\nDie Antwort lautet: 43.\n\nCategories:\n\ndatawrangling\neda\ntidyverse\ndplyr\nnum"
  },
  {
    "objectID": "posts/kausal02/kausal02.html",
    "href": "posts/kausal02/kausal02.html",
    "title": "kausal02",
    "section": "",
    "text": "Gegeben sei der DAG g (s.u.). Welche Variable/n sind zu kontrollieren, um den kausalen Effekt von x auf y zu identifizieren?\n\n\n\n\n\n\n\n\n\n\n\n\nz\nkeine, bereits identifiziert\nx\ny\nkeine, nicht identifizierbar"
  },
  {
    "objectID": "posts/kausal02/kausal02.html#answerlist",
    "href": "posts/kausal02/kausal02.html#answerlist",
    "title": "kausal02",
    "section": "",
    "text": "z\nkeine, bereits identifiziert\nx\ny\nkeine, nicht identifizierbar"
  },
  {
    "objectID": "posts/kausal02/kausal02.html#answerlist-1",
    "href": "posts/kausal02/kausal02.html#answerlist-1",
    "title": "kausal02",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal\nexam-22"
  },
  {
    "objectID": "posts/kekse02/kekse02.html",
    "href": "posts/kekse02/kekse02.html",
    "title": "kekse02",
    "section": "",
    "text": "Aufgabe\nIn Think Bayes stellt Allen Downey folgende Aufgabe:\n“Next let’s solve a cookie problem with 101 bowls:\nBowl 0 contains 0% vanilla cookies,\nBowl 1 contains 1% vanilla cookies,\nBowl 2 contains 2% vanilla cookies,\nand so on, up to\nBowl 99 contains 99% vanilla cookies, and\nBowl 100 contains all vanilla cookies.\nAs in the previous version, there are only two kinds of cookies, vanilla and chocolate. So Bowl 0 is all chocolate cookies, Bowl 1 is 99% chocolate, and so on.\nSuppose we choose a bowl at random, choose a cookie at random, and it turns out to be vanilla. What is the probability that the cookie came from Bowl \\(x\\), for each value of \\(x\\)?”\nHinweise:\n\nUntersuchen Sie die Hypothesen \\(\\pi_0 = 0, \\pi_1 = 0.1, \\pi_2 = 0.2, ..., \\pi_{10} = 1\\) für die Trefferwahrscheinlichkeit\nErstellen Sie ein Bayes-Gitter zur Lösung dieser Aufgabe.\nGehen Sie davon aus, dass Sie (apriori) indifferent gegenüber der Hypothesen zu den Parameterwerten sind.\nGeben Sie Prozentzahlen immer als Anteil an und lassen Sie die führende Null weg (z.B. .42).\n\n         \n\n\nLösung\n\nd &lt;-\n  tibble(\n    # definiere die Hypothesen (das \"Gitter\"): \n    p_Gitter = 0:100 / 101,\n    # bestimme den Priori-Wert:       \n    Priori  = 1) %&gt;%  \n    mutate(\n      # berechne Likelihood für jeden Gitterwert:\n      Likelihood = p_Gitter,\n      # berechen unstand. Posteriori-Werte:\n      unstd_Post = Likelihood * Priori,\n      # berechne stand. Posteriori-Werte (summiert zu 1):\n      Post = unstd_Post / sum(unstd_Post))  \n\n\n\n\n\n\np_Gitter\nPriori\nLikelihood\nunstd_Post\nPost\n\n\n\n\n0.00\n1\n0.00\n0.00\n0.00\n\n\n0.01\n1\n0.01\n0.01\n0.00\n\n\n0.02\n1\n0.02\n0.02\n0.00\n\n\n0.03\n1\n0.03\n0.03\n0.00\n\n\n0.04\n1\n0.04\n0.04\n0.00\n\n\n0.05\n1\n0.05\n0.05\n0.00\n\n\n0.06\n1\n0.06\n0.06\n0.00\n\n\n0.07\n1\n0.07\n0.07\n0.00\n\n\n0.08\n1\n0.08\n0.08\n0.00\n\n\n0.09\n1\n0.09\n0.09\n0.00\n\n\n0.10\n1\n0.10\n0.10\n0.00\n\n\n0.11\n1\n0.11\n0.11\n0.00\n\n\n0.12\n1\n0.12\n0.12\n0.00\n\n\n0.13\n1\n0.13\n0.13\n0.00\n\n\n0.14\n1\n0.14\n0.14\n0.00\n\n\n0.15\n1\n0.15\n0.15\n0.00\n\n\n0.16\n1\n0.16\n0.16\n0.00\n\n\n0.17\n1\n0.17\n0.17\n0.00\n\n\n0.18\n1\n0.18\n0.18\n0.00\n\n\n0.19\n1\n0.19\n0.19\n0.00\n\n\n0.20\n1\n0.20\n0.20\n0.00\n\n\n0.21\n1\n0.21\n0.21\n0.00\n\n\n0.22\n1\n0.22\n0.22\n0.00\n\n\n0.23\n1\n0.23\n0.23\n0.00\n\n\n0.24\n1\n0.24\n0.24\n0.00\n\n\n0.25\n1\n0.25\n0.25\n0.00\n\n\n0.26\n1\n0.26\n0.26\n0.01\n\n\n0.27\n1\n0.27\n0.27\n0.01\n\n\n0.28\n1\n0.28\n0.28\n0.01\n\n\n0.29\n1\n0.29\n0.29\n0.01\n\n\n0.30\n1\n0.30\n0.30\n0.01\n\n\n0.31\n1\n0.31\n0.31\n0.01\n\n\n0.32\n1\n0.32\n0.32\n0.01\n\n\n0.33\n1\n0.33\n0.33\n0.01\n\n\n0.34\n1\n0.34\n0.34\n0.01\n\n\n0.35\n1\n0.35\n0.35\n0.01\n\n\n0.36\n1\n0.36\n0.36\n0.01\n\n\n0.37\n1\n0.37\n0.37\n0.01\n\n\n0.38\n1\n0.38\n0.38\n0.01\n\n\n0.39\n1\n0.39\n0.39\n0.01\n\n\n0.40\n1\n0.40\n0.40\n0.01\n\n\n0.41\n1\n0.41\n0.41\n0.01\n\n\n0.42\n1\n0.42\n0.42\n0.01\n\n\n0.43\n1\n0.43\n0.43\n0.01\n\n\n0.44\n1\n0.44\n0.44\n0.01\n\n\n0.45\n1\n0.45\n0.45\n0.01\n\n\n0.46\n1\n0.46\n0.46\n0.01\n\n\n0.47\n1\n0.47\n0.47\n0.01\n\n\n0.48\n1\n0.48\n0.48\n0.01\n\n\n0.49\n1\n0.49\n0.49\n0.01\n\n\n0.50\n1\n0.50\n0.50\n0.01\n\n\n0.50\n1\n0.50\n0.50\n0.01\n\n\n0.51\n1\n0.51\n0.51\n0.01\n\n\n0.52\n1\n0.52\n0.52\n0.01\n\n\n0.53\n1\n0.53\n0.53\n0.01\n\n\n0.54\n1\n0.54\n0.54\n0.01\n\n\n0.55\n1\n0.55\n0.55\n0.01\n\n\n0.56\n1\n0.56\n0.56\n0.01\n\n\n0.57\n1\n0.57\n0.57\n0.01\n\n\n0.58\n1\n0.58\n0.58\n0.01\n\n\n0.59\n1\n0.59\n0.59\n0.01\n\n\n0.60\n1\n0.60\n0.60\n0.01\n\n\n0.61\n1\n0.61\n0.61\n0.01\n\n\n0.62\n1\n0.62\n0.62\n0.01\n\n\n0.63\n1\n0.63\n0.63\n0.01\n\n\n0.64\n1\n0.64\n0.64\n0.01\n\n\n0.65\n1\n0.65\n0.65\n0.01\n\n\n0.66\n1\n0.66\n0.66\n0.01\n\n\n0.67\n1\n0.67\n0.67\n0.01\n\n\n0.68\n1\n0.68\n0.68\n0.01\n\n\n0.69\n1\n0.69\n0.69\n0.01\n\n\n0.70\n1\n0.70\n0.70\n0.01\n\n\n0.71\n1\n0.71\n0.71\n0.01\n\n\n0.72\n1\n0.72\n0.72\n0.01\n\n\n0.73\n1\n0.73\n0.73\n0.01\n\n\n0.74\n1\n0.74\n0.74\n0.01\n\n\n0.75\n1\n0.75\n0.75\n0.02\n\n\n0.76\n1\n0.76\n0.76\n0.02\n\n\n0.77\n1\n0.77\n0.77\n0.02\n\n\n0.78\n1\n0.78\n0.78\n0.02\n\n\n0.79\n1\n0.79\n0.79\n0.02\n\n\n0.80\n1\n0.80\n0.80\n0.02\n\n\n0.81\n1\n0.81\n0.81\n0.02\n\n\n0.82\n1\n0.82\n0.82\n0.02\n\n\n0.83\n1\n0.83\n0.83\n0.02\n\n\n0.84\n1\n0.84\n0.84\n0.02\n\n\n0.85\n1\n0.85\n0.85\n0.02\n\n\n0.86\n1\n0.86\n0.86\n0.02\n\n\n0.87\n1\n0.87\n0.87\n0.02\n\n\n0.88\n1\n0.88\n0.88\n0.02\n\n\n0.89\n1\n0.89\n0.89\n0.02\n\n\n0.90\n1\n0.90\n0.90\n0.02\n\n\n0.91\n1\n0.91\n0.91\n0.02\n\n\n0.92\n1\n0.92\n0.92\n0.02\n\n\n0.93\n1\n0.93\n0.93\n0.02\n\n\n0.94\n1\n0.94\n0.94\n0.02\n\n\n0.95\n1\n0.95\n0.95\n0.02\n\n\n0.96\n1\n0.96\n0.96\n0.02\n\n\n0.97\n1\n0.97\n0.97\n0.02\n\n\n0.98\n1\n0.98\n0.98\n0.02\n\n\n0.99\n1\n0.99\n0.99\n0.02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCategories:\n\nprobability\nbayes-grid\nnum"
  },
  {
    "objectID": "posts/pca/pca.html",
    "href": "posts/pca/pca.html",
    "title": "pca",
    "section": "",
    "text": "Die zwei Hauptkomponenten sind nicht orthogonal (unabhängig).\nDie erste Hauptkomponente ist stets durch den kürzeren Pfeil gekennzeichnet.\nDie erste Hauptkomponente ist unkorrelliert zur ersten.\nDie erste Hauptkomponenten beinhaltet weniger Variation als die erste."
  },
  {
    "objectID": "posts/pca/pca.html#answerlist",
    "href": "posts/pca/pca.html#answerlist",
    "title": "pca",
    "section": "",
    "text": "Die zwei Hauptkomponenten sind nicht orthogonal (unabhängig).\nDie erste Hauptkomponente ist stets durch den kürzeren Pfeil gekennzeichnet.\nDie erste Hauptkomponente ist unkorrelliert zur ersten.\nDie erste Hauptkomponenten beinhaltet weniger Variation als die erste."
  },
  {
    "objectID": "posts/Logikpruefung2/Logikpruefung2.html",
    "href": "posts/Logikpruefung2/Logikpruefung2.html",
    "title": "Logikpruefung2",
    "section": "",
    "text": "Aufgabe\nWir definieren x wie folgt:\n\nx &lt;- c(-1, 0, 1)\n\nGeben Sie die Syntax an, für die Prüfung, ob x positiv ist.\n         \n\n\nLösung\n\nx &gt; 0\n\n[1] FALSE FALSE  TRUE\n\n\n\nCategories:\n\nR\n‘2023’\nLogikpruefung2"
  },
  {
    "objectID": "posts/Bayes-Ziel1/Bayes-Ziel1.html",
    "href": "posts/Bayes-Ziel1/Bayes-Ziel1.html",
    "title": "Bayes-Ziel1",
    "section": "",
    "text": "Was ist nicht Ziel oder Gegenstand einer Bayes-Analyse?\n\n\n\nupdating beliefs\nquantifying uncertainty\nincluding prior knowledge of the domain, possibly of subjective nature\ndrawing inferential conclusions solely based on the likelihood"
  },
  {
    "objectID": "posts/Bayes-Ziel1/Bayes-Ziel1.html#answerlist",
    "href": "posts/Bayes-Ziel1/Bayes-Ziel1.html#answerlist",
    "title": "Bayes-Ziel1",
    "section": "",
    "text": "updating beliefs\nquantifying uncertainty\nincluding prior knowledge of the domain, possibly of subjective nature\ndrawing inferential conclusions solely based on the likelihood"
  },
  {
    "objectID": "posts/Bayes-Ziel1/Bayes-Ziel1.html#answerlist-1",
    "href": "posts/Bayes-Ziel1/Bayes-Ziel1.html#answerlist-1",
    "title": "Bayes-Ziel1",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nWahr\n\n\nCategories:\n\nregression\nbayes"
  },
  {
    "objectID": "posts/tidymodels-lasso2/tidymodels-lasso2.html",
    "href": "posts/tidymodels-lasso2/tidymodels-lasso2.html",
    "title": "tidymodels-lasso2",
    "section": "",
    "text": "Aufgabe\n\nSchreiben Sie eine minimale Analyse für ein Vorhersagemodell mit dem Lasso.\nHinweise:\n\nVerzichten Sie auf Tuning der Penalisierung; setzen Sie den Wert auf 0.1\nVerzichten Sie auf die Unterteilung von Train- und Test-Set.\nVerzichten Sie auf Kreuzvalidierung.\nVerwenden Sie Standardwerte, wo nicht anders angegeben.\nFixieren Sie Zufallszahlen auf den Startwert 42.\nVerwenden Sie den Datensatz penguins.\nModellformel: body_mass_g ~ .\n\n         \n\n\nLösung\n\n# 2023-05-14\n\n# Setup:\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(tictoc)  # Zeitmessung\n\n\n# Data:\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\"\nd &lt;- read_csv(d_path)\n\n# drop rows with NA in outcome variable:\nd &lt;-\n  d %&gt;% \n  drop_na(body_mass_g)\n\nset.seed(42)\nd_split &lt;- initial_split(d)\n# d_train &lt;- training(d_split)\n# d_test &lt;- testing(d_split)\n\n\n# model:\nmod_lasso &lt;-\n  linear_reg(mode = \"regression\",\n             penalty = 0.1,\n             mixture = 1,\n             engine = \"glmnet\")\n\n# cv:\n# set.seed(42)\n# rsmpl &lt;- vfold_cv(d_train)\n\n\n# recipe:\nrec1_plain &lt;- \n  recipe(body_mass_g ~  ., data = d) %&gt;% \n  update_role(\"rownames\", new_role = \"id\") %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_impute_bag(all_predictors())\n\n\n# check:\nd_train_baked &lt;- \n  prep(rec1_plain) %&gt;% bake(new_data = NULL)\n\nna_n &lt;- sum(is.na(d_train_baked))\n\n\n# workflow:\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(mod_lasso) %&gt;% \n  add_recipe(rec1_plain)\n\n\n# tuning:\ntic()\nwf1_fit &lt;-\n  wf1 %&gt;% \n  fit(data = d)\ntoc()\n\n1.223 sec elapsed\n\n# best candidate:\nwf1_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_normalize()\n• step_dummy()\n• step_impute_bag()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~1) \n\n   Df  %Dev Lambda\n1   0  0.00 697.60\n2   1 12.89 635.70\n3   1 23.58 579.20\n4   1 32.47 527.70\n5   1 39.84 480.90\n6   1 45.96 438.10\n7   1 51.05 399.20\n8   2 55.36 363.80\n9   2 59.11 331.40\n10  2 62.22 302.00\n11  2 64.81 275.20\n12  2 66.95 250.70\n13  3 69.54 228.40\n14  3 72.37 208.20\n15  3 74.73 189.70\n16  3 76.68 172.80\n17  3 78.30 157.50\n18  3 79.65 143.50\n19  3 80.77 130.70\n20  3 81.70 119.10\n21  3 82.47 108.50\n22  3 83.11  98.89\n23  3 83.64  90.10\n24  3 84.08  82.10\n25  3 84.45  74.81\n26  3 84.75  68.16\n27  3 85.00  62.11\n28  3 85.21  56.59\n29  3 85.39  51.56\n30  4 85.54  46.98\n31  5 85.69  42.81\n32  5 85.80  39.00\n33  5 85.90  35.54\n34  6 86.01  32.38\n35  7 86.17  29.50\n36  7 86.31  26.88\n37  7 86.43  24.50\n38  7 86.53  22.32\n39  7 86.62  20.34\n40  7 86.68  18.53\n41  7 86.74  16.88\n42  7 86.79  15.38\n43  8 86.83  14.02\n44  8 86.92  12.77\n45  8 86.99  11.64\n46  8 87.05  10.60\n\n...\nand 24 more lines.\n\n# Modellgüte:\n\npredict(wf1_fit, new_data = d) %&gt;% \n  bind_cols(d %&gt;% select(body_mass_g)) %&gt;% \n  rmse(truth = body_mass_g,\n       estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        285.\n\n\nMan beachte: Für regulierte Modelle sind Zentrierung und Skalierung nötig.\n\nCategories:\n\ntidymodels\nstatlearning\nlasso\nlm\nsimple\nstring\ntemplate"
  },
  {
    "objectID": "posts/Bootstrap1/Bootstrap1.html",
    "href": "posts/Bootstrap1/Bootstrap1.html",
    "title": "Bootstrap1",
    "section": "",
    "text": "Die Bootstrap-Methode ist eine beliebte numerische Methode im statistischen Lernen und in der Statistik allgemein. Welche der folgenden Aussagen zum Bootstrapping ist richtig?\n\n\n\nBootstrapping ist (u.a.) eine alternative Methode zur Kreuzvalidierung.\nBootstrapping sollte nur bei normalverteilten Variablen verwendet werden.\nBootstrapping basiert auf Ziehen ohne Zurücklegen.\nBootstrapping sollte nur verwendet werden, wo gleichzeitig eine analytische (exakte) Methode bekannt ist.\nBootstrapping sollte nicht verwendet werden, um Standardfehler zu schätzen."
  },
  {
    "objectID": "posts/Bootstrap1/Bootstrap1.html#answerlist",
    "href": "posts/Bootstrap1/Bootstrap1.html#answerlist",
    "title": "Bootstrap1",
    "section": "",
    "text": "Bootstrapping ist (u.a.) eine alternative Methode zur Kreuzvalidierung.\nBootstrapping sollte nur bei normalverteilten Variablen verwendet werden.\nBootstrapping basiert auf Ziehen ohne Zurücklegen.\nBootstrapping sollte nur verwendet werden, wo gleichzeitig eine analytische (exakte) Methode bekannt ist.\nBootstrapping sollte nicht verwendet werden, um Standardfehler zu schätzen."
  },
  {
    "objectID": "posts/Bootstrap1/Bootstrap1.html#answerlist-1",
    "href": "posts/Bootstrap1/Bootstrap1.html#answerlist-1",
    "title": "Bootstrap1",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nstatlearning\nds1\nschoice"
  },
  {
    "objectID": "posts/boxhist/boxhist.html",
    "href": "posts/boxhist/boxhist.html",
    "title": "boxhist",
    "section": "",
    "text": "boxhist.csv draw a histogram, and a boxplot. Based on the graphics, answer the following questions or check the correct statements, respectively. (Comment: The tolerance for numeric answers is \\(\\pm0.3\\), the true/false statements are either about correct or clearly wrong.)\n\n\n\nThe distribution is unimodal.\nThe distribution is not unimodal.\nThe distribution is symmetric.\nThe distribution is right-skewed.\nThe distribution is left-skewed.\nThe boxplot shows outliers.\nThe boxplot shows no outliers.\nA quarter of the observations is smaller than which value?\nA quarter of the observations is greater than which value?\nHalf of the observations are greater than which value?"
  },
  {
    "objectID": "posts/boxhist/boxhist.html#answerlist",
    "href": "posts/boxhist/boxhist.html#answerlist",
    "title": "boxhist",
    "section": "",
    "text": "The distribution is unimodal.\nThe distribution is not unimodal.\nThe distribution is symmetric.\nThe distribution is right-skewed.\nThe distribution is left-skewed.\nThe boxplot shows outliers.\nThe boxplot shows no outliers.\nA quarter of the observations is smaller than which value?\nA quarter of the observations is greater than which value?\nHalf of the observations are greater than which value?"
  },
  {
    "objectID": "posts/boxhist/boxhist.html#answerlist-1",
    "href": "posts/boxhist/boxhist.html#answerlist-1",
    "title": "boxhist",
    "section": "Answerlist",
    "text": "Answerlist\n\nTrue.\nFalse.\nFalse.\nFalse.\nTrue.\nTrue.\nFalse.\n0.22.\n2.17.\n1.88.\n\n\nCategories:\n\nvis\neda\nen\ncloze"
  },
  {
    "objectID": "posts/Anteil-Apple/Anteil-Apple.html",
    "href": "posts/Anteil-Apple/Anteil-Apple.html",
    "title": "Anteil-Apple",
    "section": "",
    "text": "Exercise\nZählen Sie, wie viele der Studentis im Raum mindestens ein Apple-Gerät besitzen (iPhone, Macbook,…).\nBerechnen Sie die Posteriori-Verteilung mit der Grid-Methode!\nHinweise:\n\nErstellen Sie eine Bayes-Box (Gittermethode).\nFalls Sie keine Erhebung durchführen können oder wollen, erfinden Sie Zahlen.\nVisualisieren Sie die Post-Verteilung\n\n         \n\n\nSolution\nWir berechnen die Posteriori-Verteilung:\n\nlibrary(tidyverse)\nd &lt;-\n  tibble(\n    p_grid = seq(0,1, by = .01),\n    prior= 1,\n    Likelihood = dbinom(x = 9,\n                        size = 12,\n                        prob = p_grid),\n    post_unstand = prior * Likelihood,\n    post_stand = post_unstand / sum(post_unstand)\n  )\n\nhead(d)\n\n# A tibble: 6 × 5\n  p_grid prior Likelihood post_unstand post_stand\n   &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1   0        1   0            0          0       \n2   0.01     1   2.13e-16     2.13e-16   2.78e-17\n3   0.02     1   1.06e-13     1.06e-13   1.38e-14\n4   0.03     1   3.95e-12     3.95e-12   5.14e-13\n5   0.04     1   5.10e-11     5.10e-11   6.63e-12\n6   0.05     1   3.68e-10     3.68e-10   4.79e-11\n\n\nVisualisieren der Posteriori-Verteilung:\n\nd %&gt;% \n  ggplot(aes(x = p_grid, y = post_stand)) +\n  geom_point() +\n  geom_line()\n\n\n\n\n\n\n\n\n\nCategories:\n\nbayes\nbayes-grid"
  },
  {
    "objectID": "posts/flights-yacsda-eda/index.html",
    "href": "posts/flights-yacsda-eda/index.html",
    "title": "flights-yacsda-eda",
    "section": "",
    "text": "Diese Fallstudie zeigt einige mögliche/typische Schritte der explorativen Datenanalyse (EDA) im Hinblick auf die Forschungsfrage “Welche Variablen steht in Zusammenhang mit Flugverspätungen?”."
  },
  {
    "objectID": "posts/flights-yacsda-eda/index.html#wie-ähnlich-sind-ankunfts--und-abflugsverspätung",
    "href": "posts/flights-yacsda-eda/index.html#wie-ähnlich-sind-ankunfts--und-abflugsverspätung",
    "title": "flights-yacsda-eda",
    "section": "4.1 Wie ähnlich sind Ankunfts- und Abflugsverspätung?",
    "text": "4.1 Wie ähnlich sind Ankunfts- und Abflugsverspätung?\nDa der Datensatz so groß ist, ziehen wir eine Stichprobe (mit sample_n), dann geht alles schneller. Hier nicht wichtig, nur um etwas Zeit beim Plotten zu sparen. In der Praxis würde ich in an dieser Stelle keine Stichprobe ziehen, bzw. mit dem Gesamtdatensatz weiterarbeiten (was wir ja auch im Folgenden tun).\n\nflights_sample &lt;- \nflights |&gt; \n  sample_n(size = 1000) \n\n\n4.1.1 Diagramm mit DataExplorer\n\nflights_sample |&gt; \n  select(dep_delay, arr_delay) |&gt; \n  plot_scatterplot(by = \"arr_delay\")\n\n\n\n\n\n\n\n\n\n\n4.1.2 Diagramm mit ggplot\n\nflights_sample |&gt; \n  ggplot() +\n  aes(y = dep_delay, x = arr_delay) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n4.1.3 Statistiken\n\nflights %&gt;%\n  drop_na(dep_delay, arr_delay) %&gt;% \n  summarise(sd(dep_delay),\n            sd(arr_delay))\n\n\n\n\nsd(dep_delay)\nsd(arr_delay)\n\n\n\n\n40.06569\n44.63329\n\n\n\n\n\nDas sind ca. 10% Differenz in der Skalierung; wir können die Skalierung komplett angleichen, um Abweichungen, die auf unterschiedlichen Mustern beruhen, besser zu sehen. Dazu hilft uns die z-Transformation.\nDie beiden Variablen scheinen ziemlich stark korreliert zu sein.\n\nflights %&gt;% \n  drop_na(dep_delay, arr_delay) %&gt;% \n  summarise(cor(dep_delay, arr_delay))\n\n\n\n\ncor(dep_delay, arr_delay)\n\n\n\n\n0.9148028\n\n\n\n\n\nJa, sind sie. Dann ist es vielleicht egal, welche der beiden Variablen wir verwenden. Nehmen wir dep_delay.\n\n\n4.1.4 Vertiefung: z-Skalierung\n\nflights %&gt;% \n  select(contains(\"delay\")) %&gt;% \n  drop_na() %&gt;% \n  mutate(dep_delay = scale(dep_delay),  # z-Transformation\n         arr_delay = scale(arr_delay)) %&gt;%   # z-Transformation\n  ggplot() +\n  aes(x = arr_delay, y = dep_delay) +\n  geom_bin2d() +\n  geom_abline(linetype = \"dashed\",\n              color = \"grey60\")\n\n\n\n\n\n\n\n\nbin2d wurde hier nur aus dem Grund verwendet, da das Plotten von ein paar Hunderttausend Punkte recht lange dauert. bin2d hingegen ist sehr schnell."
  },
  {
    "objectID": "posts/flights-yacsda-eda/index.html#visualisierung",
    "href": "posts/flights-yacsda-eda/index.html#visualisierung",
    "title": "flights-yacsda-eda",
    "section": "5.1 Visualisierung",
    "text": "5.1 Visualisierung\n\nMit DataExplorerMit ggplot\n\n\n\nflights |&gt; \n  select(dep_delay) |&gt; \n  plot_density()\n\n\n\n\n\n\n\n\n\n\n\nflights %&gt;% \n  ggplot() +\n  aes(x = dep_delay) %&gt;% \n  geom_density()\n\n\n\n\n\n\n\n\n\n\n\nEin sehr langer rechter Rand; die meisten Flüge sind nicht/kaum verspätet; aber einige wenige sind sehr stark verspätet.\nZentrale deskriptive Statistiken könnte man sich mit summary ausgeben lassen:\n\nflights %&gt;%\n  filter(!is.na(dep_delay)) %&gt;%  # keine fehlenden Werte\n  summarise(depdelay_mean = mean(dep_delay),\n            depdelay_sd = sd(dep_delay),\n            depdelay_md = median(dep_delay),\n            depdelay_iqr = IQR(dep_delay)) \n\n\n\n\ndepdelay_mean\ndepdelay_sd\ndepdelay_md\ndepdelay_iqr\n\n\n\n\n12.63907\n40.21006\n-2\n16\n\n\n\n\n\n\n5.1.1 Vertiefung: Wiederholung mit across\nOder man benutzt den Befehl across, der es erlaubt, eine oder mehrere Funktionen auf eine oder mehrere Spalten wiederholtanzuwenden (Man spricht von einer “Schleife”). In diesem Beispiel wenden wir mehrere Funktionen (adressiert mit .fns) auf eine Spalte (dep_delay), adressiert mit dem Argument .cols an. Außerdem kann man die Namen der resultierenden Spalten bestimmen mit dem Argument .names. In der geschweiften Klammer steht eine interne Variable, die den Namen der jeweils berechneten Funktion ({fn}) an den Namen der neu erstellten Spalte anfügt; in der Ausgabe sieht man das gut.\n\nflights %&gt;% \n  summarise(across(\n    .cols = dep_delay,\n    .fns = list(mean = mean, \n                md = median, \n                sd = sd, \n                iqr = IQR), na.rm = TRUE,\n    .names = \"depdelay_{fn}\"\n  ))\n\n\n\n\ndepdelay_mean\ndepdelay_md\ndepdelay_sd\ndepdelay_iqr\n\n\n\n\n12.63907\n-2\n40.21006\n16"
  },
  {
    "objectID": "posts/flights-yacsda-eda/index.html#flights2-extremwerte-der-verspätung-definieren",
    "href": "posts/flights-yacsda-eda/index.html#flights2-extremwerte-der-verspätung-definieren",
    "title": "flights-yacsda-eda",
    "section": "5.2 flights2: Extremwerte (der Verspätung) definieren",
    "text": "5.2 flights2: Extremwerte (der Verspätung) definieren\nEs gibt keinen sicheren Weg, mit Extremwerten umzugehen. Häufig macht es Sinn, die Ergebnisse mehrerer Analysen zu vergleichen mit, oder ohne Extremwerten.\n\n“Wann ist ein Flug sehr verspätet?\n\n5.2.1 Boxplot-Methode\nEine Möglichkeit ist die “Boxplot-Methode”: Entferne alle Flüge, die mehr verspätet sind als als das 1.5-fache der IQR über dem 3. Quartil (75. Perzentil): \\(q75+1.5iqr\\)\nBerechnen wir zunächst das 75. Perzentil (3. Quartil):\n\nflights %&gt;% \n  summarise(q75 = quantile(dep_delay, \n                           prob = .75, \n                           na.rm = TRUE))\n\n\n\n\nq75\n\n\n\n\n11\n\n\n\n\n\nDas sind also etwa 11 Minuten, die die Grenzlinie zwischen den 75% weniger bzw. den 25% stärker verspäteten Flügen markieren.\nDann berechnen wir den IQR:\n\nflights %&gt;% \n  summarise(depdelay_iqr = IQR(dep_delay, na.rm = TRUE))\n\n\n\n\ndepdelay_iqr\n\n\n\n\n16\n\n\n\n\n\nDer Grenzwert liegt dem zufolge bei:\n\ngrenzwert &lt;- 11 + 1.5*16\n\nDas ist kein “gottgegebener” Wert, sondern ein pragmatischer Versuch, einen Grenzwert zu finden. Die Nützlichkeit dieses Grenzwerts müsste sich noch erweisen. Viele andere Grenzwerte lassen sich verteidigen.\n\nflights2 &lt;-\n  flights %&gt;% \n  mutate(is_extreme = case_when(\n    dep_delay &gt; 11 + 1.5 * 16 ~ TRUE, # Verspätung &gt; 35 Min.\n    dep_delay &lt;= 35 ~ FALSE  # in den anderen Fällen (&lt;= 35 Min.), dann kein Extremwert\n  ))"
  },
  {
    "objectID": "posts/flights-yacsda-eda/index.html#fehlende-werte-berechnen",
    "href": "posts/flights-yacsda-eda/index.html#fehlende-werte-berechnen",
    "title": "flights-yacsda-eda",
    "section": "5.3 Fehlende Werte berechnen",
    "text": "5.3 Fehlende Werte berechnen\nWie viele fehlende Werte gibt es eigentlich in dep_delay?\n\nflights |&gt; \n  describe_distribution(dep_delay)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nMean\nSD\nIQR\nMin\nMax\nSkewness\nKurtosis\nn\nn_Missing\n\n\n\n\ndep_delay\n12.63907\n40.21006\n16\n-43\n1301\n4.802541\n43.95012\n328521\n8255\n\n\n\n\n\nAlternativ, und weniger komfortabel könnte man sagen\n\nHey R,\nnimm die Tabelle flights und dann\nfasse die Spalte dep_delay zu einer Zahl zusammen und zwar\nanhand der Summe (sum) der fehlenden Werten (is.na)\n\n\nflights %&gt;% \n  summarise(sum(is.na(dep_delay)))  # fehlende Werte zählen\n\n\n\n\nsum(is.na(dep_delay))\n\n\n\n\n8255\n\n\n\n\n\nWie viele Fälle gingen verloren, wenn wir die Fälle mit fehlenden Werten bei dep_delay entfernten?\n\nflights %&gt;% \n  drop_na(dep_delay) %&gt;% \n  nrow()\n\n[1] 328521\n\n\nUnd wenn wir alle fehlenden Werte entfernen würden?\n\nflights %&gt;% \n  drop_na() %&gt;% \n  nrow()\n\n[1] 327346\n\n\nWir verlieren nicht viele Fälle mehr, wenn wir die fehlenden Werte aller Variablen (Spalten) entfernen. Also machen wir das mal.\n\n5.3.1 Vertiefung: across\nSo bekommt man die fehlenden Werte für alle Spalten auf einmal:\n\nflights %&gt;% \n  summarise(across(everything(), ~ sum(is.na(.x))))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\ncarrier\nflight\ntailnum\norigin\ndest\nair_time\ndistance\nhour\nminute\ntime_hour\n\n\n\n\n0\n0\n0\n8255\n0\n8255\n8713\n0\n9430\n0\n0\n2512\n0\n0\n9430\n0\n0\n0\n0\n\n\n\n\n\nEhrlicherweise muss man sagen, dass man mit describe_distribution auch komfortabel die fehlenden Werte für alle Spalten bekommt."
  },
  {
    "objectID": "posts/flights-yacsda-eda/index.html#flights3",
    "href": "posts/flights-yacsda-eda/index.html#flights3",
    "title": "flights-yacsda-eda",
    "section": "5.4 flights3",
    "text": "5.4 flights3\nAchtung: dieses Vorgehen hier ist gefährlich. U.U. verliert man sehr viele Zeilen (Beobachtungen).\n\nflights3 &lt;-\n  flights2 %&gt;% \n  drop_na() %&gt;% \n  select(-year)\n\nDie Spalte year ist kontant (immer der Wert “2013”); daher ist die Spalte nutzlos, sie birgt keine Information. Wir können sie gefahrlos löschen."
  },
  {
    "objectID": "posts/flights-yacsda-eda/index.html#mit-summarise",
    "href": "posts/flights-yacsda-eda/index.html#mit-summarise",
    "title": "flights-yacsda-eda",
    "section": "6.1 Mit summarise",
    "text": "6.1 Mit summarise\nDas kann machen mit summarise. Einfach, kann aber viel Tipperei bedeuten:\n\nflights2 %&gt;% \n  summarise(mean(dep_delay),\n            sd(dep_delay),\n            mean(arr_delay),\n            sd(arr_delay))  # und so weiter\n\n\n\n\nmean(dep_delay)\nsd(dep_delay)\nmean(arr_delay)\nsd(arr_delay)\n\n\n\n\nNA\nNA\nNA\nNA"
  },
  {
    "objectID": "posts/flights-yacsda-eda/index.html#mit-describe_distribution",
    "href": "posts/flights-yacsda-eda/index.html#mit-describe_distribution",
    "title": "flights-yacsda-eda",
    "section": "6.2 Mit describe_distribution",
    "text": "6.2 Mit describe_distribution\ndescribe_distribution ist sehr praktisch; man bekommt viele Statistiken auf einmal gezeigt; das spart viel Tipperei.\n\nflights %&gt;% \n  describe_distribution()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nMean\nSD\nIQR\nMin\nMax\nSkewness\nKurtosis\nn\nn_Missing\n\n\n\n\nyear\n2013.000000\n0.000000\n0\n2013\n2013\nNaN\nNaN\n336776\n0\n\n\nmonth\n6.548510\n3.414457\n6\n1\n12\n-0.0133999\n-1.1869501\n336776\n0\n\n\nday\n15.710787\n8.768607\n15\n1\n31\n0.0077445\n-1.1859454\n336776\n0\n\n\ndep_time\n1349.109947\n488.281791\n837\n1\n2400\n-0.0247435\n-1.0883200\n328521\n8255\n\n\nsched_dep_time\n1344.254840\n467.335756\n823\n106\n2359\n-0.0058581\n-1.1979031\n336776\n0\n\n\ndep_delay\n12.639070\n40.210061\n16\n-43\n1301\n4.8025405\n43.9501160\n328521\n8255\n\n\narr_time\n1502.054999\n533.264132\n836\n1\n2400\n-0.4678191\n-0.1926344\n328063\n8713\n\n\nsched_arr_time\n1536.380220\n497.457141\n821\n1\n2359\n-0.3531381\n-0.3822478\n336776\n0\n\n\narr_delay\n6.895377\n44.633292\n31\n-86\n1272\n3.7168175\n29.2330440\n327346\n9430\n\n\nflight\n1971.923620\n1632.471938\n2912\n1\n8500\n0.6616036\n-0.8485607\n336776\n0\n\n\nair_time\n150.686460\n93.688305\n110\n20\n695\n1.0707052\n0.8630770\n327346\n9430\n\n\ndistance\n1039.912604\n733.233033\n887\n17\n4983\n1.1286902\n1.1936399\n336776\n0\n\n\nhour\n13.180247\n4.661316\n8\n1\n23\n-0.0005427\n-1.2064161\n336776\n0\n\n\nminute\n26.230100\n19.300846\n36\n0\n59\n0.0929309\n-1.2350180\n336776\n0"
  },
  {
    "objectID": "posts/flights-yacsda-eda/index.html#metrische-prädiktoren",
    "href": "posts/flights-yacsda-eda/index.html#metrische-prädiktoren",
    "title": "flights-yacsda-eda",
    "section": "7.1 Metrische Prädiktoren",
    "text": "7.1 Metrische Prädiktoren\n\n7.1.1 Nur mit cor\nAm einfachsten geht es so. Der Nachteil ist mehr (viel) Tipperei:\n\nflights3 %&gt;% \n  select(where(is.numeric)) %&gt;%  # wähle alle numerischen Spalten\n  summarise(cor_month = cor(dep_delay, month),\n            cor_day = cor(dep_delay, day),\n            cor_dep_time = cor(dep_delay, dep_time))  # etc\n\n\n\n\ncor_month\ncor_day\ncor_dep_time\n\n\n\n\n-0.0200547\n0.0005914\n0.2596127\n\n\n\n\n\n\n\n7.1.2 Mit easystats\n\nflights |&gt; \n  select(where(is.numeric)) %&gt;% \n  correlation()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter1\nParameter2\nr\nCI\nCI_low\nCI_high\nt\ndf_error\np\nMethod\nn_Obs\n\n\n\n\nyear\nmonth\nNA\n0.95\nNA\nNA\nNA\n336774\nNA\nPearson correlation\n336776\n\n\nyear\nday\nNA\n0.95\nNA\nNA\nNA\n336774\nNA\nPearson correlation\n336776\n\n\nyear\ndep_time\nNA\n0.95\nNA\nNA\nNA\n328519\nNA\nPearson correlation\n328521\n\n\nyear\nsched_dep_time\nNA\n0.95\nNA\nNA\nNA\n336774\nNA\nPearson correlation\n336776\n\n\nyear\ndep_delay\nNA\n0.95\nNA\nNA\nNA\n328519\nNA\nPearson correlation\n328521\n\n\nyear\narr_time\nNA\n0.95\nNA\nNA\nNA\n328061\nNA\nPearson correlation\n328063\n\n\nyear\nsched_arr_time\nNA\n0.95\nNA\nNA\nNA\n336774\nNA\nPearson correlation\n336776\n\n\nyear\narr_delay\nNA\n0.95\nNA\nNA\nNA\n327344\nNA\nPearson correlation\n327346\n\n\nyear\nflight\nNA\n0.95\nNA\nNA\nNA\n336774\nNA\nPearson correlation\n336776\n\n\nyear\nair_time\nNA\n0.95\nNA\nNA\nNA\n327344\nNA\nPearson correlation\n327346\n\n\nyear\ndistance\nNA\n0.95\nNA\nNA\nNA\n336774\nNA\nPearson correlation\n336776\n\n\nyear\nhour\nNA\n0.95\nNA\nNA\nNA\n336774\nNA\nPearson correlation\n336776\n\n\nyear\nminute\nNA\n0.95\nNA\nNA\nNA\n336774\nNA\nPearson correlation\n336776\n\n\nmonth\nday\n0.0029423\n0.95\n-0.0004350\n0.0063197\n1.7075186\n336774\n1.0000000\nPearson correlation\n336776\n\n\nmonth\ndep_time\n-0.0039324\n0.95\n-0.0073519\n-0.0005129\n-2.2539453\n328519\n0.3388037\nPearson correlation\n328521\n\n\nmonth\nsched_dep_time\n-0.0045726\n0.95\n-0.0079499\n-0.0011953\n-2.6536338\n336774\n0.1274143\nPearson correlation\n336776\n\n\nmonth\ndep_delay\n-0.0200570\n0.95\n-0.0234749\n-0.0166386\n-11.4983104\n328519\n0.0000000\nPearson correlation\n328521\n\n\nmonth\narr_time\n-0.0025199\n0.95\n-0.0059418\n0.0009020\n-1.4433357\n328061\n1.0000000\nPearson correlation\n328063\n\n\nmonth\nsched_arr_time\n-0.0041727\n0.95\n-0.0075500\n-0.0007954\n-2.4215548\n336774\n0.2318219\nPearson correlation\n336776\n\n\nmonth\narr_delay\n-0.0173820\n0.95\n-0.0208064\n-0.0139572\n-9.9464509\n327344\n0.0000000\nPearson correlation\n327346\n\n\nmonth\nflight\n-0.0008341\n0.95\n-0.0042114\n0.0025433\n-0.4840347\n336774\n1.0000000\nPearson correlation\n336776\n\n\nmonth\nair_time\n0.0109242\n0.95\n0.0074988\n0.0143493\n6.2505247\n327344\n0.0000000\nPearson correlation\n327346\n\n\nmonth\ndistance\n0.0216356\n0.95\n0.0182596\n0.0250112\n12.5585894\n336774\n0.0000000\nPearson correlation\n336776\n\n\nmonth\nhour\n-0.0052274\n0.95\n-0.0086046\n-0.0018501\n-3.0336183\n336774\n0.0410819\nPearson correlation\n336776\n\n\nmonth\nminute\n0.0155277\n0.95\n0.0121509\n0.0189040\n9.0121387\n336774\n0.0000000\nPearson correlation\n336776\n\n\nday\ndep_time\n-0.0004674\n0.95\n-0.0038869\n0.0029522\n-0.2678812\n328519\n1.0000000\nPearson correlation\n328521\n\n\nday\nsched_dep_time\n-0.0000144\n0.95\n-0.0033917\n0.0033630\n-0.0083485\n336774\n1.0000000\nPearson correlation\n336776\n\n\nday\ndep_delay\n0.0004200\n0.95\n-0.0029995\n0.0038395\n0.2407375\n328519\n1.0000000\nPearson correlation\n328521\n\n\nday\narr_time\n-0.0055369\n0.95\n-0.0089587\n-0.0021151\n-3.1714238\n328061\n0.0273074\nPearson correlation\n328063\n\n\nday\nsched_arr_time\n-0.0024028\n0.95\n-0.0057801\n0.0009746\n-1.3943765\n336774\n1.0000000\nPearson correlation\n336776\n\n\nday\narr_delay\n-0.0003192\n0.95\n-0.0037448\n0.0031065\n-0.1826024\n327344\n1.0000000\nPearson correlation\n327346\n\n\nday\nflight\n-0.0017908\n0.95\n-0.0051681\n0.0015866\n-1.0392164\n336774\n1.0000000\nPearson correlation\n336776\n\n\nday\nair_time\n0.0022364\n0.95\n-0.0011893\n0.0056620\n1.2795263\n327344\n1.0000000\nPearson correlation\n327346\n\n\nday\ndistance\n0.0030413\n0.95\n-0.0003361\n0.0064186\n1.7649426\n336774\n1.0000000\nPearson correlation\n336776\n\n\nday\nhour\n-0.0000553\n0.95\n-0.0034326\n0.0033221\n-0.0320789\n336774\n1.0000000\nPearson correlation\n336776\n\n\nday\nminute\n0.0009867\n0.95\n-0.0023907\n0.0043640\n0.5725883\n336774\n1.0000000\nPearson correlation\n336776\n\n\ndep_time\nsched_dep_time\n0.9546169\n0.95\n0.9543125\n0.9549192\n1837.0937739\n328519\n0.0000000\nPearson correlation\n328521\n\n\ndep_time\ndep_delay\n0.2602312\n0.95\n0.2570404\n0.2634164\n154.4779649\n328519\n0.0000000\nPearson correlation\n328521\n\n\ndep_time\narr_time\n0.6607789\n0.95\n0.6588467\n0.6627023\n504.2386087\n328061\n0.0000000\nPearson correlation\n328063\n\n\ndep_time\nsched_arr_time\n0.7846824\n0.95\n0.7833648\n0.7859929\n725.5275036\n328519\n0.0000000\nPearson correlation\n328521\n\n\ndep_time\narr_delay\n0.2323057\n0.95\n0.2290624\n0.2355439\n136.6497146\n327344\n0.0000000\nPearson correlation\n327346\n\n\ndep_time\nflight\n0.0419571\n0.95\n0.0385431\n0.0453701\n24.0695668\n328519\n0.0000000\nPearson correlation\n328521\n\n\ndep_time\nair_time\n-0.0146195\n0.95\n-0.0180442\n-0.0111944\n-8.3652793\n327344\n0.0000000\nPearson correlation\n327346\n\n\ndep_time\ndistance\n-0.0139982\n0.95\n-0.0174169\n-0.0105792\n-8.0240834\n328519\n0.0000000\nPearson correlation\n328521\n\n\ndep_time\nhour\n0.9533056\n0.95\n0.9529927\n0.9536165\n1809.2355940\n328519\n0.0000000\nPearson correlation\n328521\n\n\ndep_time\nminute\n0.0915767\n0.95\n0.0881848\n0.0949665\n52.7101121\n328519\n0.0000000\nPearson correlation\n328521\n\n\nsched_dep_time\ndep_delay\n0.1988867\n0.95\n0.1956002\n0.2021688\n116.3188193\n328519\n0.0000000\nPearson correlation\n328521\n\n\nsched_dep_time\narr_time\n0.6426802\n0.95\n0.6406672\n0.6446843\n480.4710020\n328061\n0.0000000\nPearson correlation\n328063\n\n\nsched_dep_time\nsched_arr_time\n0.7833425\n0.95\n0.7820341\n0.7846440\n731.3355602\n336774\n0.0000000\nPearson correlation\n336776\n\n\nsched_dep_time\narr_delay\n0.1738962\n0.95\n0.1705721\n0.1772163\n101.0322717\n327344\n0.0000000\nPearson correlation\n327346\n\n\nsched_dep_time\nflight\n0.0364947\n0.95\n0.0331214\n0.0398672\n21.1928124\n336774\n0.0000000\nPearson correlation\n336776\n\n\nsched_dep_time\nair_time\n-0.0155321\n0.95\n-0.0189568\n-0.0121071\n-8.8876246\n327344\n0.0000000\nPearson correlation\n327346\n\n\nsched_dep_time\ndistance\n-0.0179950\n0.95\n-0.0213710\n-0.0146185\n-10.4445684\n336774\n0.0000000\nPearson correlation\n336776\n\n\nsched_dep_time\nhour\n0.9991483\n0.95\n0.9991425\n0.9991540\n14051.7706130\n336774\n0.0000000\nPearson correlation\n336776\n\n\nsched_dep_time\nminute\n0.0829598\n0.95\n0.0796047\n0.0863129\n48.3099293\n336774\n0.0000000\nPearson correlation\n336776\n\n\ndep_delay\narr_time\n0.0287288\n0.95\n0.0253094\n0.0321476\n16.4616791\n328061\n0.0000000\nPearson correlation\n328063\n\n\ndep_delay\nsched_arr_time\n0.1604885\n0.95\n0.1571552\n0.1638181\n93.1945228\n328519\n0.0000000\nPearson correlation\n328521\n\n\ndep_delay\narr_delay\n0.9148028\n0.95\n0.9142422\n0.9153599\n1295.8504088\n327344\n0.0000000\nPearson correlation\n327346\n\n\ndep_delay\nflight\n0.0547337\n0.95\n0.0513238\n0.0581424\n31.4185914\n328519\n0.0000000\nPearson correlation\n328521\n\n\ndep_delay\nair_time\n-0.0224051\n0.95\n-0.0258288\n-0.0189809\n-12.8220570\n327344\n0.0000000\nPearson correlation\n327346\n\n\ndep_delay\ndistance\n-0.0216708\n0.95\n-0.0250885\n-0.0182526\n-12.4238721\n328519\n0.0000000\nPearson correlation\n328521\n\n\ndep_delay\nhour\n0.1982259\n0.95\n0.1949385\n0.2015089\n115.9165150\n328519\n0.0000000\nPearson correlation\n328521\n\n\ndep_delay\nminute\n0.0284409\n0.95\n0.0250238\n0.0318573\n16.3079309\n328519\n0.0000000\nPearson correlation\n328521\n\n\narr_time\nsched_arr_time\n0.7889971\n0.95\n0.7877018\n0.7902853\n735.5354750\n328061\n0.0000000\nPearson correlation\n328063\n\n\narr_time\narr_delay\n0.0244821\n0.95\n0.0210582\n0.0279055\n14.0114093\n327344\n0.0000000\nPearson correlation\n327346\n\n\narr_time\nflight\n0.0250418\n0.95\n0.0216217\n0.0284613\n14.3475801\n328061\n0.0000000\nPearson correlation\n328063\n\n\narr_time\nair_time\n0.0542960\n0.95\n0.0508798\n0.0577110\n31.1108134\n327344\n0.0000000\nPearson correlation\n327346\n\n\narr_time\ndistance\n0.0469912\n0.95\n0.0435763\n0.0504051\n26.9447568\n328061\n0.0000000\nPearson correlation\n328063\n\n\narr_time\nhour\n0.6426514\n0.95\n0.6406383\n0.6446556\n480.4342878\n328061\n0.0000000\nPearson correlation\n328063\n\n\narr_time\nminute\n0.0409691\n0.95\n0.0375524\n0.0443848\n23.4854231\n328061\n0.0000000\nPearson correlation\n328063\n\n\nsched_arr_time\narr_delay\n0.1332613\n0.95\n0.1298949\n0.1366246\n76.9302376\n327344\n0.0000000\nPearson correlation\n327346\n\n\nsched_arr_time\nflight\n0.0215937\n0.95\n0.0182176\n0.0249692\n12.5342010\n336774\n0.0000000\nPearson correlation\n336776\n\n\nsched_arr_time\nair_time\n0.0789183\n0.95\n0.0755131\n0.0823217\n45.2935710\n327344\n0.0000000\nPearson correlation\n327346\n\n\nsched_arr_time\ndistance\n0.0687259\n0.95\n0.0653637\n0.0720865\n39.9777082\n336774\n0.0000000\nPearson correlation\n336776\n\n\nsched_arr_time\nhour\n0.7832825\n0.95\n0.7819738\n0.7845843\n731.1906842\n336774\n0.0000000\nPearson correlation\n336776\n\n\nsched_arr_time\nminute\n0.0503212\n0.95\n0.0469518\n0.0536895\n29.2395738\n336774\n0.0000000\nPearson correlation\n336776\n\n\narr_delay\nflight\n0.0728621\n0.95\n0.0694537\n0.0762687\n41.7983960\n327344\n0.0000000\nPearson correlation\n327346\n\n\narr_delay\nair_time\n-0.0352971\n0.95\n-0.0387181\n-0.0318753\n-20.2074620\n327344\n0.0000000\nPearson correlation\n327346\n\n\narr_delay\ndistance\n-0.0618678\n0.95\n-0.0652796\n-0.0584545\n-35.4649465\n327344\n0.0000000\nPearson correlation\n327346\n\n\narr_delay\nhour\n0.1734556\n0.95\n0.1701310\n0.1767762\n100.7683213\n327344\n0.0000000\nPearson correlation\n327346\n\n\narr_delay\nminute\n0.0215222\n0.95\n0.0180979\n0.0249460\n12.3165691\n327344\n0.0000000\nPearson correlation\n327346\n\n\nflight\nair_time\n-0.4728384\n0.95\n-0.4754938\n-0.4701743\n-307.0191395\n327344\n0.0000000\nPearson correlation\n327346\n\n\nflight\ndistance\n-0.4841654\n0.95\n-0.4867468\n-0.4815755\n-321.1194483\n336774\n0.0000000\nPearson correlation\n336776\n\n\nflight\nhour\n0.0358380\n0.95\n0.0324646\n0.0392106\n20.8109689\n336774\n0.0000000\nPearson correlation\n336776\n\n\nflight\nminute\n0.0181366\n0.95\n0.0147602\n0.0215127\n10.5268290\n336774\n0.0000000\nPearson correlation\n336776\n\n\nair_time\ndistance\n0.9906496\n0.95\n0.9905857\n0.9907132\n4154.4244720\n327344\n0.0000000\nPearson correlation\n327346\n\n\nair_time\nhour\n-0.0162773\n0.95\n-0.0197018\n-0.0128523\n-9.3141118\n327344\n0.0000000\nPearson correlation\n327346\n\n\nair_time\nminute\n0.0170318\n0.95\n0.0136070\n0.0204563\n9.7459979\n327344\n0.0000000\nPearson correlation\n327346\n\n\ndistance\nhour\n-0.0188605\n0.95\n-0.0222364\n-0.0154841\n-10.9470924\n336774\n0.0000000\nPearson correlation\n336776\n\n\ndistance\nminute\n0.0197798\n0.95\n0.0164035\n0.0231556\n11.4809042\n336774\n0.0000000\nPearson correlation\n336776\n\n\nhour\nminute\n0.0417676\n0.95\n0.0383957\n0.0451386\n24.2598646\n336774\n0.0000000\nPearson correlation\n336776\n\n\n\n\n\n\n\n7.1.3 Vertiefung: Mit across\nBerechnen wir die Korrelationen jetzt mit dem Befehl across. Der Punkt . spricht hier jeweils eine Spalte an, die von across ausgewählt wurde. Der Effekt ist, dass eine Korrelation von jeder Spalte mit dep_delay berechnet wird.\n\nflights3 %&gt;% \n  select(where(is.numeric)) %&gt;%  # nur die numerischen Spalten auswählen\n  summarise(across(\n    .cols = everything(),\n    .fns = ~ cor(., dep_delay))) %&gt;% \n  pivot_longer(everything()) %&gt;%  # von breit auf lang\n  arrange(-value)  # absteigend sortieren\n\n\n\n\nname\nvalue\n\n\n\n\ndep_delay\n1.0000000\n\n\narr_delay\n0.9148028\n\n\ndep_time\n0.2596127\n\n\nsched_dep_time\n0.1989235\n\n\nhour\n0.1982692\n\n\nsched_arr_time\n0.1604972\n\n\nflight\n0.0539697\n\n\narr_time\n0.0294210\n\n\nminute\n0.0282514\n\n\nday\n0.0005914\n\n\nmonth\n-0.0200547\n\n\ndistance\n-0.0216809\n\n\nair_time\n-0.0224051"
  },
  {
    "objectID": "posts/flights-yacsda-eda/index.html#nominale-prädiktoren",
    "href": "posts/flights-yacsda-eda/index.html#nominale-prädiktoren",
    "title": "flights-yacsda-eda",
    "section": "7.2 Nominale Prädiktoren",
    "text": "7.2 Nominale Prädiktoren\n\n7.2.1 Welche nominalen Prädiktoren gibt es?\nHey R, wähle alle nicht numerischen Spalten aus und sage mir deren Namen:\n\nflights2 %&gt;% \n  select(where(negate(is.numeric))) %&gt;% \n  names()\n\n[1] \"carrier\"    \"tailnum\"    \"origin\"     \"dest\"       \"time_hour\" \n[6] \"is_extreme\"\n\n\nJetzt kann man für jede nominale Variable die Anzahl der unterschiedlichen Ausprägungen abfragen:\n\nflights2 %&gt;% \n  summarise(n_distinct(carrier))\n\n\n\n\nn_distinct(carrier)\n\n\n\n\n16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.2.2 Carrier\ncarrier meint die Fluggesellschaft, die den jeweiligen Flug durchgeführt hat. Da stellen sich eine Reihe interessanter Fragen:\n\nWie viele verschiedene Fluggesellschaften gibt es?\nWie viele Flüge hat jede davon ausgeführt?\nWelche Fluggesellschaft hat die meisten Flüge ausgeführt?\nGibt es große Unterschiede in de Zahl der ausgeführten Flüge.\nWer hat eigentlich die flüssige Seife erfunden?\n\nFragen über Fragen…\n\nflights2_count &lt;- \nflights2 %&gt;% \n  select(carrier) %&gt;% \n  count(carrier, sort = TRUE) \n\nflights2_count\n\n\n\n\ncarrier\nn\n\n\n\n\nUA\n58665\n\n\nB6\n54635\n\n\nEV\n54173\n\n\nDL\n48110\n\n\nAA\n32729\n\n\nMQ\n26397\n\n\nUS\n20536\n\n\n9E\n18460\n\n\nWN\n12275\n\n\nVX\n5162\n\n\nFL\n3260\n\n\nAS\n714\n\n\nF9\n685\n\n\nYV\n601\n\n\nHA\n342\n\n\nOO\n32\n\n\n\n\n\nWir brauchen eine Visualisierung dazu; das beantwortet vielleicht einen Teil der obigen Fragen.\n\n\n7.2.3 Visualisierung von carrier\n\n\n7.2.4 Mit DataExplorer\n\nflights2_count |&gt; \n  plot_scatterplot(by = \"n\")\n\n\n\n\n\n\n\n\n\n\n7.2.5 Mit ggplot\n\nplot1 &lt;- \n  flights2_count |&gt; \n  ggplot() +\n  aes(y = carrier, x = n) +\n  geom_point(color = \"red\") +\n  geom_line(group = 1)\n\nplot1\n\n\n\n\n\n\n\n\nWir müssen die Werte von carrier sortieren anhand der Anzahl der Flüge, sonst ist es zu unübersichtlich.\n\n\n7.2.6 Vertiefung: Achsen-Labels anpassen\nDieser Abschnitt ist zur Vertiefung, er ist nicht inhaltlich wichtig\nSagen wir, wir möchten die Labels der X-Achse anpassen, und zwar möchten wir die Werte 25.000, 50.000, und 75.0000.\n\nplot1 +\n  scale_x_continuous(breaks = c(0, 25000, 50000, 75000),\n                     limits = c(0, 100000),\n                     labels = c(\"keine\", \"wenig\", \"mittel\", \"viel\"))\n\n\n\n\n\n\n\n\nHm, schön sieht es noch nicht aus; die limits machen nicht unbedingt Sinn. Die labels sind auch wenig sinnvoll.\nMehr zum Thema “Achsen aufhübschen” findet sich z.B. hier.\n\n\n7.2.7 “Lumpsensammler-Kategorie”\n\nflights2 &lt;-\n  flights2 %&gt;% \n  mutate(carrier = factor(carrier)) %&gt;%   # nicht `character`, sondern `factor` wollen\n  mutate(carrier_lump = fct_lump(carrier, n = 8)) \n\nHier fassen wir mit fct_lump alle Stufen von carrier zu acht Stufen (daher n = 8) zusammen plus einer “Lumpensammler-Kategorie” zusammen. Dazu muss die Variable aber als factor vorliegen, was wir in der Zeile davor erledigt haben.\nJetzt haben wir noch nur 9 (8 plus Lumpensammler-Gruppe) Gruppen:\n\nflights2_lump_count &lt;-\n  flights2 %&gt;% \n  # select(carrier) %&gt;% \n  # mutate(carrier_lump = fct_lump(carrier, n = 8)) |&gt; \n  count(carrier_lump, sort = TRUE)\n\nflights2_lump_count\n\n\n\n\ncarrier_lump\nn\n\n\n\n\nUA\n58665\n\n\nB6\n54635\n\n\nEV\n54173\n\n\nDL\n48110\n\n\nAA\n32729\n\n\nMQ\n26397\n\n\nOther\n23071\n\n\nUS\n20536\n\n\n9E\n18460\n\n\n\n\n\n\n\n7.2.8 Visualisierung der Lumpensammler\n\nflights2_count &lt;-\n  flights2_count |&gt; \n  mutate(carrier =  fct_reorder(carrier, n)) |&gt; \n  count(carrier, sort = TRUE)\n\nMit fct_reorder haben wir die Werte von carrier (UA, B6, AA, …) sortiert und zwar anhand der Werte von n, also anhand der Häufigkeit. Es resultiert eine Rangfolge: UA &gt; B6 &gt; EV &gt; DL &gt; ... etc. (Nur) Mit einer sortierten Faktorvariable lässt sich entsprechendes Diagramm gut sortiert darstellen.\n\n7.2.8.1 Mit ggplot\nLiniendiagramm:\n\nflights2_lump_count |&gt; \n  ggplot() +\n  aes(y = carrier_lump, x = n) +\n  geom_point(color = \"red\") +\n  geom_line(group = 1)\n\n\n\n\n\n\n\n\nAh, schon besser. Aber recht informationsarm, das Diagramm. Informationsreicher als das Liniendiagramm ist ein Boxplot:\n\nflights2 %&gt;% \n  filter(!is_extreme) %&gt;% \n  ggplot() +\n  aes(x = carrier_lump, \n      y = dep_delay) %&gt;% \n  geom_boxplot()\n\n\n\n\n\n\n\n\nEine alternative Darstellung wäre ein Letter Value Plot.\nSchauen wir uns mal die Mediane genauer an:\n\nflights2 %&gt;% \n  filter(!is_extreme) %&gt;% \n  group_by(carrier_lump) %&gt;% \n  summarise(dep_delay = median(dep_delay, na.rm = TRUE)) %&gt;% \n  arrange(dep_delay)\n\n\n\n\ncarrier_lump\ndep_delay\n\n\n\n\nUS\n-5\n\n\nMQ\n-4\n\n\n9E\n-3\n\n\nAA\n-3\n\n\nDL\n-3\n\n\nEV\n-3\n\n\nB6\n-2\n\n\nUA\n-1\n\n\nOther\n0\n\n\n\n\n\nDie Reihenfolge entspricht der dem obigen Diagramm."
  },
  {
    "objectID": "posts/flights-yacsda-eda/index.html#korrelation-von-carrier-mit-verspätung",
    "href": "posts/flights-yacsda-eda/index.html#korrelation-von-carrier-mit-verspätung",
    "title": "flights-yacsda-eda",
    "section": "7.3 Korrelation von carrier mit Verspätung",
    "text": "7.3 Korrelation von carrier mit Verspätung\nHier mit “Dummysierung” aller nicht-numerischer Spalten. Ein Beispiel zur Verdeutlichung:\n\nflights2 &lt;- \n  flights2 %&gt;% \n  mutate(\n    originJFK = case_when(\n      origin == \"JFK\" ~ 1,  # \"1\" wenn JFK, \n      origin != \"JFK\" ~ 0   # ansonsten 0\n    ),  \n    originLGA = case_when(\n      origin == \"LGA\" ~ 1,  # \"1\" wenn LGA,\n      TRUE ~ 0,  # in allen anderen Fällen (\"TRUE\") 0\n    )\n  )\n\n\nflights2 %&gt;% \n  select(origin, originJFK, originLGA) %&gt;% \n  slice(1:5)\n\n\n\n\norigin\noriginJFK\noriginLGA\n\n\n\n\nEWR\n0\n0\n\n\nLGA\n0\n1\n\n\nJFK\n1\n0\n\n\nJFK\n1\n0\n\n\nLGA\n0\n1\n\n\n\n\n\nDiese Art der Umwandlung von mehrstufig-nominal in eine binäre Variable (0-1-Variable, oder “Indikatorvariable”) kann man sich auch z.B. mit der Funktion dummy_cols() (aus dem Paket fastDummies) bewerkstelligen lassen:\n\nflights2 %&gt;% \n  select(origin, dep_delay) %&gt;% \n  dummy_cols() %&gt;%  # aus dem Paket `fastDummies`\n  head()  # slice(1:6)\n\n\n\n\norigin\ndep_delay\norigin_EWR\norigin_JFK\norigin_LGA\n\n\n\n\nEWR\n2\n1\n0\n0\n\n\nLGA\n4\n0\n0\n1\n\n\nJFK\n2\n0\n1\n0\n\n\nJFK\n-1\n0\n1\n0\n\n\nLGA\n-6\n0\n0\n1\n\n\nEWR\n-4\n1\n0\n0\n\n\n\n\n\nMit den “dummyisierten” Spalten können wir jetzt Korrelationen rechnen, denn jetzt haben wir Zahlen. Achtung: Die Variablen bleiben nominalskaliert, trotz der 0-1-Transformation. Auf diese Art Korrelationen zu berechnen ist nur für dummysierte Variablen (“Indikatorvariablen”) sinnvoll. Die Schiefe der Verteilung begrenzt hier übrigens die Stärke der Korrelation.\n\nflights2 %&gt;% \n  select(dep_delay, carrier) %&gt;% \n  dummy_cols() %&gt;%   # \"Dummysierung\"\n  select(-carrier) %&gt;% \n  pivot_longer(-dep_delay,\n               names_to = \"carrier\",\n               values_to = \"value\") %&gt;% \n  group_by(carrier) %&gt;% \n  summarise(cor_depdelay_carrier = cor(dep_delay, value,\n                                       use = \"complete.obs\")) %&gt;% \n  arrange(-abs(cor_depdelay_carrier)) %&gt;% \n  filter(abs(cor_depdelay_carrier) &gt; 0.10)\n\n\n\n\ncarrier\ncor_depdelay_carrier\n\n\n\n\n\n\n\nKeine Korrelation war (im Betrag) größer als 0.1. Also gab es nur vernachlässigbare Korrelationen und im Output wurde daher nichts angezeigt.\nZur Erinnerung: Es ist nicht unbedingt nötig, die “Dummyisierung” durchzuführen, ein einfaches Vergleichen der Mittelwerte (oder Mediane) mit ihrer Streuung führt zu einem ähnlichen Ergebnis. Die Regression mit lm führt für Sie automatisch die Dummyisierung durch.\n\n7.3.1 Hour\n\n7.3.1.1 Mit ggplot\n\nflights2 %&gt;% \n  filter(!is_extreme) %&gt;% \n  select(dep_delay, hour) %&gt;% \n  mutate(hour = factor(hour)) %&gt;% \n  ggplot() +\n  aes(x = fct_reorder(hour, dep_delay,\n                      na.rm = TRUE), \n      y = dep_delay) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n7.3.1.2 Mit DataExplorer\n\nflights2 %&gt;% \n  filter(!is_extreme) %&gt;% \n  select(dep_delay, hour) %&gt;% \n  mutate(hour = factor(hour)) %&gt;% \n  plot_boxplot(by = \"hour\")\n\n\n\n\n\n\n\n\n\n\n\n7.3.2 Origin\n\n7.3.2.1 Mit ggplot\n\nflights2 %&gt;% \n  filter(!is_extreme) %&gt;% \n  select(origin, dep_delay) %&gt;% \n  ggplot() +\n  aes(x = origin, y = dep_delay) %&gt;% \n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n7.3.2.2 Mit DataExplorer\n\nflights2 %&gt;% \n  filter(!is_extreme) %&gt;% \n  select(origin, dep_delay) %&gt;% \n  plot_boxplot(by = \"origin\")"
  },
  {
    "objectID": "posts/flights-yacsda-eda/index.html#drei-variablen-origin-hour-dep_delay",
    "href": "posts/flights-yacsda-eda/index.html#drei-variablen-origin-hour-dep_delay",
    "title": "flights-yacsda-eda",
    "section": "7.4 Drei Variablen: Origin, hour, dep_delay",
    "text": "7.4 Drei Variablen: Origin, hour, dep_delay\n\n7.4.0.1 Mit ggplot\n\nflights2 %&gt;% \n  filter(!is_extreme) %&gt;% \n  select(origin, dep_delay, hour) %&gt;% \n  mutate(hour = factor(hour, levels = 5:23)) %&gt;% \n  ggplot() +\n  aes(x = hour, y = dep_delay) +\n  geom_boxplot() +\n  facet_wrap(~ origin)\n\n\n\n\n\n\n\n\n\n\n7.4.0.2 Mit ggpubr\n\nflights2 %&gt;% \n  filter(!is_extreme) %&gt;% \n  select(origin, dep_delay, hour) %&gt;% \n  mutate(hour = factor(hour, levels = 5:23)) %&gt;% \n  ggboxplot(x = \"hour\", y = \"dep_delay\", facet.by = \"origin\")"
  },
  {
    "objectID": "posts/flights-yacsda-eda/index.html#vertiefung-alle-nominale-variablen",
    "href": "posts/flights-yacsda-eda/index.html#vertiefung-alle-nominale-variablen",
    "title": "flights-yacsda-eda",
    "section": "7.5 Vertiefung: Alle nominale Variablen",
    "text": "7.5 Vertiefung: Alle nominale Variablen\nNatürlich könnte man “händisch” alle nominalskalierten Variablen explizit benennen, etwa so:\n\nflights3 %&gt;% \n  select(carrier, tailnum, origin, dest, time_hour) %&gt;% \n  slice(1:3)\n\n\n\n\ncarrier\ntailnum\norigin\ndest\ntime_hour\n\n\n\n\nUA\nN14228\nEWR\nIAH\n2013-01-01 05:00:00\n\n\nUA\nN24211\nLGA\nIAH\n2013-01-01 05:00:00\n\n\nAA\nN619AA\nJFK\nMIA\n2013-01-01 05:00:00\n\n\n\n\n\nAber es geht auch etwas “cooler” mit weniger Tipperei:\n\nflights3 %&gt;% \n  select(where(~ !is.numeric(.))) %&gt;%  # wähle alle nicht-numerischen Spalten\n  names()\n\n[1] \"carrier\"    \"tailnum\"    \"origin\"     \"dest\"       \"time_hour\" \n[6] \"is_extreme\"\n\n\n\n7.5.1 flights4\n\nflights4 &lt;-\nflights3 %&gt;% \n  mutate(dest = fct_lump_prop(dest, prop = .025)) \n\nMit fct_lump_prop fassen wir alle Stufen zu einer zusammen, die jeweils weniger als 2.5% der Fääle ausmachen.\n\nflights4 %&gt;% \n  count(dest, sort = T)\n\n\n\n\ndest\nn\n\n\n\n\nOther\n172061\n\n\nATL\n16837\n\n\nORD\n16566\n\n\nLAX\n16026\n\n\nBOS\n15022\n\n\nMCO\n13967\n\n\nCLT\n13674\n\n\nSFO\n13173\n\n\nFLL\n11897\n\n\nMIA\n11593\n\n\nDCA\n9111\n\n\nDTW\n9031\n\n\nDFW\n8388\n\n\n\n\n\n\n\n7.5.2 Visualisierung im Grid\n\nflights4 %&gt;% \n  filter(!is_extreme) %&gt;% \n  select(dep_delay, dest, origin, carrier) %&gt;% \n  group_by(dest, origin, carrier) %&gt;% \n  summarise(depdelay_md = median(dep_delay, na.rm = T)) %&gt;% \n  ggplot() +\n  aes(x = origin, y = depdelay_md, color = origin) +\n  facet_grid(dest ~ carrier) +\n  geom_point()\n\n\n\n\n\n\n\n\nPuh, das Diagramm ist nicht sehr aussagekräftig. Vielleicht besser als Tabelle?\n\nflights4 %&gt;% \n  filter(!is_extreme) %&gt;% \n  select(dep_delay, dest, origin, carrier) %&gt;% \n  group_by(dest, origin, carrier) %&gt;% \n  summarise(depdelay_md = median(dep_delay, na.rm = T))\n\n# A tibble: 128 × 4\n# Groups:   dest, origin [37]\n   dest  origin carrier depdelay_md\n   &lt;fct&gt; &lt;chr&gt;  &lt;chr&gt;         &lt;dbl&gt;\n 1 ATL   EWR    9E               -6\n 2 ATL   EWR    DL               -3\n 3 ATL   EWR    EV               -2\n 4 ATL   EWR    UA               -1\n 5 ATL   JFK    9E               -2\n 6 ATL   JFK    DL               -1\n 7 ATL   LGA    DL               -3\n 8 ATL   LGA    EV               30\n 9 ATL   LGA    FL                0\n10 ATL   LGA    MQ               -4\n# ℹ 118 more rows\n\n\nHm, ist auch nicht gerade nützlich.\nDas Beispiel zeigt, dass die Datenvisualisierung bei einer größeren Zahl an Dimensionen und/oder vielen Werten an ihre Grenzen kommen kann."
  },
  {
    "objectID": "posts/flights-yacsda-eda/index.html#anzahl-von-flüge",
    "href": "posts/flights-yacsda-eda/index.html#anzahl-von-flüge",
    "title": "flights-yacsda-eda",
    "section": "7.6 Anzahl von Flüge",
    "text": "7.6 Anzahl von Flüge\n\n7.6.1 Vorbereitung\n\nflights4_sum &lt;- \n  flights4 %&gt;% \n  filter(!is_extreme) %&gt;% \n  select(month, origin, dep_delay) %&gt;% \n  drop_na() %&gt;% \n  group_by(month, origin) %&gt;% \n  summarise(delay_md = median(dep_delay),\n            delay_iqr = IQR(dep_delay),\n            delay_n = n()) %&gt;% \n  mutate(month = factor(month),\n         delay_n = as.numeric(delay_n))\n\n\n\n7.6.2 Visualisierung mit ggplot\n\nflights4 %&gt;% \n  filter(!is_extreme) %&gt;% \n  select(month, origin, dep_delay) %&gt;% \n  mutate(month = factor(month)) %&gt;% \n  drop_na() %&gt;% \n  ggplot() +\n  aes(x = month, y = dep_delay, color = origin) +\n  geom_violin() +\n  geom_point(data = flights4_sum, \n             aes(y = delay_md,\n                 x = month)) +\n  facet_wrap( ~ origin)\n\n\n\n\n\n\n\n\n\n\n7.6.3 Visualisierung mit ggpubr\n\nflights4 %&gt;% \n  filter(!is_extreme) %&gt;% \n  select(month, origin, dep_delay) %&gt;% \n  mutate(month = factor(month)) %&gt;% \n  drop_na() %&gt;% \n  ggviolin(x = \"month\", y = \"dep_delay\", facet.by = \"origin\",\n           color = \"origin\")"
  },
  {
    "objectID": "posts/Rethink2m5/Rethink2m5.html",
    "href": "posts/Rethink2m5/Rethink2m5.html",
    "title": "Rethink2m5",
    "section": "",
    "text": "Aufgabe\nThis question is taken from McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Ed.). Taylor and Francis, CRC Press.\n2M5. Now suppose there are four cards: B/B, B/W, W/W, and another B/B. Again suppose a card is drawn from the bag and a black side appears face up. Again calculate the probability that the other side is black.\n         \n\n\nLösung\nThe only difference to the question 2M4 is that we now have two bb cards, rendering the prior plausibility for balck twice as high. The rest is the same as in 2M4.\nLet’s label the cards bb (black on both sides), bw (black on one, white on the other), and ww (both sides are white), respectively.\nWanted is the probability that the second side of the card is black (2b), given one side is already identified as black (1b): \\(Pr(2b|1b)\\).\n\nlibrary(tidyverse)\n\n\nd &lt;-\n  tibble::tribble(\n  ~Hyp, ~Prior,\n  \"bb\",     2, \n  \"bw\",     1,   \n  \"ww\",     1, \n  ) %&gt;% \n  mutate(Likelihood = c(2,1,0),\n         unstand_post = Prior*Likelihood,\n         std_post = unstand_post / sum(unstand_post))\n\n\n\n\n\n\n\n\n\nHyp\nPrior\nLikelihood\nunstand_post\nstd_post\n\n\n\n\nbb\n2\n2\n4\n0.80\n\n\nbw\n1\n1\n1\n0.20\n\n\nww\n1\n0\n0\n0.00\n\n\n\n\n\n\n\nTables like this are sometimes called “Bayes Box” or “Bayes Grid”.\nAlternatively, we can write out the two black cards not in one, but in two rows, so that each black card gets it own row.\n\n\n\n\n\n\n\n\nHyp\nPrior\nLikelihood\nunstand_post\nstd_post\n\n\n\n\nbb\n1\n2\n2\n0.40\n\n\nbb\n1\n2\n2\n0.40\n\n\nbw\n1\n1\n1\n0.20\n\n\nww\n1\n0\n0\n0.00\n\n\n\n\n\n\n\nThe second Bayes Box yields the same probability as the first, which is reassuring. However, it feels a bit akward (at least to me) to write down the same hypothesis twice.\n\nCategories:\n\nprobability\nbayes\nbayes-grid\nrethink-chap2\nstring"
  },
  {
    "objectID": "posts/bike03/bike03.html",
    "href": "posts/bike03/bike03.html",
    "title": "bike03",
    "section": "",
    "text": "Kann man die Anzahl gerade verliehener Fahrräder eines entsprechenden Anbieters anhand der Temperatur vorhersagen?\nIn dieser Übung untersuchen wir diese Frage.\nSie können die Daten von der Webseite der UCI herunterladen.\nWir beziehen uns auf den Datensatz day.\nBerechnen Sie einen Entscheidungsbaum mit der Anzahl der aktuell vermieteten Räder als AV und der aktuellen Temperatur als UV!\nTunen Sie den Cp-Parameter des Baumes; lassen Sie sich 20 Tuningparameter vorschlagen.\nGeben Sie den MSE an!\nHinweise"
  },
  {
    "objectID": "posts/bike03/bike03.html#data-split",
    "href": "posts/bike03/bike03.html#data-split",
    "title": "bike03",
    "section": "Data split",
    "text": "Data split\n\nset.seed(42)\nd_split &lt;- initial_split(d, strata = cnt)\n\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)"
  },
  {
    "objectID": "posts/bike03/bike03.html#define-recipe",
    "href": "posts/bike03/bike03.html#define-recipe",
    "title": "bike03",
    "section": "Define recipe",
    "text": "Define recipe\n\nrec1 &lt;- \n  recipe(cnt ~ temp, data = d)"
  },
  {
    "objectID": "posts/bike03/bike03.html#define-model",
    "href": "posts/bike03/bike03.html#define-model",
    "title": "bike03",
    "section": "Define model",
    "text": "Define model\n\nm1 &lt;-\n  decision_tree(cost_complexity = tune(),\n                mode = \"regression\")"
  },
  {
    "objectID": "posts/bike03/bike03.html#tuning-grid",
    "href": "posts/bike03/bike03.html#tuning-grid",
    "title": "bike03",
    "section": "Tuning grid",
    "text": "Tuning grid\n\ngrid &lt;-\n  grid_regular(cost_complexity(), levels = 20)\ngrid\n\n# A tibble: 20 × 1\n   cost_complexity\n             &lt;dbl&gt;\n 1        1   e-10\n 2        2.98e-10\n 3        8.86e-10\n 4        2.64e- 9\n 5        7.85e- 9\n 6        2.34e- 8\n 7        6.95e- 8\n 8        2.07e- 7\n 9        6.16e- 7\n10        1.83e- 6\n11        5.46e- 6\n12        1.62e- 5\n13        4.83e- 5\n14        1.44e- 4\n15        4.28e- 4\n16        1.27e- 3\n17        3.79e- 3\n18        1.13e- 2\n19        3.36e- 2\n20        1   e- 1\n\n\nAlternativ:\n\ngrid &lt;-\n  grid_regular(extract_parameter_set_dials(m1), levels = 20)\ngrid\n\n# A tibble: 20 × 1\n   cost_complexity\n             &lt;dbl&gt;\n 1        1   e-10\n 2        2.98e-10\n 3        8.86e-10\n 4        2.64e- 9\n 5        7.85e- 9\n 6        2.34e- 8\n 7        6.95e- 8\n 8        2.07e- 7\n 9        6.16e- 7\n10        1.83e- 6\n11        5.46e- 6\n12        1.62e- 5\n13        4.83e- 5\n14        1.44e- 4\n15        4.28e- 4\n16        1.27e- 3\n17        3.79e- 3\n18        1.13e- 2\n19        3.36e- 2\n20        1   e- 1"
  },
  {
    "objectID": "posts/bike03/bike03.html#define-resamples",
    "href": "posts/bike03/bike03.html#define-resamples",
    "title": "bike03",
    "section": "Define Resamples",
    "text": "Define Resamples\n\nrsmpl &lt;- vfold_cv(d_train)"
  },
  {
    "objectID": "posts/bike03/bike03.html#workflow",
    "href": "posts/bike03/bike03.html#workflow",
    "title": "bike03",
    "section": "Workflow",
    "text": "Workflow\n\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(m1) %&gt;% \n  add_recipe(rec1)"
  },
  {
    "objectID": "posts/bike03/bike03.html#fit",
    "href": "posts/bike03/bike03.html#fit",
    "title": "bike03",
    "section": "Fit",
    "text": "Fit\n\ntic()\nfit1 &lt;- tune_grid(\n  object = wf1, \n  resamples = rsmpl)\ntoc()\n\n5.821 sec elapsed\n\nfit1\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits           id     .metrics          .notes          \n   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n 1 &lt;split [492/55]&gt; Fold01 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [492/55]&gt; Fold02 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [492/55]&gt; Fold03 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [492/55]&gt; Fold04 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [492/55]&gt; Fold05 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [492/55]&gt; Fold06 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [492/55]&gt; Fold07 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [493/54]&gt; Fold08 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [493/54]&gt; Fold09 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [493/54]&gt; Fold10 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;"
  },
  {
    "objectID": "posts/bike03/bike03.html#bester-kandidat",
    "href": "posts/bike03/bike03.html#bester-kandidat",
    "title": "bike03",
    "section": "Bester Kandidat",
    "text": "Bester Kandidat\n\nshow_best(fit1)\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\n\n# A tibble: 5 × 7\n  cost_complexity .metric .estimator  mean     n std_err .config              \n            &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1      0.0208     rmse    standard   1478.    10    34.7 Preprocessor1_Model09\n2      0.00220    rmse    standard   1538.    10    36.4 Preprocessor1_Model01\n3      0.000306   rmse    standard   1556.    10    40.3 Preprocessor1_Model07\n4      0.00000175 rmse    standard   1558.    10    39.7 Preprocessor1_Model02\n5      0.0000194  rmse    standard   1558.    10    39.7 Preprocessor1_Model03\n\n\n\nwf1_best &lt;-\n  wf1 %&gt;% \n  finalize_workflow(parameters = select_best(fit1))\n\nWarning: No value of `metric` was given; metric 'rmse' will be used."
  },
  {
    "objectID": "posts/bike03/bike03.html#last-fit",
    "href": "posts/bike03/bike03.html#last-fit",
    "title": "bike03",
    "section": "Last Fit",
    "text": "Last Fit\n\nfit_testsample &lt;- last_fit(wf1_best, d_split)"
  },
  {
    "objectID": "posts/bike03/bike03.html#model-performance-metrics-in-test-set",
    "href": "posts/bike03/bike03.html#model-performance-metrics-in-test-set",
    "title": "bike03",
    "section": "Model performance (metrics) in test set",
    "text": "Model performance (metrics) in test set\n\nfit_testsample %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    1430.    Preprocessor1_Model1\n2 rsq     standard       0.473 Preprocessor1_Model1\n\n\n\nMSE &lt;- fit_testsample %&gt;% collect_metrics() %&gt;% pluck(3, 1)\nMSE\n\n[1] 1430.304\n\n\nSolution: 1430.3042213\n\nCategories:\n\nstatlearning\ntidymodels\nnum"
  },
  {
    "objectID": "posts/Postvert-Regr-01/Postvert-Regr-01.html",
    "href": "posts/Postvert-Regr-01/Postvert-Regr-01.html",
    "title": "Postvert-Regr-01",
    "section": "",
    "text": "Nach der Berechnung bzw. Schätzung der Modellparameter eines Regressionsmodells (mit Methoden der Bayes-Inferenz) erhält man u.a. auf die Prädiktorwerte \\(x_i\\) (\\(i=1,2,...,n\\)) bedingte Wahrscheinlichkeiten \\(p_i\\) für die AV, \\(y_i\\), oder genauer \\(y_i|x_i,\\theta\\) (mit \\(\\theta\\) für die Modellparameter).\nBetrachten Sie dazu folgende Aussage:\n\\(Pr(y_i|x_i, \\alpha, \\beta, \\sigma) = c\\) für \\(i=1,2,...,n\\)\nWelche der Aussagen ist in diesem Zusammenhang falsch?\n\n\n\nDas Regressionsmodell hat 3 Parameter.\nDas Regressionsmodell hat 1 Prädiktor (im Sinne von 1 Inputvariablen).\n\\(Pr(y_1|x_1, \\alpha, \\beta, \\sigma) &gt; Pr(y_2|x_2, \\alpha, \\beta, \\sigma)\\)\n\\(\\sum_{y_i = -\\infty}^{+\\infty} Pr(y_i|x_i, \\alpha, \\beta, \\sigma) = 1\\)\n\\(Pr(y_i|x_i, \\alpha, \\beta, \\sigma) = p_i, \\qquad p_i \\in [0,1]\\)"
  },
  {
    "objectID": "posts/Postvert-Regr-01/Postvert-Regr-01.html#answerlist",
    "href": "posts/Postvert-Regr-01/Postvert-Regr-01.html#answerlist",
    "title": "Postvert-Regr-01",
    "section": "",
    "text": "Das Regressionsmodell hat 3 Parameter.\nDas Regressionsmodell hat 1 Prädiktor (im Sinne von 1 Inputvariablen).\n\\(Pr(y_1|x_1, \\alpha, \\beta, \\sigma) &gt; Pr(y_2|x_2, \\alpha, \\beta, \\sigma)\\)\n\\(\\sum_{y_i = -\\infty}^{+\\infty} Pr(y_i|x_i, \\alpha, \\beta, \\sigma) = 1\\)\n\\(Pr(y_i|x_i, \\alpha, \\beta, \\sigma) = p_i, \\qquad p_i \\in [0,1]\\)"
  },
  {
    "objectID": "posts/Postvert-Regr-01/Postvert-Regr-01.html#answerlist-1",
    "href": "posts/Postvert-Regr-01/Postvert-Regr-01.html#answerlist-1",
    "title": "Postvert-Regr-01",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch. Das Modell hat tatsächlich der zu schätzende Parameter: \\(\\alpha, \\beta, \\sigma\\).\nFalsch. Das Modell hat tatsächlich einen Prädiktor, \\(x_i\\).\nWahr. Die Aussage ist nicht grundsätzlich richtig.\nFalsch. Die Wahrscheinlichkeiten für alle möglichen \\(y\\) für eine bestimmte Person summiert sich tatsächlich zu 1 auf.\nFalsch. Eine Warhscheinlichkeit kann tatsächlich zwischen 0 und 1 liegen, wobei die Grenzen nur in Extremfällen vorkommen.\n\n\nCategories:\n\nregression\nbayes\npost"
  },
  {
    "objectID": "posts/tidymodels-lasso3/tidymodels-lasso3.html",
    "href": "posts/tidymodels-lasso3/tidymodels-lasso3.html",
    "title": "tidymodels-lasso3",
    "section": "",
    "text": "Schreiben Sie eine prototypische Analyse für ein Vorhersagemodell mit dem Lasso.\nBerichten Sie, welche Prädiktoren nach dem Lasso im Modell verbleiben.\nHinweise:\n\nTunen Sie die Penalisierung.\nVerwenden Sie Kreuzvalidierung.\nVerwenden Sie Standardwerte, wo nicht anders angegeben.\nFixieren Sie Zufallszahlen auf den Startwert 42.\nVerwenden Sie den Datensatz penguins.\nModellformel: body_mass_g ~ ."
  },
  {
    "objectID": "posts/tidymodels-lasso3/tidymodels-lasso3.html#standardvorgehen",
    "href": "posts/tidymodels-lasso3/tidymodels-lasso3.html#standardvorgehen",
    "title": "tidymodels-lasso3",
    "section": "Standardvorgehen",
    "text": "Standardvorgehen\n\n# 2023-05-14\n\n# Setup:\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(tictoc)  # Zeitmessung\nlibrary(vip)  # Variablenbedeutung\n\n# Data:\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\"\nd &lt;- read_csv(d_path)\n\n# drop rows with NA in outcome variable:\nd &lt;-\n  d %&gt;% \n  drop_na(body_mass_g)\n\nset.seed(42)\nd_split &lt;- initial_split(d)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\n\n# model:\nmod_lasso &lt;-\n  linear_reg(mode = \"regression\",\n             penalty = tune(),\n             mixture = 1,\n             engine = \"glmnet\")\n\n# cv:\nset.seed(42)\nrsmpl &lt;- vfold_cv(d_train)\n\n\n# recipe:\nrec1_plain &lt;- \n  recipe(body_mass_g ~  ., data = d_train) %&gt;% \n  update_role(\"rownames\", new_role = \"id\") %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_impute_bag(all_predictors())\n\n\n# check:\nd_train_baked &lt;- \n  prep(rec1_plain) %&gt;% bake(new_data = NULL)\n\nna_n &lt;- sum(is.na(d_train_baked))\n\n\n# workflow:\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(mod_lasso) %&gt;% \n  add_recipe(rec1_plain)\n\n\n# tuning:\ntic()\nwf1_fit &lt;-\n  wf1 %&gt;% \n  tune_grid(\n    resamples = rsmpl)\ntoc()\n\n11.456 sec elapsed\n\n# best candidate:\nshow_best(wf1_fit)\n\n# A tibble: 5 × 7\n   penalty .metric .estimator  mean     n std_err .config              \n     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1 1.97e-10 rmse    standard    281.    10    12.0 Preprocessor1_Model01\n2 4.54e- 9 rmse    standard    281.    10    12.0 Preprocessor1_Model02\n3 8.93e- 8 rmse    standard    281.    10    12.0 Preprocessor1_Model03\n4 1.75e- 7 rmse    standard    281.    10    12.0 Preprocessor1_Model04\n5 1.65e- 6 rmse    standard    281.    10    12.0 Preprocessor1_Model05\n\n# finalize wf:\nwf1_final &lt;-\n  wf1 %&gt;% \n  finalize_workflow(select_best(wf1_fit))\n\n\nwf1_fit_final &lt;-\n  wf1_final %&gt;% \n  last_fit(d_split)\n\n\n# Modellgüte im Test-Set:\ncollect_metrics(wf1_fit_final)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard     326.    Preprocessor1_Model1\n2 rsq     standard       0.819 Preprocessor1_Model1"
  },
  {
    "objectID": "posts/tidymodels-lasso3/tidymodels-lasso3.html#inspektion-der-tuningparameter",
    "href": "posts/tidymodels-lasso3/tidymodels-lasso3.html#inspektion-der-tuningparameter",
    "title": "tidymodels-lasso3",
    "section": "Inspektion der Tuningparameter",
    "text": "Inspektion der Tuningparameter\n\nautoplot(wf1_fit)\n\n\n\n\n\n\n\n\nDie Standard-Wahl der Tuningparameter-Werte war offenbar nicht so ideal, zumindest sieht man kaum Unterschiede zwischen der Modellgüte in Abhängigkeit von den Werten der Tuningparameter."
  },
  {
    "objectID": "posts/tidymodels-lasso3/tidymodels-lasso3.html#variablenbedeutung",
    "href": "posts/tidymodels-lasso3/tidymodels-lasso3.html#variablenbedeutung",
    "title": "tidymodels-lasso3",
    "section": "Variablenbedeutung",
    "text": "Variablenbedeutung\n\nlibrary(vip)\n\nvi_preds &lt;- \nwf1_fit_final %&gt;% \n  extract_fit_engine() %&gt;% \n  vi()\n\nvi_preds\n\n# A tibble: 9 × 3\n  Variable          Importance Sign \n  &lt;chr&gt;                  &lt;dbl&gt; &lt;chr&gt;\n1 species_Gentoo         763.  POS  \n2 sex_male               389.  POS  \n3 species_Chinstrap      284.  NEG  \n4 flipper_length_mm      268.  POS  \n5 bill_length_mm         128.  POS  \n6 bill_depth_mm           70.6 POS  \n7 island_Dream            63.8 NEG  \n8 year                    30.7 NEG  \n9 island_Torgersen         0   NEG  \n\n\n\nvi_preds %&gt;% \n  ggplot(aes(x = Importance, y = reorder(Variable, Importance), fill = Sign)) +\n  geom_col()\n\n\n\n\n\n\n\n\nMan beachte: Für regulierte Modelle sind Zentrierung und Skalierung nötig.\n\nCategories:\n\ntidymodels\nstatlearning\nlasso\nlm\nstring\ntemplate"
  },
  {
    "objectID": "posts/kausal04/kausal04.html",
    "href": "posts/kausal04/kausal04.html",
    "title": "kausal04",
    "section": "",
    "text": "Gegeben sei ein DAG g (s.u.). Was ist die minimale Menge an Variablen (minimal adjustment set), die man kontrollieren muss, um den kausalen Effekt von smoking auf arrest zu identifizieren?\n\n\n\n\n\n\n\n\n\n\n\n\n{ Cholestorol }\n{ Weight }\nkeine, da nicht identifiziferbar\n{ Cholestrol, Unhealty Lifestyle }\n{ Cholestorol, Weight }"
  },
  {
    "objectID": "posts/kausal04/kausal04.html#answerlist",
    "href": "posts/kausal04/kausal04.html#answerlist",
    "title": "kausal04",
    "section": "",
    "text": "{ Cholestorol }\n{ Weight }\nkeine, da nicht identifiziferbar\n{ Cholestrol, Unhealty Lifestyle }\n{ Cholestorol, Weight }"
  },
  {
    "objectID": "posts/kausal04/kausal04.html#answerlist-1",
    "href": "posts/kausal04/kausal04.html#answerlist-1",
    "title": "kausal04",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/count-emojis/count-emojis.html",
    "href": "posts/count-emojis/count-emojis.html",
    "title": "count-emojis",
    "section": "",
    "text": "Zählen sie die Emojis eines Textes.\nUntersuchen Sie die Rechenzeit, die die jeweiligen Verfahren benötigen.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks."
  },
  {
    "objectID": "posts/count-emojis/count-emojis.html#stringrstr_count",
    "href": "posts/count-emojis/count-emojis.html#stringrstr_count",
    "title": "count-emojis",
    "section": "stringr::str_count",
    "text": "stringr::str_count\nMan kann den Unicode-Code von Emojis ansprechen, praktische Sache:\n\nemoji_pattern &lt;- \"\\\\p{So}\" \n\n\ntest_text$text |&gt; \n  map_int(str_count, emoji_pattern)\n\n[1] 0 3 1 2\n\n\nDie Funktion map ist nicht nötig:\n\nstr_count(test_text$text, \"\\\\p{So}\")\n\n[1] 0 3 1 2\n\n\nAls neue Spalte in der Tabelle:\n\ntest_text |&gt;\n  mutate(n_emojis = str_count(text, \"\\\\p{So}\"))\n\n# A tibble: 4 × 4\n     id text            valence n_emojis\n  &lt;int&gt; &lt;chr&gt;             &lt;dbl&gt;    &lt;int&gt;\n1     1 Abbau ist jetzt       0        0\n2     2 Hello 😊🌎🚀          1        3\n3     3 🔫                   -1        1\n4     4 🔫 🔪                -2        2"
  },
  {
    "objectID": "posts/count-emojis/count-emojis.html#stringrstr_count-1",
    "href": "posts/count-emojis/count-emojis.html#stringrstr_count-1",
    "title": "count-emojis",
    "section": "stringr::str_count",
    "text": "stringr::str_count\n\ntic()\nmethod1 &lt;- germeval_train$text |&gt; \n  map_int(str_count, emoji_pattern)\ntoc()\n\n0.451 sec elapsed\n\nmethod1 |&gt; str()\n\n int [1:5009] 0 0 1 0 0 0 0 0 0 1 ...\n\n\n\nprint(method1, max = 20)\n\n [1] 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 4 0 0\n [ reached getOption(\"max.print\") -- omitted 4989 entries ]\n\n\nDie Funktion map ist nicht nötig:\n\ntic()\nmethod3 &lt;- \n  str_count(germeval_train$text, emoji_pattern)\ntoc()\n\n0.009 sec elapsed\n\n\n\nmethod3 |&gt; head()\n\n[1] 0 0 1 0 0 0\n\n\nDann geht es auch viel schneller.\nAls neue Spalte in der Tabelle:\n\ntic()\nmethod4 &lt;- \ngermeval_train |&gt; \n  mutate(n_words = str_count(text, emoji_pattern))\ntoc()\n\n0.009 sec elapsed\n\n\n\nstr(method4)\n\n'data.frame':   5009 obs. of  5 variables:\n $ id     : int  1 2 3 4 5 6 7 8 9 10 ...\n $ text   : chr  \"@corinnamilborn Liebe Corinna, wir würden dich gerne als Moderatorin für uns gewinnen! Wärst du begeisterbar?\" \"@Martin28a Sie haben ja auch Recht. Unser Tweet war etwas missverständlich. Dass das BVerfG Sachleistungen nich\"| __truncated__ \"@ahrens_theo fröhlicher gruß aus der schönsten stadt der welt theo ⚓️\" \"@dushanwegner Amis hätten alles und jeden gewählt...nur Hillary wollten sie nicht und eine Fortsetzung von Obam\"| __truncated__ ...\n $ c1     : chr  \"OTHER\" \"OTHER\" \"OTHER\" \"OTHER\" ...\n $ c2     : chr  \"OTHER\" \"OTHER\" \"OTHER\" \"OTHER\" ...\n $ n_words: int  0 0 1 0 0 0 0 0 0 1 ...\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n\nmethod4 |&gt; head()\n\n  id\n1  1\n2  2\n3  3\n4  4\n5  5\n6  6\n                                                                                                                                                                                                                                                                                          text\n1                                                                                                                                                                                @corinnamilborn Liebe Corinna, wir würden dich gerne als Moderatorin für uns gewinnen! Wärst du begeisterbar?\n2                                                                                                                                               @Martin28a Sie haben ja auch Recht. Unser Tweet war etwas missverständlich. Dass das BVerfG Sachleistungen nicht ausschließt, kritisieren wir.\n3                                                                                                                                                                                                                        @ahrens_theo fröhlicher gruß aus der schönsten stadt der welt theo ⚓️\n4                                                                                                                                                 @dushanwegner Amis hätten alles und jeden gewählt...nur Hillary wollten sie nicht und eine Fortsetzung von Obama-Politik erst recht nicht..!\n5                                                                                                                                                     @spdde kein verläßlicher Verhandlungspartner. Nachkarteln nach den Sondierzngsgesprächen - schickt diese Stümper #SPD in die Versenkung.\n6 @Dirki_M Ja, aber wo widersprechen die Zahlen denn denen, die im von uns verlinkten Artikel stehen? In unserem Tweet geht es rein um subs. Geschützte. 2017 ist der gesamte Familiennachzug im Vergleich zu 2016 - die Zahlen, die Hr. Brandner bemüht - übrigens leicht rückläufig gewesen.\n       c1     c2 n_words\n1   OTHER  OTHER       0\n2   OTHER  OTHER       0\n3   OTHER  OTHER       1\n4   OTHER  OTHER       0\n5 OFFENSE INSULT       0\n6   OTHER  OTHER       0\n\n\n\nCategories:\n\ntextmining\ntidymodels\ncount\ngermeval\nemojis\nstring"
  },
  {
    "objectID": "posts/kausal03/kausal03.html",
    "href": "posts/kausal03/kausal03.html",
    "title": "kausal03",
    "section": "",
    "text": "Gegeben sei der DAG g (s.u.). Was ist die minimale Menge an Variablen, die man kontrollieren muss, um den kausalen Effekt von x auf y zu identifizieren?\n\n\n\n\n\n\n\n\n\nHinweise:\n\nGebogene Kurven mit doppelter Pfeilspitze zeigen keine Kausaleinflüsse ein (was in DAGs nicht erlaubt wäre).\nStattdessen zeigen Sie eine Assoziation bedingt durch eine (nicht aufgeführte) Konfundierungsvariable an.\n\n\n\n\n{ w1, w2, z2 }\n{ w2, z2 }\n{ w1, w2 }\n{ w1, z2 }\n{ w1 }"
  },
  {
    "objectID": "posts/kausal03/kausal03.html#answerlist",
    "href": "posts/kausal03/kausal03.html#answerlist",
    "title": "kausal03",
    "section": "",
    "text": "{ w1, w2, z2 }\n{ w2, z2 }\n{ w1, w2 }\n{ w1, z2 }\n{ w1 }"
  },
  {
    "objectID": "posts/kausal03/kausal03.html#answerlist-1",
    "href": "posts/kausal03/kausal03.html#answerlist-1",
    "title": "kausal03",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal\nexam-22"
  },
  {
    "objectID": "posts/rf-finalize/rf-finalize.html",
    "href": "posts/rf-finalize/rf-finalize.html",
    "title": "rf-finalize",
    "section": "",
    "text": "Aufgabe\n\nBerechnen Sie ein prädiktives Modell mit dieser Modellgleichung:\nbody_mass_g ~ . (Datensatz: palmerpenguins::penguins).\nBerichten Sie den RSMSE im Test-Sample!\nHinweise: - Tunen Sie mtry - Verwenden Sie Kreuzvalidierung - Verwenden Sie Standardwerte, wo nicht anders angegeben. - Fixieren Sie Zufallszahlen auf den Startwert 42.\n         \n\n\nLösung\n\n# Setup:\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.0.8\n✔ dials        1.2.0     ✔ rsample      1.2.0\n✔ dplyr        1.1.3     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.4\n✔ lubridate 1.9.3     ✔ stringr   1.5.0\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tictoc)  # Zeitmessung\n\n\n# Data:\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\"\nd &lt;- read_csv(d_path)\n\nRows: 344 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): species, island, sex\ndbl (6): rownames, bill_length_mm, bill_depth_mm, flipper_length_mm, body_ma...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# rm NA in the dependent variable:\nd &lt;- d %&gt;% \n  drop_na(body_mass_g)\n\n\nset.seed(42)\nd_split &lt;- initial_split(d)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\n\n# model:\nmod_rf &lt;-\n  rand_forest(mode = \"regression\",\n           mtry = tune())\n\n\n# cv:\nset.seed(42)\nrsmpl &lt;- vfold_cv(d_train)\n\n\n# recipe:\nrec_plain &lt;- \n  recipe(body_mass_g ~  ., data = d_train) %&gt;% \n  step_impute_bag(all_predictors())\n\n\n# workflow:\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(mod_rf) %&gt;% \n  add_recipe(rec_plain)\n\n\n# tuning:\ntic()\nwf1_fit &lt;-\n  wf1 %&gt;% \n  tune_grid(\n    resamples = rsmpl)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\ntoc()\n\n23.078 sec elapsed\n\n# best candidate:\nshow_best(wf1_fit)\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\n\n# A tibble: 5 × 7\n   mtry .metric .estimator  mean     n std_err .config             \n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1     2 rmse    standard    282.    10   11.1  Preprocessor1_Model5\n2     3 rmse    standard    282.    10   10.6  Preprocessor1_Model7\n3     8 rmse    standard    282.    10    9.84 Preprocessor1_Model2\n4     5 rmse    standard    283.    10    9.41 Preprocessor1_Model3\n5     4 rmse    standard    283.    10    9.95 Preprocessor1_Model4\n\n# finalize wf:\nwf1_final &lt;-\n  wf1 %&gt;% \n  finalize_workflow(select_best(wf1_fit))\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\nwf1_fit_final &lt;-\n  wf1_final %&gt;% \n  last_fit(d_split)\n\n\n# Modellgüte im Test-Set:\ncollect_metrics(wf1_fit_final)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard     327.    Preprocessor1_Model1\n2 rsq     standard       0.817 Preprocessor1_Model1\n\n\nAchtung: step_impute_knn scheint Probleme zu haben, wenn es Charakter-Variablen gibt.\n\nCategories:\n\ntidymodels\nstatlearning\ntemplate\nstring"
  },
  {
    "objectID": "posts/variation01/variation01.html",
    "href": "posts/variation01/variation01.html",
    "title": "variability01",
    "section": "",
    "text": "In welchem Datensatz, \\(x1, x2, x3, x4\\), gibt es am meisten Variation?\nDatensatz x1:\n\n\n\n\n\nx1\n\n\n\n\n0.14\n\n\n-0.06\n\n\n0.04\n\n\n0.06\n\n\n0.04\n\n\n-0.01\n\n\n0.15\n\n\n-9.47e-03\n\n\n0.20\n\n\n-6.27e-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDatensatz x2:\n\n\n\n\n\nx2\n\n\n\n\n1.30\n\n\n2.29\n\n\n-1.39\n\n\n-0.28\n\n\n-0.13\n\n\n0.64\n\n\n-0.28\n\n\n-2.66\n\n\n-2.44\n\n\n1.32\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDatensatz x3:\n\n\n\n\n\nx3\n\n\n\n\n-3.07\n\n\n-17.81\n\n\n-1.72\n\n\n12.15\n\n\n18.95\n\n\n-4.30\n\n\n-2.57\n\n\n-17.63\n\n\n4.60\n\n\n-6.40\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDatensatz x4:\n\n\n\n\n\nx4\n\n\n\n\n45.55\n\n\n70.48\n\n\n103.51\n\n\n-60.89\n\n\n50.50\n\n\n-171.70\n\n\n-78.45\n\n\n-85.09\n\n\n-241.42\n\n\n3.61\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD"
  },
  {
    "objectID": "posts/variation01/variation01.html#answerlist",
    "href": "posts/variation01/variation01.html#answerlist",
    "title": "variability01",
    "section": "",
    "text": "A\nB\nC\nD"
  },
  {
    "objectID": "posts/variation01/variation01.html#answerlist-1",
    "href": "posts/variation01/variation01.html#answerlist-1",
    "title": "variability01",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nWahr\n\n\nCategories:\n\nvariablity\nbasics\nschoice"
  },
  {
    "objectID": "posts/Rethink2m3/Rethink2m3.html",
    "href": "posts/Rethink2m3/Rethink2m3.html",
    "title": "Rethink2m3",
    "section": "",
    "text": "Aufgabe\nThis question is taken from McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Ed.). Taylor and Francis, CRC Press.\n2M3. Suppose there are two globes, one for Earth and one for Mars. The Earth globe is 70% covered in water. The Mars globe is 100% land. Further suppose that one of these globes—you don’t know which—was tossed in the air and produced a “land” observatiion. Assume that each globe was equally likely to be tossed. Show that the posterior probability that the globe was the Earth, conditional on seeing “land” (Pr(Earth|land)), is 0.23.\n         \n\n\nLösung\nMan kann die Aufgabe entweder mit einer Bayes-Box lösen oder durch die Formel des Bayes’ Theorem.\n\n\nBayes-Box\n\n\n\nHyp\nPrior\nL\nPo-unstand\nPo-stand\n\n\n\n\nE\n1\n.3\n.3\n3/13\n\n\nM\n1\n1\n1\n10/13\n\n\n\n\n\nBayes-Theorem\nZur Erinnerung:\n\\[\\begin{aligned}\nPr(A) &= Pr(A \\cap B) + Pr(A \\cap B^C)  \\qquad \\text{| totale Wskt, bei disjunkten Ereignissen}\\\\\nPr(A \\cap B) &= Pr(A|B) \\cdot Pr(B)\\\\\nPr(A \\cap B^C) &= Pr(A|B^C) \\cdot Pr(B^C)\n\\end{aligned}\\]\nWobei \\(A^C\\) das komlementäre Ereignis zu \\(A\\) meint.\nThe solution is taken from this source.\n\n# probability of land, given Earth:\np_le &lt;- 0.3\n\n# probability of land, given Mars:\np_lm &lt;- 1.0\n\n# probability of Earth:\np_e &lt;- 0.5\n\n# prob. of Mars:\np_m &lt;- 0.5\n\n# probability of land:\n# totale Wahrscheinlichkeit für Land\np_l &lt;- (p_e * p_le) + (p_m * p_lm)\np_l\n\n[1] 0.65\n\n\nDann gilt also:\n\n# probability of Earth, given land (using Bayes' Theorem):\np_el &lt;- (p_le * p_e) / p_l\np_el\n\n[1] 0.2307692\n\n\nEinfacher als die Rechnung ist vielleicht ein Baumdiagramm:\n\n\n\n\n\n\n\nCategories:\n\nprobability\nbayes\nrethink-chap2\nstring"
  },
  {
    "objectID": "posts/mw-berechnen/mw-berechnen2.html",
    "href": "posts/mw-berechnen/mw-berechnen2.html",
    "title": "mw-berechnen2",
    "section": "",
    "text": "Question\n\nAufgabe\nBerechnen Sie den Mittelwert folgender Zahlenreihe; ignorieren sie etwaige fehlende Werte.\n\nzahlenreihe &lt;- c(0.25, -1.85, -0.50,  2.56,  0.90)\nzahlenreihe\n\n[1]  0.25 -1.85 -0.50  2.56  0.90\n\n\nHinweise:\n\nRunden Sie auf zwei Dezimalstellen.\nBeachten Sie die üblichen Hinweise des Datenwerks.\n\n         \n\n\nLösung\n\nmean(zahlenreihe)\n\n[1] 0.272\n\n\nAntwort: Der Mittelwert liegt bei 0.27.\n\nCategories:\n\neda\ndatawrangling\ndyn\nnum"
  },
  {
    "objectID": "posts/ames-kaggle1/ames-kaggle1.html",
    "href": "posts/ames-kaggle1/ames-kaggle1.html",
    "title": "ames-kaggle1",
    "section": "",
    "text": "Berechnen Sie ein einfaches lineare Modell für die Ames House Price Kaggle Competition.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks."
  },
  {
    "objectID": "posts/ames-kaggle1/ames-kaggle1.html#pakete-starten",
    "href": "posts/ames-kaggle1/ames-kaggle1.html#pakete-starten",
    "title": "ames-kaggle1",
    "section": "Pakete starten",
    "text": "Pakete starten\n\nlibrary(tidyverse)\nlibrary(easystats)"
  },
  {
    "objectID": "posts/ames-kaggle1/ames-kaggle1.html#daten-importieren",
    "href": "posts/ames-kaggle1/ames-kaggle1.html#daten-importieren",
    "title": "ames-kaggle1",
    "section": "Daten importieren",
    "text": "Daten importieren\n\nd_train_path_online &lt;- \"https://raw.githubusercontent.com/sebastiansauer/Lehre/main/data/ames-kaggle/train.csv\"\nd_test_path_online &lt;- \"https://raw.githubusercontent.com/sebastiansauer/Lehre/main/data/ames-kaggle/test.csv\"\nd_train &lt;- read_csv(d_train_path_online)\n\nRows: 1460 Columns: 81\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (43): MSZoning, Street, Alley, LotShape, LandContour, Utilities, LotConf...\ndbl (38): Id, MSSubClass, LotFrontage, LotArea, OverallQual, OverallCond, Ye...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nd_test &lt;- read_csv(d_test_path_online)\n\nRows: 1459 Columns: 80\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (43): MSZoning, Street, Alley, LotShape, LandContour, Utilities, LotConf...\ndbl (37): Id, MSSubClass, LotFrontage, LotArea, OverallQual, OverallCond, Ye...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "posts/ames-kaggle1/ames-kaggle1.html#model-definieren",
    "href": "posts/ames-kaggle1/ames-kaggle1.html#model-definieren",
    "title": "ames-kaggle1",
    "section": "Model definieren",
    "text": "Model definieren\n\nm1 &lt;- lm(SalePrice ~ OverallQual, data = d_train)"
  },
  {
    "objectID": "posts/ames-kaggle1/ames-kaggle1.html#neue-daten-vorhersagen",
    "href": "posts/ames-kaggle1/ames-kaggle1.html#neue-daten-vorhersagen",
    "title": "ames-kaggle1",
    "section": "Neue Daten vorhersagen",
    "text": "Neue Daten vorhersagen\n\nm1_pred &lt;- predict(m1, newdata = d_test)"
  },
  {
    "objectID": "posts/ames-kaggle1/ames-kaggle1.html#daten-einreichen",
    "href": "posts/ames-kaggle1/ames-kaggle1.html#daten-einreichen",
    "title": "ames-kaggle1",
    "section": "Daten einreichen",
    "text": "Daten einreichen\n\nd_subm &lt;-\n  d_test %&gt;% \n  select(Id) %&gt;% \n  mutate(SalePrice = m1_pred)\n\nhead(d_subm)\n\n# A tibble: 6 × 2\n     Id SalePrice\n  &lt;dbl&gt;     &lt;dbl&gt;\n1  1461   130973.\n2  1462   176409.\n3  1463   130973.\n4  1464   176409.\n5  1465   267280.\n6  1466   176409.\n\n\n\nwrite_csv(d_subm, file = \"einreichen-kaggle-modell1-yeah.csv\")\n\n\nCategories:\n\nregression\names\nkaggle\nstring"
  },
  {
    "objectID": "posts/totale-Wskt1/totale-Wskt1.html",
    "href": "posts/totale-Wskt1/totale-Wskt1.html",
    "title": "totale-Wskt1",
    "section": "",
    "text": "Aufgabe\nWas ist die Wahrscheinlichkeit, bei einem Krebstest ein positives Testergebnis (Ereignis \\(T\\)) zu bekommen?\nEs gibt zwei Möglichkeiten für ein positives Testergebnis: Wenn man Krebs hat (\\(K\\)) und wenn man nicht Krebs hat (\\(\\neg K\\)).\n\\(Pr(T|K) = 9/10\\), das ist die “Krebs-Erkenn-Sicherheit” des Tests.\n\\(Pr(T|\\neg K) = 99/891\\), das ist die “Fehlalarm-Quote” des Tests.\nDie Grundrate von Krebs sei \\(Pr(K) = .01\\).\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n         \n\n\nLösung\nDie Ereignisse \\(K\\) und K$ bilden ein vollständiges Ereignissystem. Daher ist der Satz von der totalen Wahrscheinlichkeit anzuwenden.\n\\(Pr(T) = Pr(T|K) \\cdot Pr(K) + Pr(T| \\neg K) \\cdot Pr(\\neg K)\\).\n\nPr_T_geg_K &lt;- 9/10\nPr_K &lt;- .01\nPr_NK &lt;- 1 - Pr_K  # Wskt für Nicht-Krebs\nPr_T_geg_NK &lt;- 99/891  # ca. 10 Fehlerrate\nPr_T &lt;- Pr_T_geg_K * Pr_K + Pr_T_geg_NK * Pr_NK\nPr_T\n\n[1] 0.119\n\n\nDie Lösung lautet 0.119.\n\nCategories:\n\nR\nprobability\nbayes\nnum"
  },
  {
    "objectID": "posts/tidymodels-vorlage/tidymodels-vorlage.html",
    "href": "posts/tidymodels-vorlage/tidymodels-vorlage.html",
    "title": "tidymodels-vorlage",
    "section": "",
    "text": "Aufgabe\n\nSchreiben Sie eine prototypische Analyse für ein Vorhersagemodell, das sich als Vorlage für Analysen dieser Art eignet!\nHinweise:\n\nBerechnen Sie ein Modell\nTunen Sie mind. einen Parameter des Modells\nVerwenden Sie Kreuzvalidierung\nVerwenden Sie Standardwerte, wo nicht anders angegeben.\nFixieren Sie Zufallszahlen auf den Startwert 42.\n\n         \n\n\nLösung\n\n# 2023-05-08\n\n\n# Setup:\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.0.8\n✔ dials        1.2.0     ✔ rsample      1.2.0\n✔ dplyr        1.1.3     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.4\n✔ lubridate 1.9.3     ✔ stringr   1.5.0\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tictoc)  # Zeitmessung\nlibrary(baguette)  # Bagged-Trees\n\n\n# Data:\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\"\nd &lt;- read_csv(d_path)\n\nRows: 344 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): species, island, sex\ndbl (6): rownames, bill_length_mm, bill_depth_mm, flipper_length_mm, body_ma...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nset.seed(42)\nd_split &lt;- initial_split(d)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\n\n# model:\nmod_bag &lt;-\n  bag_tree(mode = \"regression\",\n           cost_complexity = tune())\n\n\n# cv:\nset.seed(42)\nrsmpl &lt;- vfold_cv(d_train)\n\n\n# recipe:\nrec1_plain &lt;- recipe(body_mass_g ~  ., data = d_train)\n\n\n# workflow:\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(mod_bag) %&gt;% \n  add_recipe(rec1_plain)\n\n\n# tuning:\ntic()\nwf1_fit &lt;-\n  wf1 %&gt;% \n  tune_grid(\n    resamples = rsmpl)\ntoc()\n\n35.202 sec elapsed\n\n# best candidate:\nshow_best(wf1_fit)\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\n\n# A tibble: 5 × 7\n  cost_complexity .metric .estimator  mean     n std_err .config              \n            &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1        2.10e- 3 rmse    standard    302.    10    13.1 Preprocessor1_Model09\n2        1.72e-10 rmse    standard    305.    10    18.4 Preprocessor1_Model10\n3        1.71e- 9 rmse    standard    306.    10    17.3 Preprocessor1_Model07\n4        3.34e- 5 rmse    standard    311.    10    18.1 Preprocessor1_Model04\n5        7.33e- 9 rmse    standard    316.    10    15.1 Preprocessor1_Model03\n\n# finalize wf:\nwf1_final &lt;-\n  wf1 %&gt;% \n  finalize_workflow(select_best(wf1_fit))\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\nwf1_fit_final &lt;-\n  wf1_final %&gt;% \n  last_fit(d_split)\n\n\n# Modellgüte im Test-Set:\ncollect_metrics(wf1_fit_final)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard     326.    Preprocessor1_Model1\n2 rsq     standard       0.847 Preprocessor1_Model1\n\n\n\nCategories:\n\ntidymodels\nstatlearning\ntemplate\nstring"
  },
  {
    "objectID": "posts/fattails02/fattails02.html",
    "href": "posts/fattails02/fattails02.html",
    "title": "fattails02",
    "section": "",
    "text": "Exercise\nIn seinem Buch “Statistical Consequences of Fat Tails” schreibt der Autor, Nassim Taleb (S. 53):\n\nIn the summer of 1998, the hedge fund called “Long Term Capital Management” (LTCM) proved to have a very short life; it went bust from some deviations in the markets –those “of an unexpected nature”. The loss was a yuuuge deal because two of the partners received the Swedish Riksbank Prize, marketed as the “Nobel” in economics. (…) At least two of the partners made the statement that it was a “10 sigma” event (10 standard deviations), hence they should be absolved of all accusations of incompetence (I was ﬁrst hand witness of two such statements).\n\nWir testen in diesem Zusammenhang zwei Hypothesen: \\(H_N\\), dass der Finanzmarkt normalverteilt ist und \\(H_F\\), dass die Variable fat tailed ist, also nicht normalverteilt, sondernn einer Verteilung entspringt, in der “Extremereignisse” üblicher sind als in einer Normalverteilung.\nUm die Fat-Tails-Verteilung mit \\(n=10\\) zu simulieren, nutzen wir hier folgende Funktion:\n\nfat_tail_data &lt;- rt(n = 100, df = 2)\n\nDabei bedeutet df = 1, dass die Verteilung sehr randlastig (fat tailed) sein soll (genauer gesagt eine t-Verteilung mit zwei Freiheitsgraden). Details dazu sollen uns hier nicht interessieren.\nBerechnen wir die Wahrscheinlichkeit, dass die Daten einer Normalverteilung entspringen (und nicht der Fat-Tail-Verteilung).\nDie Wahrscheinlichkeit eines 10-Sigma-Events ist übrigens … klein. Taleb berichtet sie mit \\(1.31 \\cdot 10^{-23}\\):\n\nL_norm &lt;- 1.31e-23\n\nFür die t-Verteilung ist der entsprechende Wert:\n\nL_fat &lt;- 1 - pt(q = 10, df = 2)\n\nWie hoch ist die Post-Wahrscheinlichkeit, dass die Variable normalverteilt ist?\nHinweise:\n\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\nApriori soll und die Hypothese der Normalverteilung 1000 Mal plausibler sein als die der t-Verteilung.\n\n\n\nAnswerlist\n\nkleiner als 50%\nkleiner als 5%\nkleiner als 0.5%\nkleiner als 0.05%\nkleiner als 0.005%\n\n         \n\n\nSolution\n\nlibrary(tidyverse)\n\nErstellen wir erstmal den ersten Teil einer Bayes-Box:\n\nd &lt;-\n  tibble(H = c(\"Normalverteilt\", \"Randlastig verteilt\"),\n         Prior = c(1e3,1))\n\nd\n\n# A tibble: 2 × 2\n  H                   Prior\n  &lt;chr&gt;               &lt;dbl&gt;\n1 Normalverteilt       1000\n2 Randlastig verteilt     1\n\n\nDann fügen wir den Likelihood jeder Hypothese dazu:\n\nd &lt;-\n  d %&gt;% \n  mutate(L = c(L_norm, L_fat))\n\nd\n\n# A tibble: 2 × 3\n  H                   Prior        L\n  &lt;chr&gt;               &lt;dbl&gt;    &lt;dbl&gt;\n1 Normalverteilt       1000 1.31e-23\n2 Randlastig verteilt     1 4.93e- 3\n\n\nDann berechnen wir die Post-Wahrscheinlichkeit:\n\nd &lt;-\n  d %&gt;% \n  mutate(Post_unstand = Prior * L,\n         Post = Post_unstand / sum(Post_unstand))\nd\n\n# A tibble: 2 × 5\n  H                   Prior        L Post_unstand     Post\n  &lt;chr&gt;               &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 Normalverteilt       1000 1.31e-23     1.31e-20 2.66e-18\n2 Randlastig verteilt     1 4.93e- 3     4.93e- 3 1   e+ 0\n\n\nDie Wahrscheinlichkeit, dass die Variable normalverteilt ist, ist seeeeehr klein, ca. \\(10^{-18}\\).\n\n\nAnswerlist\n\nFALSE\nFALSE\nFALSE\nFALSE\nTRUE\n\n\nCategories:\n\nprobability\nsimulation\nnormal-distribution"
  },
  {
    "objectID": "posts/Gem-Wskt2/Gem-Wskt2.html",
    "href": "posts/Gem-Wskt2/Gem-Wskt2.html",
    "title": "Gem-Wskt2",
    "section": "",
    "text": "Ein renommiertes Unternehmen sucht einen Kandidaten für eine (hoch dotierte) Führungsposition. Ein Managementberatungsunternehmung führt ein Assessmentcenter durch, welches pro Kandidat/in eine positive bzw. negative Empfehlung ergibt. Aus früheren Erfahrungen heraus wissen die Berater, dass die tatsächlich geeigneten Kandidaten (Ereignis \\(E\\) wie eligible) mit \\(67\\%\\) eine positive Empfehlung für die Stelle ausgesprochen bekommen (Ereignis \\(R\\) wie recommendation). Weiterhin bekommen von den nicht geeigneten Kandidaten \\(60\\%\\) eine negative Empfehlung. Insgesamt wissen die Berater, dass \\(9\\%\\) der Bewerber/innen tatsächlich geeignet sind.\nWas ist die entsprechende Häufigkeitstabelle? Geben Sie alle vier Einträge in Prozent an!\nHinweis: Das Gegenereignis vom Ereignis \\(A\\) wird als Komplementärereignis oder kurz als Komplement bezeichnet und mit \\(A^C\\) oder \\(\\overline{A}\\) abgekürzt. Im vorliegenden Fall meint \\(\\overline{R}=R^C\\) das Ereignis, dass ein Kandidat keine Empfehlung ausgesprochen bekommt.\n\n\n\n\\(P(E \\cap R)\\)\n\\(P(\\overline{E} \\cap R)\\)\n\\(P(E \\cap \\overline{R})\\)\n\\(P(\\overline{E} \\cap \\overline{R})\\)"
  },
  {
    "objectID": "posts/Gem-Wskt2/Gem-Wskt2.html#answerlist",
    "href": "posts/Gem-Wskt2/Gem-Wskt2.html#answerlist",
    "title": "Gem-Wskt2",
    "section": "",
    "text": "\\(P(E \\cap R)\\)\n\\(P(\\overline{E} \\cap R)\\)\n\\(P(E \\cap \\overline{R})\\)\n\\(P(\\overline{E} \\cap \\overline{R})\\)"
  },
  {
    "objectID": "posts/Gem-Wskt2/Gem-Wskt2.html#answerlist-1",
    "href": "posts/Gem-Wskt2/Gem-Wskt2.html#answerlist-1",
    "title": "Gem-Wskt2",
    "section": "Answerlist",
    "text": "Answerlist\n\n\\(P(E \\cap R) =  6.03\\%\\)\n\\(P(\\overline{E} \\cap R) = 36.40\\%\\)\n\\(P(E \\cap \\overline{R}) =  2.97\\%\\)\n\\(P(\\overline{E} \\cap \\overline{R}) = 54.60\\%\\)\n\n\nCategories:\n\nprobability\nbayes\ncloze"
  },
  {
    "objectID": "posts/wskt-quiz17/wskt-quiz17.html",
    "href": "posts/wskt-quiz17/wskt-quiz17.html",
    "title": "wskt-quiz17",
    "section": "",
    "text": "Behauptung:\nHat eine Hypothese die Priori-Wahrscheinlichkeit von 0, so wird die Post-Wahrscheinlichkeit dieser Hypothese 0 sein.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n\n\n\nFalsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz17/wskt-quiz17.html#answerlist",
    "href": "posts/wskt-quiz17/wskt-quiz17.html#answerlist",
    "title": "wskt-quiz17",
    "section": "",
    "text": "Falsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz17/wskt-quiz17.html#answerlist-1",
    "href": "posts/wskt-quiz17/wskt-quiz17.html#answerlist-1",
    "title": "wskt-quiz17",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\nquiz\nprobability\nbayes\nquiz1-qm2-ws23\nschoice"
  },
  {
    "objectID": "posts/options-print/options-print.html",
    "href": "posts/options-print/options-print.html",
    "title": "options-print",
    "section": "",
    "text": "Aufgabe\nWie kann man in R kontrollieren, wie viele Zeilen eines Tibbles gedruckt werden (am Bildschrirm)?\nFixieren Sie die Werte auf min. 5 und max. 10.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n         \n\n\nLösung\n\noptions(paged.print = FALSE,\n        pillar.print_max = 15, \n        pillar.print_min = 5)\n\n\nmtcars\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\n\nlibrary(tidyverse)\n\n\nmtcars &lt;- as_tibble(mtcars)\nmtcars\n\n# A tibble: 32 × 11\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  21       6   160   110  3.9   2.62  16.5     0     1     4     4\n2  21       6   160   110  3.9   2.88  17.0     0     1     4     4\n3  22.8     4   108    93  3.85  2.32  18.6     1     1     4     1\n4  21.4     6   258   110  3.08  3.22  19.4     1     0     3     1\n5  18.7     8   360   175  3.15  3.44  17.0     0     0     3     2\n# ℹ 27 more rows\n\n\n\nCategories:\n\n2023\nR\ntidyverse\nmarkdown\nstring"
  },
  {
    "objectID": "posts/mtcars-regr01/mtcars-regr01.html",
    "href": "posts/mtcars-regr01/mtcars-regr01.html",
    "title": "mtcars-regr01",
    "section": "",
    "text": "Aufgabe\n\ndata(\"mtcars\")\n\nBetrachten Sie folgendes Modell (Datensatz mtcars):\nmpg ~ disp\nAnders gesagt: Wie gut kann man den Spritverbrauch vorhersagen auf Basis des Hubraums eines Autos?\n\nBerechnen Sie die Modellkoeffizienten! Tipp: lm()\nBerechnen Sie im Anschluss die Vorhersagen dieses Modells. Tipp: predict() mit mutate()\nVisualisieren Sie dann das Modell Tipp: ggplot() und geom_smooth() oder mittels einer anderer Methode.\nBerechnen Sie die Residuen: e = echtem Y-Wert und vorhergesagtem Y-Wert. Tipp: mutate().\nBerechnen Sie die Korrelation zwischen Spritverbrauch und Hubraum! Tipp: summarise() mitcor()`.\n\n         \n\n\nLösung\n\n\nVorbereitung\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndata(mtcars)\n\n\n\nAd 1\n\nlm1 &lt;- lm(mpg ~ disp, data = mtcars)\nlm1\n\n\nCall:\nlm(formula = mpg ~ disp, data = mtcars)\n\nCoefficients:\n(Intercept)         disp  \n   29.59985     -0.04122  \n\n\n\n\nAd 2\nNicht einfach nur predicten:\n\npredict(lm1)\n\n          Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive \n           23.00544            23.00544            25.14862            18.96635 \n  Hornet Sportabout             Valiant          Duster 360           Merc 240D \n           14.76241            20.32645            14.76241            23.55360 \n           Merc 230            Merc 280           Merc 280C          Merc 450SE \n           23.79677            22.69220            22.69220            18.23272 \n         Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental \n           18.23272            18.23272            10.14632            10.64090 \n  Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla \n           11.46520            26.35622            26.47987            26.66946 \n      Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 \n           24.64992            16.49345            17.07046            15.17456 \n   Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa \n           13.11381            26.34386            24.64168            25.68030 \n     Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E \n           15.13335            23.62366            17.19410            24.61283 \n\n\nSondern die Predictions als neue Spalte in mtcars anlegen. Viel sauberer!\n\nmtcars2  &lt;- \n  mtcars %&gt;% \n  mutate(preds_lm1 = predict(lm1))\n\n\n\nAd 3\n\nggplot(mtcars2) +\n  aes(y = mpg, x = disp) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAndere Visualisierung:\n\nmtcars2 %&gt;% \n  ggplot(aes(x = disp, y = preds_lm1)) +\n  geom_point(size = 2, alpha = .8, color = \"pink\") +\n  geom_line() +\n  labs(y = \"Vorhergesagte MPG-Werte\",\n       title = \"Vorhersage-Modell lm1\")\n\n\n\n\n\n\n\n\nOder so:\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.7.0 (red = needs update)\n✔ bayestestR  0.13.1   ✔ correlation 0.8.4 \n✖ datawizard  0.9.0    ✔ effectsize  0.8.6 \n✔ insight     0.19.7   ✔ modelbased  0.8.6 \n✔ performance 0.10.8   ✔ parameters  0.21.3\n✔ report      0.5.8    ✔ see         0.8.1 \n\nRestart the R-Session and update packages with `easystats::easystats_update()`.\n\nestimate_expectation(lm1) %&gt;% plot()\n\n\n\n\n\n\n\n\n\n\nAd 4\n\nmtcars2 &lt;- \n  mtcars2 %&gt;% \n  mutate(e = abs(mpg - preds_lm1))  # abs steht für \"Absolutwert\"\n\nDer “Absolutwert” kickt das Vorzeichen weg. Das machen wir, wenn wir meinen, dass das Vorzeichen egal ist.\n\n\nBonus-Aufgabe\nBerechnen Sie den mittleren Fehler über alle e!\n\nmtcars2 %&gt;% \n  summarise(e_avg = mean(e))\n\n     e_avg\n1 2.605473\n\n\n\n\nAd 5\n\nmtcars %&gt;% \n  summarise(cor_mpg_disp = cor(mpg, disp))\n\n  cor_mpg_disp\n1   -0.8475514\n\n\n\nCategories:\n\nlm\nmtcars\ncorrelation\nregression\nstring"
  },
  {
    "objectID": "posts/alphafehler-inflation3/alphafehler-inflation3.html",
    "href": "posts/alphafehler-inflation3/alphafehler-inflation3.html",
    "title": "alphafehler-inflation3",
    "section": "",
    "text": "Aufgabe\nEine Klettererin verwendet ein Seil, dass eine Sicherheit von \\(r=.99\\) hat: mit einer Wahrscheinlichkeit von 1% reißt das Seil. Jetzt knüpft sie mehrere dieser Seile (hintereinander, Seil an Seil) zusammen zu einem “Gesamtseil”. Wie groß ist die Gefahr, dass das „Gesamtseil“ reist?\nHinweise:\n\nEtwaige (physikalisch plausible) Verringerung der Zugfestigkeit durch (Seilbiegung aufgrund der) Knoten ist zu vernachlässigen.\nSie knüpft 10 Seile zusammen.\nBeachten Sie die sonstigen Hinweise auf dem Datenwerk.\nUnterstellen Sie Unabhängkeit der einzelnen Ereignisse.\n\n         \n\n\nLösung\nSei \\(R\\) die Wahrscheinlichkeit, dass das Gesamtseil hält (nicht reißt). \\(1-R\\) ist dann die Wahrscheinlichkeit des Gegenereignisses: das Gesamtseil reißt.\nAllgemein ist \\(R\\) bei \\(k\\) Tests (Seilen) gleich \\(r\\) hoch \\(k\\): \\(R=r^k\\). (Das Aufaddieren der Fehlalarm-Wahrscheinlichkeit bezeichnet man als Alphafehler-Inflation.)\n\nr &lt;- .99  #  Reißfestigkeit des einfaches Seils\nR10 &lt;- r^10  %&gt;% round(2)  # Reißfestigkeit des 10-fachen Seils\n\nDie Gesamtsicherheit lauten also:\n\nR10\n\n[1] 0.9\n\n\nDie Antwort (solution) ist aber \\(1-R\\):\n\nsolution &lt;- 1-R10\nsolution\n\n[1] 0.1\n\n\n\n\nVertiefung\nBetrachten wir abschließend aus Neugier die Wahrscheinlichkeit, dass die Klettererin abstürzt (\\(1-R\\)) als Funktion der Anzahl der Seie.\nDiese Überlegung ist etwas weiterführender und nicht ganz so zentral, aber ziemlich interessant.\nDefinieren wir die Parameter:\n\nanz_seile &lt;- 1:20  # von 1 bis max 20 Seile\nr &lt;- c(.9, .95, .99, .999)  # verschiedene Seil-Sicherheiten\n\nJetzt erstellen wir einen Tabelle, die alle anz_seile * r Werte kombiniert:\n\nd &lt;- \n  expand_grid(anz_seile, r)\n\nhead(d)\n\n# A tibble: 6 × 2\n  anz_seile     r\n      &lt;int&gt; &lt;dbl&gt;\n1         1 0.9  \n2         1 0.95 \n3         1 0.99 \n4         1 0.999\n5         2 0.9  \n6         2 0.95 \n\n\nJetzt berechnen wir für jede Kombination die Gesamtsicherheit R sowie die Wahrscheinlichkeit, dass das Seil reißt, \\(1-R\\):\n\nd &lt;-\n  d %&gt;% \n  mutate(R = r^anz_seile,\n         seil_reisst_prob = 1 - R)\n\nplotten das Ganze mit dem Paket ggpubr:\n\nlibrary(ggpubr)\nd &lt;-\n  d |&gt; \n  mutate(r_fctr = factor(r))  # um \"r\" zum Gruppieren zu verwenden, sollte es eine nominale Variable sein, daher wandeln wir mit \"factor\" in eine nominale Variable um.\n\nggline(d,\n       x = \"anz_seile\",\n       y = \"seil_reisst_prob\",\n       color = \"r_fctr\",\n       linetype = \"r_fctr\",\n       group = \"r_fctr\") +\n  labs(color = \"Reißfestigkeit\",\n       linetype = \"Reißfestigkeit\")\n\n\n\n\n\n\n\n\nOder mit ggplot plotten:\n\nd %&gt;% \n  ggplot(aes(x = anz_seile,\n             y = seil_reisst_prob,\n             color = factor(r))) +\n  geom_line() +\n  labs(color = \"Reißfestigkeit\")\n\n\n\n\n\n\n\n\nHat ein Seil eine Sicherheit von 90%, dann will man nicht dranhängen, wenn 20 Seile zusammengeknotet sind!\nDie Antwort lautet:\n\n\\(R_{10}= 1-r^{10} = 1 - 0.9 = 0.1\\)\n\n\nCategories:\n\nprobability\nR\ninference\nnum"
  },
  {
    "objectID": "posts/Cluster02/Cluster02.html",
    "href": "posts/Cluster02/Cluster02.html",
    "title": "Cluster02",
    "section": "",
    "text": "Für einen bestimmten Datensatz soll ein hierarchisches Clustering vorgenommen werden, einmal mittels Single Linkage und einmal mittels Complete Linkage.\nZu einem bestimmten Punkt im mittels Single-Linkage-Verfahren erstellten Dendrogramm fusionieren die Cluster \\({5}\\) und \\({6}\\). Im Dendrogramm, das durch Complete-Linkage erstellt wurde, fusionieren die Cluster \\({5}\\) und \\({6}\\) ebenfalls an einem bestimmten Punkt. Bei welchem der beiden Verfahren liegt der Fusionierungspunkt höher im Baum (Dendrogramm)?\n\n\n\nDer Fusionierungspunkt liegt auf der gleichen Höhe.\nDer Fusionierungspunkt liegt beim Single-Linkage-Verfahren höher im Baum als beim Complete-Linkage-Verfahren.\nDer Fusionierungspunkt liegt beim Single-Linkage-Verfahren tiefer im Baum als beim Complete-Linkage-Verfahren.\nDie Angaben lassen keine Aussage zu."
  },
  {
    "objectID": "posts/Cluster02/Cluster02.html#answerlist",
    "href": "posts/Cluster02/Cluster02.html#answerlist",
    "title": "Cluster02",
    "section": "",
    "text": "Der Fusionierungspunkt liegt auf der gleichen Höhe.\nDer Fusionierungspunkt liegt beim Single-Linkage-Verfahren höher im Baum als beim Complete-Linkage-Verfahren.\nDer Fusionierungspunkt liegt beim Single-Linkage-Verfahren tiefer im Baum als beim Complete-Linkage-Verfahren.\nDie Angaben lassen keine Aussage zu."
  },
  {
    "objectID": "posts/Cluster02/Cluster02.html#answerlist-1",
    "href": "posts/Cluster02/Cluster02.html#answerlist-1",
    "title": "Cluster02",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\n\n\nCategories:\nschoice"
  },
  {
    "objectID": "posts/Wskt-Schluckspecht/Wskt-Schluckspecht.html",
    "href": "posts/Wskt-Schluckspecht/Wskt-Schluckspecht.html",
    "title": "Wskt-Schluckspecht",
    "section": "",
    "text": "Prüfen Sie folgende Hypothese:\n\nAutos mit viel PS haben einen höheren Spritverbrauch als Autos mit wenig PS.\n\nQuantifizieren Sie die Wahrscheinlichkeit dieser Hypothese!\nHinweise:\n\n“viel PS” definieren wir als “mehr als der Median”.\nVerwenden Sie den Datensatz mtcars.\nNutzen Sie die Bayes-Statistik mit Stan.\nBeachten Sie die Standardhinweise des Datenwerks."
  },
  {
    "objectID": "posts/Wskt-Schluckspecht/Wskt-Schluckspecht.html#setup",
    "href": "posts/Wskt-Schluckspecht/Wskt-Schluckspecht.html#setup",
    "title": "Wskt-Schluckspecht",
    "section": "Setup",
    "text": "Setup\n\nlibrary(rstanarm)\nlibrary(easystats)\nlibrary(tidyverse)\n\n\ndata(mtcars)"
  },
  {
    "objectID": "posts/Wskt-Schluckspecht/Wskt-Schluckspecht.html#modell",
    "href": "posts/Wskt-Schluckspecht/Wskt-Schluckspecht.html#modell",
    "title": "Wskt-Schluckspecht",
    "section": "Modell",
    "text": "Modell\nDie Hypothese kann man wie folgt formalisieren:\n\\[\\text{mpg}_{PS=0} &gt; \\text{mpg}_{PS=1}\\],\nwobei \\(PS=0\\) die Autos mit wenig PS meint."
  },
  {
    "objectID": "posts/Wskt-Schluckspecht/Wskt-Schluckspecht.html#vorverarbeitung",
    "href": "posts/Wskt-Schluckspecht/Wskt-Schluckspecht.html#vorverarbeitung",
    "title": "Wskt-Schluckspecht",
    "section": "Vorverarbeitung",
    "text": "Vorverarbeitung\n\nmtcars &lt;-\n  mtcars |&gt; \n  mutate(PS = case_when(\n    mpg &gt; median(mpg) ~ 1,\n    mpg &lt;= median(mpg) ~ 0\n  ))"
  },
  {
    "objectID": "posts/Wskt-Schluckspecht/Wskt-Schluckspecht.html#modell-berechnen",
    "href": "posts/Wskt-Schluckspecht/Wskt-Schluckspecht.html#modell-berechnen",
    "title": "Wskt-Schluckspecht",
    "section": "Modell berechnen",
    "text": "Modell berechnen\n\nm &lt;- stan_glm(mpg ~ PS,\n              data = mtcars,\n              refresh = 0,\n              seed = 42)\n\n\nparameters(m)\n\nParameter   | Median |         95% CI |   pd |  Rhat |     ESS |                   Prior\n----------------------------------------------------------------------------------------\n(Intercept) |  15.68 | [13.81, 17.54] | 100% | 1.000 | 3302.00 | Normal (20.09 +- 15.07)\nPS          |   9.42 | [ 6.79, 12.12] | 100% | 1.000 | 3549.00 |  Normal (0.00 +- 29.72)\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a MCMC distribution approximation."
  },
  {
    "objectID": "posts/Wskt-Schluckspecht/Wskt-Schluckspecht.html#post-verteilung-auslesen",
    "href": "posts/Wskt-Schluckspecht/Wskt-Schluckspecht.html#post-verteilung-auslesen",
    "title": "Wskt-Schluckspecht",
    "section": "Post-Verteilung auslesen",
    "text": "Post-Verteilung auslesen\n\nm_post &lt;-\n  m |&gt;\n  as_tibble()\n\nprop &lt;- \nm_post |&gt; \n  count(PS &gt;= 0) |&gt; \n  mutate(prop = n/sum(n))\n\nprop\n\n# A tibble: 1 × 3\n  `PS &gt;= 0`     n  prop\n  &lt;lgl&gt;     &lt;int&gt; &lt;dbl&gt;\n1 TRUE       4000     1"
  },
  {
    "objectID": "posts/Wskt-Schluckspecht/Wskt-Schluckspecht.html#antwort",
    "href": "posts/Wskt-Schluckspecht/Wskt-Schluckspecht.html#antwort",
    "title": "Wskt-Schluckspecht",
    "section": "Antwort",
    "text": "Antwort\nLaut unserem Modell beträgt die Wahrscheinlichkeit für obige Hypothese 1."
  },
  {
    "objectID": "posts/wskt-quiz19/wskt-quiz19.html",
    "href": "posts/wskt-quiz19/wskt-quiz19.html",
    "title": "wskt-quiz19",
    "section": "",
    "text": "Behauptung:\nDie folgenden Variablen haben (alle) “Fat Tails”: Intelligenz (IQ) und das Einkommen von Musikern.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n\n\n\nFalsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz19/wskt-quiz19.html#answerlist",
    "href": "posts/wskt-quiz19/wskt-quiz19.html#answerlist",
    "title": "wskt-quiz19",
    "section": "",
    "text": "Falsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz19/wskt-quiz19.html#answerlist-1",
    "href": "posts/wskt-quiz19/wskt-quiz19.html#answerlist-1",
    "title": "wskt-quiz19",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\nquiz\nprobability\nbayes\ndistribution\nquiz1-qm2-ws23\nschoice"
  },
  {
    "objectID": "posts/germeval03/germeval03.html",
    "href": "posts/germeval03/germeval03.html",
    "title": "germeval03-sent-textfeatures-rand-for",
    "section": "",
    "text": "Erstellen Sie ein prädiktives Modell für Textdaten. Nutzen Sie Sentiments und TextFeatures im Rahmen von Feature-Engineering.\nNutzen Sie die GermEval-2018-Daten.\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\nDie Daten sind auch über das R-Paket PradaData zu beziehen.\n\nlibrary(tidyverse)\ndata(\"germeval_train\", package = \"pradadata\")\ndata(\"germeval_test\", package = \"pradadata\")\n\nDie AV lautet c1. Die (einzige) UV lautet: text.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\nNutzen Sie Tidymodels.\nNutzen Sie das sentiws Lexikon."
  },
  {
    "objectID": "posts/germeval03/germeval03.html#workflow",
    "href": "posts/germeval03/germeval03.html#workflow",
    "title": "germeval03-sent-textfeatures-rand-for",
    "section": "Workflow",
    "text": "Workflow\n\n# model:\nmod1 &lt;-\n  rand_forest(mode = \"classification\")\n\n# recipe:\nrec1 &lt;-\n  recipe(c1 ~ ., data = d_train) |&gt; \n  update_role(id, new_role = \"id\")  |&gt; \n  #update_role(c2, new_role = \"ignore\") |&gt; \n  update_role(text, new_role = \"ignore\") |&gt; \n  step_mutate(n_emo = get_sentiment(text,  # aus `syuzhet`\n                                    method = \"custom\",\n                                    lexicon = sentiws))  |&gt; \n  step_rm(text)  # Datensatz verschlanken\n\n\n# workflow:\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(mod1) %&gt;% \n  add_recipe(rec1)"
  },
  {
    "objectID": "posts/germeval03/germeval03.html#fit",
    "href": "posts/germeval03/germeval03.html#fit",
    "title": "germeval03-sent-textfeatures-rand-for",
    "section": "Fit",
    "text": "Fit\n\ntic()\nfit1 &lt;-\n  fit(wf1,\n      data = d_train)\ntoc()\n\n10.461 sec elapsed\n\nbeep()\n\n\nfit1\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_mutate()\n• step_rm()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      5009 \nNumber of independent variables:  1 \nMtry:                             1 \nTarget node size:                 10 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.2278091"
  },
  {
    "objectID": "posts/germeval03/germeval03.html#test-set-güte",
    "href": "posts/germeval03/germeval03.html#test-set-güte",
    "title": "germeval03-sent-textfeatures-rand-for",
    "section": "Test-Set-Güte",
    "text": "Test-Set-Güte\nVorhersagen im Test-Set:\n\ntic()\npreds &lt;-\n  predict(fit1, new_data = germeval_test)\ntoc()\n\n5.524 sec elapsed\n\n\nUnd die Vorhersagen zum Test-Set hinzufügen, damit man TRUTH und ESTIMATE vergleichen kann:\n\nd_test &lt;-\n  germeval_test |&gt; \n  bind_cols(preds) |&gt; \n  mutate(c1 = as.factor(c1))\n\n\nmy_metrics &lt;- metric_set(accuracy, f_meas)\nmy_metrics(d_test,\n           truth = c1,\n           estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.656\n2 f_meas   binary         0.129\n\n\n\nCategories:\n\n2023\ntextmining\ndatawrangling\ngermeval\nprediction\ntidymodels\nsentiment\nstring"
  },
  {
    "objectID": "posts/Typ-Fehler-R-08-name-clash/Typ-Fehler-R-08-name-clash.html",
    "href": "posts/Typ-Fehler-R-08-name-clash/Typ-Fehler-R-08-name-clash.html",
    "title": "Typ-Fehler-R-08-name-clash",
    "section": "",
    "text": "Aufgabe\nR spuckt eine komische Fehlermeldung aus. Was ist nur los? Hat R einen schlechten Tag?\nSchauen wir uns die Sache näher an:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nmtcars_small &lt;-\n  mtcars %&gt;% \n  select(hp, am)\n\nError in select(., hp, am): unused arguments (hp, am)\n\n\nOh nein! Fehler!\nWas ist nur los?\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n         \n\n\nLösung\nDas Problem ist, dass es in beiden Paketen, {MASS} und {dplyr} (dasjenige Paket im tidyverse, in dem select() wohnt), eine Funktion namens select vorhanden ist.\nEs kommt zu einem “Name Clash”, einer Namenskollision.\nWenn mehrere Funktion gleichen Namens geladen (“attached”) sind, so “gewinnt” diejenige Funktion, die als letztes geladen wurde, in unserem Fall ist das die Funktion aus {MASS}.\nEs gibt eine Reihe von Lösungen.\n\nNur das benötigte Paket starten\n\nZuerst “entladen” wir MASS, da wir es nicht benötigen:\n\ndetach(\"package:MASS\", unload = TRUE)\n\nAlternativ (und einfacher) könnten wir R neu starten: Session &gt; Restart R.\n\nlibrary(dplyr)\n#library(MASS)\n\nmtcars %&gt;% \n  select(hp, am)\n\n                     hp am\nMazda RX4           110  1\nMazda RX4 Wag       110  1\nDatsun 710           93  1\nHornet 4 Drive      110  0\nHornet Sportabout   175  0\nValiant             105  0\nDuster 360          245  0\nMerc 240D            62  0\nMerc 230             95  0\nMerc 280            123  0\nMerc 280C           123  0\nMerc 450SE          180  0\nMerc 450SL          180  0\nMerc 450SLC         180  0\nCadillac Fleetwood  205  0\nLincoln Continental 215  0\nChrysler Imperial   230  0\nFiat 128             66  1\nHonda Civic          52  1\nToyota Corolla       65  1\nToyota Corona        97  0\nDodge Challenger    150  0\nAMC Javelin         150  0\nCamaro Z28          245  0\nPontiac Firebird    175  0\nFiat X1-9            66  1\nPorsche 914-2        91  1\nLotus Europa        113  1\nFord Pantera L      264  1\nFerrari Dino        175  1\nMaserati Bora       335  1\nVolvo 142E          109  1\n\n\nUnd schon geht’s!\n\nPaketnamen vor Funktionsnamen anfügen\n\n\n#library(dplyr)\n#library(MASS)\n\nmtcars %&gt;% \n  dplyr::select(hp, am) %&gt;% \n  dplyr::filter(hp &gt; 200)\n\n                     hp am\nDuster 360          245  0\nCadillac Fleetwood  205  0\nLincoln Continental 215  0\nChrysler Imperial   230  0\nCamaro Z28          245  0\nFord Pantera L      264  1\nMaserati Bora       335  1\n\n\n\nPaket conflicted nutzen\n\nHier gibt’s dazu nähere Infos.\n\nCategories:\n\nR\nerror\nstring"
  },
  {
    "objectID": "posts/Wertberechnen2/Wertberechnen2.html",
    "href": "posts/Wertberechnen2/Wertberechnen2.html",
    "title": "Wertberechnen2",
    "section": "",
    "text": "Aufgabe\nWelchen Wert bzw. welches Ergebnis liefert folgende R-Syntax für ergebnis zurück?\nx hat zu Beginn den Wert 15.\nHinweise:\n\nsqrt(x) liefert die (positive) Quadratwurzel von x zurück.\nx^2 liefert die zweite Potenz von x zurück.\n\n\ny &lt;- 1\n\nx &lt;- x + y - 1\n\ny = x\n\ny &lt;- y * 2\n\nx &lt;- x + 1\n\nx &lt;- sqrt(x)\n\nergebnis &lt;- x^2\n\n         \n\n\nLösung\nEs wird 16 zurückgeliefert.\n\nCategories:\n\nR\ndyn\nnum"
  },
  {
    "objectID": "posts/Kaefer2/Kaefer2.html",
    "href": "posts/Kaefer2/Kaefer2.html",
    "title": "Kaefer2",
    "section": "",
    "text": "Weltsensation?! Der Insektenforscher Prof. Mügge ist der Meinung, eine bislang unbekannte Käferart entdeckt zu haben. Nach nur 18 Monaten Feldforschung im brasilianischen Regenwald gelang ihm dieser Durchbruch. Wenn es denn nun wirklich eine neue Art ist. Gerade untersucht er ein Exemplar unter dem Mikroskop. Hm, was ist das für ein Tier? 🐛 🔬\nDrei Arten kommen in Frage, \\(A_1, A_2, A_3\\).\nDabei ist die Art \\(A_1\\) sehr verbreitet und schon längst bekannt, \\(A_2\\) ist die neue Art, Exemplare dieser Art sind selten und \\(A_3\\) ist auch bekannt und eher häufig anzutreffen. Allerdings spricht das Aussehen am ehesten für \\(A_2\\), der seltenen Art.\n👉 Aufgabe: Wie groß ist die Wahrscheinlichkeit, dass Prof. Mügge wirklich einen großen Fang gemacht hat und einen unbekannten Käfer entdeckt hat?\nGeben Sie diese Wahrscheinlichkeit an!\nHier sind die genauen Vorkommenshäufigkeiten:\n\nPr_A1 &lt;- .6\nPr_A2 &lt;- .1\nPr_A3 &lt;- .4\n\nUnd hier die genauen Wahrscheinlichkeiten, wie typisch das beobachtete Objekt für einen Vertreter der jeweiligen Art ist:\n\nL_A1 &lt;- .5\nL_A2 &lt;- .9\nL_A3 &lt;- .4\n\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks."
  },
  {
    "objectID": "posts/Kaefer2/Kaefer2.html#setup",
    "href": "posts/Kaefer2/Kaefer2.html#setup",
    "title": "Kaefer2",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(prada)  # für Funktion `bayesbox`\nlibrary(easystats)\n\n\nbb &lt;- bayesbox(hyps = 1:3,\n               priors = c(Pr_A1, Pr_A2, Pr_A3),\n               liks = c(L_A1, L_A2, L_A3))\n\nbb\n\n  hyps priors liks post_unstand  post_std\n1    1    0.6  0.5         0.30 0.5454545\n2    2    0.1  0.9         0.09 0.1636364\n3    3    0.4  0.4         0.16 0.2909091\n\n\nAntwort: Die Wahrscheinlichkeit, dass der Käfer zur Art “B” gehört, ist relativ klein: 0.16.\n\nCategories:\n\nR\nbayes\nbayesbox\nnum"
  },
  {
    "objectID": "posts/DAG-Graph/DAG-Graph.html",
    "href": "posts/DAG-Graph/DAG-Graph.html",
    "title": "DAG-Graph",
    "section": "",
    "text": "Aufgabe\nFür ein Forschungsprojekt hat ein Forschungsteam die Frage getestet, ob Personen, die einen animierten Graphen zu Auswirkungen von Stress gesehen haben danach eine höhere Motivation haben, ihr Stresspensum anzugehen, als Personen, die einen statischen Graph gesehen haben. Dazu wurde jeweils in einem Fragebogen die Veränderungsbereitschaft auf das Stressniveau angepasst abgefragt, dann den jeweiligen Graphen gezeigt und danach dieselben Fragen wie davor nochmals gestellt. Die Personen wurden randomisiert den beiden Bedingungen (statisch vs. animiert) zugeordnet. Es handelt sich um ein Between-Group-Design.\nZur Auswertung wurde nun zu jeder der Fragen zur Veränderungsbereitschaft die Mittelwerte der Vor-sehen-des-Graphen-Gruppe von der Nach-sehen-des-Graphen-Gruppe abgezogen und diese Werte dann verglichen von dem animierten und dem statischen Graphen. Dabei konnte der gewünschten Effekt deutlich erkannt werden, hypothesenkonform.\nZeichnen Sie den DAG für dieses Studiendesign\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n         \n\n\nLösung\n\nlibrary(dagitty)\nlibrary(ggdag)\n\n\nAttaching package: 'ggdag'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nlibrary(ggplot2)\n\n\nmy_dag &lt;-\n  dagitty(\"dag{g -&gt; mot; u -&gt; mot}\")\n\n\ntidy_dagitty(my_dag)\n\n# A DAG with 3 nodes and 2 edges\n#\n# A tibble: 3 × 8\n  name      x     y direction to     xend  yend circular\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;   \n1 g     -2.00  1.83 -&gt;        mot   -3.08  1.43 FALSE   \n2 mot   -3.08  1.43 &lt;NA&gt;      &lt;NA&gt;  NA    NA    FALSE   \n3 u     -4.15  1.03 -&gt;        mot   -3.08  1.43 FALSE   \n\n\n\nggdag(my_dag) +\n  theme_dag()\n\n\n\n\n\n\n\n\nDie AV ist mit mot bezeichnet; die UV mit g (wie Gruppe). u steht für sonstige Einflüsse auf die AV.\n\nCategories:\n\nfopro\nresearchdesign\ncausal\nstring"
  },
  {
    "objectID": "posts/wfsets_penguins02/wfsets_penguins02.html",
    "href": "posts/wfsets_penguins02/wfsets_penguins02.html",
    "title": "wfsets_penguins02",
    "section": "",
    "text": "Berechnen Sie die Vorhersagegüte (RMSE) für folgende Lernalgorithmen:\n\nlineares Modell\nknn (neighbors: tune)\n\nModellgleichung: body_mass_g ~ bill_length_mm, data = d_train.\nTunen Sie bei neighbors folgende Werte: 5, 10, 15, 20, 35, 30 und betrachten Sie deren Modellgüte.\nNutzen Sie minimale Vorverarbeitung.\nBerichten Sie die den RSME."
  },
  {
    "objectID": "posts/wfsets_penguins02/wfsets_penguins02.html#setup",
    "href": "posts/wfsets_penguins02/wfsets_penguins02.html#setup",
    "title": "wfsets_penguins02",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidymodels)\nlibrary(tidyverse)\ndata(penguins, package = \"palmerpenguins\")"
  },
  {
    "objectID": "posts/wfsets_penguins02/wfsets_penguins02.html#daten",
    "href": "posts/wfsets_penguins02/wfsets_penguins02.html#daten",
    "title": "wfsets_penguins02",
    "section": "Daten",
    "text": "Daten\n\nd &lt;-\n  penguins %&gt;% \n  drop_na()\n\n\nd_split &lt;- initial_split(d)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)"
  },
  {
    "objectID": "posts/wfsets_penguins02/wfsets_penguins02.html#modelle",
    "href": "posts/wfsets_penguins02/wfsets_penguins02.html#modelle",
    "title": "wfsets_penguins02",
    "section": "Modelle",
    "text": "Modelle\nLineares Modell:\n\nmod_lin &lt;- linear_reg()\n\nmod_knn &lt;- nearest_neighbor(mode = \"regression\",\n                                  neighbors = tune())"
  },
  {
    "objectID": "posts/wfsets_penguins02/wfsets_penguins02.html#rezepte",
    "href": "posts/wfsets_penguins02/wfsets_penguins02.html#rezepte",
    "title": "wfsets_penguins02",
    "section": "Rezepte",
    "text": "Rezepte\n\nrec_basic &lt;- recipe(body_mass_g ~ bill_length_mm, data = d_train) %&gt;% \n         step_normalize(all_predictors())\n\nrec_basic"
  },
  {
    "objectID": "posts/wfsets_penguins02/wfsets_penguins02.html#resampling",
    "href": "posts/wfsets_penguins02/wfsets_penguins02.html#resampling",
    "title": "wfsets_penguins02",
    "section": "Resampling",
    "text": "Resampling\n\nrsmpls &lt;- vfold_cv(d_train)"
  },
  {
    "objectID": "posts/wfsets_penguins02/wfsets_penguins02.html#workflow-set",
    "href": "posts/wfsets_penguins02/wfsets_penguins02.html#workflow-set",
    "title": "wfsets_penguins02",
    "section": "Workflow Set",
    "text": "Workflow Set\n\nwf_set &lt;-\n  workflow_set(\n    preproc = list(rec_simple = rec_basic),\n    models = list(mod_lm = mod_lin,\n                  mod_nn = mod_knn)\n  )\n\nwf_set\n\n# A workflow set/tibble: 2 × 4\n  wflow_id          info             option    result    \n  &lt;chr&gt;             &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 rec_simple_mod_lm &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 rec_simple_mod_nn &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;"
  },
  {
    "objectID": "posts/wfsets_penguins02/wfsets_penguins02.html#tuningparameter-werte-bestimmen",
    "href": "posts/wfsets_penguins02/wfsets_penguins02.html#tuningparameter-werte-bestimmen",
    "title": "wfsets_penguins02",
    "section": "Tuningparameter-Werte bestimmen",
    "text": "Tuningparameter-Werte bestimmen\nWelche Tuningparameter hatten wir noch mal ausgewiesen?\n\nmod_knn %&gt;% \n  extract_parameter_set_dials()\n\nCollection of 1 parameters for tuning\n\n identifier      type    object\n  neighbors neighbors nparam[+]\n\n\nUpdaten wir die Parameter mit unseren Werten, also min. 5 Nachbarn und max. 20 Nachbarn.\n\nparams_knn &lt;- \nmod_knn %&gt;% \n  extract_parameter_set_dials() %&gt;% \n  update(neighbors = neighbors(c(5, 20)))\n\nparams_knn\n\nCollection of 1 parameters for tuning\n\n identifier      type    object\n  neighbors neighbors nparam[+]\n\n\nDiese Infos ergänzen wir jetzt in das Workflow-Set-Objekt für den Workflow mit der ID “rec_simple_mod_nn” unter der Spalte “Options”:\n\nwf_set &lt;- \nwf_set %&gt;% \n  option_add(param_info = params_knn, id = \"rec_simple_mod_nn\")  \n\nwf_set\n\n# A workflow set/tibble: 2 × 4\n  wflow_id          info             option    result    \n  &lt;chr&gt;             &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 rec_simple_mod_lm &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 rec_simple_mod_nn &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;list [0]&gt;"
  },
  {
    "objectID": "posts/wfsets_penguins02/wfsets_penguins02.html#fitten",
    "href": "posts/wfsets_penguins02/wfsets_penguins02.html#fitten",
    "title": "wfsets_penguins02",
    "section": "Fitten",
    "text": "Fitten\n\nwf_set_fit &lt;-\n  wf_set %&gt;% \n  workflow_map(resamples = rsmpls)\n\nwf_set_fit\n\n# A workflow set/tibble: 2 × 4\n  wflow_id          info             option    result   \n  &lt;chr&gt;             &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n1 rec_simple_mod_lm &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n2 rec_simple_mod_nn &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;tune[+]&gt;\n\n\nCheck:\n\nwf_set_fit %&gt;% pluck(\"result\")\n\n[[1]]\n# Resampling results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits           id     .metrics         .notes          \n   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;          \n 1 &lt;split [224/25]&gt; Fold01 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [224/25]&gt; Fold02 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [224/25]&gt; Fold03 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [224/25]&gt; Fold04 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [224/25]&gt; Fold05 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [224/25]&gt; Fold06 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [224/25]&gt; Fold07 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [224/25]&gt; Fold08 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [224/25]&gt; Fold09 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [225/24]&gt; Fold10 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n\n[[2]]\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits           id     .metrics          .notes          \n   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n 1 &lt;split [224/25]&gt; Fold01 &lt;tibble [16 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [224/25]&gt; Fold02 &lt;tibble [16 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [224/25]&gt; Fold03 &lt;tibble [16 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [224/25]&gt; Fold04 &lt;tibble [16 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [224/25]&gt; Fold05 &lt;tibble [16 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [224/25]&gt; Fold06 &lt;tibble [16 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [224/25]&gt; Fold07 &lt;tibble [16 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [224/25]&gt; Fold08 &lt;tibble [16 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [224/25]&gt; Fold09 &lt;tibble [16 × 5]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [225/24]&gt; Fold10 &lt;tibble [16 × 5]&gt; &lt;tibble [0 × 3]&gt;"
  },
  {
    "objectID": "posts/wfsets_penguins02/wfsets_penguins02.html#bester-kandidat",
    "href": "posts/wfsets_penguins02/wfsets_penguins02.html#bester-kandidat",
    "title": "wfsets_penguins02",
    "section": "Bester Kandidat",
    "text": "Bester Kandidat\n\nautoplot(wf_set_fit)\n\n\n\n\n\n\n\n\n\nrank_results(wf_set_fit, rank_metric = \"rmse\") %&gt;% \n  filter(.metric == \"rmse\")\n\n# A tibble: 9 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 rec_simple_mod_nn Prepro… rmse     645.    23.7    10 recipe       near…     1\n2 rec_simple_mod_lm Prepro… rmse     646.    26.0    10 recipe       line…     2\n3 rec_simple_mod_nn Prepro… rmse     651.    22.4    10 recipe       near…     3\n4 rec_simple_mod_nn Prepro… rmse     653.    22.7    10 recipe       near…     4\n5 rec_simple_mod_nn Prepro… rmse     656.    22.0    10 recipe       near…     5\n6 rec_simple_mod_nn Prepro… rmse     661.    22.4    10 recipe       near…     6\n7 rec_simple_mod_nn Prepro… rmse     670.    23.3    10 recipe       near…     7\n8 rec_simple_mod_nn Prepro… rmse     680.    24.8    10 recipe       near…     8\n9 rec_simple_mod_nn Prepro… rmse     699.    30.9    10 recipe       near…     9\n\n\nAm besten war das lineare Modell, aber schauen wir uns auch mal das knn-Modell an, v.a. um zu wissen, wie man den besten Tuningparameter-Wert sieht:\n\nwf_knn &lt;- \n  extract_workflow_set_result(wf_set_fit, \"rec_simple_mod_nn\")\n\n\nwf_knn %&gt;% autoplot()\n\n\n\n\n\n\n\n\n\nwf_knn %&gt;% select_best()\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\n\n# A tibble: 1 × 2\n  neighbors .config             \n      &lt;int&gt; &lt;chr&gt;               \n1        19 Preprocessor1_Model8"
  },
  {
    "objectID": "posts/wfsets_penguins02/wfsets_penguins02.html#last-fit",
    "href": "posts/wfsets_penguins02/wfsets_penguins02.html#last-fit",
    "title": "wfsets_penguins02",
    "section": "Last Fit",
    "text": "Last Fit\n\nbest_wf &lt;-\n  wf_set_fit %&gt;% \n  extract_workflow(\"rec_simple_mod_lm\")\n\nFinalisieren müssen wir diesen Workflow nicht, da er keine Tuningparameter hatte.\n\nfit_final &lt;-\n  best_wf %&gt;% \n  last_fit(d_split)"
  },
  {
    "objectID": "posts/wfsets_penguins02/wfsets_penguins02.html#modellgüte-im-test-set",
    "href": "posts/wfsets_penguins02/wfsets_penguins02.html#modellgüte-im-test-set",
    "title": "wfsets_penguins02",
    "section": "Modellgüte im Test-Set",
    "text": "Modellgüte im Test-Set\n\ncollect_metrics(fit_final)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard     670.    Preprocessor1_Model1\n2 rsq     standard       0.369 Preprocessor1_Model1\n\n\n\nCategories:\n\nR\nstatlearning\ntidymodels\nnum"
  },
  {
    "objectID": "posts/regression1a/regression1a.html",
    "href": "posts/regression1a/regression1a.html",
    "title": "regression1a",
    "section": "",
    "text": "Die folgende Frage bezieht sich auf dieses Ergebnis einer Regressionsanalyse:\n\n\n\nCall:\nlm(formula = y ~ x, data = d)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.295 -1.138  0.202  0.998  4.528 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)    0.367      0.303    1.21     0.23\nx              0.326      0.389    0.84     0.41\n\nResidual standard error: 2 on 50 degrees of freedom\nMultiple R-squared:  0.0138,    Adjusted R-squared:  -0.0059 \nF-statistic: 0.701 on 1 and 50 DF,  p-value: 0.406\n\n\nWelche der folgenden Aussagen passt am besten?\n\n\n\nWenn x=2, dann ist ein Mittelwert von y in Höhe von ca. 1.02 zu erwarten.\nDer Mittelwert der abhängigen Variaben y sinkt mit zunehmenden x.\nWenn x um 1 Einheit steigt, dann kann eine Veränderung um etwa 0.37 Einheiten in y erwartet werden (nicht kausal zu verstehen).\nWenn x=0, dann ist ein Mittelwert von y in Höhe von etwa 0.69 zu erwarten.\nWenn x=1, dann ist ein Mittelwert von y in Höhe von ca. 0.37 zu erwarten."
  },
  {
    "objectID": "posts/regression1a/regression1a.html#answerlist",
    "href": "posts/regression1a/regression1a.html#answerlist",
    "title": "regression1a",
    "section": "",
    "text": "Wenn x=2, dann ist ein Mittelwert von y in Höhe von ca. 1.02 zu erwarten.\nDer Mittelwert der abhängigen Variaben y sinkt mit zunehmenden x.\nWenn x um 1 Einheit steigt, dann kann eine Veränderung um etwa 0.37 Einheiten in y erwartet werden (nicht kausal zu verstehen).\nWenn x=0, dann ist ein Mittelwert von y in Höhe von etwa 0.69 zu erwarten.\nWenn x=1, dann ist ein Mittelwert von y in Höhe von ca. 0.37 zu erwarten."
  },
  {
    "objectID": "posts/regression1a/regression1a.html#answerlist-1",
    "href": "posts/regression1a/regression1a.html#answerlist-1",
    "title": "regression1a",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nregression\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/mariokart-max2/mariokart-max2.html",
    "href": "posts/mariokart-max2/mariokart-max2.html",
    "title": "mariokart-max2",
    "section": "",
    "text": "Aufgabe\nImportieren Sie den Datensatz mariokart in R. Berechnen Sie die maximale Verkaufspreise (total_pr) für Spiele, die mit 0, 1, 2, … Lenkräder (wheels) gekauft werden. Dieser Kennwert heiße pr_max. Berücksichtigen Sie aber nur neue Spiele. Bilden Sie von pr_max den Mittelwert und geben Sie diesen an.\nHinweise:\n\nRunden Sie auf 1 Dezimalstelle.\n\n         \n\n\nLösung\nPakete starten:\n\nlibrary(tidyverse)\nlibrary(easystats)\n\nDaten importieren:\n\nd_url &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\"\nd &lt;- data_read(d_url)\n\n\nsolution &lt;-\nd  %&gt;% \n  filter(cond == \"new\") %&gt;% \n  group_by(wheels) %&gt;% \n  summarise(pr_max = max(total_pr)) %&gt;% \n  summarise(pr_max_mean = mean(pr_max))\n\nsolution\n\n# A tibble: 1 × 1\n  pr_max_mean\n        &lt;dbl&gt;\n1        63.2\n\n\nLösung: 63.17.\n\nCategories:\n\ndatawrangling\ndplyr\neda\nnum"
  },
  {
    "objectID": "posts/iq04/iq04.html",
    "href": "posts/iq04/iq04.html",
    "title": "iq04",
    "section": "",
    "text": "Aufgabe\nIntelligenz wird häufig mittels einem IQ-Test ermittelt.\nWie intelligent muss man sein, um zu den schlauesten 2% Personen in der Allgemeinbevölkerung zu gehören?\nHinweise:\n\nNutzen Sie Simulationsmethoden.\nGehen Sie von folgender IQ-Verteilung aus: \\(IQ \\sim N(100,15)\\).\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\nSimulieren Sie \\(n=10^3\\) Stichproben.\nNutzen Sie die Zahl 42 als Startwert für Ihre Zufallszahlen (um die Reproduzierbarkeit zu gewährleisten).\nGeben Sie keine Prozentzahlen, sondern stets Anteile an.\n\n         \n\n\nLösung\n\nlibrary(tidyverse)\n\nWir simulieren die Daten:\n\nset.seed(42)\nd &lt;- tibble(\n  id = 1:10^3,\n  iq = rnorm(n = 10^3, mean = 100, sd = 15))\n\nWir filtern die schlauesten 2 Prozent:\n\nsolution_d &lt;- \n  d %&gt;% \n  arrange(iq) %&gt;% \n  slice_tail(prop = 0.02) %&gt;%  # schneide \"hinten an der Tabelle\" einen Anteil (prop) von 0.02 (2%) ab\n  summarise(min(iq))  # was ist der kleinste Wert in diesen 2%?\n\nsolution_d\n\n# A tibble: 1 × 1\n  `min(iq)`\n      &lt;dbl&gt;\n1      130.\n\n\nDie Syntax auf Deutsch übersetzt:\nDefiniere solution_d wie folgt:\nnimm die Tabelle d und dann ...\nsortiere (aufsteigend) die Spalte iq und dann ...\nschneide hinten (\"am Schwanz\") einen Anteil von 2% ab und dann ...\nfasse diese Liste an Werten zusammen zu ihrem Minimum (also dem kleinsten Wert).\nAlternativ könnte man schreiben:\n\nsolution &lt;- \n  d %&gt;% \n  summarise(iq_top_2komma3_prozent = quantile(iq, prob = .98))\n\nsolution\n\n# A tibble: 1 × 1\n  iq_top_2komma3_prozent\n                   &lt;dbl&gt;\n1                   130.\n\n\nLösung: 130.2767958.\n\nCategories:\n\nprobability\nsimulation\nnormal-distribution\nnum"
  },
  {
    "objectID": "posts/wuerfel06/wuerfel06.html",
    "href": "posts/wuerfel06/wuerfel06.html",
    "title": "wuerfel06",
    "section": "",
    "text": "Aufgabe\nWas ist die Wahrscheinlichkeit, bei 10 Wiederholungen des Werfens zweier Würfel mindestens einen Sechserpasch zu werfen?\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n         \n\n\nLösung\nSei \\(A_i\\) das Ereignis “Sechserpach” in der \\(i\\)-ten Wiederholung.\nEs gilt: \\(Pr(A_i) = 1/36\\).\nNennen wir \\(A\\) “keinen Sechserpasch in jeder Wiederholung”, wir suchen die Wahrscheinlichkeit von A.\n“Mindestens einen Sechserpasch” - Das Gegenteil davon ist “keinen Sechserpasch$.\n\\(Pr(\\neg A_i) = 35/36\\).\nNennen wir \\(X\\) eine Zufallsvariable, die die Anzahl der Sechserpasche zählt.\nDie Wiederholungen sind voneinander unabhängig, es gilt also\n\\(Pr(X=0) = Pr(\\neg A) = \\left(\\frac{35}{36} \\right)^{10}\\)\n\nPr_kein_Secherpasch &lt;- (35/36)^10\nPr_kein_Secherpasch\n\n[1] 0.7544934\n\n\nDas Gegenteil (Komplement) von \\(\\neg A\\), also \\(A\\) ist das gesuchte Ereignis.\n\nPr_A &lt;- 1 - Pr_kein_Secherpasch\nPr_A\n\n[1] 0.2455066\n\n\nDie Lösung lautet 0.2455066.\n\nCategories:\n\nR\nprobability\nnum"
  },
  {
    "objectID": "posts/mariokart-korr4/mariokart-korr4.html",
    "href": "posts/mariokart-korr4/mariokart-korr4.html",
    "title": "mariokart-korr4",
    "section": "",
    "text": "Aufgabe\nImportieren Sie den Datensatz mariokart in R. Berechnen Sie die Korrelation von mittlerem Verkaufspreis (total_pr) und Startgebot (start_pr) für Spiele, die sowohl neu sind oder über Lenkräder (wheels) verfügen.\nHinweise:\n\nRunden Sie auf 1 Dezimalstelle.\nBeachten Sie die Hinweise des Datenwerk.\n\n         \n\n\nLösung\nPakete starten:\n\nlibrary(tidyverse)\nlibrary(easystats)\n\nDaten importieren:\n\nd_url &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\"\nd &lt;- read.csv(d_url)\n\n\nsolution &lt;-\nd  %&gt;% \n  filter(cond == \"new\" | wheels &gt; 0) %&gt;% \n  summarise(pr_corr = cor(total_pr, start_pr))\n\nsolution\n\n     pr_corr\n1 0.04725486\n\n\nAlternativ kann man (komfortabel) die Korrelation z.B. so berechnen:\n\nd %&gt;% \n  select(start_pr, total_pr, cond, wheels) %&gt;% \n  filter(cond == \"new\" | wheels &gt; 0) %&gt;%   # logisches ODER\n  correlation()  # aus dem Paket `easystats`\n\n# Correlation Matrix (pearson-method)\n\nParameter1 | Parameter2 |    r |        95% CI | t(108) |       p\n-----------------------------------------------------------------\nstart_pr   |   total_pr | 0.05 | [-0.14, 0.23] |   0.49 | 0.762  \nstart_pr   |     wheels | 0.08 | [-0.10, 0.27] |   0.88 | 0.762  \ntotal_pr   |     wheels | 0.28 | [ 0.10, 0.45] |   3.09 | 0.008**\n\np-value adjustment method: Holm (1979)\nObservations: 110\n\n\nLösung: 0.0.\n\nCategories:\n\ndatawrangling\ndplyr\neda\nassociation\nnum"
  },
  {
    "objectID": "posts/germeval02/germeval02.html",
    "href": "posts/germeval02/germeval02.html",
    "title": "germeval02",
    "section": "",
    "text": "Führen Sie eine Sentiment-Analyse durch. Verwenden Sie verschiedene Verfahren.\nNutzen Sie die GermEval-2018-Daten.\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\nDie Daten sind auch über das R-Paket PradaData zu beziehen.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks."
  },
  {
    "objectID": "posts/germeval02/germeval02.html#sentimentanalyse-mit-regex",
    "href": "posts/germeval02/germeval02.html#sentimentanalyse-mit-regex",
    "title": "germeval02",
    "section": "Sentimentanalyse mit Regex",
    "text": "Sentimentanalyse mit Regex\nDie Funktion count_lexicon stammt aus {prada}.\nTipp: Mit ?count_lexicon sehen Sie den Quelltext (jeder Funktion).\n\ntest_text  |&gt; \n  mutate(n_emowords = map_int(text, prada::count_lexicon, sentiws$word))\n\n# A tibble: 8 × 4\n     id text                     n_emo n_emowords\n  &lt;int&gt; &lt;chr&gt;                    &lt;dbl&gt;      &lt;int&gt;\n1     1 Abbau, Abbruch ist jetzt     2          2\n2     2 Test heute                   0          0\n3     3 Abbruch morgen perfekt       2          2\n4     4 Abmachung lore ipsum         1          1\n5     5 boese ja                     1          0\n6     6 böse nein                    1          1\n7     7 hallo ?! hallo.              0          0\n8     8 gut schlecht                 2          2\n\n\n\ntic()\ngermeval_train |&gt; \n  mutate(n_emowords = map_int(text, ~ prada::count_lexicon(.x, sentiws$word))) |&gt; \n  head()\n\n# A tibble: 6 × 5\n     id text                                              c1    c2    n_emowords\n  &lt;int&gt; &lt;chr&gt;                                             &lt;chr&gt; &lt;chr&gt;      &lt;int&gt;\n1     1 @corinnamilborn Liebe Corinna, wir würden dich g… OTHER OTHER          2\n2     2 @Martin28a Sie haben ja auch Recht. Unser Tweet … OTHER OTHER          2\n3     3 @ahrens_theo fröhlicher gruß aus der schönsten s… OTHER OTHER          0\n4     4 @dushanwegner Amis hätten alles und jeden gewähl… OTHER OTHER          1\n5     5 @spdde kein verläßlicher Verhandlungspartner. Na… OFFE… INSU…          1\n6     6 @Dirki_M Ja, aber wo widersprechen die Zahlen de… OTHER OTHER          4\n\ntoc()\n\n61.739 sec elapsed\n\n\nPuh! Viel zu langsam."
  },
  {
    "objectID": "posts/germeval02/germeval02.html#sentimentanalyse-mit-unnest_tokens",
    "href": "posts/germeval02/germeval02.html#sentimentanalyse-mit-unnest_tokens",
    "title": "germeval02",
    "section": "Sentimentanalyse mit unnest_tokens",
    "text": "Sentimentanalyse mit unnest_tokens\nProbieren wir es mit unnest_tokens:\nJaa,… aber die Strings ohne Treffer werden ignoriert.\n\ntest_text |&gt; \n  unnest_tokens(word, text) |&gt; \n  right_join(sentiws |&gt; select(word)) |&gt; \n  count(id)\n\n# A tibble: 6 × 2\n     id     n\n  &lt;int&gt; &lt;int&gt;\n1     1     2\n2     3     2\n3     4     1\n4     6     1\n5     8     2\n6    NA  3461\n\n\nProbieren wir es so:\n\n#' Count words in a lexicon\n#' \n#' Counts how many of the words of the character vector `text` are\n#' found in a lexicon `lex` \n#' `text` is transformed via tolower.\n#'\n#' @param text corpus, character vector\n#'\n#' @return number of hits per element of the corpus\n#' @export\n#'\n#' @examples\n#' count_lex(my_text, my_lex)\ncount_lex &lt;- function(text) {\n  \n  stopifnot(class(text) == \"character\")\n  \n  doc &lt;- tibble(text = tolower(text),\n                id = 1:length(text))\n  \n  doc1 &lt;- \n    doc |&gt; \n    tidytext::unnest_tokens(word, text) |&gt; \n    dplyr::inner_join(sentiws |&gt; dplyr::select(word), by = \"word\") |&gt; \n    count(id)\n  \n  doc2 &lt;-\n    doc1 |&gt; \n    dplyr::full_join(doc |&gt; select(id), by = \"id\")\n  \n  doc2$n &lt;- ifelse(is.na(doc2$n), 0,doc2$n)\n  \n  doc2 &lt;- doc2 |&gt; dplyr::arrange(id)\n  \n  doc2 |&gt; pull(n)\n}\n\nMit dem Paket box kann man Funktionen, die nicht in Paketen stehen, importieren.\n\ncount_lex(test_text$text)\n\n[1] 2 0 2 1 0 1 0 2\n\n\nAls neue Spalte im Datensatz:\n\ntest_text |&gt; \n  mutate(n_emowords = count_lex(text))\n\n# A tibble: 8 × 4\n     id text                     n_emo n_emowords\n  &lt;int&gt; &lt;chr&gt;                    &lt;dbl&gt;      &lt;dbl&gt;\n1     1 Abbau, Abbruch ist jetzt     2          2\n2     2 Test heute                   0          0\n3     3 Abbruch morgen perfekt       2          2\n4     4 Abmachung lore ipsum         1          1\n5     5 boese ja                     1          0\n6     6 böse nein                    1          1\n7     7 hallo ?! hallo.              0          0\n8     8 gut schlecht                 2          2"
  },
  {
    "objectID": "posts/germeval02/germeval02.html#sentimentanalyse-mit-syuzhet",
    "href": "posts/germeval02/germeval02.html#sentimentanalyse-mit-syuzhet",
    "title": "germeval02",
    "section": "Sentimentanalyse mit {syuzhet}",
    "text": "Sentimentanalyse mit {syuzhet}\n\nMit dem Lexicon nrc\n\nget_nrc_sentiment(test_text$text, language = \"german\")\n\n  anger anticipation disgust fear joy sadness surprise trust negative positive\n1     0            0       0    0   0       0        0     0        0        0\n2     0            0       0    0   0       0        0     0        0        0\n3     0            3       0    0   1       0        0     1        0        1\n4     0            0       0    0   0       0        0     0        0        0\n5     0            0       0    0   0       0        0     0        0        1\n6     0            0       0    0   0       0        0     0        1        0\n7     0            0       0    0   0       0        0     0        0        0\n8     2            1       2    2   1       3        1     1        4        1\n\n\nTja, nicht so viele Treffer …\nIn der Zusammenfassung:\n\nget_nrc_values(text, language = \"german\")\n\n# A tibble: 1 × 10\n  anger anticipation disgust  fear   joy negative positive sadness surprise\n  &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1     0            0       0     0     0        0        0       0        0\n# ℹ 1 more variable: trust &lt;dbl&gt;\n\n\nTja, leider keine Treffer. Merkwürdig.\n\nget_sentiment(text,\n              method = \"nrc\",\n              language = \"german\")\n\n[1]  0  0  1  0  1 -1  0 -3\n\n\nNaja, ok.\n\n\nMit einem eigenen Lexikon\nBeispiel vom Autor des Pakets:\n\nmy_text &lt;- \"I love when I see something beautiful.  I hate it when ugly feelings creep into my head.\"\nchar_v &lt;- get_sentences(my_text)\nmethod &lt;- \"custom\"\ncustom_lexicon &lt;- data.frame(word=c(\"love\", \"hate\", \"beautiful\", \"ugly\"), value=c(1,-1,1, -1))\nmy_custom_values &lt;- get_sentiment(char_v, method = method, lexicon = custom_lexicon)\nmy_custom_values\n\n[1]  2 -2\n\n\n\nget_sentiment(text,\n              method = \"custom\",\n              lexicon = sentiws)\n\n[1] -0.0628  0.0000  0.7251  0.0040  0.0000  0.0000  0.0000 -0.3990"
  },
  {
    "objectID": "posts/germeval02/germeval02.html#test",
    "href": "posts/germeval02/germeval02.html#test",
    "title": "germeval02",
    "section": "Test",
    "text": "Test\n\ntic()\nsentiments &lt;-\n  get_sentiment(germeval_train$text,\n              method = \"custom\",\n              lexicon = sentiws)\ntoc()\n\n6.76 sec elapsed\n\n\n\nlength(sentiments)\n\n[1] 5009\n\nhead(sentiments)\n\n[1]  0.1025 -0.3426  0.0000  0.0040 -0.0048 -0.3460\n\n\nDie Geschwindigkeit scheint deutlich besser zu sein, als bei den Regex-Ansätzen."
  },
  {
    "objectID": "posts/germeval02/germeval02.html#als-spalte-in-die-tabelle",
    "href": "posts/germeval02/germeval02.html#als-spalte-in-die-tabelle",
    "title": "germeval02",
    "section": "Als Spalte in die Tabelle",
    "text": "Als Spalte in die Tabelle\n\ntic()\nd &lt;-\n  germeval_train |&gt; \n  mutate(n_emo = get_sentiment(germeval_train$text,\n              method = \"custom\",\n              lexicon = sentiws))\ntoc()\n\n6.635 sec elapsed\n\nhead(d)\n\n# A tibble: 6 × 5\n     id text                                                 c1    c2      n_emo\n  &lt;int&gt; &lt;chr&gt;                                                &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1 @corinnamilborn Liebe Corinna, wir würden dich gern… OTHER OTHER  0.103 \n2     2 @Martin28a Sie haben ja auch Recht. Unser Tweet war… OTHER OTHER -0.343 \n3     3 @ahrens_theo fröhlicher gruß aus der schönsten stad… OTHER OTHER  0     \n4     4 @dushanwegner Amis hätten alles und jeden gewählt..… OTHER OTHER  0.004 \n5     5 @spdde kein verläßlicher Verhandlungspartner. Nachk… OFFE… INSU… -0.0048\n6     6 @Dirki_M Ja, aber wo widersprechen die Zahlen denn … OTHER OTHER -0.346"
  },
  {
    "objectID": "posts/Typ-Fehler-R-07/Typ-Fehler-R-07.html",
    "href": "posts/Typ-Fehler-R-07/Typ-Fehler-R-07.html",
    "title": "Typ-Fehler-R-07",
    "section": "",
    "text": "Question"
  },
  {
    "objectID": "posts/Typ-Fehler-R-07/Typ-Fehler-R-07.html#answerlist",
    "href": "posts/Typ-Fehler-R-07/Typ-Fehler-R-07.html#answerlist",
    "title": "Typ-Fehler-R-07",
    "section": "Answerlist",
    "text": "Answerlist\n\nR ist abgestürzt; am besten neu starten.\nR verträgt im Standard nur Grüße in englischer Sprache. Sprachpakete updaten.\nR wartet auf das Ende der Text-Auszeichnung, also auf das schließende Anführungszeichen. Das muss noch eingegeben werden. Alternativ kann man “Escape” drücken.\nEs gibt kein Problem; man kann einfach den nächsten Befehl eingeben.\nR hat gewartet auf das Ende der Text-Auszeichnung, also auf das schließende Anführungszeichen. Jetzt ist R abgestürzt und muss neu gestartet werden."
  },
  {
    "objectID": "posts/Typ-Fehler-R-07/Typ-Fehler-R-07.html#answerlist-1",
    "href": "posts/Typ-Fehler-R-07/Typ-Fehler-R-07.html#answerlist-1",
    "title": "Typ-Fehler-R-07",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nRichtig\nFalsch\nFalsch\n\n\nCategories:\n\nR\nerror\nmchoice"
  },
  {
    "objectID": "posts/alphafehler-inflation2/alphafehler-inflation2.html",
    "href": "posts/alphafehler-inflation2/alphafehler-inflation2.html",
    "title": "alphafehler-inflation2",
    "section": "",
    "text": "Aufgabe\nDas “Maschinendisaster” sei als folgendes Szenario beschrieben:\nEine Maschine bestehe aus einer Menge Teile, die alle recht zuverlässig arbeiten. Außerdem arbeiten alle Teile unabhängig voneinander (vermutlich keine ganz realistische Annahme). Die Zuverlässigkeit eines Teils sei \\(r=.9999\\) für ein bestimmtes Zeitintervall \\(t\\). Mit \\(1-r\\) fällt also ein Teil innerhalb von \\(t\\) aus.\nEin interessanter Schnörkel ist, dass man “Maschine” auch als “Computerprogramm” oder “biologisches System” lesen kann.\nEine Forscherin fragt sich, aus wie vielen \\(k\\) Teilen die Maschine höchstens bestehen darf, damit es mit einer Wahrscheinlichkeit von 99% zu keinem Ausfall innerhalb von \\(t=1\\) kommt.\n         \n\n\nLösung\n\nr &lt;- .9999\nloesung &lt;- \n  log(.99, base = r) %&gt;% \n  trunc()\nloesung\n\n[1] 100\n\n\ntrunc() schneidet die Dezimalstellen ab, rundet also ab.\n\\[\n\\begin{aligned}\nr^k  &= .99 \\qquad |log_r \\\\\nlog_r(r^k) &= log_r(.99) \\\\\nk &\\approx 100\n\\end{aligned}\n\\]\nDie Lösung lautet also 100.\n\nCategories:\n\nprobability\nR\ninference\nnum"
  },
  {
    "objectID": "posts/oecd-yacsda/index.html",
    "href": "posts/oecd-yacsda/index.html",
    "title": "oecd-yacsda",
    "section": "",
    "text": "Fallstudie: Explorative Datenanalyse zum Datensatz “OECD Wellbeing”\n(YACSDA: Yet another Case Study on Data Analysis)"
  },
  {
    "objectID": "posts/oecd-yacsda/index.html#hintergrund",
    "href": "posts/oecd-yacsda/index.html#hintergrund",
    "title": "oecd-yacsda",
    "section": "1.1 Hintergrund",
    "text": "1.1 Hintergrund\nIn diesem Post untersuchen wir einige Aspekte der explorativen Datenanalyse für den Datensatz oecd wellbeing aus dem Jahr 2016.\nHinweis: Als Vertiefung gekennzeichnete Abschnitt sind nicht prüfungsrelevant."
  },
  {
    "objectID": "posts/oecd-yacsda/index.html#benötigte-pakete",
    "href": "posts/oecd-yacsda/index.html#benötigte-pakete",
    "title": "oecd-yacsda",
    "section": "1.2 Benötigte Pakete",
    "text": "1.2 Benötigte Pakete\nEin Standard-Paket zur grundlegenden Datenanalyse bzw. des Datenjudos ist tidyverse. Darüber hinaus verwenden wir noch zwei Pakete zur Visualisierung und eines für den Komfort.\n\nlibrary(tidyverse)  # Datenjudo\nlibrary(easystats)  # Komfort \nlibrary(DataExplorer)  # Data vis\nlibrary(ggpubr)  # Data vis"
  },
  {
    "objectID": "posts/oecd-yacsda/index.html#datensatz-laden",
    "href": "posts/oecd-yacsda/index.html#datensatz-laden",
    "title": "oecd-yacsda",
    "section": "1.3 Datensatz laden",
    "text": "1.3 Datensatz laden\nDer Datensatz kann hier bezogen werden.\nDoi: https://doi.org/10.1787/data-00707-en.\nFalls der Datensatz lokal (auf Ihrem Rechner) vorliegt, können Sie ihn in gewohnter Manier importieren. Geben Sie dazu den Pfad zum Datensatz ein; bei mir sieht das so aus:\n\noecd &lt;- read.csv(\"/Users/sebastiansaueruser/datasets/oecd_wellbeing.csv\")\n\nLiegt die Datendatei im gleichen Verzeichnis wie Ihre R-/Quarto-/Rmd-Datei, dann brauchen Sie nur den Dateinamen, nicht den Pfad, anzugeben.\nAlternativ können Sie die Daten direkt von einem Server beziehen:\n\noecd &lt;- read.csv(\"https://raw.githubusercontent.com/sebastiansauer/2021-sose/master/data/OECD/oecd-wellbeing.csv\")"
  },
  {
    "objectID": "posts/oecd-yacsda/index.html#erster-blick",
    "href": "posts/oecd-yacsda/index.html#erster-blick",
    "title": "oecd-yacsda",
    "section": "1.4 Erster Blick",
    "text": "1.4 Erster Blick\n\nglimpse(oecd)\n\nRows: 429\nColumns: 15\n$ Country                  &lt;chr&gt; \"Australia\", \"Australia\", \"Australia\", \"Austr…\n$ Region                   &lt;chr&gt; \"New South Wales\", \"Victoria\", \"Queensland\", …\n$ region_type              &lt;chr&gt; \"country_part\", \"country_part\", \"country_part…\n$ Code                     &lt;chr&gt; \"AU1\", \"AU2\", \"AU3\", \"AU4\", \"AU5\", \"AU6\", \"AU…\n$ Education                &lt;dbl&gt; 8.0, 8.1, 7.8, 7.3, 7.6, 6.5, 8.1, 9.5, 8.8, …\n$ Jobs                     &lt;dbl&gt; 8.1, 7.9, 8.1, 7.8, 8.8, 7.6, 8.7, 9.3, 7.8, …\n$ Income                   &lt;dbl&gt; 6.8, 5.9, 6.3, 6.1, 7.9, 5.4, 8.2, 10.0, 5.7,…\n$ Safety                   &lt;dbl&gt; 8.8, 9.5, 9.5, 9.0, 8.6, 8.8, 0.0, 10.0, 9.7,…\n$ Health                   &lt;dbl&gt; 9.0, 9.5, 8.3, 8.5, 9.3, 5.4, 2.4, 9.3, 6.7, …\n$ Environment              &lt;dbl&gt; 9.8, 8.6, 9.9, 9.4, 9.6, 10.0, 9.2, 9.1, 3.5,…\n$ Civic_engagement         &lt;dbl&gt; 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 8.4, 10.0…\n$ Accessiblity_to_services &lt;dbl&gt; 7.2, 7.5, 7.7, 7.2, 7.8, 6.8, 7.8, 8.7, 8.0, …\n$ Housing                  &lt;dbl&gt; 7.2, 7.8, 8.3, 8.3, 8.9, 8.3, 5.6, 8.3, 6.1, …\n$ Community                &lt;dbl&gt; 8.9, 9.3, 8.6, 8.6, 8.5, 8.6, 10.0, 9.8, 8.3,…\n$ Life_satisfaction        &lt;dbl&gt; 7.8, 8.5, 8.1, 8.5, 7.8, 9.6, 7.0, 9.6, 7.8, …\n\n\nWie glimpse() aufzeigt, liegen also einige qualitative (kategoriale, chr, vom Typ “Text”) und einige quantitative (metrische, dbl) Variablen vor. Die qualitativen Variablen sind für eine direkte Analyse weniger interessant; vielmehr ist es interessant, die Statistiken auf die Gruppen (Stufen, Level) der qualitativen Variablen aufzusplitten.\nBetrachten wir aber zu Beginn die metrischen Variablen einzeln (univariat)."
  },
  {
    "objectID": "posts/oecd-yacsda/index.html#deskriptive-statistiken-zu-den-metrischen-variablen-einzeln-univariat",
    "href": "posts/oecd-yacsda/index.html#deskriptive-statistiken-zu-den-metrischen-variablen-einzeln-univariat",
    "title": "oecd-yacsda",
    "section": "1.5 Deskriptive Statistiken zu den metrischen Variablen, einzeln (univariat)",
    "text": "1.5 Deskriptive Statistiken zu den metrischen Variablen, einzeln (univariat)\nZentrale Statistiken zu den metrischen Variablen lassen sich auf mehreren Wegen mit R berechnen. Hier ist ein Weg:\n\ndescribe_distribution(oecd)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nMean\nSD\nIQR\nMin\nMax\nSkewness\nKurtosis\nn\nn_Missing\n\n\n\n\nEducation\n6.81\n2.97\n3.50\n0\n10\n-1.08\n-0.02\n426\n3\n\n\nJobs\n6.45\n2.41\n2.90\n0\n10\n-0.96\n0.48\n429\n0\n\n\nIncome\n4.11\n2.70\n3.40\n0\n10\n0.44\n-0.38\n429\n0\n\n\nSafety\n7.05\n3.23\n3.80\n0\n10\n-1.18\n0.10\n429\n0\n\n\nHealth\n5.73\n2.93\n5.20\n0\n10\n-0.33\n-1.15\n429\n0\n\n\nEnvironment\n5.40\n2.73\n4.35\n0\n10\n-0.11\n-0.89\n429\n0\n\n\nCivic_engagement\n5.05\n2.85\n4.10\n0\n10\n-0.03\n-0.91\n429\n0\n\n\nAccessiblity_to_services\n6.53\n2.65\n2.60\n0\n10\n-1.13\n0.50\n429\n0\n\n\nHousing\n4.80\n3.02\n5.50\n0\n10\n-0.06\n-1.12\n427\n2\n\n\nCommunity\n6.88\n2.75\n3.25\n0\n10\n-1.13\n0.32\n425\n4\n\n\nLife_satisfaction\n5.80\n2.90\n4.90\n0\n10\n-0.47\n-0.87\n425\n4\n\n\n\n\n\nJetzt gehen wir weiter zur Visualisierung der Verteilung der metrischen Variablen. Auch hier gibt es wieder viele Lösungen.\nEs reicht, wenn Sie mit einer Lösung vertraut sind.\n\n1.5.1 Mit ggpubr\n\noecd |&gt; \ngghistogram(x = \"Life_satisfaction\")\n\n\n\n\n\n\n\n\n\n\n1.5.2 Mit DataExplorer\n\noecd |&gt; \n  select(Life_satisfaction) |&gt; \n  plot_histogram()\n\n\n\n\n\n\n\n\n\n\n1.5.3 Mit ggplot\nDas R-Paket ggplot wird durch das “Meta-Paket” tidyverse gestartet. Sie müssen es also nicht extra starten.\n\noecd %&gt;% \n  ggplot(aes(x = Life_satisfaction)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\nEine ähnliche Aussage liefert das Dichte-Diagramm:\n\noecd %&gt;% \n  ggplot(aes(x = Life_satisfaction)) +\n  geom_density()\n\n\n\n\n\n\n\n\nDie Dichte gibt an, welcher Anteil der Beobachtungen an der jeweiligen Stelle der X-Achse lägen, wenn man eine Einheit betrachtet (z.B. die Lebenszufriedenheit von 5-6)."
  },
  {
    "objectID": "posts/oecd-yacsda/index.html#histogramm-nach-gruppen-lebenszufriedenheit-in-de-und-fr",
    "href": "posts/oecd-yacsda/index.html#histogramm-nach-gruppen-lebenszufriedenheit-in-de-und-fr",
    "title": "oecd-yacsda",
    "section": "1.6 Histogramm nach Gruppen: Lebenszufriedenheit in De und Fr",
    "text": "1.6 Histogramm nach Gruppen: Lebenszufriedenheit in De und Fr\nAngenommen, man möchte Deutschland mit Frankreich vergleichen im Hinblick auf die Lebenszufriedenheit.\nZunächst filtern wir den OECD-Datensatz, so dass nur die beiden genannten Länder enthalten bleiben:\n\noecd_de_fr &lt;- \noecd %&gt;% \n  filter(Country == \"Germany\" | Country == \"France\") \n\nDann visualisieren wir wieder.\n\n1.6.1 Mit ggpubr\n\noecd_de_fr |&gt; \n  gghistogram(x = \"Life_satisfaction\", facet.by = \"Country\")\n\n\n\n\n\n\n\n\n\n\n1.6.2 Mit DataExplorer\nLeider unterstützt DataExplorer nicht direkt den Vergleich von Grupen mit einem Histogramm. Man könnte aber einen Boxplot verwenden stattdessen.\n\noecd_de_fr |&gt; \n  select(Life_satisfaction, Country) |&gt; \n  plot_boxplot(by = \"Country\")\n\n\n\n\n\n\n\n\n\n\n1.6.3 Mit ggplot\n\noecd_de_fr %&gt;% \n  ggplot(aes(x = Life_satisfaction)) +\n  geom_histogram(bins = 15) +\n  facet_wrap(~ Country)"
  },
  {
    "objectID": "posts/oecd-yacsda/index.html#histogramm-für-alle-metrischen-variablen-auf-einmal",
    "href": "posts/oecd-yacsda/index.html#histogramm-für-alle-metrischen-variablen-auf-einmal",
    "title": "oecd-yacsda",
    "section": "1.7 Histogramm für alle metrischen Variablen auf einmal",
    "text": "1.7 Histogramm für alle metrischen Variablen auf einmal\nUm einen Überblick über die Verteilungen zu bekommen, bietet es sich an, sich alle Verteilungen anzuschauen. Malen wir einmal alle Histogramme auf einmal. Das geht wiederum mit DataExplorer sehr einfach:\n\noecd |&gt; \n  plot_histogram()"
  },
  {
    "objectID": "posts/oecd-yacsda/index.html#vertiefung-histogramm-für-alle-variablen-auf-kompliziert",
    "href": "posts/oecd-yacsda/index.html#vertiefung-histogramm-für-alle-variablen-auf-kompliziert",
    "title": "oecd-yacsda",
    "section": "1.8 VERTIEFUNG: Histogramm für alle Variablen auf kompliziert",
    "text": "1.8 VERTIEFUNG: Histogramm für alle Variablen auf kompliziert\n\n\n\n\n\n\nNote\n\n\n\nDieser Abschnitt ist eine Vertiefung; Sie können in überspringen, ohne den Anschluss zu den folgenden Abschnitten zu verlieren. \\(\\square\\)\n\n\nAls erstes erzeugen wir einen langen Dataframe (der nur aus metrischen Variablen besteht):\n\noecd_de_fr %&gt;% \n  select(where(is.numeric)) %&gt;%  # wähle alle Spalten aus, wo sich Nummern finden\n  pivot_longer(everything()) %&gt;%  # baue alle Variablen in ein langes Format um\n  slice(1:10) # zeige die Zeilen 1 bis 10\n\n# A tibble: 10 × 2\n   name                     value\n   &lt;chr&gt;                    &lt;dbl&gt;\n 1 Education                  7.8\n 2 Jobs                       6  \n 3 Income                     6.3\n 4 Safety                     8.6\n 5 Health                    10  \n 6 Environment                3.5\n 7 Civic_engagement           7.4\n 8 Accessiblity_to_services   8.6\n 9 Housing                    3.3\n10 Community                  7.9\n\n\nDann plotten wir Histogramme, wobei wir nach den Ländern (key) gruppieren. Aber zuerst speichern wir uns den “langen” Datensatz ab:\n\noecd_de_fr_long &lt;- \noecd_de_fr %&gt;% \n  select(where(is.numeric)) %&gt;%  # wähle alle Spalten aus, wo sich Nummern finden\n  pivot_longer(everything()) \n\nBetrachten Sie diesen Daten einmal zur Übung.\nDann plotten wir in gewohnter Manier:\n\noecd_de_fr_long %&gt;% \n  ggplot(aes(x = value)) +\n  geom_histogram() +\n  facet_wrap(~ name)"
  },
  {
    "objectID": "posts/oecd-yacsda/index.html#hintergrund-1",
    "href": "posts/oecd-yacsda/index.html#hintergrund-1",
    "title": "oecd-yacsda",
    "section": "2.1 Hintergrund",
    "text": "2.1 Hintergrund\nHat Deutschland in Vergleich zu anderen Ländern eine hohe Lebenszufriedenheit?\nDie Frage ist noch recht unpräzise formuliert, aber dafür gibt sie Raum für eine Menge von Untersuchungsansätzen.\n\n2.1.1 Datensatz filtern - nur Länder, keine Landesteile\nDer Datensatz in seiner aktuellen Form verstößt gegen die Regel der “Normalform”, dass in jeder Zeile (genau) eine Beobachtungseinheit steht und in jeder Zeile (genau) eine Variable. In einigen Zeilen stehen Länder, in den meisten anderen aber Landesteile (wie Bayern, Baden-Württemberg etc.). Filtern wir uns nur die Länder, und exkdluieren die Landesteile:\n\noecd_short &lt;-\n  filter(oecd, region_type == \"country_whole\") \n\nDie Anzahl der Zeilen dieses Datensatz oecd_short gibt uns Aufschluss über die Anzahl der untersuchten Länder.\n\n\n2.1.2 Visualisierung der Lebenszufriedenheit der Länder\n\n2.1.2.1 Mit DataExplorer\n\noecd_short |&gt; \n  select(Country, Life_satisfaction) |&gt; \n  plot_scatterplot(by = \"Life_satisfaction\")\n\n\n\n\n\n\n\n\n\n\n2.1.2.2 Mit ggplot\n\noecd_short %&gt;% \n  ggplot(aes(x = Country, y = Life_satisfaction)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n2.1.2.3 Sieht nicht so schön aus\nHm, unser Punktediagramm sieht nicht übersichtlich aus. Besser wäre es, die Punkte absteigend zu sortieren.\nBetrachten wir dazu die Variable country näher: Es handelt sich um eine Character-Variable:\n\nstr(oecd)\n\n'data.frame':   429 obs. of  15 variables:\n $ Country                 : chr  \"Australia\" \"Australia\" \"Australia\" \"Australia\" ...\n $ Region                  : chr  \"New South Wales\" \"Victoria\" \"Queensland\" \"South Australia\" ...\n $ region_type             : chr  \"country_part\" \"country_part\" \"country_part\" \"country_part\" ...\n $ Code                    : chr  \"AU1\" \"AU2\" \"AU3\" \"AU4\" ...\n $ Education               : num  8 8.1 7.8 7.3 7.6 6.5 8.1 9.5 8.8 8.5 ...\n $ Jobs                    : num  8.1 7.9 8.1 7.8 8.8 7.6 8.7 9.3 7.8 8.2 ...\n $ Income                  : num  6.8 5.9 6.3 6.1 7.9 5.4 8.2 10 5.7 5.9 ...\n $ Safety                  : num  8.8 9.5 9.5 9 8.6 8.8 0 10 9.7 9.8 ...\n $ Health                  : num  9 9.5 8.3 8.5 9.3 5.4 2.4 9.3 6.7 6.6 ...\n $ Environment             : num  9.8 8.6 9.9 9.4 9.6 10 9.2 9.1 3.5 2.6 ...\n $ Civic_engagement        : num  10 10 10 10 10 10 8.4 10 8.6 8.1 ...\n $ Accessiblity_to_services: num  7.2 7.5 7.7 7.2 7.8 6.8 7.8 8.7 8 7.4 ...\n $ Housing                 : num  7.2 7.8 8.3 8.3 8.9 8.3 5.6 8.3 6.1 5.6 ...\n $ Community               : num  8.9 9.3 8.6 8.6 8.5 8.6 10 9.8 8.3 7.8 ...\n $ Life_satisfaction       : num  7.8 8.5 8.1 8.5 7.8 9.6 7 9.6 7.8 8.1 ...\n\n\nEine Variable des Typs character steht für Text, z.B. \"Germany\".\nOffensichtlich sind diese alphabetisch geordnet – nach dieser Ordnung richtet sich die Ordnung im Diagramm.\n\n\n\n2.1.3 Umwandling in eine Faktor-Variable\nIn solchen Fällen bietet es sich an, die Character-Variable in eine Factor-Variable umzuwandeln; dann geht das Weitere einfacher.\n\noecd_short &lt;- \noecd_short %&gt;% \n  mutate(Country = factor(Country))\n\nÜbrigens: Möchte man wissen, wie viele unterschiedliche Werte eine Variable enthält, dann kann die Funktion distinct() verwenden:\n\noecd_short %&gt;% \n  distinct(Country)\n\n           Country\n1        Australia\n2          Austria\n3          Belgium\n4           Canada\n5            Chile\n6   Czech Republic\n7          Denmark\n8          Estonia\n9          Finland\n10          France\n11         Germany\n12          Greece\n13         Hungary\n14         Iceland\n15         Ireland\n16          Israel\n17           Italy\n18           Japan\n19           Korea\n20      Luxembourg\n21          Mexico\n22     Netherlands\n23     New Zealand\n24          Norway\n25          Poland\n26        Portugal\n27 Slovak Republic\n28        Slovenia\n29           Spain\n30          Sweden\n31     Switzerland\n32          Turkey\n33  United Kingdom\n34   United States"
  },
  {
    "objectID": "posts/oecd-yacsda/index.html#ranking-und-top-10-prozent-der-zufriedenheit",
    "href": "posts/oecd-yacsda/index.html#ranking-und-top-10-prozent-der-zufriedenheit",
    "title": "oecd-yacsda",
    "section": "2.2 Ranking und Top-10-Prozent der Zufriedenheit",
    "text": "2.2 Ranking und Top-10-Prozent der Zufriedenheit\n\n2.2.1 Top-10\nSchauen wir uns die “Happy-Top-10” an, die 10 Länder mit der höchsten Lebenszufriedenheit:\n\noecd_short %&gt;% \n  arrange(-Life_satisfaction) %&gt;%  # absteigend sortieren\n  select(Country, Life_satisfaction) %&gt;% \n  slice(1:10)\n\n       Country Life_satisfaction\n1      Denmark              10.0\n2  Switzerland              10.0\n3      Finland               9.7\n4  Netherlands               9.7\n5       Norway               9.7\n6       Canada               9.3\n7      Iceland               9.3\n8       Sweden               9.3\n9    Australia               8.8\n10     Austria               8.8\n\n\n\n\n2.2.2 Die oberen 10% der Zufriedenheit\nMit welcher Lebenszufriedenheit gehört ein Land zu den Top-10-Prozent der zufriedenen Länder?\n\noecd_short %&gt;% \n  summarise(quantile(Life_satisfaction, probs = .90))\n\n  quantile(Life_satisfaction, probs = 0.9)\n1                                      9.7\n\n\nAh, Länder mit einer Lebenszufriedenheit von mind. 9.7 gehören zu den oberen Top-10-Prozent. Filtern wir mal entsprechend:\n\noecd_short %&gt;% \n  filter(Life_satisfaction &gt;= 9.7) %&gt;% \n  select(Country, Life_satisfaction)\n\n      Country Life_satisfaction\n1     Denmark              10.0\n2     Finland               9.7\n3 Netherlands               9.7\n4      Norway               9.7\n5 Switzerland              10.0"
  },
  {
    "objectID": "posts/oecd-yacsda/index.html#vertiefung",
    "href": "posts/oecd-yacsda/index.html#vertiefung",
    "title": "oecd-yacsda",
    "section": "2.3 Vertiefung",
    "text": "2.3 Vertiefung\nÄndern wir die Sortierung! Mit reorder() kann man die Sortierung ändern (re-ordnen, daher der Name):\n\noecd_short_reordered &lt;- \noecd_short %&gt;% \n  mutate(Country_sorted = reorder(Country, Life_satisfaction)) \n\nIst das jetzt geordnet? str() (wie structure) verrät es uns:\n\nstr(oecd_short_reordered)\n\n'data.frame':   34 obs. of  16 variables:\n $ Country                 : Factor w/ 34 levels \"Australia\",\"Austria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ Region                  : chr  \"Australia\" \"Austria\" \"Belgium\" \"Canada\" ...\n $ region_type             : chr  \"country_whole\" \"country_whole\" \"country_whole\" \"country_whole\" ...\n $ Code                    : chr  \"AUS\" \"AUT\" \"BEL\" \"CAN\" ...\n $ Education               : num  7.6 8.4 7.5 9.2 7.2 10 6.7 9.6 8.7 7.6 ...\n $ Jobs                    : num  7.9 7.7 5 6.6 6.2 7.3 7.9 6.6 6.4 5.2 ...\n $ Income                  : num  9.5 7.9 6.2 7.4 0.4 2.8 5.1 1 5.6 7 ...\n $ Safety                  : num  8.4 10 5.7 6.5 0.6 6.1 10 0 10 8.4 ...\n $ Health                  : num  9 7.2 6.6 8.5 4.2 2.7 5.8 2.1 7.3 9.3 ...\n $ Environment             : num  9.7 2.8 1.9 7.4 8.5 1.4 5.8 6.4 7.9 4.5 ...\n $ Civic_engagement        : num  10 6.2 9.7 4.7 0.1 2.6 8.9 3.7 4.8 7.5 ...\n $ Accessiblity_to_services: num  6.9 7.2 7.6 8.1 0 6.7 8.3 7.6 9 6.9 ...\n $ Housing                 : num  9.5 5.1 8.8 10 1.5 2.9 6.6 1.5 6.6 5.1 ...\n $ Community               : num  8.8 7.6 7.7 8.5 2.5 5.1 9.6 4.6 8.7 7.6 ...\n $ Life_satisfaction       : num  8.8 8.8 7.9 9.3 4.9 5.3 10 0.4 9.7 6.2 ...\n $ Country_sorted          : Factor w/ 34 levels \"Hungary\",\"Portugal\",..: 23 24 20 27 12 13 33 4 30 15 ...\n  ..- attr(*, \"scores\")= num [1:34(1d)] 8.8 8.8 7.9 9.3 4.9 5.3 10 0.4 9.7 6.2 ...\n  .. ..- attr(*, \"dimnames\")=List of 1\n  .. .. ..$ : chr [1:34] \"Australia\" \"Austria\" \"Belgium\" \"Canada\" ...\n\n\nWie man sieht, ist Country_sorted jetzt anders sortiert.\nVisualisieren wir das Ergebnis:\n\n2.3.1 Mit DataExplorer\n\noecd_short_reordered |&gt; \n  select(Country_sorted, Life_satisfaction) |&gt; \n  plot_scatterplot(by = \"Life_satisfaction\")\n\n\n\n\n\n\n\n\nOh, DataExplorer macht die Reihenfolge wieder kaputt.\n\n\n2.3.2 Mit ggpubr\n\noecd_short_reordered |&gt; \n  ggscatter(x = \"Country_sorted\",\n            y = \"Life_satisfaction\")\n\n\n\n\n\n\n\n\n\n\n2.3.3 Mit ggplot\n\nplot_sorted &lt;- oecd_short_reordered %&gt;% \n  ggplot(aes(x = Country_sorted, y = Life_satisfaction)) +\n  geom_point()\n\nplot_sorted\n\n\n\n\n\n\n\n\nSchon besser. Man kann z.B. die Achsen nicht lesen 😿. Was könnte man da bloß tun?\n\n\n2.3.4 Achsenu um 90 Grad drehen\nMit + coord_flip() lassen sich die Achsen um 90 Grad drehen:\n\nplot_sorted + coord_flip()\n\n\n\n\n\n\n\n\nSchön 😄.\nMan hätte das Sortieren und Achsen drehen auch in einem Haps machen können:\n\noecd_short_reordered %&gt;% \n  ggplot(aes(x = Country_sorted, y = Life_satisfaction)) +\n  geom_point() + coord_flip()\n\n\n\n\n\n\n\n\nAber übersichtlicher ist es, die Dinge nacheinander zu tun.\n\n\n2.3.5 Mittelwert ins Diagramm\nSchön wäre es noch, im Bild den Mittelwert o.Ä. im Diagramm zu sehen:\n\noecd_short_reordered %&gt;% \n  ggplot(aes(x = Country_sorted, y = Life_satisfaction)) +\n  geom_point() +\n  geom_hline(yintercept = 6.08, data = NA, color = \"firebrick\") + \n  coord_flip()\n\n\n\n\n\n\n\n\nTja, die Wünsche hören nie auf… Wäre es nicht noch nett, wenn “Deutschland” hervorgehoben wäre, optisch, so dass es im Diagramm hervorsticht. Nehmen wir an, wir sind an diesem Land besonders interessiert.\n\noecd_short_reordered &lt;- \n  oecd_short_reordered %&gt;% \n  mutate(is_Germany = Country == \"Germany\")\n\nDamit haben wir eine Spalte erstellt, die angibt, ob ein Land Deutschland ist (TRUE) oder nicht (FALSE). Diese neue Variable nehmen wir her, um die Farbe, Größe und Form der Punkte zu bestimmen:\n\noecd_short_reordered %&gt;% \n  ggplot(aes(x = Country_sorted, y = Life_satisfaction)) +\n  geom_point(aes(color = is_Germany, shape = is_Germany, size = is_Germany)) +\n  geom_hline(yintercept = 6.08, data = NA, color = \"firebrick\") + \n  geom_hline(yintercept = 6.08, data = NA, color = \"grey60\") %&gt;% \n  geom_vline(xintercept = 16, data = NA, color = \"grey80\") +\n  coord_flip()"
  },
  {
    "objectID": "posts/oecd-yacsda/index.html#zusammenhang-zweier-metrischer-variablen-punktediagramm",
    "href": "posts/oecd-yacsda/index.html#zusammenhang-zweier-metrischer-variablen-punktediagramm",
    "title": "oecd-yacsda",
    "section": "2.4 Zusammenhang zweier metrischer Variablen – Punktediagramm",
    "text": "2.4 Zusammenhang zweier metrischer Variablen – Punktediagramm\nHängt die Lebenszufriedenheit mit Civic_engagment zusammen?\nVisualisieren wir diesen (möglichen) Zusammenhang.\n\n2.4.1 Mit ggpubr\n\noecd_short_reordered |&gt; \n  ggscatter(x = \"Civic_engagement\",\n            y = \"Life_satisfaction\")\n\n\n\n\n\n\n\n\n\n\n2.4.2 Mit DataExplorer\n\noecd_short_reordered |&gt; \n  select(Civic_engagement, Life_satisfaction) |&gt; \n  plot_scatterplot(by = \"Life_satisfaction\")\n\n\n\n\n\n\n\n\nDataExplorer bietet den Vorteil, dass man einfach überprüfen kann, ob irgendeine Variable mit Lebenszufriedenheit zusammenhängt:\n\noecd_short_reordered |&gt; \n  #select(Civic_engagement, Life_satisfaction) |&gt; \n  plot_scatterplot(by = \"Life_satisfaction\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4.3 Mit ggplot\n\noecd_short_reordered %&gt;% \n  ggplot(aes(x = Civic_engagement, y = Life_satisfaction)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n2.4.4 Insgesamt wenig Zusammenhang\nHm, es ist kein starker Trend zu erkennen.\nWas sagt die Korrelation dazu:\n\noecd_short_reordered %&gt;% \n  summarise(cor_ce_ls = cor(Civic_engagement, Life_satisfaction))\n\n  cor_ce_ls\n1 0.4021292\n\n\nImmerhin, kein ganz unwesentlicher Wert.\n\n\n2.4.5 Und so weiter\nDieses Prinzip mit dem Punktediagramm könnte man jetzt weiterführen ad nauseam.\n\n\n2.4.6 Korrelationsdiagramm\n\noecd_short_reordered |&gt; \n  plot_correlation()"
  },
  {
    "objectID": "posts/oecd-yacsda/index.html#zusammenhang-zweier-variablen-unter-berücksichtigung-von-drittvariablen",
    "href": "posts/oecd-yacsda/index.html#zusammenhang-zweier-variablen-unter-berücksichtigung-von-drittvariablen",
    "title": "oecd-yacsda",
    "section": "2.5 Zusammenhang zweier Variablen unter Berücksichtigung von Drittvariablen",
    "text": "2.5 Zusammenhang zweier Variablen unter Berücksichtigung von Drittvariablen\nOben haben wir gesehen, dass Lebenszufriedenheit und Civiv Engagement zusammenhängen (zumindest ein bisschen).\nAber vielleicht hängt dieser Zusammenhang wiederum von der finanziellen Absicherung ab? Nur wenn man materiell abgesichert ist, so könnte man argumentieren, wird bürgerliches Engagement (bzw. die Möglichkeit zu) eine Einflussgröße auf die Lebenszufriedenheit.\nAnders gesagt: Man könnte behaupten, der Zusammenahng von Lebenszufriedenheit und Civiv Engagement ist abhängig von einer dritten Variable, dem Einkommen.\nUm diese Frage zu untersuchen, teilen wir Income in zwei Stufen, hoch und gering. Dann untersuchen wir jeweils den Zusammenhang von Lebenszufriedenheit und bürgerlichem Engagement.\nAchtung! Eine metrische Variablen in zwei Hälften zu spalten birgt einen hohen Informationsverlust. Da wir aber nur eine grobe Untersuchung vorhaben (und uns noch nicht fortgeschrittener Technik bedienen wollen), bleiben wir erstmal bei dieser sog. Dichotomisierung.\nNehmen wir den Median des Einkommen als Teilungspunkt; man spricht von einem “Mediansplit”:\n\noecd_short %&gt;% \n  summarise(Income_md = median(Income))\n\n  Income_md\n1      5.15\n\n\nZuerst erstellen wir eine Variable Income_high mit den Stufen 0 (nein) und 1 (ja):\n\noecd_short_reordered &lt;- \n  oecd_short_reordered %&gt;% \n    mutate(Income_high = \n           case_when( Income &gt;= median(Income) ~ 1,\n                      Income &lt; median(Income) ~ 0))\n\n\n2.5.1 Visualisierung\nJetzt plotten wir den Zusammenhang:\n\n2.5.1.1 Mit ggpubr\n\noecd_short_reordered |&gt; \n  ggscatter(y = \"Country_sorted\",\n            x = \"Life_satisfaction\",\n            facet.by = \"Income_high\")\n\n\n\n\n\n\n\n\n\n\n2.5.1.2 Mit ggplot\n\nincome_labels &lt;- c(`0` = \"arm\", \n                   `1` =\"reich\")\n\noecd_short_reordered %&gt;% \n  ggplot(aes(x = Country_sorted, y = Life_satisfaction)) +\n  geom_point() +\n  facet_wrap(~ Income_high, \n             labeller = labeller(Income_high = income_labels))  +\n  coord_flip() +\n  labs(y = \"Länder\",\n       x = \"Lebenszufriedenheit\",\n       title = \"Lebenszufriedenheit in armen und reichen Ländern \") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n2.5.2 Vertiefung: Korrelation pro Gruppe\nUm die Korrelation pro Gruppe zu erhalten, könnten wir jeweils einen Dataframe pro Gruppe erzeugen (mit filter()) und dann jeweils die Korrelation von Zufriedenheit und Engagement berechnen.\nEine andere, etwas elegantere Möglichkeit kann so aussehen:\n\noecd_short_reordered %&gt;%\n  group_by(Income_high) %&gt;% \n  summarise(cor_zuf_eng = cor(Life_satisfaction, Civic_engagement))\n\n# A tibble: 2 × 2\n  Income_high cor_zuf_eng\n        &lt;dbl&gt;       &lt;dbl&gt;\n1           0       0.367\n2           1       0.140\n\n\nInteressanterweise ist die Korrelation durchaus verschieden in den beiden Gruppen.\nNatürlich sind die beiden Gruppen nur Stichproben - es stellt sich die Frage, ob die Unterschiede nur durch Zufälligkeiten des Stichprobenziehens entstanden sind oder auch in der Grundgesatmtheit der “reichen” und “armen” Ländern existieren? Dazu später mehr!"
  },
  {
    "objectID": "posts/oecd-yacsda/index.html#deskriptive-statistiken-nach-ländern",
    "href": "posts/oecd-yacsda/index.html#deskriptive-statistiken-nach-ländern",
    "title": "oecd-yacsda",
    "section": "2.6 Deskriptive Statistiken nach Ländern",
    "text": "2.6 Deskriptive Statistiken nach Ländern\n\n2.6.1 Lebenszufriedenheit\n\n2.6.1.1 Mit easystats\nDas ist relativ einfach:\n\noecd_short_reordered %&gt;% \n  select(Life_satisfaction) %&gt;% \n  describe_distribution()\n\nVariable          | Mean |   SD |  IQR |         Range | Skewness | Kurtosis |  n | n_Missing\n---------------------------------------------------------------------------------------------\nLife_satisfaction | 6.07 | 3.38 | 6.33 | [0.00, 10.00] |    -0.58 |    -1.08 | 34 |         0\n\n\n\n\n2.6.1.2 Mit tidyverse\n\noecd_short_reordered %&gt;% \n  summarise(satis_mean = mean(Life_satisfaction),\n            satis_median = median(Life_satisfaction),\n            satis_sd = sd(Life_satisfaction),\n            satis_iqr = IQR(Life_satisfaction))\n\n  satis_mean satis_median satis_sd satis_iqr\n1   6.070588          7.3 3.379397     5.975"
  },
  {
    "objectID": "posts/Def-Statistik01/Def-Statistik01.html",
    "href": "posts/Def-Statistik01/Def-Statistik01.html",
    "title": "Def-Statistik01",
    "section": "",
    "text": "Welche Definition von Statistik passt am besten?\n\n\n\nStatistik fasst Daten zusammen.\nStatistik vervielfacht Daten.\nStatistik fasst Daten zusammen, um wesentliche Informationen den Daten zu entnehmen .\nStatistik fasst Daten zusammen, um wesentliche Informationen den Daten zu entnehmen und beschreibt die Ungewissheit unserer Schlüsse."
  },
  {
    "objectID": "posts/Def-Statistik01/Def-Statistik01.html#answerlist",
    "href": "posts/Def-Statistik01/Def-Statistik01.html#answerlist",
    "title": "Def-Statistik01",
    "section": "",
    "text": "Statistik fasst Daten zusammen.\nStatistik vervielfacht Daten.\nStatistik fasst Daten zusammen, um wesentliche Informationen den Daten zu entnehmen .\nStatistik fasst Daten zusammen, um wesentliche Informationen den Daten zu entnehmen und beschreibt die Ungewissheit unserer Schlüsse."
  },
  {
    "objectID": "posts/Def-Statistik01/Def-Statistik01.html#answerlist-1",
    "href": "posts/Def-Statistik01/Def-Statistik01.html#answerlist-1",
    "title": "Def-Statistik01",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nWahr\n\n\nCategories:\nschoice"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-03/Verteilungen-Quiz-03.html",
    "href": "posts/Verteilungen-Quiz-03/Verteilungen-Quiz-03.html",
    "title": "Verteilungen-Quiz-03",
    "section": "",
    "text": "Ist folgende Aussage wahr?\nWenn eine Verteilung einer stetigen Zufallsvariablen \\(X\\) (z.B. die Posteriori-Verteilung einer Bayes-Analyse) normalverteilt ist, gilt dann \\(Pr(X \\ge\\bar{x}) = 1/2\\).\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-03/Verteilungen-Quiz-03.html#answerlist",
    "href": "posts/Verteilungen-Quiz-03/Verteilungen-Quiz-03.html#answerlist",
    "title": "Verteilungen-Quiz-03",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-03/Verteilungen-Quiz-03.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-03/Verteilungen-Quiz-03.html#answerlist-1",
    "title": "Verteilungen-Quiz-03",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/nyc_casestudy/nyc_casestudy.html",
    "href": "posts/nyc_casestudy/nyc_casestudy.html",
    "title": "nyc_casestudy",
    "section": "",
    "text": "Aufgabe\nFallstudie\nEine Analystin untersucht Verspätungen der New Yorker Flüge. Sie gibt folgenden Code ein und erhält unten stehendes Ergebnis.\n\nlibrary(sjmisc)\nlibrary(tidyverse)\nlibrary(caret)\ndata(flights, package = \"nycflights13\")\n\nmy_crossval &lt;- trainControl(method = \"cv\",\n                            number = 5,\n                            allowParallel = TRUE,\n                            verboseIter = FALSE)\n\ndoMC::registerDoMC(cores = 2)\n\n\nflights2 &lt;- flights %&gt;%\n  select_if(is.numeric) %&gt;% \n  drop_na() %&gt;% \n  select(-c(year, dep_delay)) %&gt;% \n  std(suffix = \"\")  \n\nn_uebung &lt;- round(.8 * nrow(flights2), digits = 0)\n\nuebung_index &lt;- sample(1:nrow(flights2), size = n_uebung)\n\nuebung_df &lt;- filter(flights2, row_number() %in% uebung_index)\ntest_df &lt;- filter(flights2, !(row_number() %in% uebung_index))\n\nlm_fit1 &lt;- train(arr_delay ~ .,\n                 data = uebung_df,\n                 method = \"lm\",\n                 trControl = my_crossval)\n\nsummary(lm_fit1)\n\n\nCall:\nlm(formula = .outcome ~ ., data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.1233 -0.4714 -0.2037  0.1607 29.2360 \n\nCoefficients: (1 not defined because of singularities)\n                 Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept)    -0.0001968  0.0017957   -0.110   0.9127    \nmonth          -0.0029446  0.0018005   -1.635   0.1020    \nday             0.0003989  0.0017964    0.222   0.8243    \ndep_time        0.8217620  0.0061960  132.627   &lt;2e-16 ***\nsched_dep_time -0.6018870  0.0441828  -13.623   &lt;2e-16 ***\narr_time       -0.2867776  0.0029510  -97.179   &lt;2e-16 ***\nsched_arr_time  0.0979923  0.0036413   26.911   &lt;2e-16 ***\nflight          0.0372636  0.0020564   18.121   &lt;2e-16 ***\nair_time        1.4256039  0.0132562  107.542   &lt;2e-16 ***\ndistance       -1.4444033  0.0133051 -108.561   &lt;2e-16 ***\nhour            0.1019498  0.0436140    2.338   0.0194 *  \nminute                 NA         NA       NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.919 on 261866 degrees of freedom\nMultiple R-squared:  0.1533,    Adjusted R-squared:  0.1533 \nF-statistic:  4742 on 10 and 261866 DF,  p-value: &lt; 2.2e-16\n\n\nIm Folgenden untersucht sie die prädiktive Güte am Testdatensatz.\n\nlm1_pred &lt;- predict(lm_fit1, newdata = test_df)\n\nWarning in predict.lm(modelFit, newdata): prediction from a rank-deficient fit\nmay be misleading\n\nlm1_pred_fit &lt;- postResample(pred = lm1_pred, obs = test_df$arr_delay)\nlm1_pred_fit\n\n     RMSE  Rsquared       MAE \n0.9298718 0.1443675 0.5492924 \n\n\nSchließlich berechnet sie noch ein Random-Forest-Modell. Um Zeit zu sparen, verringert sie den Datensatz in dieser ersten Analyse.\n\nrf_grid &lt;- data.frame(\n  .mtry = c(4, 5, 6, 7),\n  .splitrule = \"variance\",\n  .min.node.size = 5)\n\nuebung_df_small &lt;- sample_n(uebung_df, size = 1000)\n\nrf_fit1 &lt;- train(arr_delay ~ .,\n                 data = uebung_df_small,\n                 method = \"ranger\",\n                 trControl = my_crossval)\n\nAuch hier lässt sie sich wieder die Gütekoeffizienten ausgeben.\n\nrf1_pred &lt;- predict(rf_fit1, newdata = test_df)\nrf1_pred_fit &lt;- postResample(pred = rf1_pred, obs = test_df$arr_delay)\nrf1_pred_fit\n\n     RMSE  Rsquared       MAE \n0.5536435 0.7294604 0.3300325 \n\n\n\nInterpretieren Sie die Koeffizienten des Prädiktors sched_dep_time des Modells lm1 (s. Spalten Estimate bis Pr(&gt;|t|))!\nWelcher Prädiktor des Modells lm1 ist am wichtigsten? Begründen Sie Ihre Antwort!\nInterpretieren Sie die Ausgabe des Objekts lm1_pred_fit!\nWelche Gefahren bzw. Probleme können damit verbunden sein, dass die Analystin die Stichprobe auf \\(n=1000\\) verkleinert?\nVergleichen Sie die Gütekoeffizienten der beiden Modelle im Test-Datensatz!\nDiskutieren Sie einen (möglichen) Grund für die Unterschiede in den Gütekriterien zwischen den beiden Modellen!\n\n         \n\n\nLösung\nInterpretieren Sie die Koeffizienten des Prädiktors sched_dep_time des Modells lm1!\n\nEstimate: Punktschätzer des Einflussgewichts\nStd. Err: Standardfehler, ein Koeffizient zur Beurteilung der (Un-)Genauigkeit des Punktschätzers\nt value: Estimate geteilt durch Std. Error, Signal-Noise-Ratio, z-Wert\nPr: p-Wert\n\nWelcher Prädiktor des Modells lm1 ist am wichtigsten? Begründen Sie Ihre Antwort!\n\nEine Möglichkeit zur Bestimmung der Prädiktorenrelevanz besteht darin, den Prädiktor mit höchstem Absolutwert in der Spalte t value heranzuziehen.\nIn diesem Fall ist das dep_time.\n\nInterpretieren Sie die Ausgabe des Objekts lm1_pred_fit!\n\nEs werden drei Gütekriterien berichtet: R-Quadrat, MSE und MAE. MSE gibt den mittleren Quadratfehler der vorhersage an; MAE den mittleren Absolutfehler.\nJe höher \\(R^2\\) und je geringer MAE bzw. MSE sind, desto besser ist das Modell.\n\nWelche Gefahren bzw. Probleme können damit verbunden sein, dass die Analystin die Stichprobe auf \\(n=1000\\) verkleinert?\n\nKleinere Stichproben schätzen die Population ungenauer.\nDurch die Stichprobenziehung könnten sich die Verteilungen verändern, was wiederum einen Einfluss auf die Vorhersagen haben kann.\n\nVergleichen Sie die Gütekoeffizienten der beiden Modelle im Test-Datensatz!\n\nDas Random-Forest-Modell hat deutlich besser abgeschnitten als das lineare Modell.\n\nDiskutieren Sie einen (möglichen) Grund für die Unterschiede in den Gütekriterien zwischen den beiden Modellen!\n\nEin lineares Modell wird dort gute Vorhersagen leisten, wo seine Voraussetzungen erfüllt sind. Eine wichtige Vorausssetzung sind lineare Beziehungen der Prädiktoren zum Kriterium.\nHier könnte der Fall vorliegen, dass die Beziehungen nicht linear sind, so dass ein Modell - wie das Random-Forest-Modell - das nicht auf lineare Beziehungen abzielt, bessere Vorhersagen treffen kann.\n\n\nsol &lt;- \"s. text\"\n\n\nCategories:\n\nds1\ntidymodels\nstatlearning\nstring"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-04/Verteilungen-Quiz-04.html",
    "href": "posts/Verteilungen-Quiz-04/Verteilungen-Quiz-04.html",
    "title": "Verteilungen-Quiz-04",
    "section": "",
    "text": "Ist folgende Aussage wahr?\nIst eine stetige Verteilung symmetrisch, dann ist sie normalverteilt.\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-04/Verteilungen-Quiz-04.html#answerlist",
    "href": "posts/Verteilungen-Quiz-04/Verteilungen-Quiz-04.html#answerlist",
    "title": "Verteilungen-Quiz-04",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-04/Verteilungen-Quiz-04.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-04/Verteilungen-Quiz-04.html#answerlist-1",
    "title": "Verteilungen-Quiz-04",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/Diamonds-Histogramm-Vergleich2/Diamonds-Histogramm-Vergleich2.html",
    "href": "posts/Diamonds-Histogramm-Vergleich2/Diamonds-Histogramm-Vergleich2.html",
    "title": "Diamonds-Histogramm-Vergleich2",
    "section": "",
    "text": "Die Daten beziehen sich auf den Datensatz diamonds und sind hier einzusehen bzw. können bei Interesse dort heruntergeladen werden.\nAlternativ kann der Datensatz von hier bezogen werden.\n\n\n\n\n\n\n\n\n\n\n\n\nIm Diagramm A wird ein Maß der zentralen Tendenz gruppenbezogen gezeigt, also den jeweiligen Kennwert der Gruppe (Schliffart) wiedergegeben.\nIm Diagramm B wird die Gesamtverteilung über die drei Gruppen hinweg (in hellgrau) dargestellt; in den kräftigeren Farbtönen wird die Verteilung pro Gruppe (Schliffart) dargestellt.\nInsgesamt sind die Verteilung linksschief.\nInsgesamt sind die Verteilung rechtssteil."
  },
  {
    "objectID": "posts/Diamonds-Histogramm-Vergleich2/Diamonds-Histogramm-Vergleich2.html#answerlist",
    "href": "posts/Diamonds-Histogramm-Vergleich2/Diamonds-Histogramm-Vergleich2.html#answerlist",
    "title": "Diamonds-Histogramm-Vergleich2",
    "section": "",
    "text": "Im Diagramm A wird ein Maß der zentralen Tendenz gruppenbezogen gezeigt, also den jeweiligen Kennwert der Gruppe (Schliffart) wiedergegeben.\nIm Diagramm B wird die Gesamtverteilung über die drei Gruppen hinweg (in hellgrau) dargestellt; in den kräftigeren Farbtönen wird die Verteilung pro Gruppe (Schliffart) dargestellt.\nInsgesamt sind die Verteilung linksschief.\nInsgesamt sind die Verteilung rechtssteil."
  },
  {
    "objectID": "posts/Diamonds-Histogramm-Vergleich2/Diamonds-Histogramm-Vergleich2.html#answerlist-1",
    "href": "posts/Diamonds-Histogramm-Vergleich2/Diamonds-Histogramm-Vergleich2.html#answerlist-1",
    "title": "Diamonds-Histogramm-Vergleich2",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\nFalsch\nFalsch\n\n\nCategories:\n\nvis\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/chatgpt-sentiment-loop-all/chatgpt-sentiment-loop-all.html",
    "href": "posts/chatgpt-sentiment-loop-all/chatgpt-sentiment-loop-all.html",
    "title": "chatgpt-sentiment-loop-all",
    "section": "",
    "text": "Fragen Sie ChatGPT via API zum Sentiment der Texte aus dem Germeval-2018-Datensatz (Test).\nHinweise:\n\nBeachten Sie die Standardhinweise des Datenwerks.\nNutzen Sie Python, nicht R.\nDas Verwenden der OpenAI-API kostet Geld. 💸 Informieren Sie sich vorab über die Preise von OpenAI. Um auf die API zugreifen zu können, müssen Sie sich ein Konto angelegt haben und über ein Guthaben verfügen. Sie können unter https://platform.openai.com/usage Ihre Kosten prüfen."
  },
  {
    "objectID": "posts/chatgpt-sentiment-loop-all/chatgpt-sentiment-loop-all.html#achtung",
    "href": "posts/chatgpt-sentiment-loop-all/chatgpt-sentiment-loop-all.html#achtung",
    "title": "chatgpt-sentiment-loop-all",
    "section": "Achtung",
    "text": "Achtung\n\nOpenAI hat eine neue API (Stand: 2023-11-23), V1.3.5. Der Code der alten API bricht. 💔 \\(\\square\\)"
  },
  {
    "objectID": "posts/chatgpt-sentiment-loop-all/chatgpt-sentiment-loop-all.html#setup",
    "href": "posts/chatgpt-sentiment-loop-all/chatgpt-sentiment-loop-all.html#setup",
    "title": "chatgpt-sentiment-loop-all",
    "section": "Setup",
    "text": "Setup\nDie richtige venv nutzen:\n\nlibrary(reticulate)\n#virtualenv_create(\"chatgpt\")\nuse_virtualenv(\"chatgpt\")\n\nCheck zu Python:\n\nreticulate::py_config()\n\npython:         /Users/sebastiansaueruser/.virtualenvs/chatgpt/bin/python\nlibpython:      /Users/sebastiansaueruser/.pyenv/versions/3.11.1/lib/libpython3.11.dylib\npythonhome:     /Users/sebastiansaueruser/.virtualenvs/chatgpt:/Users/sebastiansaueruser/.virtualenvs/chatgpt\nversion:        3.11.1 (main, Oct  4 2023, 18:12:06) [Clang 15.0.0 (clang-1500.0.40.1)]\nnumpy:          /Users/sebastiansaueruser/.virtualenvs/chatgpt/lib/python3.11/site-packages/numpy\nnumpy_version:  1.26.2\n\nNOTE: Python version was forced by use_python() function\n\n\nGgf. noch Module installieren:\n\n#reticulate::py_install(\"pandas\")\n#py_install(\"tiktoken\")\n#py_install(\"datar\")\n#py_install(\"scikit-learn\")"
  },
  {
    "objectID": "posts/chatgpt-sentiment-loop-all/chatgpt-sentiment-loop-all.html#r-pakete-und-python-module",
    "href": "posts/chatgpt-sentiment-loop-all/chatgpt-sentiment-loop-all.html#r-pakete-und-python-module",
    "title": "chatgpt-sentiment-loop-all",
    "section": "R-Pakete und Python-Module",
    "text": "R-Pakete und Python-Module\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\nModule importieren:\n\nfrom openai import OpenAI\nimport pandas as pd\nimport numpy as np\nimport time\nfrom datetime import datetime\n#from sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\n\nVersionen der importierten Module:\n\npd.__version__\n\n'2.1.3'\n\n\n\n```{zsh openai-version-zsh}\npip list | grep openai\n```\n\n\n[notice] A new release of pip is available: 23.3.1 -&gt; 23.3.2\n[notice] To update, run: pip install --upgrade pip\nopenai             1.3.5\n\n\nWir brauchen &gt;= 1.35.\nDer Operator | ist die “Pfeife” der Kommandozeile, also sozusagen der “UND-DANN-Befehl”."
  },
  {
    "objectID": "posts/chatgpt-sentiment-loop-all/chatgpt-sentiment-loop-all.html#daten",
    "href": "posts/chatgpt-sentiment-loop-all/chatgpt-sentiment-loop-all.html#daten",
    "title": "chatgpt-sentiment-loop-all",
    "section": "Daten",
    "text": "Daten\nDaten importieren:\n\ncsv_file_path_test = 'https://github.com/sebastiansauer/pradadata/raw/master/data-raw/germeval_test.csv'\n\ngermeval_test = pd.read_csv(csv_file_path_test)\n\nDie ersten paar Texte herausziehen:\n\nstart_pos = 0\nend_pos = 3531\ntweets = germeval_test[\"text\"].iloc[start_pos:(end_pos+1)].tolist()"
  },
  {
    "objectID": "posts/chatgpt-sentiment-loop-all/chatgpt-sentiment-loop-all.html#prompt",
    "href": "posts/chatgpt-sentiment-loop-all/chatgpt-sentiment-loop-all.html#prompt",
    "title": "chatgpt-sentiment-loop-all",
    "section": "Prompt",
    "text": "Prompt\nPrompt definieren:\n\nprompt_stem  = \"Als KI mit Exertise in natürlicher Sprache und Emotionserkennung ist es Ihre Aufgabe, das Sentiment des folgenden Textes einzuschätzen. Bitte antworten Sie nur mit einem einzigen Wort, entweder 'positiv', 'neutral' oder 'negativ'. Ihre Antwort soll Ihre Insgesamt-Einschätzung zum Sentiments des Textes zusammenfassen. Nach dem Doppelpunkt folgt der Text, dessen Sentiment Sie einschätzen sollen: \"\n\nGute Prompts können helfen, gute Antworten vom Modell zu erhalten.\nMit “List Comprehension” können wir die Tweets jeweils mit dem Prompt verknüpfen:\n\nprompts = [prompt_stem + tweet for tweet in tweets]\nprompts[0]\n\n\"Als KI mit Exertise in natürlicher Sprache und Emotionserkennung ist es Ihre Aufgabe, das Sentiment des folgenden Textes einzuschätzen. Bitte antworten Sie nur mit einem einzigen Wort, entweder 'positiv', 'neutral' oder 'negativ'. Ihre Antwort soll Ihre Insgesamt-Einschätzung zum Sentiments des Textes zusammenfassen. Nach dem Doppelpunkt folgt der Text, dessen Sentiment Sie einschätzen sollen: Meine Mutter hat mir erzählt, dass mein Vater einen Wahlkreiskandidaten nicht gewählt hat, weil der gegen die Homo-Ehe ist ☺\"\n\n\nCheck: Wie viele Elemente hat die Liste prompts?\n\nlen(prompts)\n\n3532\n\n\nLaut OpenAI kostet 1k Token für das Modell gpt-3.5-turbo-1106 $0.001."
  },
  {
    "objectID": "posts/chatgpt-sentiment-loop-all/chatgpt-sentiment-loop-all.html#authentifizieren",
    "href": "posts/chatgpt-sentiment-loop-all/chatgpt-sentiment-loop-all.html#authentifizieren",
    "title": "chatgpt-sentiment-loop-all",
    "section": "Authentifizieren",
    "text": "Authentifizieren\nAnmelden bei OpenAI:\n\nclient = OpenAI()\n\n\n\n\n\n\n\nNote\n\n\n\nDieses Anmeldeverfahren setzt voraus, dass in .Renviron die Variable OPENAI_API_KEY hinterlegt ist. \\(\\square\\)\n\n\nAnfrage an die API, in eine Funktion gepackt:\n\ndef get_completion(prompt, client_instance, model=\"gpt-3.5-turbo\"):\n  messages = [{\"role\": \"user\", \"content\": prompt}]\n  response = client_instance.chat.completions.create(\n    model=model,\n    messages=messages,\n    max_tokens=50,\n    temperature=0,\n  )\n  return response.choices[0].message.content"
  },
  {
    "objectID": "posts/chatgpt-sentiment-loop-all/chatgpt-sentiment-loop-all.html#api-anfragen",
    "href": "posts/chatgpt-sentiment-loop-all/chatgpt-sentiment-loop-all.html#api-anfragen",
    "title": "chatgpt-sentiment-loop-all",
    "section": "API anfragen",
    "text": "API anfragen\nUnd jetzt als Schleife. Ergebnisliste anlegen, am Anfang noch leer:\n\npredicted_values = []\n\n\nstart_time = time.time()\n\nfor prompt in prompts:\n  result = get_completion(prompt, client) \n  predicted_values.append(result)\n\nend_time = time.time()\nend_time - start_time\n\nVoilà:\n\nprint(predicted_values[:5])\n\n[]"
  },
  {
    "objectID": "posts/chatgpt-sentiment-loop-all/chatgpt-sentiment-loop-all.html#als-csv-speichern",
    "href": "posts/chatgpt-sentiment-loop-all/chatgpt-sentiment-loop-all.html#als-csv-speichern",
    "title": "chatgpt-sentiment-loop-all",
    "section": "Als CSV speichern",
    "text": "Als CSV speichern\n\nid_seq = [i for i in range(start_pos, end_pos + 1)]\npredicted_values_df = pd.DataFrame(id_seq, columns = [\"id\"])\npredicted_values_df[\"pred\"] = predicted_values\n\nnow = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\ncsv_output_name = \"germeval_test_preds_at_\" + now\npredicted_values_df.to_csv(csv_output_name)"
  },
  {
    "objectID": "posts/chatgpt-sentiment-loop-all/chatgpt-sentiment-loop-all.html#oder-vorhersagen-aus-csv-importieren",
    "href": "posts/chatgpt-sentiment-loop-all/chatgpt-sentiment-loop-all.html#oder-vorhersagen-aus-csv-importieren",
    "title": "chatgpt-sentiment-loop-all",
    "section": "Oder Vorhersagen aus CSV importieren",
    "text": "Oder Vorhersagen aus CSV importieren\n\npreds_path = 'https://raw.githubusercontent.com/sebastiansauer/Datenwerk2/main/posts/chatgpt-sentiment-loop-all/germeval_test_preds_at_2023-12-20%2014%3A06%3A00'\n\npreds = pd.read_csv(preds_path)\n\npreds.head()\n\n   Unnamed: 0  id     pred\n0           0   0  neutral\n1           1   1  negativ\n2           2   2  negativ\n3           3   3  neutral\n4           4   4  negativ\n\n\nMan kann eine Python-Variable an R übergeben:\n\npreds_r &lt;- py$preds"
  },
  {
    "objectID": "posts/chatgpt-sentiment-loop-all/chatgpt-sentiment-loop-all.html#vorhersagen-predictions-betrachten",
    "href": "posts/chatgpt-sentiment-loop-all/chatgpt-sentiment-loop-all.html#vorhersagen-predictions-betrachten",
    "title": "chatgpt-sentiment-loop-all",
    "section": "Vorhersagen (Predictions) betrachten",
    "text": "Vorhersagen (Predictions) betrachten\nZählen wir mal kurz aus:\n\npreds_r |&gt; \n  count(pred) |&gt; \n  slice(3:5)  # zwei komische, kaputte Zeilen, weg damit\n\n     pred    n\n1 negativ 1792\n2 neutral 1354\n3 positiv  384\n\n\nOder in Python:\n\npreds[\"pred\"].value_counts()\n\npred\nnegativ                                                                             1792\nneutral                                                                             1354\npositiv                                                                              384\nmuss. #AfD #Grenzschutz #Deutschland: negativ                                          1\nkommen, in den eigenen vier Wänden haben. Das ist doch wohl klar. #ltwlsa #ltw21       1\nName: count, dtype: int64\n\n\nPuh, das ist ein bisschen was kaput gegangen."
  },
  {
    "objectID": "posts/chatgpt-sentiment-loop-all/chatgpt-sentiment-loop-all.html#predictions-reparieren",
    "href": "posts/chatgpt-sentiment-loop-all/chatgpt-sentiment-loop-all.html#predictions-reparieren",
    "title": "chatgpt-sentiment-loop-all",
    "section": "Predictions reparieren",
    "text": "Predictions reparieren\n\nallowed_preds = [\"positiv\", \"neutral\", \"negativ\"]\npreds.loc[~preds[\"pred\"].isin(allowed_preds), \"pred\"] = np.nan\n\nCheck:\n\npreds[\"pred\"].value_counts()\n\npred\nnegativ    1792\nneutral    1354\npositiv     384\nName: count, dtype: int64\n\n\nPasst!"
  },
  {
    "objectID": "posts/chatgpt-sentiment-loop-all/chatgpt-sentiment-loop-all.html#scoring-vorbereiten",
    "href": "posts/chatgpt-sentiment-loop-all/chatgpt-sentiment-loop-all.html#scoring-vorbereiten",
    "title": "chatgpt-sentiment-loop-all",
    "section": "Scoring vorbereiten",
    "text": "Scoring vorbereiten\nWas waren noch mal die Variablen unser Tabelle?\n\ngermeval_test.columns\n\nIndex(['id', 'text', 'c1', 'c2'], dtype='object')\n\n\nDie ersten paar Werte:\n\ngermeval_test.head()\n\n   id                                               text       c1     c2\n0   1  Meine Mutter hat mir erzählt, dass mein Vater ...    OTHER  OTHER\n1   2  @Tom174_ @davidbest95 Meine Reaktion; |LBR| Ni...    OTHER  OTHER\n2   3  #Merkel rollt dem Emir von #Katar, der islamis...    OTHER  OTHER\n3   4  „Merle ist kein junges unschuldiges Mädchen“ K...    OTHER  OTHER\n4   5  @umweltundaktiv Asylantenflut bringt eben nur ...  OFFENSE  ABUSE\n\n\nRescore im Test-Set:\n\ndf = germeval_test\ndf[\"c1\"] = df[\"c1\"].replace({\"OFFENSE\": \"negativ\"})\n\ndf[\"c1\"].value_counts()\n\nc1\nOTHER      2330\nnegativ    1202\nName: count, dtype: int64\n\n\nRescore in den Vorhersagen\n\npreds[\"pred\"] = preds[\"pred\"].replace({\"neutral\": \"OTHER\", \"positiv\": \"OTHER\"})\n\npreds[\"pred\"].value_counts()\n\npred\nnegativ    1792\nOTHER      1738\nName: count, dtype: int64\n\n\n\npreds_list = preds[\"pred\"].tolist()\n\nHier ist die Liste der wahren Werte:\n\ny = df[\"c1\"].values.tolist()"
  },
  {
    "objectID": "posts/chatgpt-sentiment-loop-all/chatgpt-sentiment-loop-all.html#scoring",
    "href": "posts/chatgpt-sentiment-loop-all/chatgpt-sentiment-loop-all.html#scoring",
    "title": "chatgpt-sentiment-loop-all",
    "section": "Scoring",
    "text": "Scoring\n\naccuracy = accuracy_score(y, preds_list)\nprint(\"Accuracy:\", accuracy)\n\nAccuracy: 0.7355605889014722\n\n\nOder mit tidymodels; zuerst aufbereiten:\n\ny_truth = as.factor(py$y)\ny_pred = py$preds_list \n\n# replace NAN with NA and convert to factor:\ny_pred = as.character(y_pred) \ny_pred[is.nan(y_pred)] &lt;- NA\ny_pred[!y_pred %in% c(\"negativ\", \"OTHER\")] &lt;- NA\ny_pred &lt;- as.factor(y_pred)\n\ntable(y_pred)\n\ny_pred\nnegativ   OTHER \n   1792    1738 \n\n\n\naccuracy_vec(truth = y_truth,\n             estimate = y_pred)\n\n[1] 0.7359773"
  },
  {
    "objectID": "posts/chatgpt-sentiment-loop-all/chatgpt-sentiment-loop-all.html#fun",
    "href": "posts/chatgpt-sentiment-loop-all/chatgpt-sentiment-loop-all.html#fun",
    "title": "chatgpt-sentiment-loop-all",
    "section": "Fun",
    "text": "Fun\n\nfig &lt;- plot_ly(\n  domain = list(x = c(0, 1), y = c(0, 1)),\n  value = 74,\n  title = list(text = \"Accuracy\"),\n  type = \"indicator\",\n  mode = \"gauge+number\") \nfig &lt;- fig %&gt;%\n  layout(margin = list(l=20,r=30))\n\nfig"
  },
  {
    "objectID": "posts/Klausuren-bestehen/Klausuren-bestehen.html",
    "href": "posts/Klausuren-bestehen/Klausuren-bestehen.html",
    "title": "Klausuren-bestehen",
    "section": "",
    "text": "Aufgabe\nEine Studentin hat zwei Klausuren, \\(A\\) und \\(B\\) geschrieben. Sie schätzt ihre Chancen zu bestehen auf 35% bzw. auf 60%. Unterstellen Sie Unabhängigkeit der Ereignisse.\nAufgabe: Wie groß ist die Chance, mindestens eine der beiden Klausuren zu bestehen?\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n         \n\n\nLösung\n\nPr_A &lt;- .35\nPr_B &lt;- .6\n\nDie Wahrscheinlichkeit, beide Klausuren zu bestehen:\n\nPr_AB &lt;- Pr_A * Pr_B\nPr_AB\n\n[1] 0.21\n\n\nDie Wahrscheinlichkeit, durch beide Klausuren durchzurasseln nennen wir Pr_negA_negB:\n\nPr_NA &lt;- 1 - Pr_A\nPr_NB &lt;- 1 - Pr_B\n\nPr_negA_negB &lt;- Pr_NA * Pr_NB\nPr_negA_negB\n\n[1] 0.26\n\n\nDas Gegenteil von Pr_negA_negB ist, mindestens eine Klausur zu bestehen:\n\nPr_mind1_bestanden &lt;- 1 - Pr_negA_negB\nPr_mind1_bestanden\n\n[1] 0.74\n\n\nDie Lösung lautet 0.74.\n\nCategories:\n\nR\nprobability\nnum"
  },
  {
    "objectID": "posts/Flex-vs-nichtflex-Methode/Flex-vs-nichtflex-Methode.html",
    "href": "posts/Flex-vs-nichtflex-Methode/Flex-vs-nichtflex-Methode.html",
    "title": "Flex-vs-nichtflex-Methode",
    "section": "",
    "text": "Algorithmen des statistischen Lernens lassen sich unterteilen hinsichtlich ihrer Flexibilität; es gibt mehr bzw. weniger flexible Algorithmen.\nWelche der folgenden Aussagen ist in diesem Zusammenhang korrekt?\n\n\n\nBei großer Stichprobe (\\(n\\)) und kleiner Zahl an Prädiktoren (\\(p\\)) haben flexible Algorithmen im Vergleich zu weniger flexiblen Methoden eine geringere Verzerrung (bias).\nBei großer Stichprobe (\\(n\\)) und kleiner Zahl an Prädiktoren (\\(p\\)) haben flexible Algorithmen im Vergleich zu weniger flexiblen Methoden eine höhere Verzerrung (bias).\nBei großer Stichprobe (\\(n\\)) und kleiner Zahl an Prädiktoren (\\(p\\)) haben flexible Algorithmen im Vergleich zu weniger flexiblen Methoden eine kleinere Varianz.\nBei großer Stichprobe (\\(n\\)) und kleiner Zahl an Prädiktoren (\\(p\\)) haben flexible Algorithmen im Vergleich zu weniger flexiblen Methoden eine kleinere Varianz und eine höhere Verzerrung.\nBei großer Stichprobe (\\(n\\)) und kleiner Zahl an Prädiktoren (\\(p\\)) haben flexible Algorithmen im Vergleich zu weniger flexiblen Methoden eine höhere Varianz und eine höhere Verzerrung."
  },
  {
    "objectID": "posts/Flex-vs-nichtflex-Methode/Flex-vs-nichtflex-Methode.html#answerlist",
    "href": "posts/Flex-vs-nichtflex-Methode/Flex-vs-nichtflex-Methode.html#answerlist",
    "title": "Flex-vs-nichtflex-Methode",
    "section": "",
    "text": "Bei großer Stichprobe (\\(n\\)) und kleiner Zahl an Prädiktoren (\\(p\\)) haben flexible Algorithmen im Vergleich zu weniger flexiblen Methoden eine geringere Verzerrung (bias).\nBei großer Stichprobe (\\(n\\)) und kleiner Zahl an Prädiktoren (\\(p\\)) haben flexible Algorithmen im Vergleich zu weniger flexiblen Methoden eine höhere Verzerrung (bias).\nBei großer Stichprobe (\\(n\\)) und kleiner Zahl an Prädiktoren (\\(p\\)) haben flexible Algorithmen im Vergleich zu weniger flexiblen Methoden eine kleinere Varianz.\nBei großer Stichprobe (\\(n\\)) und kleiner Zahl an Prädiktoren (\\(p\\)) haben flexible Algorithmen im Vergleich zu weniger flexiblen Methoden eine kleinere Varianz und eine höhere Verzerrung.\nBei großer Stichprobe (\\(n\\)) und kleiner Zahl an Prädiktoren (\\(p\\)) haben flexible Algorithmen im Vergleich zu weniger flexiblen Methoden eine höhere Varianz und eine höhere Verzerrung."
  },
  {
    "objectID": "posts/Flex-vs-nichtflex-Methode/Flex-vs-nichtflex-Methode.html#answerlist-1",
    "href": "posts/Flex-vs-nichtflex-Methode/Flex-vs-nichtflex-Methode.html#answerlist-1",
    "title": "Flex-vs-nichtflex-Methode",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nstatlearning\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/chatgpt-sentiment-simple/chatgpt-sentiment-simple.html",
    "href": "posts/chatgpt-sentiment-simple/chatgpt-sentiment-simple.html",
    "title": "chatgpt-sentiment-simple",
    "section": "",
    "text": "Aufgabe\nFragen Sie ChatGPT via API zum Sentiment des ersten Texts aus dem Germeval-2018-Datensatz (Train).\n\n\n\n\n\nHinweise:\n\nBeachten Sie die Standardhinweise des Datenwerks.\nNutzen Sie Python, nicht R.\nDas Verwenden der OpenAI-API kostet Geld. 💸 Informieren Sie sich vorab. Um auf die API zugreifen zu können, müssen Sie sich ein Konto angelegt haben und über ein Guthaben verfügen.\n\n         \n\n\nLösung\n\nOpenAI hat eine neue API (Stand: 2023-11-23). Der Code der alten API bricht. 💔 \\(\\square\\)\n\nModule importieren:\n\nfrom openai import OpenAI\n\nAnmelden bei OpenAI:\n\nclient = OpenAI()\n\n\n\n\n\n\n\nNote\n\n\n\nDieses Verfahren setzt voraus, dass in .Renviron die Variable OPENAI_API_KEY hinterlegt ist. \\(\\square\\)\n\n\nTextschnipsel, das zu klassifizieren ist:\n\ntext = \"@corinnamilborn Liebe Corinna, wir würden dich gerne als Moderatorin für uns gewinnen! Wärst du begeisterbar?\"\n\nPrompt definieren:\n\nmy_prompt  = f\"Analysieren Sie das Sentiment des folgenden Texts:\\n{text}\"\n\nAnfrage an die API, in eine Funktion gepackt:\n\ndef get_completion(prompt, client_instance, model=\"gpt-3.5-turbo\"):\n  messages = [{\"role\": \"user\", \"content\": prompt}]\n  response = client_instance.chat.completions.create(\n  model=model,\n  messages=messages,\n  max_tokens=50,\n  temperature=0,\n  )\n  return response.choices[0].message.content\n\nUnd los:\n\nget_completion(my_prompt, client) \n\n'Basierend auf dem gegebenen Text kann das Sentiment als positiv eingestuft werden. Der Text drückt Interesse und Begeisterung aus, indem er Corinna Milborn als Moderatorin gewinnen möchte.'"
  },
  {
    "objectID": "posts/modellguete-testset/modellguete-testset.html",
    "href": "posts/modellguete-testset/modellguete-testset.html",
    "title": "modellguete-testset",
    "section": "",
    "text": "Berechnen Sie die Modellgüte (RMSE) im Test-Sample.\nGehen Sie von folgenden Annahmen aus.\n\nDieses Test-Sample\nAV: count\nDieses Train-Sample\nGehen Sie als Vorhersage vom Mittelwert der AV im Train-Sample aus (für alle Beobachtungen im Test-Sample).\n\nHinweise:\n\nHier finden Sie ein Data-Dictionary.\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks."
  },
  {
    "objectID": "posts/modellguete-testset/modellguete-testset.html#setup",
    "href": "posts/modellguete-testset/modellguete-testset.html#setup",
    "title": "modellguete-testset",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.0.8\n✔ dials        1.2.0     ✔ rsample      1.2.0\n✔ dplyr        1.1.3     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\n\n\nd_train &lt;- read.csv(\"https://raw.githubusercontent.com/sebastiansauer/yacsda-bikerental/main/data/bikeshare_train.csv\")\nd_test &lt;- read.csv(\"https://raw.githubusercontent.com/sebastiansauer/yacsda-bikerental/main/data/bikeshare_control.csv\")"
  },
  {
    "objectID": "posts/modellguete-testset/modellguete-testset.html#mittelwert-der-av-im-train-sample-berechnen",
    "href": "posts/modellguete-testset/modellguete-testset.html#mittelwert-der-av-im-train-sample-berechnen",
    "title": "modellguete-testset",
    "section": "Mittelwert der AV im Train-Sample berechnen",
    "text": "Mittelwert der AV im Train-Sample berechnen\n\nmean_count_train_sample &lt;- \n  d_train |&gt; \n  summarise(count_avg = mean(count))\n\nmean_count_train_sample\n\n  count_avg\n1  703.7913\n\n\n\nd_test &lt;-\n  d_test |&gt; \n  mutate(pred = 704)\n\nAnstelle von 704 könnten Sie auch Ihre eigenen Vorhersagen Ihrer Modelle einsetzen, etwa:\n\nd_test &lt;-\n  d_test |&gt; \n  mutate(pred = meine_vorhersagen)"
  },
  {
    "objectID": "posts/modellguete-testset/modellguete-testset.html#modellgüte-im-test-sample-berechnen",
    "href": "posts/modellguete-testset/modellguete-testset.html#modellgüte-im-test-sample-berechnen",
    "title": "modellguete-testset",
    "section": "Modellgüte im Test-Sample berechnen",
    "text": "Modellgüte im Test-Sample berechnen\n\nd_test |&gt; \n  rmse(truth = count,\n       estimate = pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        646.\n\n\nFür R-Quadrat geht das analog:\n\nd_test |&gt; \n  rsq(truth = count,\n       estimate = pred)\n\nWarning: A correlation computation is required, but `estimate` is constant and\nhas 0 standard deviation, resulting in a divide by 0 error. `NA` will be\nreturned.\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard          NA\n\n\nLeider ist das R-Quadrat in diesem Fall (per Definition) Null: Der Mittelwert als Vorhersagewert ist was “R-Quadrat gleich Null” meint.\n(Darüber hinaus wird das R-Quadrat hier auf Basis der Korrelation berechnet, und wir haben einen konstanten Wert bei pred).\n\nCategories:\n\nregression\nmodelling\nperformance\nrmse\nstring"
  },
  {
    "objectID": "posts/lm1/lm1.html",
    "href": "posts/lm1/lm1.html",
    "title": "lm1",
    "section": "",
    "text": "Laden Sie den Datensatz mtcars aus dieser Quelle.\nBerechnen Sie eine Regression mit mpg als Ausgabevariable und hp als Eingabevariable!\nWelche Aussage ist für diese Analyse richtig?\n\n\n\nmpg und hp sind positiv korreliert laut dem Modell.\nDer Achsenabschnitt ist nahe Null.\nDie Analyse beinhaltet einen nominal skalierten Prädiktor.\nDas geschätzte Betagewicht für hp liegt bei 30.099.\nDas geschätzte Betagewicht für hp liegt bei -0.068."
  },
  {
    "objectID": "posts/lm1/lm1.html#answerlist",
    "href": "posts/lm1/lm1.html#answerlist",
    "title": "lm1",
    "section": "",
    "text": "mpg und hp sind positiv korreliert laut dem Modell.\nDer Achsenabschnitt ist nahe Null.\nDie Analyse beinhaltet einen nominal skalierten Prädiktor.\nDas geschätzte Betagewicht für hp liegt bei 30.099.\nDas geschätzte Betagewicht für hp liegt bei -0.068."
  },
  {
    "objectID": "posts/lm1/lm1.html#answerlist-1",
    "href": "posts/lm1/lm1.html#answerlist-1",
    "title": "lm1",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nFalsch\nRichtig\n\n\nCategories:\n\nregression\nlm\nstats-nutshell\nschoice"
  },
  {
    "objectID": "posts/lm-mario2/lm-mario2.html",
    "href": "posts/lm-mario2/lm-mario2.html",
    "title": "lm-mario2",
    "section": "",
    "text": "Sagen Sie den Verkaufspreis vorher für ein Spiel mit 3 Euro Versandkosten (ship_pr)!"
  },
  {
    "objectID": "posts/lm-mario2/lm-mario2.html#setup",
    "href": "posts/lm-mario2/lm-mario2.html#setup",
    "title": "lm-mario2",
    "section": "Setup",
    "text": "Setup\n\nmariokart &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\")\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.6.0 (red = needs update)\n✔ bayestestR  0.13.1   ✔ correlation 0.8.4 \n✔ datawizard  0.9.0    ✔ effectsize  0.8.6 \n✔ insight     0.19.6   ✔ modelbased  0.8.6 \n✔ performance 0.10.8   ✔ parameters  0.21.3\n✔ report      0.5.7    ✖ see         0.8.0 \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`."
  },
  {
    "objectID": "posts/lm-mario2/lm-mario2.html#regressionsgerade-berechnen",
    "href": "posts/lm-mario2/lm-mario2.html#regressionsgerade-berechnen",
    "title": "lm-mario2",
    "section": "Regressionsgerade berechnen",
    "text": "Regressionsgerade berechnen\n\nlm_mariokart &lt;- lm(total_pr ~ ship_pr, data = mariokart) # \"lm\" wie *l*lineares *M*odell, also eine Gerade.\nlm_mariokart\n\n\nCall:\nlm(formula = total_pr ~ ship_pr, data = mariokart)\n\nCoefficients:\n(Intercept)      ship_pr  \n     36.246        4.337"
  },
  {
    "objectID": "posts/lm-mario2/lm-mario2.html#vorhersagen",
    "href": "posts/lm-mario2/lm-mario2.html#vorhersagen",
    "title": "lm-mario2",
    "section": "Vorhersagen",
    "text": "Vorhersagen\nVorhersagen funktionieren mit dem Befehl predict:\n\nneues_spiel &lt;- tibble(ship_pr = 3)  # oder \"data.frame\" statt \"tibble\"\nneues_spiel\n\n# A tibble: 1 × 1\n  ship_pr\n    &lt;dbl&gt;\n1       3\n\n\n\npredict(lm_mariokart, neues_spiel)  # predicte mir den Verkaufspreis\n\n      1 \n49.2572 \n\n\n\nCategories:\n\nR\nlm\npredict\nnum"
  },
  {
    "objectID": "posts/Wertzuweisen_mc/Wertzuweisen_mc.html",
    "href": "posts/Wertzuweisen_mc/Wertzuweisen_mc.html",
    "title": "Wertzuweisen_mc",
    "section": "",
    "text": "Welche der folgenden Syntax-Varianten ist/sind formal (“syntaktisch”) korrekt?\n\n\n\nloesung &lt;-42\nloesung &lt; - 42\nloesung-&gt;42\nloesung==42\nloesung&lt;-\"42\""
  },
  {
    "objectID": "posts/Wertzuweisen_mc/Wertzuweisen_mc.html#answerlist",
    "href": "posts/Wertzuweisen_mc/Wertzuweisen_mc.html#answerlist",
    "title": "Wertzuweisen_mc",
    "section": "",
    "text": "loesung &lt;-42\nloesung &lt; - 42\nloesung-&gt;42\nloesung==42\nloesung&lt;-\"42\""
  },
  {
    "objectID": "posts/Wertzuweisen_mc/Wertzuweisen_mc.html#answerlist-1",
    "href": "posts/Wertzuweisen_mc/Wertzuweisen_mc.html#answerlist-1",
    "title": "Wertzuweisen_mc",
    "section": "Answerlist",
    "text": "Answerlist\n\nRichtig\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nR\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/regex02/regex02.html",
    "href": "posts/regex02/regex02.html",
    "title": "regex02",
    "section": "",
    "text": "Aufgabe\nMatchen Sie Elemente des Vektors txt3, die nur aus Ziffern bestehen:\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n         \n\n\nLösung\n\n\n[1] FALSE FALSE  TRUE  TRUE FALSE FALSE\n\n\nMan kann z.B. auch \\\\d verwenden anstelle von [:digit:].\nSicherlich gibt es noch mehrere weitere Lösungen.\n\nCategories:\n\ntextmining\nregex\nstring"
  },
  {
    "objectID": "posts/tidy1/tidy1.html",
    "href": "posts/tidy1/tidy1.html",
    "title": "tidy1",
    "section": "",
    "text": "Das Konzept von “tidy” Daten (“Tidyformat”) spielt in der Datenanalyse eine wichtige Rolle.\nBetrachten Sie die Tabellen im Folgenden. Welche ist “tidy”?\nHinweise:\n\nAlle Variablen sollen nicht konstant sein, also mehr als einen uniquen Wert aufweisen.\nAlle Variablen sollen keine fehlenden Werte aufweisen, also komplett sein.\nAlle Variablen sollen numerisch sein.\n\nTabelle A:\n\n\n\n\n\n\n\n\nTabelle A\n\n\ngroup\ny\nid1\nid2\n\n\n\n\n1\n10\n1\n2\n\n\n2\n20\n2\n2\n\n\n1\n30\n3\n2\n\n\n2\n40\n4\n2\n\n\n\n\n\n\n\nTabelle B:\n\n\n\n\n\n\n\n\nTabelle B\n\n\ngroup\ny\nid1\nid2\n\n\n\n\n1\n10\n1\nA\n\n\n2\n20\n2\nB\n\n\n1\n30\n3\nC\n\n\n2\n40\n4\nD\n\n\n\n\n\n\n\nTabelle C:\n\n\n\n\n\n\n\n\nTabelle C\n\n\ngroup\ny\nid1\nid2\n\n\n\n\n1\n10\n1\nid2\n\n\n2\n20\n2\nid2\n\n\n1\n30\n3\n1,2\n\n\n2\n40\n4\nid2\n\n\n\n\n\n\n\nTabelle D:\n\n\n\n\n\n\n\n\nTabelle D\n\n\ngroup\ny\nid1\nid2\n\n\n\n\n1\n10\n1\n1\n\n\n2\n20\n2\n1\n\n\n1\n30\n3\n2\n\n\n2\n40\n4\n2\n\n\n\n\n\n\n\nTabelle E:\n\n\n\n\n\n\n\n\nTabelle E\n\n\ngroup\n\ny\nid1\nid2\n\n\n\n\n1\nNA\n10\n1\n1\n\n\n2\nNA\n20\n2\n1\n\n\n1\nNA\n30\n3\n2\n\n\n2\nNA\n40\n4\n2\n\n\n\n\n\n\n\n\n\n\nTabelle A\nTabelle B\nTabelle C\nTabelle D\nTabelle E"
  },
  {
    "objectID": "posts/tidy1/tidy1.html#answerlist",
    "href": "posts/tidy1/tidy1.html#answerlist",
    "title": "tidy1",
    "section": "",
    "text": "Tabelle A\nTabelle B\nTabelle C\nTabelle D\nTabelle E"
  },
  {
    "objectID": "posts/tidy1/tidy1.html#answerlist-1",
    "href": "posts/tidy1/tidy1.html#answerlist-1",
    "title": "tidy1",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch. Eine Spalte soll nicht aus einem uniquen Wert bestehen.\nFalsch. Alle Werte sollen numerisch sein\nFalsch. Die Spalte id2weißt einen nicht erlaubten Wert auf.\nRichtig. Das ist ein ‘tidy Tibble’.\nFalsch. In einem Tidy-Tibble darf keine leere Spalte vorkommen.\n\n\nCategories:\n\ntidy\ndatawrangling\nschoice"
  },
  {
    "objectID": "posts/wrangle7/wrangle7.html",
    "href": "posts/wrangle7/wrangle7.html",
    "title": "wrangle7",
    "section": "",
    "text": "Welche Aussage zur Funktion filter() aus dem R-Paket dplyr ist richtig?\n\n\n\nfilter() filtert (behält) Zeilen, für die eine Prüfung TRUE ergibt\nfilter() filtert (behält)Zeilen, für die eine Prüfung FALSE ergibt\nfilter() filtert (behält) Zeilen, für die eine Prüfung TRUE oder NA ergibt\nMöchte man nur nicht-fehlende Zeilen aus der Variable x aus dem Dataframe df filtern (behalten), so formuliert man filter(df, x == NA).\nMöchte man nur nicht-fehlende Zeilen aus der Variable x aus dem Dataframe df filtern (behalten), so formuliert man filter(df, is.na(x))."
  },
  {
    "objectID": "posts/wrangle7/wrangle7.html#answerlist",
    "href": "posts/wrangle7/wrangle7.html#answerlist",
    "title": "wrangle7",
    "section": "",
    "text": "filter() filtert (behält) Zeilen, für die eine Prüfung TRUE ergibt\nfilter() filtert (behält)Zeilen, für die eine Prüfung FALSE ergibt\nfilter() filtert (behält) Zeilen, für die eine Prüfung TRUE oder NA ergibt\nMöchte man nur nicht-fehlende Zeilen aus der Variable x aus dem Dataframe df filtern (behalten), so formuliert man filter(df, x == NA).\nMöchte man nur nicht-fehlende Zeilen aus der Variable x aus dem Dataframe df filtern (behalten), so formuliert man filter(df, is.na(x))."
  },
  {
    "objectID": "posts/wrangle7/wrangle7.html#answerlist-1",
    "href": "posts/wrangle7/wrangle7.html#answerlist-1",
    "title": "wrangle7",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\neda\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/ttest-skalenniveau/ttest-skalenniveau.html",
    "href": "posts/ttest-skalenniveau/ttest-skalenniveau.html",
    "title": "ttest-skalenniveau",
    "section": "",
    "text": "Der t-Test ist ein inferenzstatistisches Verfahren des Frequentismus. Welches Skalenniveau passt zu diesem Verfahren?\nHinweisse:\n\nDie folgende Abbildung gibt Tipps.\nInformationen, die zur Lösung einer Aufgabe nicht nötig sind, sollte man ignorieren.\n\n\n\n\n\n\n\n\n\n\n\n\n\nUV: nominal (mehrstufig), AV: metrisch\nUV: metrisch, AV: nominal (zweistufig)\nUV: nominal (mehrstufig), AV: nominal (mehrstufig)\nUV: metrisch, AV: nominal (zweistufig)\nUV: nominal (zweistufig), AV: metrisch"
  },
  {
    "objectID": "posts/ttest-skalenniveau/ttest-skalenniveau.html#answerlist",
    "href": "posts/ttest-skalenniveau/ttest-skalenniveau.html#answerlist",
    "title": "ttest-skalenniveau",
    "section": "",
    "text": "UV: nominal (mehrstufig), AV: metrisch\nUV: metrisch, AV: nominal (zweistufig)\nUV: nominal (mehrstufig), AV: nominal (mehrstufig)\nUV: metrisch, AV: nominal (zweistufig)\nUV: nominal (zweistufig), AV: metrisch"
  },
  {
    "objectID": "posts/ttest-skalenniveau/ttest-skalenniveau.html#answerlist-1",
    "href": "posts/ttest-skalenniveau/ttest-skalenniveau.html#answerlist-1",
    "title": "ttest-skalenniveau",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nFalsch\nWahr\n\n\nCategories:\n\nttest\nregression\nvariable-levels"
  },
  {
    "objectID": "posts/nasa07/index.html",
    "href": "posts/nasa07/index.html",
    "title": "nasa07",
    "section": "",
    "text": "Aufgabe\nViele Quellen berichten Klimadaten unserer Erde, z.B. auch National Aeronautics and Space Administration - Goddard Institute for Space Studies.\nVon dieser Quelle beziehen wir diesen Datensatz.\nDie Datensatz sind auf der Webseite wie folgt beschrieben:\nTables of Global and Hemispheric Monthly Means and Zonal Annual Means\nCombined Land-Surface Air and Sea-Surface Water Temperature Anomalies (Land-Ocean Temperature Index, L-OTI)\nThe following are plain-text files in tabular format of temperature anomalies, i.e. deviations from the corresponding 1951-1980 means.\n\nGlobal-mean monthly, seasonal, and annual means, 1880-present, updated through most recent month: TXT, CSV\n\nStarten Sie zunächst das R-Paket tidyverse falls noch nicht geschehen.\n\nlibrary(tidyverse)\n\nImportieren Sie dann die Daten:\n\ndata_path &lt;- \"https://data.giss.nasa.gov/gistemp/tabledata_v4/GLB.Ts+dSST.csv\"\nd &lt;- read_csv(data_path, skip = 1)\n\nWir lassen die 1. Zeile des Datensatzes aus (Argument skip), da dort Metadaten stehen, also keine Daten, sondern Informationen (Daten) zu den eigentlichen Daten.\nAufgabe\nBerechnen Sie die folgende Statistiken pro Dekade:\n\nMittelwert der Temperatur im Januar\nSD der Temperatur im Januar\n\nHinweise:\n\nSie müssen zuerst die Dekade als neue Spalte berechnen.\n\n         \n\n\nLösung\nDekade (Jahrzehnt) berechnen:\n\nd &lt;-\n  d %&gt;% \n  mutate(decade = round(Year/10))\n\nDas ist ein möglicher Weg, um aus einer Jahreszahl die Dekade zu berechnen.\nStatistiken pro Dekade:\n\nd_summarized &lt;- \n  d %&gt;% \n  group_by(decade) %&gt;% \n  summarise(temp_mean = mean(Jan),\n            temp_sd = sd(Jan))\n\nd_summarized\n\n\n\n\n\n\n\n\n\ndecade\ntemp_mean\ntemp_sd\n\n\n\n\n188\n−0.20\n0.24\n\n\n189\n−0.44\n0.22\n\n\n190\n−0.26\n0.16\n\n\n191\n−0.39\n0.22\n\n\n192\n−0.28\n0.15\n\n\n193\n−0.14\n0.22\n\n\n194\n0.03\n0.21\n\n\n195\n−0.05\n0.18\n\n\n196\n0.03\n0.15\n\n\n197\n−0.07\n0.17\n\n\n198\n0.21\n0.19\n\n\n199\n0.35\n0.13\n\n\n200\n0.52\n0.19\n\n\n201\n0.64\n0.21\n\n\n202\n0.98\n0.15\n\n\n\n\n\n\n\nZur Veranschaulichung visualisieren wir die Ergebnisse:\n\nd_summarized %&gt;% \n  pivot_longer(-decade) %&gt;% \n  ggplot(aes(x = decade, y = value)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~ name)\n\n\n\n\n\n\n\n\nAlternativ können Sie zum Visualisieren der Daten z.B. das Paket DataExplorer oder das Paket ggpubr nutzen:\n\nlibrary(DataExplorer)\nd_summarized |&gt; \n  select(decade, temp_mean) |&gt; \n  plot_scatterplot(by = \"temp_mean\")\n\nd_summarized |&gt; \n  select(decade, temp_sd) |&gt; \n  plot_scatterplot(by = \"temp_sd\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ggpubr)\nggscatter(d_summarized, x = \"decade\", y = \"temp_mean\", add = \"reg.line\")\nggscatter(d_summarized, x = \"decade\", y = \"temp_sd\", add = \"reg.line\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFalls Sie Teile der R-Syntax nicht kennen: Machen Sie sich nichts daraus. 😄\n\nCategories:\n\ndata\neda\nlagemaße\nvariability\nstring"
  },
  {
    "objectID": "posts/Bed-Wskt2/Bed-Wskt2.html",
    "href": "posts/Bed-Wskt2/Bed-Wskt2.html",
    "title": "Bed-Wskt2",
    "section": "",
    "text": "(article?){apter_mass_2010, title = {A mass observation study of student and teacher behaviour in British primary classrooms}, volume = {26}, issn = {0266-7363}, url = {https://doi.org/10.1080/02667361003768518}, doi = {10.1080/02667361003768518}, abstract = {A large scale observational study by educational psychologists of 141 {UK} primary classrooms used a partial interval time‐sampling observational schedule to record the frequency and type of verbal behaviour of teachers and whether students were “on‐task” (following the teacher’s directions) or “off‐task” (not following the teacher’s directions). Results were analysed and comparisons made between lessons that followed National Literacy Strategy or Numeracy guidelines and those that did not; between schools from different geographical contexts, e.g. rural or inner‐city; between classes where there was one, or more than one, adult present; between schools with different percentages of free school meals; and between a.m. and p.m. lessons. A range of findings included higher rates of students being on‐task than found by previous studies, and correlations between high on‐task rates and teachers who used high levels of verbal behaviour including positive academic feedback. Teachers used three times more verbal approval for desired social behaviour in the classroom than has been reported in previous studies. It was found that teachers verbally interacted more with students during National Literacy and Numeracy Strategy lessons but that this did not lead to statistically significantly higher on‐task rate. Similarly, teachers in inner‐city schools interacted more with students, but on‐task rates in inner‐city schools were not significantly higher. Reasons for this effect are discussed.}, pages = {151–171}, number = {2}, journaltitle = {Educational Psychology in Practice}, author = {Apter, Brian and Arnold, Christopher and Swinson, Jeremy}, urldate = {2023-08-30}, date = {2010-06-01}, note = {Publisher: Routledge _eprint: https://doi.org/10.1080/02667361003768518}, keywords = {observational study, off‐task rates, on‐task rates, {UK} primary classrooms}, }"
  },
  {
    "objectID": "posts/Bed-Wskt2/Bed-Wskt2.html#answerlist",
    "href": "posts/Bed-Wskt2/Bed-Wskt2.html#answerlist",
    "title": "Bed-Wskt2",
    "section": "Answerlist",
    "text": "Answerlist\n\nZeichnen Sie (per Hand) ein Baumdiagramm, um die gemeinsamen Wahrscheinlichkeiten darzustellen. Weiterhin sollen die Randwahrscheinlichkeiten für \\(A\\) dargestellt sein.\nZeichnen Sie (per Hand) ein Baumdiagramm, um diesen Sachverhalt darzustellen.\nGeben Sie die Wahrscheinlichkeit des gesuchten Ereignisses an."
  },
  {
    "objectID": "posts/count-lexicon/count-lexicon.html",
    "href": "posts/count-lexicon/count-lexicon.html",
    "title": "count-lexicon",
    "section": "",
    "text": "Aufgabe\nGegeben eines (mehrelementigen) Strings, my_string, und eines Lexicons, my_lexicon, zählen Sie, wie häufig sich ein Wort aus dem Lexikon in einem Element des Strings wiederfindet.\n\nmy_string &lt;-\n  c(\"Heute ist ein schöner Tag\", \"Was geht in dieser Woche?\")\n\n\nmy_lexicon &lt;- c(\"Tag\", \"Woche\", \"Jahr\")\n\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\nNutzen Sie die Funktion count_lexicon aus {prada}. Das Paket können Sie hier herunterladen/installieren.\n\n         \n\n\nLösung\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\nPaket prada\nEine Möglichkeit ist es, die Funktion count_lexion aus dem Paket prada zu nutzen.\nMan kann es so installieren: remotes::install_github(\"sebastiansauer/prada\").\n\nlibrary(prada)\n\n\nmap_int(my_string,  \n        ~ count_lexicon(.x, my_lexicon))\n\n[1] 1 1\n\n\nSo können Sie sich den Quellcode einer Funktion, z.B. count_lexicon() anschauen:\n\ncount_lexicon\n\nfunction(txt, lexicon){\n  # convert strings to lower letters:\n  txt &lt;- tolower(txt)\n  lexicon &lt;- tolower(lexicon)\n\n  # build regex query:\n  lexicon_regex &lt;- paste0(\"^\", lexicon, \"$\", collapse = \"|\")\n\n  # split string into words:\n  string_in_words &lt;- unlist(stringr::str_split(txt, pattern = stringr::boundary(\"word\")))\n\n  # search:\n  pattern_detected_in_string_count &lt;- sum(stringr::str_detect(string_in_words, pattern = lexicon_regex))\n\n  # return:\n  return(pattern_detected_in_string_count)\n}\n&lt;bytecode: 0x7fb814c232d0&gt;\n&lt;environment: namespace:prada&gt;\n\n\nIn dem Paket gibt es noch zwei Varianten für diese Funktion, die auf einem Join aufbauen.\n\n\nSelbstgestrickt\nHier ist eine zweite Variante ohne besondere Pakete (außer stringr).\nWir definieren eine entsprechende Funktion:\n\n# Funktion, um die Anzahl der Übereinstimmungen eines Lexikons in einem String zu zählen\ncount_lexicon_matches &lt;- function(string, lexicon) {\n  # Verketten Sie die Wörter im Lexikon mit dem |-Operator, um ein reguläres Ausdrucksmuster zu erstellen\n  lexicon_pattern &lt;- paste(lexicon, collapse = \"|\")\n  # Verwenden Sie str_count, um die Anzahl der Übereinstimmungen zu zählen\n  matches &lt;- str_count(string, lexicon_pattern)\n  return(matches)\n}\n\nWir zählen die Übereinstimmungen in jedem Element des Strings:\n\nmatches_per_element &lt;- sapply(my_string, count_lexicon_matches, lexicon = my_lexicon)\n\n# Ergebnis ausgeben:\nprint(matches_per_element)\n\nHeute ist ein schöner Tag Was geht in dieser Woche? \n                        1                         1 \n\n\nAnstelle von sapply kann man das tidyverse-Pendant, map nutzen:\n\nmap_int(my_string,  \n        ~ count_lexicon_matches(.x, my_lexicon))\n\n[1] 1 1\n\n\n\nCategories:\n\ntextmining\nnlp\nregex\nstring"
  },
  {
    "objectID": "posts/Regression2/Regression2.html",
    "href": "posts/Regression2/Regression2.html",
    "title": "Regression2",
    "section": "",
    "text": "Ein Streudiagramm von \\(x\\) und \\(y\\) ergibt folgende Abbildung:\n\n\n\n\n\n\n\n\n\nWählen Sie das am besten passende Modell aus der Liste aus!\n\n\n\n\\(y = 40 + 10 \\cdot x + \\epsilon\\)\n\\(y = 40 + -10 \\cdot x + \\epsilon\\)\n\\(y = -40 + 10 \\cdot x + \\epsilon\\)\n\\(y = -40 + -10 \\cdot x + \\epsilon\\)\n\\(y = 0 + -40 \\cdot x + \\epsilon\\)"
  },
  {
    "objectID": "posts/Regression2/Regression2.html#answerlist",
    "href": "posts/Regression2/Regression2.html#answerlist",
    "title": "Regression2",
    "section": "",
    "text": "\\(y = 40 + 10 \\cdot x + \\epsilon\\)\n\\(y = 40 + -10 \\cdot x + \\epsilon\\)\n\\(y = -40 + 10 \\cdot x + \\epsilon\\)\n\\(y = -40 + -10 \\cdot x + \\epsilon\\)\n\\(y = 0 + -40 \\cdot x + \\epsilon\\)"
  },
  {
    "objectID": "posts/Regression2/Regression2.html#answerlist-1",
    "href": "posts/Regression2/Regression2.html#answerlist-1",
    "title": "Regression2",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nRichtig\nFalsch\nFalsch\n\n\nCategories:\n\nregression\ndyn"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-17/Verteilungen-Quiz-17.html",
    "href": "posts/Verteilungen-Quiz-17/Verteilungen-Quiz-17.html",
    "title": "Verteilungen-Quiz-17",
    "section": "",
    "text": "Ei Forschi untersucht die mittlere Körpergröße eines bis dato unbekannten Urwaldvolks. Dabei findet sich aposteriori (also als Ergebnis der Untersuchung) \\(\\bar{x} \\sim N(160,5)\\) (in Zentimetern).\nDis Forschi resümiert: “Mit sehr hoher Wahrscheinlichkeit, also 95%, sind diese Menschen im Schnitt größer als 1 Meter 60 Zentimeter groß”.\nIst diese Aussage korrekt (gegeben der Angaben)?\nHinweise:\n\nNutzen Sie Simulationsmethoden zur Lösung\nFixieren Sie die Zufallszahlen auf die Startzahl 42.\nZiehen Sie \\(10^5\\) Zufallszahlen aus der gegebenen Verteilung.\n\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-17/Verteilungen-Quiz-17.html#answerlist",
    "href": "posts/Verteilungen-Quiz-17/Verteilungen-Quiz-17.html#answerlist",
    "title": "Verteilungen-Quiz-17",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-17/Verteilungen-Quiz-17.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-17/Verteilungen-Quiz-17.html#answerlist-1",
    "title": "Verteilungen-Quiz-17",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/sd-vergleich/sd-vergleich.html",
    "href": "posts/sd-vergleich/sd-vergleich.html",
    "title": "sd-vergleich",
    "section": "",
    "text": "Welches der folgenden Diagramm hat die größte Streuung, gemessen in Standardabweichung (sd, sigma)?\n\n\n\n\n\n\n\n\n\n\n\n\nA\nB\nC\nalle gleich\nkeine Antwort möglich"
  },
  {
    "objectID": "posts/sd-vergleich/sd-vergleich.html#answerlist",
    "href": "posts/sd-vergleich/sd-vergleich.html#answerlist",
    "title": "sd-vergleich",
    "section": "",
    "text": "A\nB\nC\nalle gleich\nkeine Antwort möglich"
  },
  {
    "objectID": "posts/sd-vergleich/sd-vergleich.html#answerlist-1",
    "href": "posts/sd-vergleich/sd-vergleich.html#answerlist-1",
    "title": "sd-vergleich",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch. Dieses Diagramm hat die kleinste Streuung\nFalsch.\nWahr.\nFalsch. Die Streuungen sind unterschiedlich.\nFalsch.\n\n\nCategories:\n\ndatawrangling\neda\ntidyverse\nvis\nvariability\nschoice"
  },
  {
    "objectID": "posts/wskt-quiz05/wskt-quiz05.html",
    "href": "posts/wskt-quiz05/wskt-quiz05.html",
    "title": "wskt-quiz05",
    "section": "",
    "text": "Wasserplanet entdeckt (Er wurde auf den Namen “Bath42” getauft)! Die ganze Oberfläche besteht aus Wasser. Jemand presentiert uns die Probe von diesem Planeten: Wasser! Allerdings ohne zu sagen, ob die Probe vom Wasserplaneten oder von der Erde (E) kommt. Hm.\nGilt die folgende Gleichung: \\(Pr(W|E) = Pr(E|W)\\)?\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n\n\n\nFalsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz05/wskt-quiz05.html#answerlist",
    "href": "posts/wskt-quiz05/wskt-quiz05.html#answerlist",
    "title": "wskt-quiz05",
    "section": "",
    "text": "Falsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz05/wskt-quiz05.html#answerlist-1",
    "href": "posts/wskt-quiz05/wskt-quiz05.html#answerlist-1",
    "title": "wskt-quiz05",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\nquiz\nprobability\ndistribution\nquiz1-qm2-ws23\nschoice"
  },
  {
    "objectID": "posts/saratoga-cor1/saratoga-cor1.html",
    "href": "posts/saratoga-cor1/saratoga-cor1.html",
    "title": "saratoga-cor1",
    "section": "",
    "text": "Importieren Sie den Datensatz saratoga.\nGruppieren Sie den Datensatz in die Quartile für livingArea.\nBerechnen Sie dann den Zusammenhang zwischen price und bedrooms pro Quartil von livingArea.\nHinweise:\n\nBeachten Sie die Standardhinweise des Datenwerks.\nTipp: Die Funktion ntile aus {dplyr} teilt eine Variable var in Quartile auf, wenn Sie schreiben ntile(var, 4)."
  },
  {
    "objectID": "posts/saratoga-cor1/saratoga-cor1.html#setup",
    "href": "posts/saratoga-cor1/saratoga-cor1.html#setup",
    "title": "saratoga-cor1",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(ggpubr)\n\n\ndata(\"SaratogaHouses\", package = \"mosaicData\")"
  },
  {
    "objectID": "posts/saratoga-cor1/saratoga-cor1.html#gruppieren",
    "href": "posts/saratoga-cor1/saratoga-cor1.html#gruppieren",
    "title": "saratoga-cor1",
    "section": "Gruppieren",
    "text": "Gruppieren\n\nd2 &lt;-\n  SaratogaHouses |&gt; \n  mutate(q = ntile(livingArea, 4)) |&gt; \n  group_by(q)"
  },
  {
    "objectID": "posts/saratoga-cor1/saratoga-cor1.html#statistiken",
    "href": "posts/saratoga-cor1/saratoga-cor1.html#statistiken",
    "title": "saratoga-cor1",
    "section": "Statistiken",
    "text": "Statistiken\n\nd2 |&gt; \n  summarise(korrelation = cor(bedrooms, price))\n\n# A tibble: 4 × 2\n      q korrelation\n  &lt;int&gt;       &lt;dbl&gt;\n1     1      0.126 \n2     2      0.0781\n3     3     -0.143 \n4     4     -0.0478"
  },
  {
    "objectID": "posts/saratoga-cor1/saratoga-cor1.html#visualisierung",
    "href": "posts/saratoga-cor1/saratoga-cor1.html#visualisierung",
    "title": "saratoga-cor1",
    "section": "Visualisierung",
    "text": "Visualisierung\n\nggscatter(d2, \n          x = \"bedrooms\",\n          y = \"price\",\n          facet.by = \"q\",\n          add = \"reg.line\")"
  },
  {
    "objectID": "posts/tidymodels-ames-05/tidymodels-ames-05.html",
    "href": "posts/tidymodels-ames-05/tidymodels-ames-05.html",
    "title": "tidymodels-ames-05",
    "section": "",
    "text": "Aufgabe\nBerechnen Sie ein knn-Modell mit tidymodels und zwar anhand des ames Datensatzes.\nModellgleichung: log(Sale_Price) ~ ., data = ames_train.\nGesucht ist R-Quadrat als Maß für die Modellgüte im TEST-Sample.\nHinweise:\n\nFixieren Sie die Zufallszahlen auf den Startwert 42.\nDenken Sie daran, die nominal skalierten Variablen in Dummy-Variablen umzurechnen.\nDenken Sie daran, dass kNN gleich skalierte Prädiktoren benötigt.\nNutzen Sie eine v=10,r=1 CV.\nVerzichten Sie auf weitere Schritte der Vorverarbeitung.\n\n         \n\n\nLösung\nSetup:\n\nlibrary(tidymodels)\nlibrary(tictoc)  # Rechenzeit messen, optional\ndata(ames)\n\nAV loggen:\n\names &lt;-\n  ames %&gt;% \n  mutate(Sale_Price = log(Sale_Price, base = 10))\n\nDatensatz aufteilen:\n\nset.seed(42)\ndata_split &lt;- initial_split(ames, strata = \"Sale_Price\")\names_train &lt;- training(data_split)\names_test &lt;- testing(data_split)\n\nWorkflow:\n\names_rec &lt;-\n  recipe(Sale_Price ~ ., data = ames_train) %&gt;%\n  # step_log(Sale_Price, base = 10) %&gt;%   No!\n  step_other(Neighborhood, threshold = .1)  %&gt;%\n  step_dummy(all_nominal()) %&gt;%\n  step_zv(all_predictors()) \n\nknn_model2 &lt;-\n  nearest_neighbor(\n    mode = \"regression\",\n    neighbors = tune()  # Wir tunen \"neighbors\"\n  ) \n\names_wflow2 &lt;-\n  workflow() %&gt;%\n  add_recipe(ames_rec) %&gt;%\n  add_model(knn_model2)\n\names_wflow2\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_other()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-Nearest Neighbor Model Specification (regression)\n\nMain Arguments:\n  neighbors = tune()\n\nComputational engine: kknn \n\n\nCV:\n\nset.seed(42)\names_folds &lt;- vfold_cv(ames_train, strata = \"Sale_Price\", v = 2)\names_folds\n\n#  2-fold cross-validation using stratification \n# A tibble: 2 × 2\n  splits              id   \n  &lt;list&gt;              &lt;chr&gt;\n1 &lt;split [1098/1099]&gt; Fold1\n2 &lt;split [1099/1098]&gt; Fold2\n\n\nTunen:\n\ntic()\names_grid_search &lt;-\n  tune_grid(\n    knn_model2,\n    ames_rec,\n    resamples = ames_folds,\n    control = control_grid(save_workflow = TRUE),\n    grid = 2,  # 2 Tuningparameterwerte, hier nur zum Zeit sparen\n  )\ntoc()\n\n5.253 sec elapsed\n\names_grid_search\n\n# Tuning results\n# 2-fold cross-validation using stratification \n# A tibble: 2 × 4\n  splits              id    .metrics         .notes          \n  &lt;list&gt;              &lt;chr&gt; &lt;list&gt;           &lt;list&gt;          \n1 &lt;split [1098/1099]&gt; Fold1 &lt;tibble [4 × 5]&gt; &lt;tibble [0 × 3]&gt;\n2 &lt;split [1099/1098]&gt; Fold2 &lt;tibble [4 × 5]&gt; &lt;tibble [0 × 3]&gt;\n\n\nModellgüte im Train-Samples über die Tuningparameter hinweg:\n\nautoplot(ames_grid_search)\n\n\n\n\n\n\n\n\nFitte besten Modellkandidaten (Paket tune &gt;= V1.1.0 benötigt):\n\nfit1_final &lt;- fit_best(ames_grid_search)\n\nVorhersagen:\n\npreds &lt;-\n  predict(fit1_final, ames_test)\n\nModellgüte im Test-Sample:\n\nfit1_metrics &lt;-\n  preds %&gt;% \n  bind_cols(ames_test %&gt;% select(Sale_Price)) %&gt;% \n  rsq(truth = Sale_Price, estimate = .pred)\n\nfit1_metrics\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.739\n\n\nR-Quadrat:\n\nsol &lt;- fit1_metrics %&gt;% pull(.estimate)\nsol\n\n[1] 0.739015\n\n\n\nCategories:\n\nds1\ntidymodels\nprediction\nyacsda\nstatlearning\nnum"
  },
  {
    "objectID": "posts/tidymodels-ames-02/tidymodels-ames-02.html",
    "href": "posts/tidymodels-ames-02/tidymodels-ames-02.html",
    "title": "tidymodels-ames-02",
    "section": "",
    "text": "Aufgabe\nBerechnen Sie ein lineares Modell mit tidymodels und zwar anhand des ames Datensatzes.\nModellgleichung: Sale_Price ~ Gr_Liv_Area, data = ames.\nBerechnen Sie ein multiplikatives (exponenzielles) Modell.\nGesucht ist R-Quadrat als Maß für die Modellgüte im TEST-Sample.\nHinweise:\n\nFixieren Sie die Zufallszahlen auf den Startwert 42.\n\n         \n\n\nLösung\n\nlibrary(tidymodels)\ndata(ames)\n\nMultiplikatives Modell:\n\names &lt;- \n  ames %&gt;% \n  mutate(Sale_Price = log10(Sale_Price)) %&gt;% \n  select(Sale_Price, Gr_Liv_Area)\n\nNicht vergessen: AV-Transformation in beiden Samples!\nDatensatz aufteilen:\n\nset.seed(42)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\nModell definieren:\n\nm1 &lt;-\n  linear_reg() # engine ist \"lm\" im Default\n\nModell fitten:\n\nfit1 &lt;-\n  m1 %&gt;% \n  fit(Sale_Price ~ Gr_Liv_Area, data = ames)\n\n\nfit1 %&gt;% pluck(\"fit\") \n\n\nCall:\nstats::lm(formula = Sale_Price ~ Gr_Liv_Area, data = data)\n\nCoefficients:\n(Intercept)  Gr_Liv_Area  \n  4.8552133    0.0002437  \n\n\nModellgüte im Train-Sample:\n\nfit1_performance &lt;-\n  fit1 %&gt;% \n  extract_fit_engine()  # identisch zu pluck(\"fit\")\n\nModellgüte im Train-Sample:\n\nfit1_performance %&gt;% summary()\n\n\nCall:\nstats::lm(formula = Sale_Price ~ Gr_Liv_Area, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.02587 -0.06577  0.01342  0.07202  0.39231 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 4.855e+00  7.355e-03  660.12   &lt;2e-16 ***\nGr_Liv_Area 2.437e-04  4.648e-06   52.43   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1271 on 2928 degrees of freedom\nMultiple R-squared:  0.4842,    Adjusted R-squared:  0.484 \nF-statistic:  2749 on 1 and 2928 DF,  p-value: &lt; 2.2e-16\n\n\nR-Quadrat via easystats:\n\nlibrary(easystats)\nfit1_performance %&gt;% r2()  # rmse()\n\n# R2 for Linear Regression\n       R2: 0.484\n  adj. R2: 0.484\n\n\n\ntidy(fit1_performance)  # ähnlich zu parameters()\n\n# A tibble: 2 × 5\n  term        estimate  std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) 4.86     0.00736        660.        0\n2 Gr_Liv_Area 0.000244 0.00000465      52.4       0\n\n\nVorhersagen im Test-Sample:\n\npreds &lt;- predict(fit1, new_data = ames_test)  # liefert TABELLE (tibble) zurück\nhead(preds)\n\n# A tibble: 6 × 1\n  .pred\n  &lt;dbl&gt;\n1  5.07\n2  5.18\n3  5.31\n4  5.11\n5  5.18\n6  5.10\n\n\npreds ist ein Tibble, also müssen wir noch die Spalte .pred. herausziehen, z.B. mit pluck(preds, \".pred\"):\n\npreds_vec &lt;- preds$.pred\n\n\names_test2 &lt;-\n  ames_test %&gt;% \n  mutate(preds = pluck(preds, \".pred\"),  # pluck aus der Tabelle rausziehen\n         .pred = preds_vec)  # oder  mit dem Dollar-Operator\n\nhead(ames_test2)\n\n# A tibble: 6 × 4\n  Sale_Price Gr_Liv_Area preds .pred\n       &lt;dbl&gt;       &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1       5.02         896  5.07  5.07\n2       5.24        1329  5.18  5.18\n3       5.60        1856  5.31  5.31\n4       5.15        1056  5.11  5.11\n5       5.26        1337  5.18  5.18\n6       4.98         987  5.10  5.10\n\n\nOder mit unnest:\n\names_test2 &lt;-\n  ames_test %&gt;% \n  mutate(preds = preds) %&gt;% \n  unnest(preds) # Listenspalte \"entschachteln\"\n\nhead(ames_test2)\n\n# A tibble: 6 × 3\n  Sale_Price Gr_Liv_Area .pred\n       &lt;dbl&gt;       &lt;int&gt; &lt;dbl&gt;\n1       5.02         896  5.07\n2       5.24        1329  5.18\n3       5.60        1856  5.31\n4       5.15        1056  5.11\n5       5.26        1337  5.18\n6       4.98         987  5.10\n\n\nOder wir binden einfach die Spalte an den Tibble:\n\names_test2 &lt;-\n  ames_test %&gt;% \n  bind_cols(preds = preds)  # nimmt Tabelle und bindet die Spalten dieser Tabelle an eine Tabelle\n\nhead(ames_test2)\n\n# A tibble: 6 × 3\n  Sale_Price Gr_Liv_Area .pred\n       &lt;dbl&gt;       &lt;int&gt; &lt;dbl&gt;\n1       5.02         896  5.07\n2       5.24        1329  5.18\n3       5.60        1856  5.31\n4       5.15        1056  5.11\n5       5.26        1337  5.18\n6       4.98         987  5.10\n\n\nModellgüte im Test-Sample:\n\nrsq(ames_test2,\n    truth = Sale_Price,\n    estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.517\n\n\n\nsol &lt;- 0.51679\n\nZur Interpretation von Log10-Werten\n\n5e5\n\n[1] 5e+05\n\n5*10^5 - 500000\n\n[1] 0\n\n\n\nCategories:\n\nds1\ntidymodels\nprediction\nyacsda\nstatlearning\nnum"
  },
  {
    "objectID": "posts/breiman/breiman.html",
    "href": "posts/breiman/breiman.html",
    "title": "breiman",
    "section": "",
    "text": "In einem berühmten Paper mit dem Titel “Statistical Modeling: The Two Culture” hat Leo Breiman zwei Arten der Datenanalyse vorgestellt und kritisch diskutiert. Dieser Artikel wurde vielfach diskutiert, weil er zentrale Fragen des Fachgebiets anstieß bzw. intensivierte. Welche der folgenden Kontroversen zeichnen das Feld der Datenanalyse nicht?\nWelche Aussagen sind in diesem Zusammenhang falsch?\n\n\n\nZentrale Fragestellungen der Datenanalyse kann man mit den Zielen Vorhersage und Erklärung (des Kausalmodells) kontrastierend vorstellen.\nMan kann zwei “Lager” oder Fraktionen innerhalb der Datenanalyse ausmachen: Die mehr an Mathematik orientierten Statistiker und die mehr an Informatik orientierten Datenwissenschaftler.\nStatistische Modelle (bzw. die Modelle der Statistiker) basieren auf der Wahrscheinlichkeitsrechnung.\nDie Modelle der Datenwissenschaftler bezeichnet man auch als algorithmische Modelle.\nDie Modelle der Datenwissenschaftler sind zumeist “Black-Box-Modelle”.\nEin Beispiel für ein Black-Box-Modell ist die klassische Regression (Methode der kleinsten Quadrate)."
  },
  {
    "objectID": "posts/breiman/breiman.html#answerlist",
    "href": "posts/breiman/breiman.html#answerlist",
    "title": "breiman",
    "section": "",
    "text": "Zentrale Fragestellungen der Datenanalyse kann man mit den Zielen Vorhersage und Erklärung (des Kausalmodells) kontrastierend vorstellen.\nMan kann zwei “Lager” oder Fraktionen innerhalb der Datenanalyse ausmachen: Die mehr an Mathematik orientierten Statistiker und die mehr an Informatik orientierten Datenwissenschaftler.\nStatistische Modelle (bzw. die Modelle der Statistiker) basieren auf der Wahrscheinlichkeitsrechnung.\nDie Modelle der Datenwissenschaftler bezeichnet man auch als algorithmische Modelle.\nDie Modelle der Datenwissenschaftler sind zumeist “Black-Box-Modelle”.\nEin Beispiel für ein Black-Box-Modell ist die klassische Regression (Methode der kleinsten Quadrate)."
  },
  {
    "objectID": "posts/regex03/regex03.html",
    "href": "posts/regex03/regex03.html",
    "title": "regex03",
    "section": "",
    "text": "Aufgabe\nGegeben sein ein String-Vektor, x. Dieser Vektor enthält Vornamen mehrerer Personen. Extrahieren Sie den ersten Vornamen jeder Person.\n\nx &lt;-\n  c(\"Anna\",\n    \"Berta Brigitte\",\n    \"Carla-Klara\",\n    \"Dana Dora Diana\",\n    \"Emilia E\",\n    \"F-Franziska\",\n    \" Gabi\",\n    \"Jana die Erste\")\n\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n         \n\n\nLösung\n\nlibrary(stringr)  # Teil von tidyvserse\nlibrary(purrr)\n\nLiest man alle Zeichen vom Typ w aus, so sind Bindestriche nicht enthalten:\n\nstr_match(x, \"\\\\w+\")\n\n     [,1]    \n[1,] \"Anna\"  \n[2,] \"Berta\" \n[3,] \"Carla\" \n[4,] \"Dana\"  \n[5,] \"Emilia\"\n[6,] \"F\"     \n[7,] \"Gabi\"  \n[8,] \"Jana\"  \n\n\nDaher macht es vermutlich mehr Sinn, umgekehrt zu sagen, was man nicht will, nämlich Leerzeichen, also s:\n\nstr_match(x, \"[^\\\\s]+\")\n\n     [,1]         \n[1,] \"Anna\"       \n[2,] \"Berta\"      \n[3,] \"Carla-Klara\"\n[4,] \"Dana\"       \n[5,] \"Emilia\"     \n[6,] \"F-Franziska\"\n[7,] \"Gabi\"       \n[8,] \"Jana\"       \n\n\nDie Ausgabe kann man noch vereinfachen, in dem wir aus der resultieren Matrix (Tabelle) die ersten Spalte auswählen:\n\n\n[1] \"Anna\"        \"Berta\"       \"Carla-Klara\" \"Dana\"        \"Emilia\"     \n[6] \"F-Franziska\" \"Gabi\"        \"Jana\"       \n\n\n\nCategories:\n\nregex\ntextmining\nstring"
  },
  {
    "objectID": "posts/affairs-dplyr/affairs-dplyr.html",
    "href": "posts/affairs-dplyr/affairs-dplyr.html",
    "title": "affairs-dplyr",
    "section": "",
    "text": "Aufgabe\nLaden Sie den Datensatz affairs:\n\naffairs_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/AER/Affairs.csv\"\n\n\naffairs &lt;- read.csv(affairs_path)\n\nLesen Sie das Data Dictionnary hier.\nWir definieren als “Halodrie” eine Person mit mindestens einer Affäre (laut Datensatz).\nBearbeiten Sie folgende Aufgaben:\n\nFiltern Sie mal nach Halodries!\nSortieren Sie (absteigend) nach Anzahl der Affären!\nWählen Sie die Spalten zu Anzahl der Affären, ob es Kinder in der Ehe gibt und die Zufriedenheit mit der Ehe. Dann sortieren Sie dann nach Anzahl der Kinder und danach nach der Anzahl der Affären.\nBerechnen Sie die mittlere Anzahl der Affären!\nBerechnen Sie die mittlere Anzahl der Affären pro Geschlecht und aufgeteilt auf Partnerschaften mit bzw. ohne Kinder.\nGeben Sie für jede Person die höhere der zwei Zahlen von Religiösität und Ehezufriedenheit aus!\nBerechnen Sie jeweils das Heiratsalter!\n\n         \n\n\nLösung\n\nlibrary(tidyverse)\n\nAd 1.\n\naffairs %&gt;% \n  filter(affairs &gt; 0) %&gt;% \n  head(10)\n\n  rownames affairs gender age yearsmarried children religiousness education\n1        6       3   male  27          1.5       no             3        18\n2       12       3 female  27          4.0      yes             3        17\n  occupation rating\n1          4      4\n....\n\n\nHinweis: head(10) begrenzt die Ausgabe auf 10 Zeilen, einfach um den Bildschirm nicht vollzumüllen.\nAd 2.\n\naffairs %&gt;% \n  arrange(-affairs) %&gt;% \n  head(10)\n\n  rownames affairs gender age yearsmarried children religiousness education\n1       53      12 female  32           10      yes             3        17\n2      122      12   male  37           15      yes             4        14\n  occupation rating\n1          5      2\n2          5      2\n [ reached 'max' / getOption(\"max.print\") -- omitted 8 rows ]\n\n\nAd 3.\n\naffairs %&gt;% \n  select(affairs, rating, children) %&gt;% \n  arrange(children, affairs) %&gt;% \n  head(10)\n\n  affairs rating children\n1       0      4       no\n2       0      4       no\n3       0      3       no\n4       0      5       no\n5       0      3       no\n6       0      5       no\n [ reached 'max' / getOption(\"max.print\") -- omitted 4 rows ]\n\n\nAd 4.\n\naffairs %&gt;% \n  summarise(affairs_mean = mean(affairs)) %&gt;% \n  head(10)\n\n  affairs_mean\n1     1.455907\n\n\nAd 5.\n\naffairs %&gt;% \n  group_by(gender, children) %&gt;% \n  summarise(affairs_mean = mean(affairs)) %&gt;% \n  head(10)\n\n# A tibble: 4 × 3\n# Groups:   gender [2]\n  gender children affairs_mean\n  &lt;chr&gt;  &lt;chr&gt;           &lt;dbl&gt;\n1 female no              0.838\n2 female yes             1.69 \n3 male   no              1.01 \n4 male   yes             1.66 \n\n\nAd 6.\n\naffairs %&gt;% \n  rowwise() %&gt;% \n  summarise(max(c(religiousness, rating))) %&gt;% \n  head(10)\n\n# A tibble: 10 × 1\n   `max(c(religiousness, rating))`\n                             &lt;int&gt;\n 1                               4\n 2                               4\n 3                               4\n 4                               5\n 5                               3\n 6                               5\n 7                               3\n....\n\n\nAd 7.\n\naffairs %&gt;% \n  mutate(heiratsalter = age - yearsmarried) %&gt;%\n  head(10)\n\n  rownames affairs gender age yearsmarried children religiousness education\n1        4       0   male  37           10       no             3        18\n  occupation rating heiratsalter\n1          7      4           27\n [ reached 'max' / getOption(\"max.print\") -- omitted 9 rows ]\n\n\n\nCategories:\n\ndatawrangling\neda\nstring"
  },
  {
    "objectID": "posts/iq10/iq10.html",
    "href": "posts/iq10/iq10.html",
    "title": "iq10",
    "section": "",
    "text": "Aufgabe\nEine Forscherin, Prof. Weiss-Ois, untersucht den Effekt von Cannabis auf die Intelligenz.\nDazu untersucht Sie die Intelligenz langjähriger Konsumentis.\nProf. Weiss-Ois geht apriori von gleichverteilter Intelligenz aus. Ihre zentrale Hypothese ist \\(\\mu = 90\\pm5\\).\nMit großer Spannung wurden die Messdaten zur Intelligenz erwartet (die erst nach langem Streit über die zu verwendenden Intelligenztests erhoben werden konnten). Insgesamt wurden \\(N=541\\) Personen untersucht.\nTatsächlich sei die wahre IQ-Verteilung jener Cannabis-Konsumentis wie folgt: \\(IQ \\sim N(92.5, 7.5)\\). Natürlich kennt die Forscherin diese Verteilung nicht.\nWie wahrscheinlich ist die Hypothese der Forscherin im Lichte der Daten?\nHinweise:\n\nNutzen Sie Simulationsmethoden.\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\nSimulieren Sie \\(n=10^4\\) Stichproben.\nNutzen Sie die Zahl 42 als Startwert für Ihre Zufallszahlen (um die Reproduzierbarkeit zu gewährleisten).\nGitterwerte für die Intelligenz könnten z.B. 75 bis 130 sein.\n\n         \n\n\nLösung\n\nlibrary(tidyverse)\n\nZunächst basteln wir die Bayes-Box:\n\nn &lt;- 100\n\nset.seed(42)\npostvert &lt;-\n  tibble(p_grid = seq(from = 75, to = 130, length.out = n),\n         prior  = 1) %&gt;% \n  mutate(likelihood = dnorm(x = p_grid, mean = 92.5, sd = 7.5)) %&gt;% \n  mutate(unstand_post = likelihood * prior,\n         post = unstand_post / sum(unstand_post))\n\nWarum 75 bis 130? Das ist ein beliebiger Wert, in dem Sinne, dass Sie sich inhaltlich überlegen müssen, welchen Wertebereich Sie für plausibel halten. Untersucht man IQ-Mittelwerte so erscheint (mir) ein Wertebereich von 75 bis 130 mehr als ausreichend. Untersucht man Mittelwerte der Körpergröße (heutiger Menschen in bekannten Zivilisationen) so erscheint (mir) ein Wertebereich von 140 cm bis 200cm als ausreichend.\nAus der Post-Verteilung ziehen wir Stichproben:\n\nset.seed(42)\npostvert_stipros &lt;-\n  postvert %&gt;% \n  slice_sample(\n    n = 1e4,\n    weight_by = post,\n    replace = T) %&gt;% \n  select(p_grid)\n\nDamit haben wir unsere Stichproben-Post-Verteilung.\nDa slice_sample auch zufällig Stichproben zieht, müssen wir auch hier die Zufallszahlen fixieren, wenn wir die exakt gleichen Ergebnisse reproduzieren wollen.\nJetzt schauen wir (mit Spannung), wie hoch die Wahrscheinlichkeit ist für Parameterwete (p_grid) innerhalb des Intervalls wie von der Forscherin vorgegeben.\n\npostvert_stipros %&gt;% \n  count(between(p_grid, left = 85, right = 95))\n\n# A tibble: 2 × 2\n  `between(p_grid, left = 85, right = 95)`     n\n  &lt;lgl&gt;                                    &lt;int&gt;\n1 FALSE                                     5002\n2 TRUE                                      4998\n\n\nbetween ist eine Komfortfuntion; ins Deutsche übersetzt sagt die Syntax:\nNimm die Tabelle postvert_stipros und dann ...\n  zähle den Anteil der Werte von p_grid zwischen 85 und 95.\nDie Wahrscheinlichkeit der Hypothese der Forscherin beträgt also ca. 50%.\nOb das viel oder weniger ist, ist eine subjektive Frage. Das beste Vorgehen wäre jetzt, die Hypothesen anderer Forschis dagegen zu legen. Dann würde man sehen, welche Hypothese am besten zu den Daten passt.\nBasteln wir zum Vergleich eine Bayes-Box mit Gitterwerte von von 60 bis 145:\n\nn &lt;- 100\n\nset.seed(42)\npostvert2 &lt;-\n  tibble(p_grid = seq(from = 60, to = 145, length.out = n),\n         prior  = 1) %&gt;% \n  mutate(likelihood = dnorm(x = p_grid, mean = 92.5, sd = 7.5)) %&gt;% \n  mutate(unstand_post = likelihood * prior,\n         post = unstand_post / sum(unstand_post))\n\nset.seed(42)\npostvert_stipros2 &lt;-\n  postvert2 %&gt;% \n  slice_sample(\n    n = 1e4,\n    weight_by = post,\n    replace = T) %&gt;% \n  select(p_grid)\n\nOb sich das Ergebnis ändert, jetzt, da wir einen breiteren Bereich an Gitterwerten untersuchzen?\nJetzt schauen wir wieder (mit Spannung), wie hoch die Wahrscheinlichkeit ist für Parameterwete (p_grid) innerhalb des Intervalls wie von der Forscherin vorgegeben.\n\npostvert_stipros2 %&gt;% \n  count(between(p_grid, left = 85, right = 95)) %&gt;% \n  mutate(prop = n/sum(n))\n\n# A tibble: 2 × 3\n  `between(p_grid, left = 85, right = 95)`     n  prop\n  &lt;lgl&gt;                                    &lt;int&gt; &lt;dbl&gt;\n1 FALSE                                     5455 0.546\n2 TRUE                                      4545 0.454\n\n\nDie Ergebnisse sind leicht anders als oben, wo wir den engeren Wertebereich als mögliche Werte angegeben haben.\nAber bedenken Sie: Wir behaupten in postvert2 ernsthaft, dass ein Mittelwert der Intelligenz in dieser Population mit gleicher Wascheinlichkeit 60 oder 145 sein könnte! Laut Wikipedia beginnt mit 130 die Hochbegabung und unter 85 fängt die Lernbehinderung an. Dass diese Menschen super schlau oder mental behindert sind, erscheint uns gleich plausibel. Das ist eine sehr starke Apriori-Verteilung!\nViel konservativer wäre zu sagen: “Okay, vermutlich sind diese Menschen so schlau wie alle anderen auch, vielleicht etwas mehr oder etwas weniger. Aber mit sehr hoher Wahrscheinlichkeit sind sie im Durchschnitt keine Einsteins oder nicht extrem mental eingeschränkt.”\nWir brauchen also besser nicht gleichverteilte Priori-Verteilungen. Dazu an anderer Stelle mehr.\n\nCategories:\n\nprobability\nsimulation\nnormal-distribution\nbayes\nbayes-grid\nnum"
  },
  {
    "objectID": "posts/lm-mario3/lm-mario3.html",
    "href": "posts/lm-mario3/lm-mario3.html",
    "title": "lm-mario3",
    "section": "",
    "text": "Sagen Sie den Verkaufspreis vorher für Spiele mit 1, 2, bzw. 3 Euro Versandkosten (ship_pr)!\nGeben Sie den Durchschnitt der Vorhersagen als Lösung an!"
  },
  {
    "objectID": "posts/lm-mario3/lm-mario3.html#setup",
    "href": "posts/lm-mario3/lm-mario3.html#setup",
    "title": "lm-mario3",
    "section": "Setup",
    "text": "Setup\n\nmariokart &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\")\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.6.0 (red = needs update)\n✔ bayestestR  0.13.1   ✔ correlation 0.8.4 \n✔ datawizard  0.9.0    ✔ effectsize  0.8.6 \n✔ insight     0.19.6   ✔ modelbased  0.8.6 \n✔ performance 0.10.8   ✔ parameters  0.21.3\n✔ report      0.5.7    ✖ see         0.8.0 \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`."
  },
  {
    "objectID": "posts/lm-mario3/lm-mario3.html#regressionsgerade-berechnen",
    "href": "posts/lm-mario3/lm-mario3.html#regressionsgerade-berechnen",
    "title": "lm-mario3",
    "section": "Regressionsgerade berechnen",
    "text": "Regressionsgerade berechnen\n\nlm_mariokart &lt;- lm(total_pr ~ ship_pr, data = mariokart) # \"lm\" wie *l*lineares *M*odell, also eine Gerade.\nlm_mariokart\n\n\nCall:\nlm(formula = total_pr ~ ship_pr, data = mariokart)\n\nCoefficients:\n(Intercept)      ship_pr  \n     36.246        4.337"
  },
  {
    "objectID": "posts/lm-mario3/lm-mario3.html#vorhersagen",
    "href": "posts/lm-mario3/lm-mario3.html#vorhersagen",
    "title": "lm-mario3",
    "section": "Vorhersagen",
    "text": "Vorhersagen\nVorhersagen funktionieren mit dem Befehl predict.\n\nneue_spiele &lt;- tibble(ship_pr = c(1,2,3))\nneue_spiele\n\n# A tibble: 3 × 1\n  ship_pr\n    &lt;dbl&gt;\n1       1\n2       2\n3       3\n\n\nAnstelle von tibble können Sie auch data.frame verwenden. Mit c erstellt man einen “Vektor”, also eine “Liste” zusammengehöriger Werte.\n\nvorhersagen &lt;- predict(lm_mariokart, neue_spiele)  # predicte mir den Verkaufspreis\n\nvorhersagen\n\n       1        2        3 \n40.58276 44.91998 49.25720 \n\n\n\nloesung &lt;- mean(vorhersagen)\nloesung\n\n[1] 44.91998\n\n\nDie Lösung lautet: 44.9199833.\n\nCategories:\n\nR\nlm\npredict\nnum"
  },
  {
    "objectID": "posts/mtcars-post3/mtcars-post3.html",
    "href": "posts/mtcars-post3/mtcars-post3.html",
    "title": "mtcars-post3",
    "section": "",
    "text": "Im Datensatz mtcars: Wie groß ist die Wahrscheinlichkeit, dass der Effekt der UV vs auf die AV mpg positiv ist? Berechnen Sie das dazu passende Modell mit Methoden der Bayes-Statistik.\nHinweise\nWählen Sie die am besten passende Option:\n\n\n\n.42\n.73\n.23\n1\n0"
  },
  {
    "objectID": "posts/mtcars-post3/mtcars-post3.html#answerlist",
    "href": "posts/mtcars-post3/mtcars-post3.html#answerlist",
    "title": "mtcars-post3",
    "section": "",
    "text": ".42\n.73\n.23\n1\n0"
  },
  {
    "objectID": "posts/mtcars-post3/mtcars-post3.html#answerlist-1",
    "href": "posts/mtcars-post3/mtcars-post3.html#answerlist-1",
    "title": "mtcars-post3",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nWahr\nFalsch\n\n\nCategories:\n\nbayes\nregression\npost\nexam-22"
  },
  {
    "objectID": "posts/wskt-quiz03/wskt-quiz03.html",
    "href": "posts/wskt-quiz03/wskt-quiz03.html",
    "title": "wskt-quiz03",
    "section": "",
    "text": "Wirft man eine faire Münze 10-fach, so gilt \\(Pr(\\text{10 mal Kopf}) = Pr(\\text{10 mal Zahl}) &gt; .01\\).\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n\n\n\nFalsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz03/wskt-quiz03.html#answerlist",
    "href": "posts/wskt-quiz03/wskt-quiz03.html#answerlist",
    "title": "wskt-quiz03",
    "section": "",
    "text": "Falsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz03/wskt-quiz03.html#answerlist-1",
    "href": "posts/wskt-quiz03/wskt-quiz03.html#answerlist-1",
    "title": "wskt-quiz03",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\nquiz\nprobability\nquiz1-qm2-ws23\nschoice"
  },
  {
    "objectID": "posts/nasa01/nasa01.html",
    "href": "posts/nasa01/nasa01.html",
    "title": "nasa01",
    "section": "",
    "text": "Viele Quellen berichten Klimadaten unserer Erde, z.B. auch National Aeronautics and Space Administration - Goddard Institute for Space Studies.\nVon dieser Quelle beziehen wir diesen Datensatz.\nDie Datensatz sind auf der Webseite wie folgt beschrieben:\nTables of Global and Hemispheric Monthly Means and Zonal Annual Means\nCombined Land-Surface Air and Sea-Surface Water Temperature Anomalies (Land-Ocean Temperature Index, L-OTI)\nThe following are plain-text files in tabular format of temperature anomalies, i.e. deviations from the corresponding 1951-1980 means.\n\nGlobal-mean monthly, seasonal, and annual means, 1880-present, updated through most recent month: TXT, CSV\n\nStarten Sie zunächst das R-Paket tidyverse falls noch nicht geschehen.\n\nlibrary(tidyverse)\n\nImportieren Sie dann die Daten:\n\ndata_path &lt;- \"https://data.giss.nasa.gov/gistemp/tabledata_v4/GLB.Ts+dSST.csv\"\nd &lt;- read.csv(data_path, skip = 1)\n\nWir lassen die 1. Zeile des Datensatzes aus (Argument skip), da dort Metadaten stehen, also keine Daten, sondern Informationen (Daten) zu den eigentlichen Daten.\nAufgabe\nBerechnen und visualisieren Sie die folgende Statistiken pro Dekade:\n\nMittelwert der Temperatur im Januar\nSD der Temperatur im Januar\n\nHinweise:\n\nSie müssen zuerst die Dekade als neue Spalte berechnen."
  },
  {
    "objectID": "posts/nasa01/nasa01.html#setup",
    "href": "posts/nasa01/nasa01.html#setup",
    "title": "nasa01",
    "section": "Setup",
    "text": "Setup\n\nlibrary(ggpubr)\nlibrary(DataExplorer)"
  },
  {
    "objectID": "posts/nasa01/nasa01.html#daten-aufbereiten",
    "href": "posts/nasa01/nasa01.html#daten-aufbereiten",
    "title": "nasa01",
    "section": "Daten aufbereiten",
    "text": "Daten aufbereiten\nDekade berechnen:\n\nd &lt;-\n  d %&gt;% \n  mutate(decade = round(Year/10))\n\nDas ist ein möglicher Weg, um aus einer Jahreszahl die Dekade zu berechnen."
  },
  {
    "objectID": "posts/nasa01/nasa01.html#statistiken-berechnen",
    "href": "posts/nasa01/nasa01.html#statistiken-berechnen",
    "title": "nasa01",
    "section": "Statistiken berechnen",
    "text": "Statistiken berechnen\nStatistiken pro Dekade:\n\nd_summarized &lt;- \n  d %&gt;% \n  group_by(decade) %&gt;% \n  summarise(temp_mean = mean(Jan),\n            temp_sd = sd(Jan))\n\nd_summarized\n\n\n\n\n\n\n\n\n\ndecade\ntemp_mean\ntemp_sd\n\n\n\n\n188\n−0.20\n0.24\n\n\n189\n−0.44\n0.22\n\n\n190\n−0.26\n0.16\n\n\n191\n−0.39\n0.22\n\n\n192\n−0.28\n0.15\n\n\n193\n−0.14\n0.22\n\n\n194\n0.03\n0.21\n\n\n195\n−0.05\n0.18\n\n\n196\n0.03\n0.15\n\n\n197\n−0.07\n0.17\n\n\n198\n0.21\n0.19\n\n\n199\n0.35\n0.13\n\n\n200\n0.52\n0.19\n\n\n201\n0.64\n0.21\n\n\n202\n0.98\n0.15"
  },
  {
    "objectID": "posts/nasa01/nasa01.html#statistiken-visualisieren",
    "href": "posts/nasa01/nasa01.html#statistiken-visualisieren",
    "title": "nasa01",
    "section": "Statistiken visualisieren",
    "text": "Statistiken visualisieren\n\nMit DataExplorer\n\nd_summarized |&gt; \n  select(decade, temp_mean) |&gt; \n  plot_scatterplot(by = \"temp_mean\")\n\nd_summarized |&gt; \n  select(decade, temp_sd) |&gt; \n  plot_scatterplot(by = \"temp_sd\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMit ggpubr\n\nd_summarized |&gt; \n  ggline(x = \"decade\", y = \"temp_mean\")\n\nd_summarized |&gt; \n  ggline(x = \"decade\", y = \"temp_sd\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nd  |&gt; \n  ggerrorplot(x = \"decade\", y = \"Jan\")\n\n\n\n\n\n\n\n\nFalls Sie Teile der R-Syntax nicht kennen: Machen Sie sich nichts daraus. 😄\n\nCategories:\n\ndata\neda\nlagemaße\nvariability\nstring"
  },
  {
    "objectID": "posts/ds-quiz/ds-quiz.html",
    "href": "posts/ds-quiz/ds-quiz.html",
    "title": "ds-quiz",
    "section": "",
    "text": "Im Folgenden sind mehrere Aussagen zum Thema maschinelles Lernen dargestellt. Wählen Sie alle korrekten Aussagen aus!\nHinweise:\n\nAlle Aussagen sind entweder richtig oder falsch, aber nicht beides.\nBeziehen Sie sich im Zweifel auf den Stoff wie im Unterricht dargestellt.\n\n\n\n\nDecision Trees (Baummodelle) sind Overfitting (Überanpassung) mehr ausgesetzt als lineare Modelle.\nEin Resampling-Schema mit \\(v=10\\) Faltungen und \\(r=5\\) Wiederholungen ist identisch zu einem Schema mit \\(v=50\\) Faltungen und \\(r=1\\) Wiederholungen.\n“Normale” (nicht regularisierte) lineare Modelle sind besser interpretierbar als L1-regularisierte lineare Modelle.\n“Normale” lineare Modelle verfügen nicht über Tuningparameter.\nJe größer die Anzahl der Bäume in einem Random Forest, desto größer die Gefahr des Overfittings."
  },
  {
    "objectID": "posts/ds-quiz/ds-quiz.html#answerlist",
    "href": "posts/ds-quiz/ds-quiz.html#answerlist",
    "title": "ds-quiz",
    "section": "",
    "text": "Decision Trees (Baummodelle) sind Overfitting (Überanpassung) mehr ausgesetzt als lineare Modelle.\nEin Resampling-Schema mit \\(v=10\\) Faltungen und \\(r=5\\) Wiederholungen ist identisch zu einem Schema mit \\(v=50\\) Faltungen und \\(r=1\\) Wiederholungen.\n“Normale” (nicht regularisierte) lineare Modelle sind besser interpretierbar als L1-regularisierte lineare Modelle.\n“Normale” lineare Modelle verfügen nicht über Tuningparameter.\nJe größer die Anzahl der Bäume in einem Random Forest, desto größer die Gefahr des Overfittings."
  },
  {
    "objectID": "posts/ds-quiz/ds-quiz.html#answerlist-1",
    "href": "posts/ds-quiz/ds-quiz.html#answerlist-1",
    "title": "ds-quiz",
    "section": "Answerlist",
    "text": "Answerlist\n\nRichtig. Decision Trees neigen zum Overfitting, sie sind sehr flexibel und haben daher viel Varianz in ihren Entscheidungen.\nFalsch. Bei 10 Faltungen beinhaltet das Test-Sample 1/10 der Beobachtungen und entsprechend bei 50 Faltungen nur 1/50 der Beobachtungen. Außerdem erreicht man durch die Wiederholungen eine robustere (präzisere) Schätzung der Vorhersagegüte.\nFalsch. Durch die L1-Regularisierung (Lasso) werden (oft) Prädiktoren entfernt (ihr Gewicht auf Null gesetzt), so dass das resultierende Modell einfacher und damit auch einfacher interpretierbarer ist.\nRichtig.\nFalsch. Normalerweise steigt die Modellgüte mit der Anzahl der Bäume bis zu einem Sättigungsniveau, ab welchem zusätzliche Bäume die Vorhersagegüte nicht mehr beeinflussen (allerdings die Rechenzeit schon).\n\n\nCategories:\n\nds1\ntidymodels\nstatlearning\nmchoice"
  },
  {
    "objectID": "posts/nasa06/nasa06.html",
    "href": "posts/nasa06/nasa06.html",
    "title": "nasa06",
    "section": "",
    "text": "Aufgabe\nViele Quellen berichten Klimadaten unserer Erde, z.B. auch National Aeronautics and Space Administration - Goddard Institute for Space Studies.\nVon dieser Quelle beziehen wir diesen Datensatz.\nDie Datensatz sind auf der Webseite wie folgt beschrieben:\nTables of Global and Hemispheric Monthly Means and Zonal Annual Means\nCombined Land-Surface Air and Sea-Surface Water Temperature Anomalies (Land-Ocean Temperature Index, L-OTI)\nThe following are plain-text files in tabular format of temperature anomalies, i.e. deviations from the corresponding 1951-1980 means.\n\nGlobal-mean monthly, seasonal, and annual means, 1880-present, updated through most recent month: TXT, CSV\n\nStarten Sie zunächst das R-Paket tidyverse falls noch nicht geschehen.\n\nlibrary(tidyverse)\n\nImportieren Sie dann die Daten:\n\ndata_path &lt;- \"https://data.giss.nasa.gov/gistemp/tabledata_v4/GLB.Ts+dSST.csv\"\nd &lt;- read_csv(data_path, skip = 1)\n\nWir lassen die 1. Zeile des Datensatzes aus (Argument skip), da dort Metadaten stehen, also keine Daten, sondern Informationen (Daten) zu den eigentlichen Daten.\nAufgaben\n\nFassen Sie immer 10 Jahre (eine Dekade) an Jahren zusammen.\nPräsentieren Sie gängige Statistiken pro Dekade für alle Monate.\n\nHinweise:\n\nSie müssen zuerst die Dekade als neue Spalte berechnen.\nTreffen Sie Annahmen, wo nötig.\nBeachten Sie die Hinweise.\n\n         \n\n\nLösung\nTabelle in die Lang-Form bringen:\n\nd_long &lt;- \n  d %&gt;% \n  select(Year, Jan:Dec) %&gt;% \n  mutate(across(Jan:Dec, as.numeric)) %&gt;% \n  pivot_longer(cols = Jan:Dec, values_to = \"temp\", names_to = \"month\") \n\nEin Blick in die Tabelle:\n\nhead(d_long)\n\n# A tibble: 6 × 3\n   Year month  temp\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1  1880 Jan   -0.18\n2  1880 Feb   -0.24\n3  1880 Mar   -0.09\n4  1880 Apr   -0.16\n5  1880 May   -0.1 \n6  1880 Jun   -0.21\n\n\nDekade berechnen:\n\nd_long2 &lt;-\n  d_long %&gt;% \n  mutate(decade = round(Year/10) * 10)\n\n\ntail(d_long2)  # letzten paar Zeilen der Tabelle \"d_long2\"\n\n# A tibble: 6 × 4\n   Year month  temp decade\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1  2024 Jul      NA   2020\n2  2024 Aug      NA   2020\n3  2024 Sep      NA   2020\n4  2024 Oct      NA   2020\n5  2024 Nov      NA   2020\n6  2024 Dec      NA   2020\n\n\n\nd_summarized &lt;-\nd_long2 %&gt;% \n  group_by(decade, month) %&gt;% \n  summarise(temp_mean = mean(temp, na.rm = TRUE),\n            temp_sd = sd(temp, na.rm = TRUE))\n\n\n\n# A tibble: 6 × 4\n# Groups:   decade [1]\n  decade month temp_mean temp_sd\n   &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1   1880 Apr      -0.213   0.175\n2   1880 Aug      -0.157   0.113\n3   1880 Dec      -0.185   0.118\n4   1880 Feb      -0.17    0.187\n5   1880 Jan      -0.203   0.240\n6   1880 Jul      -0.173   0.132\n\n\nZum Visualisieren gibt es meist viele Wege. Hier ist ein Weg mit ggplot2:\n\nd_summarized %&gt;% \n  ggplot(aes(x = decade, y = temp_mean)) +\n  geom_point(color = \"darkblue\") +\n  geom_errorbar(aes(ymin = temp_mean - temp_sd, ymax = temp_mean + temp_sd)) +\n  geom_line(alpha = .7) +\n  facet_wrap(~ month) +\n  labs(caption = \"Fehlerbalken zeigen plus/minus 1 SD\")\n\n\n\n\n\n\n\n\nMonate zu einem Jahreswert zusammen:\n\nd_summarized2 &lt;- \n  d_summarized %&gt;% \n  group_by(decade) %&gt;% \n  summarise(temp_mean = mean(temp_mean),\n            temp_sd = sd(temp_sd))\n\nd_summarized2\n\n# A tibble: 15 × 3\n   decade temp_mean temp_sd\n    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1   1880  -0.192    0.0580\n 2   1890  -0.265    0.0540\n 3   1900  -0.226    0.0216\n 4   1910  -0.357    0.0452\n 5   1920  -0.272    0.0580\n 6   1930  -0.191    0.0470\n 7   1940   0.0324   0.0289\n 8   1950  -0.0671   0.0357\n 9   1960  -0.0341   0.0308\n10   1970  -0.00759  0.0401\n11   1980   0.145    0.0304\n12   1990   0.308    0.0236\n13   2000   0.51     0.0235\n14   2010   0.655    0.0401\n15   2020   0.963    0.0484\n\n\nAlternativ können Sie zum Visualisieren der Daten z.B. das Paket ggpubr nutzen:\n\nlibrary(ggpubr)\nggscatter(d_summarized2, x = \"decade\", y = \"temp_mean\", add = \"reg.line\")\n\n\n\n\n\n\n\n\nOder auch mit den Facetten pro Monat:\n\nggscatter(d_summarized, x = \"decade\", y = \"temp_mean\", add = \"reg.line\",\n          facet.by = \"month\")\n\n\n\n\n\n\n\n\nÄhnlich sieht es mit DataExplorer aus:\n\nlibrary(DataExplorer)\nd_summarized2 |&gt; \n  select(temp_mean, decade) |&gt; \n  plot_scatterplot(by = \"decade\")\n\nd_summarized2 |&gt; \n  select(temp_sd, decade) |&gt; \n  plot_scatterplot(by = \"decade\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDie weltweilte Temperatur steigt. Bleibt u.a. die Frage, ob eine lineare Funktion angemessen ist, oder ob die Steigung nicht vielleicht prozentual steigt (das wäre eine exponenzielle Steigerung)?\nFalls Sie Teile der R-Syntax nicht kennen: Machen Sie sich nichts daraus. 😄\n\nCategories:\n\ndata\neda\nlagemaße\nstring"
  },
  {
    "objectID": "posts/Linearitaet1a/Linearitaet1a.html",
    "href": "posts/Linearitaet1a/Linearitaet1a.html",
    "title": "Linearitaet1a",
    "section": "",
    "text": "Bei welcher der Abbildungen ist eine Regression mit linearem Graph angemessen?\n\n\n\nA\nB\nC\nD"
  },
  {
    "objectID": "posts/Linearitaet1a/Linearitaet1a.html#answerlist",
    "href": "posts/Linearitaet1a/Linearitaet1a.html#answerlist",
    "title": "Linearitaet1a",
    "section": "",
    "text": "A\nB\nC\nD"
  },
  {
    "objectID": "posts/Linearitaet1a/Linearitaet1a.html#answerlist-1",
    "href": "posts/Linearitaet1a/Linearitaet1a.html#answerlist-1",
    "title": "Linearitaet1a",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nRichtig\nFalsch\nFalsch\n\n\nCategories:\n\nlm\nregression\nlinear\nschoice"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-18/Verteilungen-Quiz-18.html",
    "href": "posts/Verteilungen-Quiz-18/Verteilungen-Quiz-18.html",
    "title": "Verteilungen-Quiz-18",
    "section": "",
    "text": "Ei Forschi untersucht den Effekt eines Intelligenztraining auf den IQ.\nDabei findet sich aposteriori (also als Ergebnis der Untersuchung) \\(\\bar{x} \\sim N(3,5)\\) (in Standard-IQ-Punkten). Wir messen dabei die Erhöhung des Intelligenzwerts.\nDis Forschi resümiert: “Mit einer Wahrscheinlichkeit von 95% profitiert man von diesem Training”.\nIst diese Aussage korrekt (gegeben der Angaben)?\nHinweise:\n\nNutzen Sie Simulationsmethoden zur Lösung\nFixieren Sie die Zufallszahlen auf die Startzahl 42.\nZiehen Sie \\(10^5\\) Zufallszahlen aus der gegebenen Verteilung.\n\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-18/Verteilungen-Quiz-18.html#answerlist",
    "href": "posts/Verteilungen-Quiz-18/Verteilungen-Quiz-18.html#answerlist",
    "title": "Verteilungen-Quiz-18",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-18/Verteilungen-Quiz-18.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-18/Verteilungen-Quiz-18.html#answerlist-1",
    "title": "Verteilungen-Quiz-18",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/Regression4/Regression4.html",
    "href": "posts/Regression4/Regression4.html",
    "title": "Regression4",
    "section": "",
    "text": "Aufgabe\nBerechnen Sie \\(\\hat{y}\\) für das unten ausgegeben Modell!\nNutzen Sie dafür folgende Werte:\n\n\\(g=1\\)\n\\(x=17\\).\n\n\n\n\n\n\nterm\nestimate\nstd_error\nstatistic\np_value\nlower_ci\nupper_ci\n\n\n\n\nintercept\n43.215\n3.695\n11.696\n0.000\n35.838\n50.592\n\n\nx\n-10.312\n0.363\n-28.401\n0.000\n-11.037\n-9.587\n\n\ng\n-1.017\n4.954\n-0.205\n0.838\n-10.909\n8.875\n\n\nx:g\n10.336\n0.486\n21.258\n0.000\n9.365\n11.307\n\n\n\n\n\n\n\nHinweis: Ein Interaktionseffekt der Variablen \\(x\\) und \\(g\\) ist mit x:g gekennzeichnet. Runden Sie zur nächsten ganzen Zahl.\n         \n\n\nLösung\n\\(\\hat{y}\\) beträgt im Fall der vorliegenden Parameter und dem vorliegenden Modell \\(43\\).\n\nCategories:\n\ndyn\nregression\nlm\nnum"
  },
  {
    "objectID": "posts/purrr-map01/purrr-map01.html",
    "href": "posts/purrr-map01/purrr-map01.html",
    "title": "purrr-map01",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nExercise\nErstellen Sie einen Tibble mit folgenden Spalten:\n\nBuchstaben A-Z, so dass in der 1. Zeile “A” steht, in der 2. Zeile “B” etc.\nBuchstaben a-z, so dass in der 1. Zeile “a” steht, in der 2. Zeile “b” etc.\nBuchstabenkombination der ersten beiden Spalten, so dass in der 1. Zeile “A-a” steht, in der 2. Zeile “B-b” etc.\n\n         \n\n\nSolution\nGeht es vielleicht so?\n\nd &lt;-\n  tibble(\n    letter1 = LETTERS,\n    letter2 = letters,\n    letters = paste(letter1, letter2, collapse = \"-\")\n  )\n\nhead(d)\n\n# A tibble: 6 × 3\n  letter1 letter2 letters                                                       \n  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;                                                         \n1 A       a       A a-B b-C c-D d-E e-F f-G g-H h-I i-J j-K k-L l-M m-N n-O o-P…\n2 B       b       A a-B b-C c-D d-E e-F f-G g-H h-I i-J j-K k-L l-M m-N n-O o-P…\n3 C       c       A a-B b-C c-D d-E e-F f-G g-H h-I i-J j-K k-L l-M m-N n-O o-P…\n4 D       d       A a-B b-C c-D d-E e-F f-G g-H h-I i-J j-K k-L l-M m-N n-O o-P…\n5 E       e       A a-B b-C c-D d-E e-F f-G g-H h-I i-J j-K k-L l-M m-N n-O o-P…\n6 F       f       A a-B b-C c-D d-E e-F f-G g-H h-I i-J j-K k-L l-M m-N n-O o-P…\n\n\nNein, leider nicht.\nOK, neuer Versuch:\n\nd &lt;-\n  tibble(\n    letter1 = LETTERS,\n    letter2 = letters) %&gt;% \n  unite(\"letters\", c(letter1, letter2), remove = FALSE)\n\n\nhead(d)\n\n# A tibble: 6 × 3\n  letters letter1 letter2\n  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  \n1 A_a     A       a      \n2 B_b     B       b      \n3 C_c     C       c      \n4 D_d     D       d      \n5 E_e     E       e      \n6 F_f     F       f      \n\n\nProbieren wir es mit purrr::map():\n\nd &lt;-\n  tibble(\n    letter1 = LETTERS,\n    letter2 = letters,\n    letters = map2_chr(letter1, letter2, ~ paste(c(.x, .y), collapse =\"-\"))\n  )\n\nhead(d)\n\n# A tibble: 6 × 3\n  letter1 letter2 letters\n  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  \n1 A       a       A-a    \n2 B       b       B-b    \n3 C       c       C-c    \n4 D       d       D-d    \n5 E       e       E-e    \n6 F       f       F-f    \n\n\nInfos zur Funktion paste() findet sich z.B. hier.\n\nCategories:\n\nR\nmap\ntidyverse"
  },
  {
    "objectID": "posts/tmdb06/tmdb06.html",
    "href": "posts/tmdb06/tmdb06.html",
    "title": "tmdb06",
    "section": "",
    "text": "Aufgabe\nMelden Sie sich an für die Kaggle Competition TMDB Box Office Prediction - Can you predict a movie’s worldwide box office revenue?.\nSie benötigen dazu ein Konto; es ist auch möglich, sich mit seinem Google-Konto anzumelden.\nBei diesem Prognosewettbewerb geht es darum, vorherzusagen, wieviel Umsatz wohl einige Filme machen werden. Als Prädiktoren stehen einige Infos wie Budget, Genre, Titel etc. zur Verfügung. Eine klassische “predictive Competition” also :-) Allerdings können immer ein paar Schwierigkeiten auftreten ;-)\nAufgabe\nErstellen Sie ein Lineares Modell mit Tidymodels!\nHinweise\n\n\nVerzichten Sie auf Vorverarbeitung.\nVerzichten Sie auf Tuning.\nReichen Sie das Modell ein und berichten Sie Ihren Score.\nBegrenzen Sie sich auf folgende Prädiktoren.\nVerwenden Sie (langweiligerweise) nur ein lineares Modell.\n\n\npreds_chosen &lt;- \n  c(\"id\", \"budget\", \"popularity\", \"runtime\")\n\n         \n\n\nLösung\n\n\nPakete starten\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(tictoc)\nlibrary(finetune)  # Anova Race\nlibrary(doParallel)  # parallele Verarbeitung\n\n\n\nDaten importieren\n\nd_train_path &lt;- \"https://raw.githubusercontent.com/sebastiansauer/Lehre/main/data/tmdb-box-office-prediction/train.csv\"\nd_test_path &lt;- \"https://raw.githubusercontent.com/sebastiansauer/Lehre/main/data/tmdb-box-office-prediction/test.csv\"\n\nd_train &lt;- read_csv(d_train_path)\nd_test &lt;- read_csv(d_test_path)\n\nWerfen wir einen Blick in die Daten:\n\nglimpse(d_train)\n\nRows: 3,000\nColumns: 23\n$ id                    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ belongs_to_collection &lt;chr&gt; \"[{'id': 313576, 'name': 'Hot Tub Time Machine C…\n$ budget                &lt;dbl&gt; 1.40e+07, 4.00e+07, 3.30e+06, 1.20e+06, 0.00e+00…\n$ genres                &lt;chr&gt; \"[{'id': 35, 'name': 'Comedy'}]\", \"[{'id': 35, '…\n$ homepage              &lt;chr&gt; NA, NA, \"http://sonyclassics.com/whiplash/\", \"ht…\n$ imdb_id               &lt;chr&gt; \"tt2637294\", \"tt0368933\", \"tt2582802\", \"tt182148…\n$ original_language     &lt;chr&gt; \"en\", \"en\", \"en\", \"hi\", \"ko\", \"en\", \"en\", \"en\", …\n$ original_title        &lt;chr&gt; \"Hot Tub Time Machine 2\", \"The Princess Diaries …\n$ overview              &lt;chr&gt; \"When Lou, who has become the \\\"father of the In…\n$ popularity            &lt;dbl&gt; 6.575393, 8.248895, 64.299990, 3.174936, 1.14807…\n$ poster_path           &lt;chr&gt; \"/tQtWuwvMf0hCc2QR2tkolwl7c3c.jpg\", \"/w9Z7A0GHEh…\n$ production_companies  &lt;chr&gt; \"[{'name': 'Paramount Pictures', 'id': 4}, {'nam…\n$ production_countries  &lt;chr&gt; \"[{'iso_3166_1': 'US', 'name': 'United States of…\n$ release_date          &lt;chr&gt; \"2/20/15\", \"8/6/04\", \"10/10/14\", \"3/9/12\", \"2/5/…\n$ runtime               &lt;dbl&gt; 93, 113, 105, 122, 118, 83, 92, 84, 100, 91, 119…\n$ spoken_languages      &lt;chr&gt; \"[{'iso_639_1': 'en', 'name': 'English'}]\", \"[{'…\n$ status                &lt;chr&gt; \"Released\", \"Released\", \"Released\", \"Released\", …\n$ tagline               &lt;chr&gt; \"The Laws of Space and Time are About to be Viol…\n$ title                 &lt;chr&gt; \"Hot Tub Time Machine 2\", \"The Princess Diaries …\n$ Keywords              &lt;chr&gt; \"[{'id': 4379, 'name': 'time travel'}, {'id': 96…\n$ cast                  &lt;chr&gt; \"[{'cast_id': 4, 'character': 'Lou', 'credit_id'…\n$ crew                  &lt;chr&gt; \"[{'credit_id': '59ac067c92514107af02c8c8', 'dep…\n$ revenue               &lt;dbl&gt; 12314651, 95149435, 13092000, 16000000, 3923970,…\n\nglimpse(d_test)\n\nRows: 4,398\nColumns: 22\n$ id                    &lt;dbl&gt; 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, …\n$ belongs_to_collection &lt;chr&gt; \"[{'id': 34055, 'name': 'Pokémon Collection', 'p…\n$ budget                &lt;dbl&gt; 0.00e+00, 8.80e+04, 0.00e+00, 6.80e+06, 2.00e+06…\n$ genres                &lt;chr&gt; \"[{'id': 12, 'name': 'Adventure'}, {'id': 16, 'n…\n$ homepage              &lt;chr&gt; \"http://www.pokemon.com/us/movies/movie-pokemon-…\n$ imdb_id               &lt;chr&gt; \"tt1226251\", \"tt0051380\", \"tt0118556\", \"tt125595…\n$ original_language     &lt;chr&gt; \"ja\", \"en\", \"en\", \"fr\", \"en\", \"en\", \"de\", \"en\", …\n$ original_title        &lt;chr&gt; \"ディアルガVSパルキアVSダークライ\", \"Attack of t…\n$ overview              &lt;chr&gt; \"Ash and friends (this time accompanied by newco…\n$ popularity            &lt;dbl&gt; 3.851534, 3.559789, 8.085194, 8.596012, 3.217680…\n$ poster_path           &lt;chr&gt; \"/tnftmLMemPLduW6MRyZE0ZUD19z.jpg\", \"/9MgBNBqlH1…\n$ production_companies  &lt;chr&gt; NA, \"[{'name': 'Woolner Brothers Pictures Inc.',…\n$ production_countries  &lt;chr&gt; \"[{'iso_3166_1': 'JP', 'name': 'Japan'}, {'iso_3…\n$ release_date          &lt;chr&gt; \"7/14/07\", \"5/19/58\", \"5/23/97\", \"9/4/10\", \"2/11…\n$ runtime               &lt;dbl&gt; 90, 65, 100, 130, 92, 121, 119, 77, 120, 92, 88,…\n$ spoken_languages      &lt;chr&gt; \"[{'iso_639_1': 'en', 'name': 'English'}, {'iso_…\n$ status                &lt;chr&gt; \"Released\", \"Released\", \"Released\", \"Released\", …\n$ tagline               &lt;chr&gt; \"Somewhere Between Time & Space... A Legend Is B…\n$ title                 &lt;chr&gt; \"Pokémon: The Rise of Darkrai\", \"Attack of the 5…\n$ Keywords              &lt;chr&gt; \"[{'id': 11451, 'name': 'pok√©mon'}, {'id': 1155…\n$ cast                  &lt;chr&gt; \"[{'cast_id': 3, 'character': 'Tonio', 'credit_i…\n$ crew                  &lt;chr&gt; \"[{'credit_id': '52fe44e7c3a368484e03d683', 'dep…\n\n\npreds_chosen sind alle Prädiktoren im Datensatz, oder nicht? Das prüfen wir mal kurz:\n\npreds_chosen %in% names(d_train) %&gt;% \n  all()\n\n[1] TRUE\n\n\nJa, alle Elemente von preds_chosen sind Prädiktoren im (Train-)Datensatz.\n\n\nCV\nWir brauchen keine CV, da wir keine Tuningparameter haben.\n\ncv_scheme &lt;- vfold_cv(d_train)\n\n\n\nRezept\n\nrec1 &lt;- \n  recipe(revenue ~ budget + popularity + runtime, data = d_train) %&gt;% \n  step_impute_bag(all_predictors()) %&gt;% \n  step_naomit(all_predictors()) \nrec1\n\nMan beachte, dass noch 21 Prädiktoren angezeigt werden, da das Rezept noch nicht auf den Datensatz angewandt (“gebacken”) wurde.\n\ntidy(rec1)\n\n# A tibble: 2 × 6\n  number operation type       trained skip  id              \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;      &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;           \n1      1 step      impute_bag FALSE   FALSE impute_bag_sBkeX\n2      2 step      naomit     FALSE   TRUE  naomit_NxiQP    \n\n\nRezept checken:\n\nprep(rec1)\n\n\nd_train_baked &lt;-\n  rec1 %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL)\n\nglimpse(d_train_baked)\n\nRows: 3,000\nColumns: 4\n$ budget     &lt;dbl&gt; 1.40e+07, 4.00e+07, 3.30e+06, 1.20e+06, 0.00e+00, 8.00e+06,…\n$ popularity &lt;dbl&gt; 6.575393, 8.248895, 64.299990, 3.174936, 1.148070, 0.743274…\n$ runtime    &lt;dbl&gt; 93, 113, 105, 122, 118, 83, 92, 84, 100, 91, 119, 98, 122, …\n$ revenue    &lt;dbl&gt; 12314651, 95149435, 13092000, 16000000, 3923970, 3261638, 8…\n\n\nFehlende Werte noch übrig?\n\nlibrary(easystats)\ndescribe_distribution(d_train_baked) %&gt;% \n  select(Variable, n_Missing)\n\nVariable   | n_Missing\n----------------------\nbudget     |         0\npopularity |         0\nruntime    |         0\nrevenue    |         0\n\n\n\n\nModell\n\nmodel_lm &lt;- linear_reg()\n\n\n\nWorkflow\n\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(model_lm) %&gt;% \n  add_recipe(rec1)\n\n\n\nModell fitten (und tunen)\n\n#doParallel::registerDoParallel(4)\ntic()\nlm_fit1 &lt;-\n  wf1 %&gt;% \n  fit(d_train)\ntoc()\n\n0.645 sec elapsed\n\n\n\npreds &lt;-\n  lm_fit1 %&gt;% \n  predict(d_test)\n\n\n\nSubmission df\n\nsubmission_df &lt;-\n  d_test %&gt;% \n  select(id) %&gt;% \n  bind_cols(preds) %&gt;% \n  rename(revenue = .pred)\n\nhead(submission_df)\n\n# A tibble: 6 × 2\n     id   revenue\n  &lt;dbl&gt;     &lt;dbl&gt;\n1  3001 -4147506.\n2  3002 -8808140.\n3  3003  8523980.\n4  3004 31675099.\n5  3005  -504355.\n6  3006 13531355.\n\n\nAbspeichern und einreichen:\n\n#write_csv(submission_df, file = \"submission.csv\")\n\n\n\nKaggle Score\nDiese Submission erzielte einen Score von Score: 6.14787 (RMSLE).\n\nsol &lt;- 6.14787\n\n\nCategories:\n\nds1\ntidymodels\nstatlearning\ntmdb\nrandom-forest\nnum"
  },
  {
    "objectID": "posts/penguins-vis-bodymass1/index.html",
    "href": "posts/penguins-vis-bodymass1/index.html",
    "title": "penguins-vis-bodymass1",
    "section": "",
    "text": "Aufgabe\nIm Datensatz palmerpenguins: Welche der folgenden Variablen korreliert am stärksten mit dem Körpergewicht der Pinguine?\nBeantworten Sie diese Frage mit Hilfe einer Visualisierung!\nSie können den Datensatz so beziehen:\n\n#install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\ndata(\"penguins\")\nd &lt;- penguins \n\nOder so:\n\nd &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\")\n\nEin Codebook finden Sie hier.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n\n\nLösung\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(DataExplorer)\n\n\nd &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\")\n\n\nd |&gt; \n  select(bill_depth_mm, bill_length_mm, flipper_length_mm, body_mass_g) |&gt; \n  plot_scatterplot(by = \"body_mass_g\")\n\nWarning: Removed 6 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nEs sieht so aus, also ob flipper_length_mm am stärksten mit dem Körpergewicht zusammenhängt."
  },
  {
    "objectID": "posts/penguins-stan-03/penguins-stan-03.html",
    "href": "posts/penguins-stan-03/penguins-stan-03.html",
    "title": "penguins-stan-03",
    "section": "",
    "text": "Aufgabe\nWir untersuchen Einflussfaktoren bzw. Prädiktoren auf das Körpergewicht von Pinguinen. In dieser Aufgabe untersuchen wir den Zusammenhang von Schnabellänge (als UV) und Körpergewicht (als AV).\nWie groß ist der statistische Einfluss der UV auf die AV?\nGeben Sie den Punktschätzer des Effekts an!\nHinweise:\n\nNutzen Sie den Datensatz zu den Palmer Penguins.\nSie können den Datensatz z.B. hier beziehen oder über das R-Paket palmerpenguins.\nBeziehen Sie sich auf den Median-Schätzwert.\nWeitere Hinweise\n\n         \n\n\nLösung\nZentrieren ist eigentlich immer nützlich, aber hier streng genommen nicht unbedingt nötig. Der Hauptgrund dafür ist, dass Stan für uns den Prior für den Intercept festlegt, und zwar auf Basis der Daten, wir uns also nicht um die komische Frage zu kümmern brauchen, welchen Prior wir für den unzentrierten Achsenabschnitt vergeben wollten: Wie schwer sind Pinguins der Schnabellänge Null? Mit zentrierten Prädiktoren ist die Frage nach dem Prior viel einfacher zu beantworten: Wie schwer ist ein Pinguin mit mittelgroßem Schnabel?\nSetup:\n\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(rstanarm)\n\ndata(\"penguins\", package = \"palmerpenguins\")\n\nEs wird in dieser Aufgabe vorausgesetzt, dass Sie den Datensatz selbständig importieren können. Tipp: Kurzes Googeln hilft ggf., den Datensatz zu finden.\nAlternativ könnten Sie den Datensatz als CSV-Datei importieren:\n\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\"\npenguins &lt;- data_read(d_path)\n\nEin Blick in die Daten zur Kontrolle, ob das Importieren richtig funktioniert hat:\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nVertrauen ist gut, aber - was Golems betrifft - ist Kontrolle eindeutig besser ;-)\n\nm1 &lt;- stan_glm(body_mass_g ~  bill_length_mm,  # Regressionsgleichung\n               data = penguins, #  Daten\n               seed = 42,  # Repro.\n               refresh = 0)  # nicht so viel Output\n\n\nparameters(m1, ci_method = \"hdi\", ci = .9, keep = \"bill_length_mm\")  # nur für \"bill_length_mm\"\n\nParameter      | Median |         90% CI |   pd |  Rhat |     ESS |                Prior\n----------------------------------------------------------------------------------------\nbill_length_mm |  87.45 | [77.17, 98.51] | 100% | 0.999 | 3931.00 | Normal (0 +- 367.22)\n\n\n\nUncertainty intervals (highest-density) and p-values (two-tailed)\n  computed using a MCMC distribution approximation.\n\n\nDie Lösung lautet also, wie aus der Ausgabe zu den Parametern ersichtlich, 87.45.\n\nCategories:\n\nbayes\nregression\nexam-22"
  },
  {
    "objectID": "posts/Regr-Bayes-interpret/Regr-Bayes-interpret.html",
    "href": "posts/Regr-Bayes-interpret/Regr-Bayes-interpret.html",
    "title": "Regr-Bayes-interpret",
    "section": "",
    "text": "Exercise\nBerechnen Sie das Modell und interpretieren Sie die Ausgabe des folgenden Regressionsmodells. Geben Sie für jeden Regressionskoeffizienten an, wie sein Wert zu verstehen ist!\nmpg ~ hp + am + hp:am\nHinweise:\n\nFixieren Sie die Zufallszahlen.\nVerwenden Sie Stan zur Berechnung.\nRunden Sie auf 2 Dezimalstellen.\n\n         \n\n\nSolution\n\nlibrary(tidyverse)  # Datenjudo\nlibrary(rstanarm)  # Stan, komm her\nlibrary(easystats)  # Komfort\n\n\nm1 &lt;- \n  stan_glm(mpg ~ hp + am + hp:am, \n           seed = 42,\n           refresh = 0,\n           data = mtcars)\n\ncoef(m1)\n\n  (Intercept)            hp            am         hp:am \n26.6170440514 -0.0588639914  5.2553599666  0.0003348108 \n\n\n\nIntercept: Ein Auto mit 0 PS und Automatikantrieb (am=0, s. Hilfe zum Datensatz: help(mtcars)) kann laut Modell mit einer Gallone Sprit ca. 26.62 Meilen fahren.\nhp: Pro zusätzlichem PS kann ein Auto mit Automatikantrieb pro Gallone Sprit ca. 0.06 Meilen weniger weit fahren.\nam: Ein Auto mit 0 PS und Schaltgetriebe (am=1) kommt pro Gallone Sprit ca. 5.26 Meilen weiter als ein Auto mit Automatikantrieb.\nhp:am: Der Interaktionseffekt ist praktisch Null: Der Zusammenhang von PS-Zahl und Spritverbrauch unterscheidet sich nicht (wesentlich) zwischen Autos mit bzw. ohne Automatikantrieb.\n\n\nCategories:\n\nbayes\nregression"
  },
  {
    "objectID": "posts/twitter07/twitter07.html",
    "href": "posts/twitter07/twitter07.html",
    "title": "twitter07",
    "section": "",
    "text": "Exercise\nLaden Sie \\(n=10^k\\) Tweets von Twitter herunter (mit \\(k=2\\)) und zwar pro Nutzerkonto wie unten angegeben . die Tweets sollen jeweils an eine prominente Person gerichtet sein.\nBeziehen Sie sich auf diese Politikis.\n         \n         \n\n\nSolution\nWir starten die benötigten R-Pakete:\n\nlibrary(academictwitteR)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(askpass)\nlibrary(rio)\n\nHier ist der Datensatz mit den Twitterkonten, für die wir die Daten herunterladen sollen:\n\npoliticians_path &lt;- \"https://raw.githubusercontent.com/sebastiansauer/datascience-text/main/data/twitter-german-politicians.csv\"\npoliticians &lt;- import(politicians_path)\npoliticians\n\n                                               name  party      screenname\n1                                   Karl Lauterbach    SPD Karl_Lauterbach\n2                                       Olaf Scholz    SPD      OlafScholz\n3                                 Annalena Baerback Gruene       ABaerbock\n4  Bundesministerium für Wirtschaft und Klimaschutz Gruene            BMWK\n5                                    Friedrich Merz    CDU  _FriedrichMerz\n6                                      Markus Söder    CSU   Markus_Soeder\n7                                       Cem Özdemir Gruene    cem_oezdemir\n8                                    Janine Wissler  Linke  Janine_Wissler\n9                                 Martin Schirdewan  Linke      schirdewan\n10                                Christian Lindner    FDP       c_lindner\n11                    Marie-Agnes Strack-Zimmermann    FDP      MAStrackZi\n12                                   Tino Chrupalla    AFD  Tino_Chrupalla\n13                                     Alice Weidel    AFD    Alice_Weidel\n                                  comment\n1                                    &lt;NA&gt;\n2                                    &lt;NA&gt;\n3                                    &lt;NA&gt;\n4  Robert Habeck ist der Minister im BMWK\n5                                CDU-Chef\n6                                CSU-Chef\n7                                    BMEL\n8                            Linke-Chefin\n9                              Linke-Chef\n10                               FDP-Chef\n11     Vorsitzende Verteidigungsausschuss\n12                     AFD-Bundessprecher\n13                   AFD-Bundessprecherin\n\n\nWir müssen noch das Passwort bereitstellen:\n\nbearer_token &lt;- askpass::askpass(\"bearer token\")\n\nUnd dann definieren wir eine Funktion, die das Gewichtheben für uns erledigt:\n\nget_all_tweets_politicians &lt;- function(screenname, n = 1e1) {\n  get_all_tweets(query = paste0(\"to:\", screenname, \" -is:retweet\"),\n                 start_tweets = \"2021-01-01T00:00:00Z\",\n                 end_tweets = \"2021-12-31T23:59:59Z\",\n                 bearer_token = bearer_token,\n                 file = glue::glue(\"~/datasets/Twitter/hate-speech/tweets_to_{screenname}_2021.rds\"),\n                 data_path = glue::glue(\"~/datasets/Twitter/hate-speech/{screenname}\"),\n                 n = n)\n}\n\nJetzt wenden wir die Funktion auf jedes Twitterkonto unserer Liste (alle Politikis) an:\n\nd &lt;- politicians$screenname %&gt;% \n  map(get_all_tweets_politicians)\n\n\nCategories:\n\ntextmining\ntwitter\nprogramming"
  },
  {
    "objectID": "posts/Bed-Post-Wskt1/Bed-Post-Wskt1.html",
    "href": "posts/Bed-Post-Wskt1/Bed-Post-Wskt1.html",
    "title": "Bed-Post-Wskt1",
    "section": "",
    "text": "Beziehen Sie sich auf das Regressionsmodell, für das die Ausgabe mit stan_glm() hier dargestellt ist:\n\n\nParameter   | Median |           95% CI |   pd |  Rhat |     ESS |                    Prior\n-------------------------------------------------------------------------------------------\n(Intercept) | 146.12 | [145.15, 147.09] | 100% | 1.000 | 3857.00 | Normal (154.60 +- 19.36)\nweight_c    |   0.90 | [  0.82,   0.98] | 100% | 1.000 | 3889.00 |    Normal (0.00 +- 3.00)\n\n\nBetrachten Sie folgende Beziehung (Gleichung bzw. Ungleichung):\n\\[Pr(\\text{height}_i = 155|\\text{weightcentered}_i=10, \\alpha, \\beta, \\sigma) \\quad \\Box \\quad Pr(\\text{height}_i = 160|\\text{weightcentered}_i=10, \\alpha, \\beta, \\sigma)\\] Die in der obigen Beziehung angegebenen Parameter beziehen sich auf das oben dargestellt Modell.\nErgänzen Sie das korrekte Zeichen in das Rechteck \\(\\Box\\)!\n\n\n\n\\(\\lt\\)\n\\(\\le\\)\n\\(\\gt\\)\n\\(\\ge\\)\n\\(=\\)"
  },
  {
    "objectID": "posts/Bed-Post-Wskt1/Bed-Post-Wskt1.html#answerlist",
    "href": "posts/Bed-Post-Wskt1/Bed-Post-Wskt1.html#answerlist",
    "title": "Bed-Post-Wskt1",
    "section": "",
    "text": "\\(\\lt\\)\n\\(\\le\\)\n\\(\\gt\\)\n\\(\\ge\\)\n\\(=\\)"
  },
  {
    "objectID": "posts/Bed-Post-Wskt1/Bed-Post-Wskt1.html#answerlist-1",
    "href": "posts/Bed-Post-Wskt1/Bed-Post-Wskt1.html#answerlist-1",
    "title": "Bed-Post-Wskt1",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nWahr\nFalsch\nFalsch\n\n\nCategories:\n\nregression\nbayes\npost"
  },
  {
    "objectID": "posts/diamonds-tidymodels01/diamonds-tidymodels01.html",
    "href": "posts/diamonds-tidymodels01/diamonds-tidymodels01.html",
    "title": "diamonds-tidymodels01",
    "section": "",
    "text": "Aufgabe\nFinden Sie ein möglichst “gutes” prädiktives Modell zur Vorhersage des Diamantenpreises im Datensatz diamonds!\nGegenstand dieser Aufgabe ist die Modellierung; Datenvorverarbeitug (wie explorative Datenanalyse) steht nicht im Fokus.\nHinweise:\n\nVerwenden Sie die Methoden aus tidymodels.\nHohe Modellgüte (“gutes Modell”) sei definiert über \\(R^2\\), RMSE und MAE\nVerwenden Sie verschiedene Algorithmen (lineare Modell, kNN, …) und verschiedene Rezepte.\nResampling und Tuning ist hier noch nicht nötig.s\n\nDer Datensatz ist hier zu beziehen. Außerdem ist er Teil von ggplot2 bzw. des Tidyverse und daher mit data() zu laden, wenn das entsprechende Paket vorhanden ist.\n         \n\n\nLösung\n\n\nSetup\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nDaten laden:\n\ndata(diamonds, package = \"ggplot2\")\n\nOder so:\n\ndiamonds &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/ggplot2/diamonds.csv\")\n\nRows: 53940 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): cut, color, clarity\ndbl (8): rownames, carat, depth, table, price, x, y, z\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nTrain- vs. Testdaten:\n\nd_split &lt;- initial_split(diamonds, strata = price)\n\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\n\n\nModelle:\n\nlin_mod &lt;-\n  linear_reg()\n\n\nknn_mod &lt;-\n  nearest_neighbor(mode = \"regression\")\n\nHilfe zu kNN findet sich z.B. hier.\n\n\nRezepte:\n\nrec1 &lt;-\n  recipe(price ~ ., data = d_train) %&gt;% \n  update_role(1, new_role = \"id\") %&gt;% \n  step_naomit() %&gt;% \n  step_log(all_outcomes())\n\n\n\nRezept prüfen (preppen und backen)\n\nrec1_prepped &lt;-\n  rec1 %&gt;% \n  prep()\n\nrec1_prepped\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 9\nid:        1\n\n\n\n\n\n── Training information \n\n\nTraining data contained 40453 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Removing rows with NA values in: &lt;none&gt; | Trained\n\n\n• Log transformation on: price | Trained\n\n\n\nd_train_baked &lt;-\n  bake(rec1_prepped, new_data = d_train)\n\nEinen Überblick zu steps findet sich z.B. hier.\nRollen-Definitionen in Tidymodels-Rezepten kann man hier nachlesen.\n\nrec2 &lt;-\n  recipe(price ~ ., data = d_train) %&gt;% \n  update_role(1, new_role = \"id\") %&gt;% \n  step_impute_knn() %&gt;% \n  step_log(all_outcomes())\n\n\n\nWorkflows:\n\nwf1 &lt;-\n  workflow() %&gt;% \n  add_recipe(rec1) %&gt;% \n  add_model(lin_mod)\n\n\nwf2 &lt;-\n  wf1 %&gt;% \n  update_model(knn_mod)\n\n\n\nFitting\n\nfit1 &lt;-\n  wf1 %&gt;% \n  fit(d_train)\nfit1\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_naomit()\n• step_log()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n (Intercept)         carat       cutGood      cutIdeal    cutPremium  \n   -2.913222     -0.540689      0.091189      0.155213      0.108878  \ncutVery Good        colorE        colorF        colorG        colorH  \n    0.124711     -0.061150     -0.091230     -0.157784     -0.259018  \n      colorI        colorJ     clarityIF    claritySI1    claritySI2  \n   -0.386879     -0.528595      1.093164      0.607816      0.440536  \n  clarityVS1    clarityVS2   clarityVVS1   clarityVVS2         depth  \n    0.817124      0.751466      1.000923      0.935170      0.050243  \n       table             x             y             z  \n    0.009026      1.156195      0.012648      0.040728  \n\n\n\n\nFitten des Test-Samples\n\nfit1_test &lt;-\n  wf1 %&gt;% \n  last_fit(d_split)\nfit1_test\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits                id             .metrics .notes   .predictions .workflow \n  &lt;list&gt;                &lt;chr&gt;          &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [40453/13487]&gt; train/test sp… &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n\n\n\nModellgüte\n\ncollect_metrics(fit1_test)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.159 Preprocessor1_Model1\n2 rsq     standard       0.976 Preprocessor1_Model1\n\n\nDe-logarithmieren, wenn man Vorhersagen in den Rohwerten haben möchte:\n\ncollect_predictions(fit1_test) %&gt;% \n  head()\n\n# A tibble: 6 × 5\n  id               .pred  .row price .config             \n  &lt;chr&gt;            &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;               \n1 train/test split  5.81     5  5.81 Preprocessor1_Model1\n2 train/test split  5.86     6  5.82 Preprocessor1_Model1\n3 train/test split  5.89     8  5.82 Preprocessor1_Model1\n4 train/test split  6.10     9  5.82 Preprocessor1_Model1\n5 train/test split  5.85    21  5.86 Preprocessor1_Model1\n6 train/test split  5.90    25  5.87 Preprocessor1_Model1\n\n\n\nd_test_w_preds &lt;- \ncollect_predictions(fit1_test) %&gt;% \n  mutate(pred_raw = exp(.pred)) \n\nhead(d_test_w_preds)\n\n# A tibble: 6 × 6\n  id               .pred  .row price .config              pred_raw\n  &lt;chr&gt;            &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;                   &lt;dbl&gt;\n1 train/test split  5.81     5  5.81 Preprocessor1_Model1     334.\n2 train/test split  5.86     6  5.82 Preprocessor1_Model1     352.\n3 train/test split  5.89     8  5.82 Preprocessor1_Model1     360.\n4 train/test split  6.10     9  5.82 Preprocessor1_Model1     447.\n5 train/test split  5.85    21  5.86 Preprocessor1_Model1     346.\n6 train/test split  5.90    25  5.87 Preprocessor1_Model1     364.\n\n\n\nCategories:\n\nds1\ntidymodels\nstatlearning\nstring"
  },
  {
    "objectID": "posts/Streuung-Histogramm/Streuung-Histogramm.html",
    "href": "posts/Streuung-Histogramm/Streuung-Histogramm.html",
    "title": "Streuung-Histogramm",
    "section": "",
    "text": "Wählen Sie das Diagramm, in dem der vertikale gestrichelte Linie am genauesten die Position des Medians (\\(Md\\)) widerspiegelt.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD"
  },
  {
    "objectID": "posts/Streuung-Histogramm/Streuung-Histogramm.html#answerlist",
    "href": "posts/Streuung-Histogramm/Streuung-Histogramm.html#answerlist",
    "title": "Streuung-Histogramm",
    "section": "",
    "text": "A\nB\nC\nD"
  },
  {
    "objectID": "posts/Streuung-Histogramm/Streuung-Histogramm.html#answerlist-1",
    "href": "posts/Streuung-Histogramm/Streuung-Histogramm.html#answerlist-1",
    "title": "Streuung-Histogramm",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\nFalsch\nFalsch\n\n\nCategories:\n\neda\nstreuungsmaß\nvariability\ndyn\nschoice"
  },
  {
    "objectID": "posts/Nullhyp-Beispiel/Nullhyp-Beispiel.html",
    "href": "posts/Nullhyp-Beispiel/Nullhyp-Beispiel.html",
    "title": "Nullhyp-Beispiel",
    "section": "",
    "text": "Welches der folgenden Beispiele ist kein Beispiel für eine Nullhypothese?\n\n\n\n\\(\\beta_1 &lt;= 0\\)\n\\(\\mu_1 = \\mu_2\\)\n\\(\\mu_1 = \\mu_2 = ... = \\mu_k\\)\n\\(\\rho = 0\\)\n\\(\\pi_1 = \\pi_2\\)"
  },
  {
    "objectID": "posts/Nullhyp-Beispiel/Nullhyp-Beispiel.html#answerlist",
    "href": "posts/Nullhyp-Beispiel/Nullhyp-Beispiel.html#answerlist",
    "title": "Nullhyp-Beispiel",
    "section": "",
    "text": "\\(\\beta_1 &lt;= 0\\)\n\\(\\mu_1 = \\mu_2\\)\n\\(\\mu_1 = \\mu_2 = ... = \\mu_k\\)\n\\(\\rho = 0\\)\n\\(\\pi_1 = \\pi_2\\)"
  },
  {
    "objectID": "posts/Nullhyp-Beispiel/Nullhyp-Beispiel.html#answerlist-1",
    "href": "posts/Nullhyp-Beispiel/Nullhyp-Beispiel.html#answerlist-1",
    "title": "Nullhyp-Beispiel",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nnull\ninference"
  },
  {
    "objectID": "posts/mariokart-sd1/mariokart-sd1.html",
    "href": "posts/mariokart-sd1/mariokart-sd1.html",
    "title": "mariokart-sd1",
    "section": "",
    "text": "Aufgabe\nImportieren Sie den Datensatz mariokart in R. Berechnen Sie die SD des Verkaufspreis (total_pr) für Spiele, die neu sind oder (auch) über Lenkräder (wheels) verfügen.\nHinweise:\n\nRunden Sie auf 1 Dezimalstelle.\n\n         \n\n\nLösung\nPakete starten:\n\nlibrary(tidyverse)\nlibrary(easystats)\n\nDaten importieren:\n\nd_url &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\"\nmariokart &lt;- data_read(d_url)\n\n\nsolution &lt;-\nmariokart  %&gt;% \n  filter(cond == \"new\" | wheels &gt; 0) %&gt;% \n  summarise(pr_mean = sd(total_pr))\n\nsolution\n\n   pr_mean\n1 27.54928\n\n\nLösung: 27.5.\n\nCategories:\n\ndatawrangling\ndplyr\neda\nvariability\nnum"
  },
  {
    "objectID": "posts/Skalenniveau1a/Skalenniveau1a.html",
    "href": "posts/Skalenniveau1a/Skalenniveau1a.html",
    "title": "Skalenniveau1a",
    "section": "",
    "text": "Verfügt die Variable Alter über ein metrisches Skalenniveau?\n\n\n\nnein\nja"
  },
  {
    "objectID": "posts/Skalenniveau1a/Skalenniveau1a.html#answerlist",
    "href": "posts/Skalenniveau1a/Skalenniveau1a.html#answerlist",
    "title": "Skalenniveau1a",
    "section": "",
    "text": "nein\nja"
  },
  {
    "objectID": "posts/Priorwahl1/Priorwahl1.html",
    "href": "posts/Priorwahl1/Priorwahl1.html",
    "title": "Priorwahl1",
    "section": "",
    "text": "Exercise\nEi Forschi wählt für ein Regressionsmodell \\(\\beta \\sim \\mathcal{N}(0,500)\\) (Priori), wobei die empirischen Variablen z-standardisiert sind. Beziehen Sie Stellung zu diesem Prior.\n         \n\n\nSolution\nDie Priori-Verteilung ist nicht sinnvoll spezifiziert. Die Streuung der Normalverteilung ist so groß, dass sie fast schon uniform verteilt ist. Dieser Priori-Verteilung nimmt z.B. an, \\(Pr(|\\beta| &lt; 250) &lt; Pr(|\\beta| &gt; 250)\\), was eine sehr wilde Vorstellung ist. Man könnte sagen: Die Verteilung nimmt an, dass es wahrscheinlicher ist, dass ihr bester Freund 100 Millionen Lichtjahre entfernt lebt, als dass er näher als diese Distanz bei Ihnen lebt.\nWeitere Hinweise hier\nZur Verdeutlichung: Wie wahrscheinlich ist \\(q=1,2,...,10\\) bei einer Normalverteilung zu betrachten?\nFür \\(q=1\\) beträgt die Wahrscheinlichkeit für einen Wert nicht höher als \\(q=1\\) etwa 84%:\n\npnorm(q = 1)\n\n[1] 0.8413447\n\n\nAllgemeiner:\n\noptions(digits = 20)  # Mehr Nachkommastellen\npnorm(q = 1:10)\n\n [1] 0.84134474606854292578 0.97724986805182079141 0.99865010196836989653\n [4] 0.99996832875816688002 0.99999971334842807646 0.99999999901341229958\n [7] 0.99999999999872013490 0.99999999999999933387 1.00000000000000000000\n[10] 1.00000000000000000000\n\n\nDie Wahrscheinlichkeiten für Sigma-Ereignisse bis zu ±7 finden sich z.B. hier.\n\noptions(digits = 2)\n\nVertiefung:\nNassim Taleb hat dieses Argument in seinem Buch “Statistical Consequences of Fat Tails” aufgegriffen (ein anspruchsvolles Buch). Hier finden Sie eine interessante Darstellung eines Arguments daraus.\n\nCategories:\n\nfat-tails\ndistributions"
  },
  {
    "objectID": "posts/mtcars-abhaengig/mtcars-abhaengig.html",
    "href": "posts/mtcars-abhaengig/mtcars-abhaengig.html",
    "title": "mtcars-abhaengig",
    "section": "",
    "text": "Exercise\nOb wohl die PS-Zahl (Ereignis \\(A\\)) und der Spritverbrauch (Ereignis \\(B\\)) voneinander abhängig sind? Was meinen Sie? Was ist Ihre Einschätzung dazu? Vermutlich haben Sie ein (wenn vielleicht auch implizites) Vorab-Wissen zu dieser Frage. Lassen wir dieses Vorab-Wissen aber einmal außen vor und schauen uns rein Daten dazu an. Vereinfachen wir die Frage etwas, indem wir fragen, ob die Ereignisse “hoher Spritverbrauch” (A) und “hohe PS-Zahl” voneinander abhängig sind.\nUm es konkret zu machen, nutzen wir den Datensatz mtcars:\n\nlibrary(tidyverse)\ndata(mtcars)\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n\nWeitere Infos zum Datensatz bekommen Sie mit help(mtcars) in R.\nDefinieren wir uns das Ereignis “hohe PS-Zahl” (und nennen wir es hp_high, klingt cooler). Sagen wir, wenn die PS-Zahl größer ist als der Median, dann trifft hp_high zu, ansonsten nicht:\n\nmtcars %&gt;% \n  summarise(median(hp))\n\n  median(hp)\n1        123\n\n\nMit dieser “Wenn-Dann-Abfrage” können wir die Variable hp_high mit den Stufen TRUE und FALSE definieren:\n\nmtcars &lt;-\n  mtcars %&gt;% \n  mutate(hp_high = case_when(\n    hp &gt; 123 ~ TRUE,\n    hp &lt;= 123 ~ FALSE\n  ))\n\nGenauso gehen wir mit dem Spritverbrauch vor (mpg_high):\n\nmtcars &lt;- \n  mtcars %&gt;% \n  mutate(mpg_high = case_when(\n    mpg &gt; median(mpg) ~ TRUE,\n    mpg &lt;= median(mpg) ~ FALSE\n  ))\n\n\nSchauen Sie im Datensatz nach, ob unser Vorgehen (Erstellung von hp_high und mpg_high) überhaupt funktioniert hat. Probieren geht über Studieren.\nVisualisieren Sie in geeigneter Form den Zusammenhang.\nBerechnen Sie \\(Pr(\\text{mpg high}|\\text{hphigh})\\) und \\(Pr(\\text{mpg high}|\\neg \\text{hp high})\\) !\n\n         \n\n\nSolution\n\nSchauen wir mal in den Datensatz:\n\n\nmtcars %&gt;% \n  select(hp, hp_high, mpg, mpg_high) %&gt;% \n  slice_head(n = 5)\n\n                   hp hp_high  mpg mpg_high\nMazda RX4         110   FALSE 21.0     TRUE\nMazda RX4 Wag     110   FALSE 21.0     TRUE\nDatsun 710         93   FALSE 22.8     TRUE\nHornet 4 Drive    110   FALSE 21.4     TRUE\nHornet Sportabout 175    TRUE 18.7    FALSE\n\n\n\n\n\n\nmtcars %&gt;% \n  #select(hp_high, mpg_high) %&gt;% \n  ggplot() +\n  aes(x = hp_high, fill = mpg_high) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\nHey, sowas von abhängig voneinander, die zwei Variablen, mpg_high und hp_high!\nDer rechte Balken zeigt \\(Pr(\\text{mpghigh}|\\text{hp high})\\) und \\(Pr(\\neg \\text{mpg high}|\\text{hp high})\\).Der linke Balken zeigt \\(Pr(\\text{mpg high}|\\neg \\text{hp high})\\) und \\(Pr(\\neg \\text{mpg high}|\\neg \\text{hp high})\\).\n\nBerechnen wir die relevanten Anteile:\n\n\nmtcars %&gt;% \n  #select(hp_high, mpg_high) %&gt;% \n  count(hp_high, mpg_high) %&gt;%  # Anzahl pro Zelle der Kontingenztabelle\n  group_by(hp_high) %&gt;%  # die Anteile pro \"Balken\" s. Diagramm\n  mutate(prop = n / sum(n))\n\n# A tibble: 4 × 4\n# Groups:   hp_high [2]\n  hp_high mpg_high     n   prop\n  &lt;lgl&gt;   &lt;lgl&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 FALSE   FALSE        3 0.176 \n2 FALSE   TRUE        14 0.824 \n3 TRUE    FALSE       14 0.933 \n4 TRUE    TRUE         1 0.0667\n\n\nAm besten, Sie führen den letzten Code Schritt für Schritt aus und schauen sich jeweils das Ergebnis an, das hilft beim Verstehen.\nAlternativ kann man sich die Häufigkeiten auch schön bequem ausgeben lassen:\n\nlibrary(mosaic)\ntally(mpg_high ~ hp_high, \n      data = mtcars, \n      format = \"proportion\")\n\n        hp_high\nmpg_high       TRUE      FALSE\n   TRUE  0.06666667 0.82352941\n   FALSE 0.93333333 0.17647059\n\n\n\nCategories:\n\nprobability\ndependent"
  },
  {
    "objectID": "posts/kausal26/kausal26.html",
    "href": "posts/kausal26/kausal26.html",
    "title": "kausal26",
    "section": "",
    "text": "Gegeben sei der DAG g (s. u.). Der DAG verfügt über 7 Variablen, die als Knoten im Graph dargestellt sind (mit \\(x_1, x_2, \\ldots x_n\\) bezeichnet) und über Kanten verbunden sind.\nWelche minimale Variablenmenge muss kontrolliert werden, um den kausalen Effekt von der UV zur AV zu identifizieren?\nUV: x4.\nAV: x5.\nHinweise:\n\nMengen sind mittels geschweifter Klammern gekennzeichnet, z.B. {x8, x9} meint die Menge mit den zwei Elementen x8 und x9.\nDie leere Menge { } bedeutet, dass keine Variable kontrolliert werden muss, um den kausalen Effekt zu identifizieren.\nAlle Variablen werden als gemessen vorausgesetzt.\nEs ist möglich, dass es keine Lösung gibt, dass es also keine Adjustierungsmenge gibt, um den kausalen Effekt zu identifizieren. Wenn dies der Fall sein sollte, wählen Sie “keine Lösung”.\nEs ist möglich, dass einzelne Variablen keine Kanten besitzen, also keine Verbindung zu anderen Variablen (Knoten) haben.\n\n\n\n\n{ x1, x2 }\n/\n{ x2 }\n{ x6, x7 }\n{ x3 }"
  },
  {
    "objectID": "posts/kausal26/kausal26.html#answerlist",
    "href": "posts/kausal26/kausal26.html#answerlist",
    "title": "kausal26",
    "section": "",
    "text": "{ x1, x2 }\n/\n{ x2 }\n{ x6, x7 }\n{ x3 }"
  },
  {
    "objectID": "posts/kausal26/kausal26.html#answerlist-1",
    "href": "posts/kausal26/kausal26.html#answerlist-1",
    "title": "kausal26",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nRichtig\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/Nerd-gelockert/Nerd-gelockert.html",
    "href": "posts/Nerd-gelockert/Nerd-gelockert.html",
    "title": "Nerd-gelockert",
    "section": "",
    "text": "Aufgabe\nIn einer Studie werden Persönlichkeitsmerkmale von Professoren untersucht. Laut der Studie wird bei 12% extreme Nerdigkeit festgestellt, bei 8% stark gelockerte Assoziation (vulgo: Schraube locker). Bei 4% wurden beide Merkmale festgestellt.\nAufgabe: Wie groß ist die Wahrscheinlichkeit, dass ein extrem nerdiger Prof gelockerte Assoziationen aufweist?\nAnders gesagt: Die Wahrscheinlichkeit von gelockerter Assoziation gegeben dass der Prof extermer Nerd ist.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n         \n\n\nLösung\nSei \\(N\\) “extreme Nerdigkeit”.\nSei \\(A\\) “gelockerte Assoziation”.\n\\(Pr(A|N) = \\frac{Pr(N \\cap A)}{Pr(N)} = \\frac{Pr(NA)}{Pr(N)}\\)\n\nPr_N &lt;- .12\nPr_A &lt;- .08\nPr_NA &lt;- .04\nPr_A_geg_N &lt;- (Pr_NA) / Pr_N\nPr_A_geg_N\n\n[1] 0.3333333\n\n\nDie Lösung lautet 0.3333333.\n\nCategories:\n\nR\nprobability\nnum"
  },
  {
    "objectID": "posts/kausal10/kausal10.html",
    "href": "posts/kausal10/kausal10.html",
    "title": "kausal10",
    "section": "",
    "text": "Ein Forschungsteam aus Psychologen und Medizinern untersucht die Frage, ob (höhere) Bereitschaft für eine OP und zu Veränderung in ihrer Lebensführung, nach einem Jahr über einen (höheren) Schmerzrückgang führt. Das hießt, Patienten geringerer Bereitschaft sollten es entsprechend zu weniger Schmerzrückgang kommen. Die Bereitschaft der Patienten (ein theoretisches Konstrukt, was nicht direkt beobachtbar ist) wurde mittels eines psychometrisch validierten Fragebogen erhoben. Die Studie umfasst ausschließlich Patienten, die eine OP wegen Rückenschmerzen durchlaufen sind (s. DAG).\nDas Studiendesign impliziert, dass nur Patienten, die eine OP durchlaufen haben, in die Studie aufgenommen wurde. Damit wird per Design diese Variable stratifiziert (kontrolliert).\n\n\n\n\n\n\n\n\n\nDurch die Stratifizierung wird ein Hintertürpfad geöffnet; dieser muss geschlossen werden. Wie sollte dies geschehen (in diesem Modell)?\nIm folgenden Diagramm ist der Kollisionsbias kenntlich gemacht, der durch die Stratifizierung von Surgical Status entsteht:\n\n\n\n\n\n\n\n\n\nHinweis:\n\nWenn von “kausaler Effekt” gesprochen wird, ist stets der (totale) kausale Effekt wie oben definiert gemeint.\nGehen Sie davon aus, dass die Daten zur Studie wie oben dargestellt erhoben und zugänglich sind; die Datenerhebung aber abgeschlossen ist.\n\n\n\n\nEs sollte vom Forschungsteam auf Baseline Pane kontrolliert werden, um den kausalen Effekt zu identifizieren.\nEs sollte vom Forschungsteam auf Underlying Readiness kontrolliert werden, um den kausalen Effekt zu identifizieren.\nEs sollte vom Forschungsteam auf Surgical Status kontrolliert werden, um den kausalen Effekt zu identifizieren.\nEs sollte vom Forschungsteam auf Change in Pain kontrolliert werden, um den kausalen Effekt zu identifizieren.\nEs sollte vom Forschungsteam auf Measured Readiness kontrolliert werden, um den kausalen Effekt zu identifizieren."
  },
  {
    "objectID": "posts/kausal10/kausal10.html#answerlist",
    "href": "posts/kausal10/kausal10.html#answerlist",
    "title": "kausal10",
    "section": "",
    "text": "Es sollte vom Forschungsteam auf Baseline Pane kontrolliert werden, um den kausalen Effekt zu identifizieren.\nEs sollte vom Forschungsteam auf Underlying Readiness kontrolliert werden, um den kausalen Effekt zu identifizieren.\nEs sollte vom Forschungsteam auf Surgical Status kontrolliert werden, um den kausalen Effekt zu identifizieren.\nEs sollte vom Forschungsteam auf Change in Pain kontrolliert werden, um den kausalen Effekt zu identifizieren.\nEs sollte vom Forschungsteam auf Measured Readiness kontrolliert werden, um den kausalen Effekt zu identifizieren."
  },
  {
    "objectID": "posts/kausal10/kausal10.html#answerlist-1",
    "href": "posts/kausal10/kausal10.html#answerlist-1",
    "title": "kausal10",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/Boosting1/Boosting1.html",
    "href": "posts/Boosting1/Boosting1.html",
    "title": "Boosting1",
    "section": "",
    "text": "Es handelt sich um ein lineares Modell, wenn man unter linear ein Modell folgender Form versteht:\n\\[f(X)=\\sum_{j=1}^p f_i(X_J)\\]\nWählen Sie die am besten passende Begründung, warum man unter Boosting ein lineares Modell versteht bzw. verstehen kann.\n\n\n\nBoosting-Modelle bestehen aus einer Sequenz von Bäumen mit jeweils nur einer Variablen (Gabelung; internal nodes) und sind daher linear.\nBaumbasierte Modelle sind stets linear; Boosting ist ein baumbasiertes Modell, daher ist Boosting ein lineares Modell.\nBaumbasierte Modelle haben stets den oben skizzierten Funktionsterm \\(f(X)\\).\nBoosting gleicht einem Random-Forest-Modell, nur dass die Bäume sequenzielle Modelle darstellen und nicht parallel (gleichzeitig) in ein Modell einfließen. Daher ist Boosting ein lineares Modell.\nAlle Boosting-Modelle erfüllen obige Funktionsgleichung und sind daher immer linear."
  },
  {
    "objectID": "posts/Boosting1/Boosting1.html#answerlist",
    "href": "posts/Boosting1/Boosting1.html#answerlist",
    "title": "Boosting1",
    "section": "",
    "text": "Boosting-Modelle bestehen aus einer Sequenz von Bäumen mit jeweils nur einer Variablen (Gabelung; internal nodes) und sind daher linear.\nBaumbasierte Modelle sind stets linear; Boosting ist ein baumbasiertes Modell, daher ist Boosting ein lineares Modell.\nBaumbasierte Modelle haben stets den oben skizzierten Funktionsterm \\(f(X)\\).\nBoosting gleicht einem Random-Forest-Modell, nur dass die Bäume sequenzielle Modelle darstellen und nicht parallel (gleichzeitig) in ein Modell einfließen. Daher ist Boosting ein lineares Modell.\nAlle Boosting-Modelle erfüllen obige Funktionsgleichung und sind daher immer linear."
  },
  {
    "objectID": "posts/Boosting1/Boosting1.html#answerlist-1",
    "href": "posts/Boosting1/Boosting1.html#answerlist-1",
    "title": "Boosting1",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nWahr\nWahr\nWahr\nWahr\n\n\nCategories:\nmchoice"
  },
  {
    "objectID": "posts/log-y-regr3/log-y-regr3.html",
    "href": "posts/log-y-regr3/log-y-regr3.html",
    "title": "log-y-regression3",
    "section": "",
    "text": "library(tidyverse)\nlibrary(easystats)\n\nIn dieser Aufgabe modellieren wir den (kausalen) Effekt von Schulbildung auf das Einkommen.\nImportieren Sie zunächst den Datensatz und verschaffen Sie sich einen Überblick.\n\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Treatment.csv\"\n\nd &lt;- data_read(d_path)\n\nDokumentation und Quellenangaben zum Datensatz finden sich hier.\n\nglimpse(d)\n\nRows: 2,675\nColumns: 11\n$ rownames &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ treat    &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n$ age      &lt;int&gt; 37, 30, 27, 33, 22, 23, 32, 22, 19, 21, 18, 27, 17, 19, 27, 2…\n$ educ     &lt;int&gt; 11, 12, 11, 8, 9, 12, 11, 16, 9, 13, 8, 10, 7, 10, 13, 10, 12…\n$ ethn     &lt;chr&gt; \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\"…\n$ married  &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,…\n$ re74     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ re75     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ re78     &lt;dbl&gt; 9930.05, 24909.50, 7506.15, 289.79, 4056.49, 0.00, 8472.16, 2…\n$ u74      &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n$ u75      &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n\n\nWelcher der Prädiktoren hat den stärkesten Einfluss auf das Einkommen?\nHinweise:\n\nVerwenden Sie lm zur Modellierung.\nOperationalisieren Sie das Einkommen mit der Variable re74.\nGehen Sie von einem kausalen Effekt der Prädiktoren aus.\nGehen Sie von einem multiplikativen Modell aus (log-y).\nLassen Sie die Variablen zur Arbeitslosigkeit außen vor.\n\n\n\n\ntreat\nage\neduc\nethn\nmarried"
  },
  {
    "objectID": "posts/log-y-regr3/log-y-regr3.html#answerlist",
    "href": "posts/log-y-regr3/log-y-regr3.html#answerlist",
    "title": "log-y-regression3",
    "section": "",
    "text": "treat\nage\neduc\nethn\nmarried"
  },
  {
    "objectID": "posts/log-y-regr3/log-y-regr3.html#answerlist-1",
    "href": "posts/log-y-regr3/log-y-regr3.html#answerlist-1",
    "title": "log-y-regression3",
    "section": "Answerlist",
    "text": "Answerlist\n\nTRUE\nFALSE\nFALSE\nFALSE\nFALSE\n\n\nCategories:\n\nstats-nutshell\nqm2\nregression\nlog"
  },
  {
    "objectID": "posts/ReThink4e2/ReThink4e2.html",
    "href": "posts/ReThink4e2/ReThink4e2.html",
    "title": "ReThink4e2",
    "section": "",
    "text": "Wie viele Parameter sind im folgenden Modell zu schätzen?\nLikelihood: \\(h_i \\sim \\mathcal{N}(\\mu, \\sigma)\\)\nPrior für \\(\\mu\\): \\(\\mu \\sim \\mathcal{N}(178, 20)\\)\nPrior für \\(\\sigma\\): \\(\\sigma \\sim \\mathcal{U}(0, 50)\\)\nQuelle: McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Aufl.). Taylor and Francis, CRC Press.\n\n\n\n0\n1\n2\n3\nmehr"
  },
  {
    "objectID": "posts/ReThink4e2/ReThink4e2.html#answerlist",
    "href": "posts/ReThink4e2/ReThink4e2.html#answerlist",
    "title": "ReThink4e2",
    "section": "",
    "text": "0\n1\n2\n3\nmehr"
  },
  {
    "objectID": "posts/ReThink4e2/ReThink4e2.html#answerlist-1",
    "href": "posts/ReThink4e2/ReThink4e2.html#answerlist-1",
    "title": "ReThink4e2",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nWahr\nFalsch\nFalsch\n\n\nCategories:\n\nprobability\nbayes"
  },
  {
    "objectID": "posts/tidymodels-remove-na/tidymodels-remove-na.html",
    "href": "posts/tidymodels-remove-na/tidymodels-remove-na.html",
    "title": "tidymodels-remove-na",
    "section": "",
    "text": "Aufgabe\n\nErstellen Sie ein Rezept, dass die fehlenden Werte aus dem Datensatz penguins entfernt.\nHinweise:\n\nVerwenden Sie tidymodels.\nVerwenden Sie Standardwerte, wo nicht anders angegeben.\nFixieren Sie Zufallszahlen auf den Startwert 42.\n\n         \n\n\nLösung\n\n# Setup:\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.0.8\n✔ dials        1.2.0     ✔ rsample      1.2.0\n✔ dplyr        1.1.3     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.4\n✔ lubridate 1.9.3     ✔ stringr   1.5.0\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tictoc)  # Zeitmessung\n\n\n\n# Data:\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\"\nd &lt;- read_csv(d_path)\n\nRows: 344 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): species, island, sex\ndbl (6): rownames, bill_length_mm, bill_depth_mm, flipper_length_mm, body_ma...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# recipe:\nrec1 &lt;- recipe(body_mass_g ~  ., data = d) |&gt; \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_normalize(all_predictors()) |&gt; \n  step_naomit(all_predictors()) \n\nAls Check: Das gepreppte/bebackene Rezept:\n\nrec1_prepped &lt;- prep(rec1)\n\nWarning: There are new levels in a factor: NA\n\nd_train_baked &lt;- bake(rec1_prepped, new_data = NULL)\n\n\nd_train_baked |&gt; \n  head()\n\n# A tibble: 6 × 11\n  rownames bill_length_mm bill_depth_mm flipper_length_mm  year body_mass_g\n     &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n1    -1.72         -0.883         0.784            -1.42  -1.26        3750\n2    -1.71         -0.810         0.126            -1.06  -1.26        3800\n3    -1.70         -0.663         0.430            -0.421 -1.26        3250\n4    -1.68         -1.32          1.09             -0.563 -1.26        3450\n5    -1.67         -0.847         1.75             -0.776 -1.26        3650\n6    -1.66         -0.920         0.329            -1.42  -1.26        3625\n# ℹ 5 more variables: species_Chinstrap &lt;dbl&gt;, species_Gentoo &lt;dbl&gt;,\n#   island_Dream &lt;dbl&gt;, island_Torgersen &lt;dbl&gt;, sex_male &lt;dbl&gt;\n\n\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.6.0 (red = needs update)\n✔ bayestestR  0.13.1   ✔ correlation 0.8.4 \n✔ datawizard  0.9.0    ✔ effectsize  0.8.6 \n✔ insight     0.19.6   ✔ modelbased  0.8.6 \n✔ performance 0.10.8   ✔ parameters  0.21.3\n✔ report      0.5.7    ✖ see         0.8.0 \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`.\n\ndescribe_distribution(d_train_baked)\n\nVariable          |      Mean |     SD |     IQR |              Range | Skewness | Kurtosis |   n | n_Missing\n-------------------------------------------------------------------------------------------------------------\nrownames          |      0.02 |   0.99 |    1.71 |      [-1.72, 1.72] |     0.01 |    -1.19 | 333 |         0\nbill_length_mm    |      0.01 |   1.00 |    1.69 |      [-2.17, 2.87] |     0.05 |    -0.88 | 333 |         0\nbill_depth_mm     |  6.94e-03 |   1.00 |    1.57 |      [-2.05, 2.20] |    -0.15 |    -0.89 | 333 |         0\nflipper_length_mm |  3.68e-03 |   1.00 |    1.64 |      [-2.06, 2.14] |     0.36 |    -0.96 | 333 |         0\nyear              |      0.02 |   0.99 |    2.44 |      [-1.26, 1.19] |    -0.08 |    -1.48 | 333 |         0\nbody_mass_g       |   4207.06 | 805.22 | 1237.50 | [2700.00, 6300.00] |     0.47 |    -0.73 | 333 |         0\nspecies_Chinstrap |      0.02 |   1.01 |    0.00 |      [-0.50, 2.01] |     1.47 |     0.17 | 333 |         0\nspecies_Gentoo    | -6.46e-03 |   1.00 |    2.08 |      [-0.75, 1.33] |     0.60 |    -1.65 | 333 |         0\nisland_Dream      |      0.02 |   1.01 |    2.08 |      [-0.75, 1.33] |     0.54 |    -1.71 | 333 |         0\nisland_Torgersen  |     -0.03 |   0.97 |    0.00 |      [-0.42, 2.37] |     2.07 |     2.30 | 333 |         0\nsex_male          |  8.40e-17 |   1.00 |    2.00 |      [-1.01, 0.99] |    -0.02 |    -2.01 | 333 |         0\n\n\n\nCategories:\n\ntidymodels\nstatlearning\ntemplate\nstring"
  },
  {
    "objectID": "posts/wrangle10/wrangle10.html",
    "href": "posts/wrangle10/wrangle10.html",
    "title": "wrangle10",
    "section": "",
    "text": "Aufgabe\nBetrachten Sie folgende Tabelle:\n\ndf &lt;- tibble(\n  groesse = c(180, 190, 160, 170),\n  geschlecht = c(\"m\", \"m\", \"f\", \"f\")\n)\ndf\n\n\n\n\ngroesse\ngeschlecht\n\n\n\n\n180\nm\n\n\n190\nm\n\n\n160\nf\n\n\n170\nf\n\n\n\n\n\nHinweis: Der Befehl tibble erstellt einen Tibble (Dataframe).\nWas ist er erste Wert, den der folgende Ausdruck zurückliefert?\n\ndf_grouped &lt;- group_by(df, geschlecht)\n\nsummarise(df_grouped, ergebnis = mean(groesse))\n\n         \n\n\nLösung\nDie Werte werden alphabetisch (bzw. alphanumerisch) sortiert. “f” kommt vor “m” im Alphabet.\nAntwort: 165\n\ndf_grouped &lt;- group_by(df, geschlecht)\n\nsummarise(df_grouped, ergebnis = mean(groesse))\n\n\n\n\ngeschlecht\nergebnis\n\n\n\n\nf\n165\n\n\nm\n185\n\n\n\n\n\n\nCategories:\n\neda\nlagemaße\nnum"
  },
  {
    "objectID": "posts/summarise06/summarise06.html",
    "href": "posts/summarise06/summarise06.html",
    "title": "summarise06",
    "section": "",
    "text": "Aufgabe\nIm Datensatz mariokart:\nFassen Sie die Spalte total_pr zusammen und zwar zu verschiedenene Maßen der Streuung (keine Gruppierung).\nWelchem Koeffizienten der Streuung schenken Sie am meisten Vertrauen in diesem Fall? Geben Sie den Wert als Antwort an.\n         \n\n\nLösung\nPakete starten:\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.6.0 (red = needs update)\n✔ bayestestR  0.13.1   ✔ correlation 0.8.4 \n✔ datawizard  0.9.0    ✔ effectsize  0.8.6 \n✔ insight     0.19.6   ✔ modelbased  0.8.6 \n✔ performance 0.10.8   ✔ parameters  0.21.3\n✔ report      0.5.7    ✖ see         0.8.0 \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`.\n\nlibrary(tidyverse)  # startet das Paket tidyverse\n\nDaten importieren:\n\nmariokart &lt;- data_read(\"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\")\n\nOder so:\n\ndata(mariokart, package = \"openintro\")  # aus dem Paket \"openintro\"\n\nDazu muss das Paket openintro auf Ihrem Computer installiert sein.\nZusammenfassen:\n\nlibrary(DescTools)\nmariokart_summarised &lt;- summarise(mariokart, \n                                  pr_sd = sd(total_pr),\n                                  pr_iqr = IQR(total_pr),\n                                  pr_maa = mean(abs(total_pr - mean(total_pr))),\n                                  pr_maa2 = MeanAD(total_pr)\n)  # zusammenfassen\nmariokart_summarised\n\n# A tibble: 1 × 4\n  pr_sd pr_iqr pr_maa pr_maa2\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1  25.7   12.8   10.0    10.0\n\n\nMöchte man den MAA nicht von Hand ausrechnen, so kann man die Funktion MeanAD aus dem Paket DescTools nutzen (Denken Sie daran, dass Sie das Paket einmalig installiert haben müssen.)\nDa es Extremwerte gibt in total_pr wird die SD besonders hoch sein. Der Grund ist, dass die SD eine Statistik ist, die auf einem Mittelwert beruht. Außerdem werden bei der Berechnung der SD die einzelnen Werte quadriert, was große Werte überproportional vergrößert. Aus diesem Grund könnte der IQR hier gegenüber anderen Maßen bevorzugt werden.\nLösung: 12.82\n\nCategories:\n\ndatawrangling\neda\ntidyverse\ndplyr\nvariability\nnum"
  },
  {
    "objectID": "posts/tidymodels-poly01/tidymodels-poly01.html",
    "href": "posts/tidymodels-poly01/tidymodels-poly01.html",
    "title": "tidymodels-poly01",
    "section": "",
    "text": "Aufgabe\nFitten Sie ein Polynomial-Modell für folgende Modellgleichung:\nbody_mass_g ~ bill_length_mm.\nGesucht ist der optimale Polynomgrad im Train-Sample (optimal hinsichtlich minimalem Prognosefehler).\nHinweise:\n\nDatensatz penguins (palmerpenguins)\nVerwenden Sie Tidymodels\nFitten Sie Polynome des Grades 1 bis 10.\nDefinieren Sie die Polynomegrade als Tuningparameter.\nBeziehen Sie sich auf RMSE als Kennzahl der Modellgüte.\nEntfernen Sie fehlende Werte in den Prädiktoren\n\n         \n\n\nLösung\nSetup:\n\nlibrary(tidymodels)\ndata(penguins, package = \"palmerpenguins\")\n\nRezept:\n\nrec1 &lt;- \n  recipe(body_mass_g ~ bill_length_mm, data = penguins) %&gt;% \n  step_naomit(all_predictors()) %&gt;% \n  step_poly(all_predictors(), degree = tune()) %&gt;% \n  update_role(contains(\"_poly_\"), new_role = \"predictor\")\n\nWarning: No columns were selected in `update_role()`.\n\n\nCheck:\n\nd_baked &lt;- bake(prep(rec1), new_data = NULL)\n\nWorkflow:\n\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(linear_reg()) %&gt;% \n  add_recipe(rec1)\n\nRezepte mit Tuningparametern kann man nicht preppen/backen.\nTuning:\n\ntune1 &lt;-\n  tune_grid(\n    wf1,\n    resamples = vfold_cv(data = penguins),\n    metrics = metric_set(rmse),\n    grid = grid_regular(degree(range = c(1, 10)),\n                               levels = 10)\n  )\n\n\nautoplot(tune1)\n\n\n\n\n\n\n\n\n\nshow_best(tune1)\n\n# A tibble: 5 × 7\n  degree .metric .estimator  mean     n std_err .config              \n   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1      2 rmse    standard    642.    10    23.5 Preprocessor02_Model1\n2      1 rmse    standard    644.    10    27.4 Preprocessor01_Model1\n3      5 rmse    standard    646.    10    23.9 Preprocessor05_Model1\n4      4 rmse    standard    648.    10    25.4 Preprocessor04_Model1\n5      3 rmse    standard    654.    10    27.8 Preprocessor03_Model1\n\n\n\nsol &lt;- show_best(tune1)$degree[1]\nsol\n\n[1] 2\n\n\nDie Antwort lautet: 2.\n\nCategories:\n\nR\nstatlearning\ntidymodels\nnum"
  },
  {
    "objectID": "posts/penguins-stan-05/penguins-stan-05.html",
    "href": "posts/penguins-stan-05/penguins-stan-05.html",
    "title": "penguins-stan-05",
    "section": "",
    "text": "Aufgabe\nWir untersuchen Einflussfaktoren bzw. Prädiktoren auf das Körpergewicht von Pinguinen. In dieser Aufgabe untersuchen wir den Zusammenhang von Schnabellänge (als UV) und Körpergewicht (als AV).\nAufgabe: Wie breit ist das 95%-ETI, wenn Sie nur die Spezies Adelie untersuchen?\nHinweise:\n\nSie können den Datensatz z.B. hier beziehen oder über das R-Paket palmerpenguins.\nWeitere Hinweise\n\n         \n\n\nLösung\nZentrieren ist eigentlich immer nützlich, aber hier streng genommen nicht unbedingt nötig. Der Hauptgrund dafür ist, dass Stan für uns den Prior für den Intercept festlegt, und zwar auf Basis der Daten, wir uns also nicht um die komische Frage zu kümmern brauchen, welchen Prior wir für den unzentrierten Achsenabschnitt vergeben wollten: Wie schwer sind Pinguins der Schnabellänge Null? Mit zentrierten Prädiktoren ist die Frage nach dem Prior viel einfacher zu beantworten: Wie schwer ist ein Pinguin mit mittelgroßem Schnabel?\nSetup:\n\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(rstanarm)\n\ndata(\"penguins\", package = \"palmerpenguins\")\n\nEs wird in dieser Aufgabe vorausgesetzt, dass Sie den Datensatz selbständig importieren können. Tipp: Kurzes Googeln hilft ggf., den Datensatz zu finden.\nAlternativ könnten Sie den Datensatz als CSV-Datei importieren:\n\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\"\n\nEin Blick in die Daten zur Kontrolle, ob das Importieren richtig funktioniert hat:\n\npenguins &lt;- data_read(d_path)\npenguins_adelie &lt;- \n  penguins %&gt;% \n  filter(species == \"Adelie\")\n\nglimpse(penguins)\n\nRows: 344\nColumns: 9\n$ rownames          &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n$ species           &lt;chr&gt; \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"A…\n$ island            &lt;chr&gt; \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", …\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;chr&gt; \"male\", \"female\", \"female\", \"\", \"female\", \"male\", \"f…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nVertrauen ist gut, aber - was Golems betrifft - ist Kontrolle eindeutig besser ;-)\n\nm1 &lt;- stan_glm(body_mass_g ~  bill_length_mm,  # Regressionsgleichung\n               data = penguins_adelie, #  Daten\n               seed = 42,  # Repro.\n               refresh = 0)  # nicht so viel Output\n\n\nparameters(m1, ci = .95, ci_method = \"eti\")\n\nParameter      | Median |            95% CI |     pd |  Rhat |     ESS |                       Prior\n----------------------------------------------------------------------------------------------------\n(Intercept)    |  18.20 | [-881.48, 930.13] | 51.75% | 1.000 | 3988.00 | Normal (3700.66 +- 1146.42)\nbill_length_mm |  94.71 | [  71.21, 118.31] |   100% | 1.000 | 3962.00 |     Normal (0.00 +- 430.43)\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a MCMC distribution approximation.\n\n\nDie Lösung lautet also, wie in der Ausgabe zu den Parametern ersichtlich, 47.1.\n\nCategories:\n\nbayes\nregression\nexam-22"
  },
  {
    "objectID": "posts/tmdb07/tmdb07.html",
    "href": "posts/tmdb07/tmdb07.html",
    "title": "tmdb07",
    "section": "",
    "text": "Aufgabe\nMelden Sie sich an für die Kaggle Competition TMDB Box Office Prediction - Can you predict a movie’s worldwide box office revenue?.\nSie benötigen dazu ein Konto; es ist auch möglich, sich mit seinem Google-Konto anzumelden.\nBei diesem Prognosewettbewerb geht es darum, vorherzusagen, wieviel Umsatz wohl einige Filme machen werden. Als Prädiktoren stehen einige Infos wie Budget, Genre, Titel etc. zur Verfügung. Eine klassische “predictive Competition” also :-) Allerdings können immer ein paar Schwierigkeiten auftreten ;-)\nAufgabe\nErstellen Sie ein Lineares-Modell mit Regularisierung mit Tidymodels!\nHinweise\n\n\nVerzichten Sie auf Vorverarbeitung.\nTunen Sie die typischen Parameter.\nReichen Sie das Modell ein und berichten Sie Ihren Score.\nBegrenzen Sie sich auf folgende Prädiktoren.\n\n\npreds_chosen &lt;- \n  c(\"id\", \"budget\", \"popularity\", \"runtime\")\n\n         \n\n\nLösung\n\n\nPakete starten\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(finetune)\nlibrary(doParallel)\nlibrary(tictoc)\n\n\n\nDaten importieren\n\nd_train_path &lt;- \"https://raw.githubusercontent.com/sebastiansauer/Lehre/main/data/tmdb-box-office-prediction/train.csv\"\nd_test_path &lt;- \"https://raw.githubusercontent.com/sebastiansauer/Lehre/main/data/tmdb-box-office-prediction/test.csv\"\n\nd_train &lt;- read_csv(d_train_path)\nd_test &lt;- read_csv(d_test_path)\n\nWerfen wir einen Blick in die Daten:\n\nglimpse(d_train)\n\nRows: 3,000\nColumns: 23\n$ id                    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ belongs_to_collection &lt;chr&gt; \"[{'id': 313576, 'name': 'Hot Tub Time Machine C…\n$ budget                &lt;dbl&gt; 1.40e+07, 4.00e+07, 3.30e+06, 1.20e+06, 0.00e+00…\n$ genres                &lt;chr&gt; \"[{'id': 35, 'name': 'Comedy'}]\", \"[{'id': 35, '…\n$ homepage              &lt;chr&gt; NA, NA, \"http://sonyclassics.com/whiplash/\", \"ht…\n$ imdb_id               &lt;chr&gt; \"tt2637294\", \"tt0368933\", \"tt2582802\", \"tt182148…\n$ original_language     &lt;chr&gt; \"en\", \"en\", \"en\", \"hi\", \"ko\", \"en\", \"en\", \"en\", …\n$ original_title        &lt;chr&gt; \"Hot Tub Time Machine 2\", \"The Princess Diaries …\n$ overview              &lt;chr&gt; \"When Lou, who has become the \\\"father of the In…\n$ popularity            &lt;dbl&gt; 6.575393, 8.248895, 64.299990, 3.174936, 1.14807…\n$ poster_path           &lt;chr&gt; \"/tQtWuwvMf0hCc2QR2tkolwl7c3c.jpg\", \"/w9Z7A0GHEh…\n$ production_companies  &lt;chr&gt; \"[{'name': 'Paramount Pictures', 'id': 4}, {'nam…\n$ production_countries  &lt;chr&gt; \"[{'iso_3166_1': 'US', 'name': 'United States of…\n$ release_date          &lt;chr&gt; \"2/20/15\", \"8/6/04\", \"10/10/14\", \"3/9/12\", \"2/5/…\n$ runtime               &lt;dbl&gt; 93, 113, 105, 122, 118, 83, 92, 84, 100, 91, 119…\n$ spoken_languages      &lt;chr&gt; \"[{'iso_639_1': 'en', 'name': 'English'}]\", \"[{'…\n$ status                &lt;chr&gt; \"Released\", \"Released\", \"Released\", \"Released\", …\n$ tagline               &lt;chr&gt; \"The Laws of Space and Time are About to be Viol…\n$ title                 &lt;chr&gt; \"Hot Tub Time Machine 2\", \"The Princess Diaries …\n$ Keywords              &lt;chr&gt; \"[{'id': 4379, 'name': 'time travel'}, {'id': 96…\n$ cast                  &lt;chr&gt; \"[{'cast_id': 4, 'character': 'Lou', 'credit_id'…\n$ crew                  &lt;chr&gt; \"[{'credit_id': '59ac067c92514107af02c8c8', 'dep…\n$ revenue               &lt;dbl&gt; 12314651, 95149435, 13092000, 16000000, 3923970,…\n\nglimpse(d_test)\n\nRows: 4,398\nColumns: 22\n$ id                    &lt;dbl&gt; 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, …\n$ belongs_to_collection &lt;chr&gt; \"[{'id': 34055, 'name': 'Pokémon Collection', 'p…\n$ budget                &lt;dbl&gt; 0.00e+00, 8.80e+04, 0.00e+00, 6.80e+06, 2.00e+06…\n$ genres                &lt;chr&gt; \"[{'id': 12, 'name': 'Adventure'}, {'id': 16, 'n…\n$ homepage              &lt;chr&gt; \"http://www.pokemon.com/us/movies/movie-pokemon-…\n$ imdb_id               &lt;chr&gt; \"tt1226251\", \"tt0051380\", \"tt0118556\", \"tt125595…\n$ original_language     &lt;chr&gt; \"ja\", \"en\", \"en\", \"fr\", \"en\", \"en\", \"de\", \"en\", …\n$ original_title        &lt;chr&gt; \"ディアルガVSパルキアVSダークライ\", \"Attack of t…\n$ overview              &lt;chr&gt; \"Ash and friends (this time accompanied by newco…\n$ popularity            &lt;dbl&gt; 3.851534, 3.559789, 8.085194, 8.596012, 3.217680…\n$ poster_path           &lt;chr&gt; \"/tnftmLMemPLduW6MRyZE0ZUD19z.jpg\", \"/9MgBNBqlH1…\n$ production_companies  &lt;chr&gt; NA, \"[{'name': 'Woolner Brothers Pictures Inc.',…\n$ production_countries  &lt;chr&gt; \"[{'iso_3166_1': 'JP', 'name': 'Japan'}, {'iso_3…\n$ release_date          &lt;chr&gt; \"7/14/07\", \"5/19/58\", \"5/23/97\", \"9/4/10\", \"2/11…\n$ runtime               &lt;dbl&gt; 90, 65, 100, 130, 92, 121, 119, 77, 120, 92, 88,…\n$ spoken_languages      &lt;chr&gt; \"[{'iso_639_1': 'en', 'name': 'English'}, {'iso_…\n$ status                &lt;chr&gt; \"Released\", \"Released\", \"Released\", \"Released\", …\n$ tagline               &lt;chr&gt; \"Somewhere Between Time & Space... A Legend Is B…\n$ title                 &lt;chr&gt; \"Pokémon: The Rise of Darkrai\", \"Attack of the 5…\n$ Keywords              &lt;chr&gt; \"[{'id': 11451, 'name': 'pok√©mon'}, {'id': 1155…\n$ cast                  &lt;chr&gt; \"[{'cast_id': 3, 'character': 'Tonio', 'credit_i…\n$ crew                  &lt;chr&gt; \"[{'credit_id': '52fe44e7c3a368484e03d683', 'dep…\n\n\n\n\nResampling / Cross-Validation-Scheme\n\ncv_scheme &lt;- vfold_cv(d_train)\n\nKleine Werte für \\(v\\) wie \\(v=3\\) kann man wählen, um Rechenzeit zu sparen. Das ist gerade fürs Debuggen nützlich. Für die “Wirklichkeit” ist ein höherer Wert besser, z.B. \\(v=10\\) (der Defaultwert)\n\n\nRezept\n\nrec1 &lt;- \n  recipe(revenue ~ budget + popularity + runtime, data = d_train) %&gt;% \n  step_impute_bag(all_predictors()) %&gt;% \n  step_naomit(all_predictors()) \nrec1\n\n\n\nModell\n\nmodel_lm &lt;- linear_reg(penalty = tune(),\n                       engine = \"glmnet\")\n\n\n\nWorkflow\n\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(model_lm) %&gt;% \n  add_recipe(rec1)\n\n\n\nModell fitten (und tunen)\nParallele Verarbeitung starten:\n\ncl &lt;- makePSOCKcluster(4)  # Create 4 clusters\nregisterDoParallel(cl)\n\n\ntic()\nlm_fit1 &lt;-\n  wf1 %&gt;% \n  tune_race_anova(resamples = cv_scheme)\ntoc()\n\n18.691 sec elapsed\n\n\n\nlm_fit1 %&gt;% show_best()\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\n\n# A tibble: 5 × 7\n   penalty .metric .estimator      mean     n  std_err .config              \n     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;                \n1 1.62e-10 rmse    standard   84540989.    10 5365801. Preprocessor1_Model01\n2 3.06e- 9 rmse    standard   84540989.    10 5365801. Preprocessor1_Model02\n3 1.51e- 8 rmse    standard   84540989.    10 5365801. Preprocessor1_Model03\n4 9.21e- 7 rmse    standard   84540989.    10 5365801. Preprocessor1_Model04\n5 2.83e- 6 rmse    standard   84540989.    10 5365801. Preprocessor1_Model05\n\n\n\n\nFinalisieren\n\nwf1_final &lt;-\n  wf1 %&gt;% \n  finalize_workflow(select_best(lm_fit1, metric = \"rmse\"))\n\n\n\nFinal Fit\n\nfit1_final &lt;-\n  wf1_final %&gt;% \n  fit(d_train)\n\nfit1_final\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_impute_bag()\n• step_naomit()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\") \n\n   Df  %Dev    Lambda\n1   0  0.00 103500000\n2   1  9.63  94340000\n3   1 17.62  85960000\n4   1 24.25  78320000\n5   1 29.76  71370000\n6   1 34.33  65030000\n7   1 38.13  59250000\n8   1 41.28  53990000\n9   1 43.90  49190000\n10  1 46.07  44820000\n11  2 48.25  40840000\n12  2 50.48  37210000\n13  2 52.34  33900000\n14  2 53.88  30890000\n15  2 55.15  28150000\n16  2 56.21  25650000\n17  2 57.09  23370000\n18  2 57.82  21290000\n19  2 58.43  19400000\n20  2 58.93  17680000\n21  2 59.35  16110000\n22  2 59.70  14680000\n23  2 59.99  13370000\n24  2 60.23  12180000\n25  2 60.42  11100000\n26  2 60.59  10120000\n27  2 60.73   9217000\n28  2 60.84   8398000\n29  2 60.93   7652000\n30  2 61.01   6973000\n31  2 61.08   6353000\n32  2 61.13   5789000\n33  2 61.18   5274000\n34  2 61.21   4806000\n35  3 61.25   4379000\n36  3 61.29   3990000\n37  3 61.32   3635000\n38  3 61.34   3313000\n39  3 61.36   3018000\n40  3 61.38   2750000\n41  3 61.39   2506000\n42  3 61.40   2283000\n43  3 61.41   2080000\n44  3 61.42   1896000\n45  3 61.43   1727000\n46  3 61.43   1574000\n\n...\nand 12 more lines.\n\n\n\npreds &lt;-\n  fit1_final %&gt;% \n  predict(d_test)\n\n\n\nSubmission df\n\nsubmission_df &lt;-\n  d_test %&gt;% \n  select(id) %&gt;% \n  bind_cols(preds) %&gt;% \n  rename(revenue = .pred)\n\nhead(submission_df)\n\n# A tibble: 6 × 2\n     id   revenue\n  &lt;dbl&gt;     &lt;dbl&gt;\n1  3001 -3508554.\n2  3002 -7712533.\n3  3003  8857329.\n4  3004 31400199.\n5  3005   101521.\n6  3006 13470119.\n\n\nAbspeichern und einreichen:\n\n#write_csv(submission_df, file = \"submission.csv\")\n\n\n\nKaggle Score\nDiese Submission erzielte einen Score von Score: 6.14787 (RMSLE).\n\nsol &lt;- 6.14787\n\n\nCategories:\n\nds1\ntidymodels\nstatlearning\ntmdb\nnum"
  },
  {
    "objectID": "posts/Ridges-vergleichen/Ridges-vergleichen.html",
    "href": "posts/Ridges-vergleichen/Ridges-vergleichen.html",
    "title": "Ridges-vergleichen",
    "section": "",
    "text": "Die Mittelwerte der Histogramme sind identisch.\nDie Mediane der Histogramme sind identisch.\nDie Histogramme sind (alle) linksschief.\nDie Histogramme sind (alle) rechtsschief.\nDie Färbung (Füllfarbe) kodiert die Schliffart (cut).\nEinige Histogramme sind normalverteilt."
  },
  {
    "objectID": "posts/Ridges-vergleichen/Ridges-vergleichen.html#answerlist",
    "href": "posts/Ridges-vergleichen/Ridges-vergleichen.html#answerlist",
    "title": "Ridges-vergleichen",
    "section": "",
    "text": "Die Mittelwerte der Histogramme sind identisch.\nDie Mediane der Histogramme sind identisch.\nDie Histogramme sind (alle) linksschief.\nDie Histogramme sind (alle) rechtsschief.\nDie Färbung (Füllfarbe) kodiert die Schliffart (cut).\nEinige Histogramme sind normalverteilt."
  },
  {
    "objectID": "posts/Ridges-vergleichen/Ridges-vergleichen.html#answerlist-1",
    "href": "posts/Ridges-vergleichen/Ridges-vergleichen.html#answerlist-1",
    "title": "Ridges-vergleichen",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nRichtig\nFalsch\nFalsch\n\n\nCategories:\n\nvis\ndyn\nschoice"
  },
  {
    "objectID": "posts/regr-tree01/regr-tree01.html",
    "href": "posts/regr-tree01/regr-tree01.html",
    "title": "regr-tree01",
    "section": "",
    "text": "library(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.0.8\n✔ dials        1.2.0     ✔ rsample      1.2.0\n✔ dplyr        1.1.3     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/"
  },
  {
    "objectID": "posts/regr-tree01/regr-tree01.html#setup",
    "href": "posts/regr-tree01/regr-tree01.html#setup",
    "title": "regr-tree01",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidymodels)\ndata(mtcars)\nlibrary(tictoc)  # Zeitmessung\n\nFür Klassifikation verlangt Tidymodels eine nominale AV, keine numerische:\n\nmtcars &lt;-\n  mtcars %&gt;% \n  mutate(am = factor(am))"
  },
  {
    "objectID": "posts/regr-tree01/regr-tree01.html#daten-teilen",
    "href": "posts/regr-tree01/regr-tree01.html#daten-teilen",
    "title": "regr-tree01",
    "section": "Daten teilen",
    "text": "Daten teilen\n\nd_split &lt;- initial_split(mtcars)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)"
  },
  {
    "objectID": "posts/regr-tree01/regr-tree01.html#modelle",
    "href": "posts/regr-tree01/regr-tree01.html#modelle",
    "title": "regr-tree01",
    "section": "Modell(e)",
    "text": "Modell(e)\n\nmod_tree &lt;-\n  decision_tree(mode = \"classification\",\n                cost_complexity = tune())"
  },
  {
    "objectID": "posts/regr-tree01/regr-tree01.html#rezepte",
    "href": "posts/regr-tree01/regr-tree01.html#rezepte",
    "title": "regr-tree01",
    "section": "Rezept(e)",
    "text": "Rezept(e)\n\nrec1 &lt;- \n  recipe(am ~ ., data = d_train)"
  },
  {
    "objectID": "posts/regr-tree01/regr-tree01.html#resampling",
    "href": "posts/regr-tree01/regr-tree01.html#resampling",
    "title": "regr-tree01",
    "section": "Resampling",
    "text": "Resampling\n\nrsmpl &lt;- vfold_cv(d_train, v = 2)"
  },
  {
    "objectID": "posts/regr-tree01/regr-tree01.html#workflow",
    "href": "posts/regr-tree01/regr-tree01.html#workflow",
    "title": "regr-tree01",
    "section": "Workflow",
    "text": "Workflow\n\nwf1 &lt;-\n  workflow() %&gt;%  \n  add_recipe(rec1) %&gt;% \n  add_model(mod_tree)"
  },
  {
    "objectID": "posts/regr-tree01/regr-tree01.html#tuningfitting",
    "href": "posts/regr-tree01/regr-tree01.html#tuningfitting",
    "title": "regr-tree01",
    "section": "Tuning/Fitting",
    "text": "Tuning/Fitting\n\nfit1 &lt;-\n  tune_grid(object = wf1,\n            resamples = rsmpl)"
  },
  {
    "objectID": "posts/regr-tree01/regr-tree01.html#bester-kandidat",
    "href": "posts/regr-tree01/regr-tree01.html#bester-kandidat",
    "title": "regr-tree01",
    "section": "Bester Kandidat",
    "text": "Bester Kandidat\n\nautoplot(fit1)\n\n\n\n\n\n\n\n\n\nshow_best(fit1)\n\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n\n\n# A tibble: 5 × 7\n  cost_complexity .metric .estimator  mean     n std_err .config              \n            &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1   0.0000000194  roc_auc binary       0.5     2       0 Preprocessor1_Model01\n2   0.00000000417 roc_auc binary       0.5     2       0 Preprocessor1_Model02\n3   0.000107      roc_auc binary       0.5     2       0 Preprocessor1_Model03\n4   0.000522      roc_auc binary       0.5     2       0 Preprocessor1_Model04\n5   0.00000542    roc_auc binary       0.5     2       0 Preprocessor1_Model05"
  },
  {
    "objectID": "posts/regr-tree01/regr-tree01.html#finalisieren",
    "href": "posts/regr-tree01/regr-tree01.html#finalisieren",
    "title": "regr-tree01",
    "section": "Finalisieren",
    "text": "Finalisieren\n\nwf1_finalized &lt;-\n  wf1 %&gt;% \n  finalize_workflow(select_best(fit1))\n\nWarning: No value of `metric` was given; metric 'roc_auc' will be used."
  },
  {
    "objectID": "posts/regr-tree01/regr-tree01.html#last-fit",
    "href": "posts/regr-tree01/regr-tree01.html#last-fit",
    "title": "regr-tree01",
    "section": "Last Fit",
    "text": "Last Fit\n\nfinal_fit &lt;- \n  last_fit(object = wf1_finalized, d_split)\n\ncollect_metrics(final_fit)\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.875 Preprocessor1_Model1\n2 roc_auc  binary         0.75  Preprocessor1_Model1\n\n\n\nCategories:\n\nstatlearning\ntrees\ntidymodels\nstring"
  },
  {
    "objectID": "posts/fat-tails-Artikel/fat-tails-Artikel.html",
    "href": "posts/fat-tails-Artikel/fat-tails-Artikel.html",
    "title": "fat-tails-Artikel",
    "section": "",
    "text": "Exercise\nIn diesem Diagramm sehen Sie etwas Nomenklatur für eine Verteilung: Gipfel (Peak), Schultern (shoulders) und Ränder (tails). Bitte klicken Sie den Link, um sich das Diagramm anzuschauen.\nQuelle: Taleb, N. N. (2019). The statistical consequences of fat tails, papers and commentaries. https://nassimtaleb.org/2020/01/final-version-fat-tails/\nZwar sind viele Daten in der Welt normalverteilt, aber längst nicht alle. In jüngerer Zeit sind sog. “Fat Tails” in die Aufmerksamkeit gerückt. Das sind Variablen, bei denen Werte in den Rändern (tails) wahrscheinlicher sind als bei einer Normalverteilung; ein Beispiel für eine Fat-Tail-Verteilung ist die t-Verteilung mit 1 Freiheitsgrad. Sie müssen diese Verteilung nicht weiter kennen, es ist aber nützlich, zu wissen, wozu diese Verteilung nützt\nRecherchieren Sie (Fach-)Artikel, die argumentieren, dass ein bestimmtes Phänomen Fat-Tails zeigt!\n         \n\n\nSolution\n\nKriege\nPandemien\nErfolg auf der Singlebörse Tinder\nKapitelmarkt\n\n\nCategories:\n\nprobability\ndistributions\nfat-tails"
  },
  {
    "objectID": "posts/kausal27/kausal27.html",
    "href": "posts/kausal27/kausal27.html",
    "title": "kausal27",
    "section": "",
    "text": "Gegeben sei der DAG g (s. u.). Der DAG verfügt über 7 Variablen, die als Knoten im Graph dargestellt sind (mit \\(x_1, x_2, \\ldots x_n\\) bezeichnet) und über Kanten verbunden sind.\nWelche minimale Variablenmenge muss kontrolliert werden, um den kausalen Effekt von der UV zur AV zu identifizieren?\nUV: x1.\nAV: x3.\nHinweise:\n\nMengen sind mittels geschweifter Klammern gekennzeichnet, z.B. {x8, x9} meint die Menge mit den zwei Elementen x8 und x9.\nDie leere Menge { } bedeutet, dass keine Variable kontrolliert werden muss, um den kausalen Effekt zu identifizieren.\nAlle Variablen werden als gemessen vorausgesetzt.\nEs ist möglich, dass es keine Lösung gibt, dass es also keine Adjustierungsmenge gibt, um den kausalen Effekt zu identifizieren. Wenn dies der Fall sein sollte, wählen Sie “keine Lösung”.\nEs ist möglich, dass einzelne Variablen keine Kanten besitzen, also keine Verbindung zu anderen Variablen (Knoten) haben.\n\n\n\n\n{ x5 }\n{ x2 }\n{ x6, x7 }\n{ }\n{ x4, x6 }"
  },
  {
    "objectID": "posts/kausal27/kausal27.html#answerlist",
    "href": "posts/kausal27/kausal27.html#answerlist",
    "title": "kausal27",
    "section": "",
    "text": "{ x5 }\n{ x2 }\n{ x6, x7 }\n{ }\n{ x4, x6 }"
  },
  {
    "objectID": "posts/kausal27/kausal27.html#answerlist-1",
    "href": "posts/kausal27/kausal27.html#answerlist-1",
    "title": "kausal27",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nRichtig\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/tidymodels-penguins04/tidymodels-penguins04.html",
    "href": "posts/tidymodels-penguins04/tidymodels-penguins04.html",
    "title": "tidymodels-penguins04",
    "section": "",
    "text": "Aufgabe\nBerechnen Sie ein kNN-Modell mit tidymodels und zwar anhand des penguins Datensatzes.\nModellgleichung: body_mass_g ~ bill_length_mm, data = d_train.\nGesucht ist R-Quadrat als Maß für die Modellgüte im TEST-Sample.\nHinweise:\n\nFixieren Sie die Zufallszahlen auf den Startwert 42.\nNutzen Sie eine v=5,r=2 CV.\nTunen Sie \\(K\\) (Default-Tuning)\nEntfernen Sie fehlende Werte in den Variablen.\nVerzichten Sie auf weitere Schritte der Vorverarbeitung.\n\n         \n\n\nLösung\nSetup:\n\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(tictoc)  # Rechenzeit messen, optional\n# data(penguins, package = \"palmerpenguins\")\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/modeldata/penguins.csv\"\nd &lt;- read_csv(d_path)\n\nDatensatz auf NAs prüfen:\n\nd2 &lt;-\n  d %&gt;% \n  drop_na() \n\nDatensatz aufteilen:\n\nset.seed(42)\nd_split &lt;- initial_split(d2)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\nWorkflow:\n\nrec1 &lt;-\n  recipe(body_mass_g ~ bill_length_mm, data = d_train) %&gt;% \n  step_naomit(all_numeric())\n\nknn_model &lt;-\n  nearest_neighbor(\n    mode = \"regression\",\n    neighbors = tune()\n  ) \n\nwflow &lt;-\n  workflow() %&gt;%\n  add_recipe(rec1) %&gt;%\n  add_model(knn_model)\n\nwflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_naomit()\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-Nearest Neighbor Model Specification (regression)\n\nMain Arguments:\n  neighbors = tune()\n\nComputational engine: kknn \n\n\nBacken:\n\nd_baked &lt;- prep(rec1) %&gt;% bake(new_data = NULL)\nd_baked %&gt;% head()\n\n# A tibble: 6 × 2\n  bill_length_mm body_mass_g\n           &lt;dbl&gt;       &lt;dbl&gt;\n1           34.5        2900\n2           52.2        3450\n3           45.4        4800\n4           42.1        4000\n5           50          5350\n6           41.5        4000\n\n\nAuf NA prüfen:\n\nsum(is.na(d_baked))\n\n[1] 0\n\n\nCV:\n\nset.seed(42)\nfolds &lt;- vfold_cv(d_train, v = 5, repeats = 2)\nfolds\n\n#  5-fold cross-validation repeated 2 times \n# A tibble: 10 × 3\n   splits           id      id2  \n   &lt;list&gt;           &lt;chr&gt;   &lt;chr&gt;\n 1 &lt;split [199/50]&gt; Repeat1 Fold1\n 2 &lt;split [199/50]&gt; Repeat1 Fold2\n 3 &lt;split [199/50]&gt; Repeat1 Fold3\n 4 &lt;split [199/50]&gt; Repeat1 Fold4\n 5 &lt;split [200/49]&gt; Repeat1 Fold5\n 6 &lt;split [199/50]&gt; Repeat2 Fold1\n 7 &lt;split [199/50]&gt; Repeat2 Fold2\n 8 &lt;split [199/50]&gt; Repeat2 Fold3\n 9 &lt;split [199/50]&gt; Repeat2 Fold4\n10 &lt;split [200/49]&gt; Repeat2 Fold5\n\n\nTunen:\n\nd_resamples &lt;-\n  tune_grid(\n    wflow,\n    resamples = folds,\n    control = control_grid(save_workflow = TRUE)\n  )\n\nd_resamples\n\n# Tuning results\n# 5-fold cross-validation repeated 2 times \n# A tibble: 10 × 5\n   splits           id      id2   .metrics          .notes          \n   &lt;list&gt;           &lt;chr&gt;   &lt;chr&gt; &lt;list&gt;            &lt;list&gt;          \n 1 &lt;split [199/50]&gt; Repeat1 Fold1 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [199/50]&gt; Repeat1 Fold2 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [199/50]&gt; Repeat1 Fold3 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [199/50]&gt; Repeat1 Fold4 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [200/49]&gt; Repeat1 Fold5 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [199/50]&gt; Repeat2 Fold1 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [199/50]&gt; Repeat2 Fold2 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [199/50]&gt; Repeat2 Fold3 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [199/50]&gt; Repeat2 Fold4 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [200/49]&gt; Repeat2 Fold5 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n\n\nBester Kandidat:\n\nshow_best(d_resamples)\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\n\n# A tibble: 5 × 7\n  neighbors .metric .estimator  mean     n std_err .config              \n      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1        15 rmse    standard    676.    10    15.6 Preprocessor1_Model10\n2        13 rmse    standard    684.    10    15.7 Preprocessor1_Model09\n3        11 rmse    standard    692.    10    16.4 Preprocessor1_Model08\n4        10 rmse    standard    694.    10    16.5 Preprocessor1_Model07\n5         9 rmse    standard    702.    10    15.8 Preprocessor1_Model06\n\n\n\nfitbest &lt;- fit_best(d_resamples)\nfitbest\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_naomit()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nkknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(15L,     data, 5))\n\nType of response variable: continuous\nminimal mean absolute error: 526.1741\nMinimal mean squared error: 416047.6\nBest kernel: optimal\nBest k: 15\n\n\nLast Fit:\n\nfit_last &lt;- last_fit(fitbest, d_split)\nfit_last\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits           id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;           &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [249/84]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n\nModellgüte im Test-Sample:\n\nfit_last %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard     605.    Preprocessor1_Model1\n2 rsq     standard       0.385 Preprocessor1_Model1\n\n\nR-Quadrat:\n\nsol &lt;- collect_metrics(fit_last)[[\".estimate\"]][2]\nsol\n\n[1] 0.3849557\n\n\n\nCategories:\n\nds1\ntidymodels\nprediction\nyacsda\nstatlearning\nnum"
  },
  {
    "objectID": "posts/tidymodels-penguins03/tidymodels-penguins03.html",
    "href": "posts/tidymodels-penguins03/tidymodels-penguins03.html",
    "title": "tidymodels-penguins03",
    "section": "",
    "text": "Aufgabe\nBerechnen Sie ein kNN-Modell mit tidymodels und zwar anhand des penguins Datensatzes.\nModellgleichung: body_mass_g ~ bill_length_mm, data = d_train.\nGesucht ist R-Quadrat als Maß für die Modellgüte im TEST-Sample.\nHinweise:\n\nFixieren Sie die Zufallszahlen auf den Startwert 42.\nNutzen Sie eine v=5,r=1 CV.\nTunen Sie \\(K\\) (Default-Tuning)\nEntfernen Sie fehlende Werte in den Variablen.\nVerzichten Sie auf weitere Schritte der Vorverarbeitung.\n\n         \n\n\nLösung\nSetup:\n\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(tictoc)  # Rechenzeit messen, optional\n# data(penguins, package = \"palmerpenguins\")\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/modeldata/penguins.csv\"\nd &lt;- read_csv(d_path)\n\nDatensatz auf NAs prüfen:\n\nd2 &lt;-\n  d %&gt;% \n  drop_na() \n\nDatensatz aufteilen:\n\nset.seed(42)\nd_split &lt;- initial_split(d2)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\nWorkflow:\n\nrec1 &lt;-\n  recipe(body_mass_g ~ bill_length_mm, data = d_train) %&gt;% \n  step_naomit(all_numeric())\n\nknn_model &lt;-\n  nearest_neighbor(\n    mode = \"regression\",\n    neighbors = tune()\n  ) \n\nwflow &lt;-\n  workflow() %&gt;%\n  add_recipe(rec1) %&gt;%\n  add_model(knn_model)\n\nwflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_naomit()\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-Nearest Neighbor Model Specification (regression)\n\nMain Arguments:\n  neighbors = tune()\n\nComputational engine: kknn \n\n\nBacken:\n\nd_baked &lt;- prep(rec1) %&gt;% bake(new_data = NULL)\nd_baked %&gt;% head()\n\n# A tibble: 6 × 2\n  bill_length_mm body_mass_g\n           &lt;dbl&gt;       &lt;dbl&gt;\n1           34.5        2900\n2           52.2        3450\n3           45.4        4800\n4           42.1        4000\n5           50          5350\n6           41.5        4000\n\n\nAuf NA prüfen:\n\nsum(is.na(d_baked))\n\n[1] 0\n\n\nCV:\n\nset.seed(43)\nfolds &lt;- vfold_cv(d_train, v = 5)\nfolds\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits           id   \n  &lt;list&gt;           &lt;chr&gt;\n1 &lt;split [199/50]&gt; Fold1\n2 &lt;split [199/50]&gt; Fold2\n3 &lt;split [199/50]&gt; Fold3\n4 &lt;split [199/50]&gt; Fold4\n5 &lt;split [200/49]&gt; Fold5\n\n\nTunen:\n\nd_resamples &lt;-\n  tune_grid(\n    wflow,\n    resamples = folds,\n    control = control_grid(save_workflow = TRUE)\n  )\n\nd_resamples\n\n# Tuning results\n# 5-fold cross-validation \n# A tibble: 5 × 4\n  splits           id    .metrics          .notes          \n  &lt;list&gt;           &lt;chr&gt; &lt;list&gt;            &lt;list&gt;          \n1 &lt;split [199/50]&gt; Fold1 &lt;tibble [18 × 5]&gt; &lt;tibble [0 × 3]&gt;\n2 &lt;split [199/50]&gt; Fold2 &lt;tibble [18 × 5]&gt; &lt;tibble [0 × 3]&gt;\n3 &lt;split [199/50]&gt; Fold3 &lt;tibble [18 × 5]&gt; &lt;tibble [0 × 3]&gt;\n4 &lt;split [199/50]&gt; Fold4 &lt;tibble [18 × 5]&gt; &lt;tibble [0 × 3]&gt;\n5 &lt;split [200/49]&gt; Fold5 &lt;tibble [18 × 5]&gt; &lt;tibble [0 × 3]&gt;\n\n\nBester Kandidat:\n\nshow_best(d_resamples)\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\n\n# A tibble: 5 × 7\n  neighbors .metric .estimator  mean     n std_err .config             \n      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1        14 rmse    standard    664.     5    22.7 Preprocessor1_Model9\n2        12 rmse    standard    671.     5    23.2 Preprocessor1_Model8\n3        11 rmse    standard    675.     5    24.1 Preprocessor1_Model7\n4         9 rmse    standard    685.     5    22.3 Preprocessor1_Model6\n5         8 rmse    standard    688.     5    22.9 Preprocessor1_Model5\n\n\n\nfitbest &lt;- fit_best(d_resamples)\nfitbest\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_naomit()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nkknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(14L,     data, 5))\n\nType of response variable: continuous\nminimal mean absolute error: 526.4603\nMinimal mean squared error: 416216.1\nBest kernel: optimal\nBest k: 14\n\n\nLast Fit:\n\nfit_last &lt;- last_fit(fitbest, d_split)\nfit_last\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits           id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;           &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [249/84]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n\nModellgüte im Test-Sample:\n\nfit_last %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard     606.    Preprocessor1_Model1\n2 rsq     standard       0.382 Preprocessor1_Model1\n\n\nR-Quadrat:\n\nsol &lt;- collect_metrics(fit_last)[[\".estimate\"]][2]\nsol\n\n[1] 0.38246\n\n\n\nCategories:\n\nds1\ntidymodels\nprediction\nyacsda\nstatlearning\nnum"
  },
  {
    "objectID": "posts/adjustieren1/adjustieren1.html",
    "href": "posts/adjustieren1/adjustieren1.html",
    "title": "adjustieren1",
    "section": "",
    "text": "Exercise\nBetrachten Sie folgendes Modell, das den Zusammenhang von PS-Zahl und Spritverbrauch untersucht (Datensatz mtcars).\nAber zuerst zentrieren wir den metrischen Prädiktor hp, um den Achsenabschnitt besser interpretieren zu können.\n\nmtcars &lt;-\n  mtcars %&gt;% \n  mutate(hp_z = hp - mean(hp))\n\n\nlibrary(rstanarm)\nlm1 &lt;- stan_glm(mpg ~ hp_z, data = mtcars,\n                refresh = 0)\nsummary(lm1)\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 20.1    0.7 19.2  20.1  21.0 \nhp_z        -0.1    0.0 -0.1  -0.1  -0.1 \nsigma        4.0    0.5  3.4   3.9   4.7 \nJetzt können wir aus dem Achsenabschnitt (Intercept) herauslesen, dass ein Auto mit hp_z = 0 - also mit mittlerer PS-Zahl - vielleicht gut 20 Meilen weit mit einer Gallone Sprit kommt.\nZur Verdeutlichung ein Diagramm zum Modell:\n\nmtcars %&gt;% \n  ggplot() +\n  aes(x = hp_z, y = mpg) +\n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAdjustieren Sie im Modell die PS-Zahl um die Art des Schaltgetriebes (am), so dass das neue Modell den statistischen Effekt (nicht notwendig auch kausal) der PS-Zahl bereinigt bzw. unabhängig von der Art des Schaltgetriebes widerspiegelt!\nHinweise:\n\nam=0 ist ein Auto mit Automatikgetriebe.\nWir gehen davon aus, dass der Regressionseffekt gleich stark ist auf allen (beiden) Stufen von am. M.a.W.: Es liegt kein Interaktionseffekt vor.\n\n         \n\n\nSolution\n\nlibrary(rstanarm)\nlm2 &lt;- stan_glm(mpg ~ hp_z + am, data = mtcars,\n                refresh = 0)\nsummary(lm2)\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 26.6    1.5 24.7  26.6  28.5 \nhp          -0.1    0.0 -0.1  -0.1   0.0 \nam           5.3    1.1  3.8   5.3   6.6 \nsigma        3.0    0.4  2.5   3.0   3.5 \nDie Spalte mean gibt den mittleren geschätzten Wert für den jeweiligen Koeffizienten an, also den Schätzwert zum Koeffizienten.\nDie Koeffizienten zeigen, dass der Achsenabschnitt für Autos mit Automatikgetriebe um etwa 5 Meilen geringer ist als für Autos mit manueller Schaltung: Ein durchschnittliches Auto mit manueller Schaltung kommt also etwa 5 Meilen weiter als ein Auto mit Automatikschaltung, glaubt unser Modell.\n\nmtcars %&gt;% \n  mutate(am = factor(am)) %&gt;% \n  ggplot() +\n  aes(x = hp_z, y = mpg, color = am) +\n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nMan könnte hier noch einen Interaktionseffekt ergänzen.\n\nCategories:\n\nqm2\nlm\nbayes\nstats-nutshell"
  },
  {
    "objectID": "posts/count-words01/count-words01.html",
    "href": "posts/count-words01/count-words01.html",
    "title": "count-words01",
    "section": "",
    "text": "Zählen sie die Wörter eines Textes. Verwenden Sie verschiedene Verfahren. Untersuchen Sie die Rechenzeit, die die jeweiligen Verfahren benötigen.\nNutzen Sie die GermEval-2018-Daten. Die Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)), Die Daten sind auch über das R-Paket PradaData zu beziehen.\n\nlibrary(tidyverse)\nd_train &lt;- read_csv(\"https://raw.githubusercontent.com/sebastiansauer/pradadata/master/data-raw/germeval_train.csv\")\nd_test &lt;- read_csv(\"https://raw.githubusercontent.com/sebastiansauer/pradadata/master/data-raw/germeval_test.csv\")\n\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks."
  },
  {
    "objectID": "posts/count-words01/count-words01.html#stringrstr_count",
    "href": "posts/count-words01/count-words01.html#stringrstr_count",
    "title": "count-words01",
    "section": "stringr::str_count",
    "text": "stringr::str_count\n\ntest_text$text |&gt; \n  map_int(str_count, \"\\\\w+\")\n\n[1] 3 2 2 3\n\n\nOder so:\n\ntest_text$text |&gt; \n  map_int(str_count, boundary(\"word\"))\n\n[1] 3 2 2 3\n\n\nDie Funktion map ist nicht nötig:\n\nstr_count(test_text$text, boundary(\"word\"))\n\n[1] 3 2 2 3\n\n\nAls neue Spalte in der Tabelle:\n\ntest_text |&gt; \n  mutate(n_words = str_count(text, boundary(\"word\")))\n\n# A tibble: 4 × 4\n     id text                 valence n_words\n  &lt;int&gt; &lt;chr&gt;                  &lt;dbl&gt;   &lt;int&gt;\n1     1 Abbau ist jetzt           -1       3\n2     2 Test heute                 0       2\n3     3 Abbruch morgen            -1       2\n4     4 Abmachung lore ipsum       1       3"
  },
  {
    "objectID": "posts/count-words01/count-words01.html#tokenizerscount_words",
    "href": "posts/count-words01/count-words01.html#tokenizerscount_words",
    "title": "count-words01",
    "section": "tokenizers::count_words",
    "text": "tokenizers::count_words\n\ntokenizers::count_words(test_text$text)\n\n[1] 3 2 2 3\n\n\n\ntest_text |&gt; \n  mutate(n_words = tokenizers::count_words(text))\n\n# A tibble: 4 × 4\n     id text                 valence n_words\n  &lt;int&gt; &lt;chr&gt;                  &lt;dbl&gt;   &lt;int&gt;\n1     1 Abbau ist jetzt           -1       3\n2     2 Test heute                 0       2\n3     3 Abbruch morgen            -1       2\n4     4 Abmachung lore ipsum       1       3"
  },
  {
    "objectID": "posts/count-words01/count-words01.html#qdapwc",
    "href": "posts/count-words01/count-words01.html#qdapwc",
    "title": "count-words01",
    "section": "qdap:wc",
    "text": "qdap:wc\n\nqdap::wc(test_text$text)\n\n[1] 3 2 2 3"
  },
  {
    "objectID": "posts/count-words01/count-words01.html#stringrstr_count-1",
    "href": "posts/count-words01/count-words01.html#stringrstr_count-1",
    "title": "count-words01",
    "section": "stringr::str_count",
    "text": "stringr::str_count\n\ntic()\nmethod1 &lt;- germeval_train$text |&gt; \n  map_int(str_count, \"\\\\w+\")\ntoc()\n\n0.206 sec elapsed\n\nmethod1 |&gt; str()\n\n int [1:5009] 15 19 11 21 15 44 34 8 13 14 ...\n\n\n\nprint(method1, max = 20)\n\n [1] 15 19 11 21 15 44 34  8 13 14 14 28 39 22 43 28 19 23 25 13\n [ reached getOption(\"max.print\") -- omitted 4989 entries ]\n\n\nOder so:\n\ntic()\nmethod2 &lt;- germeval_train$text |&gt; \n  map_int(str_count, boundary(\"word\"))\ntoc()\n\n0.172 sec elapsed\n\n\n\nhead(method2)\n\n[1] 15 19 10 21 15 44\n\n\nDie Funktion map ist nicht nötig:\n\ntic()\nmethod3 &lt;- str_count(germeval_train$text, boundary(\"word\"))\ntoc()\n\n0.028 sec elapsed\n\n\n\nmethod3 |&gt; head()\n\n[1] 15 19 10 21 15 44\n\n\nDann geht es auch viel schneller.\nAls neue Spalte in der Tabelle:\n\ntic()\nmethod4 &lt;- \ngermeval_train |&gt; \n  mutate(n_words = str_count(text, boundary(\"word\")))\ntoc()\n\n0.028 sec elapsed\n\n\n\nstr(method4)\n\n'data.frame':   5009 obs. of  5 variables:\n $ id     : int  1 2 3 4 5 6 7 8 9 10 ...\n $ text   : chr  \"@corinnamilborn Liebe Corinna, wir würden dich gerne als Moderatorin für uns gewinnen! Wärst du begeisterbar?\" \"@Martin28a Sie haben ja auch Recht. Unser Tweet war etwas missverständlich. Dass das BVerfG Sachleistungen nich\"| __truncated__ \"@ahrens_theo fröhlicher gruß aus der schönsten stadt der welt theo ⚓️\" \"@dushanwegner Amis hätten alles und jeden gewählt...nur Hillary wollten sie nicht und eine Fortsetzung von Obam\"| __truncated__ ...\n $ c1     : chr  \"OTHER\" \"OTHER\" \"OTHER\" \"OTHER\" ...\n $ c2     : chr  \"OTHER\" \"OTHER\" \"OTHER\" \"OTHER\" ...\n $ n_words: int  15 19 10 21 15 44 34 8 13 14 ...\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n\nmethod4 |&gt; head()\n\n  id\n1  1\n2  2\n3  3\n4  4\n5  5\n6  6\n                                                                                                                                                                                                                                                                                          text\n1                                                                                                                                                                                @corinnamilborn Liebe Corinna, wir würden dich gerne als Moderatorin für uns gewinnen! Wärst du begeisterbar?\n2                                                                                                                                               @Martin28a Sie haben ja auch Recht. Unser Tweet war etwas missverständlich. Dass das BVerfG Sachleistungen nicht ausschließt, kritisieren wir.\n3                                                                                                                                                                                                                        @ahrens_theo fröhlicher gruß aus der schönsten stadt der welt theo ⚓️\n4                                                                                                                                                 @dushanwegner Amis hätten alles und jeden gewählt...nur Hillary wollten sie nicht und eine Fortsetzung von Obama-Politik erst recht nicht..!\n5                                                                                                                                                     @spdde kein verläßlicher Verhandlungspartner. Nachkarteln nach den Sondierzngsgesprächen - schickt diese Stümper #SPD in die Versenkung.\n6 @Dirki_M Ja, aber wo widersprechen die Zahlen denn denen, die im von uns verlinkten Artikel stehen? In unserem Tweet geht es rein um subs. Geschützte. 2017 ist der gesamte Familiennachzug im Vergleich zu 2016 - die Zahlen, die Hr. Brandner bemüht - übrigens leicht rückläufig gewesen.\n       c1     c2 n_words\n1   OTHER  OTHER      15\n2   OTHER  OTHER      19\n3   OTHER  OTHER      10\n4   OTHER  OTHER      21\n5 OFFENSE INSULT      15\n6   OTHER  OTHER      44"
  },
  {
    "objectID": "posts/count-words01/count-words01.html#tokenizerscount_words-1",
    "href": "posts/count-words01/count-words01.html#tokenizerscount_words-1",
    "title": "count-words01",
    "section": "tokenizers::count_words",
    "text": "tokenizers::count_words\n\ntic()\nmethod5 &lt;- tokenizers::count_words(germeval_train$text)\ntoc()\n\n0.027 sec elapsed\n\nhead(method5)\n\n[1] 15 19 10 21 15 44\n\n\n\ntic()\nmethod6 &lt;-\ngermeval_train |&gt; \n  mutate(n_words = tokenizers::count_words(text))\ntoc()\n\n0.03 sec elapsed\n\nmethod6 |&gt; head()\n\n  id\n1  1\n2  2\n3  3\n4  4\n5  5\n6  6\n                                                                                                                                                                                                                                                                                          text\n1                                                                                                                                                                                @corinnamilborn Liebe Corinna, wir würden dich gerne als Moderatorin für uns gewinnen! Wärst du begeisterbar?\n2                                                                                                                                               @Martin28a Sie haben ja auch Recht. Unser Tweet war etwas missverständlich. Dass das BVerfG Sachleistungen nicht ausschließt, kritisieren wir.\n3                                                                                                                                                                                                                        @ahrens_theo fröhlicher gruß aus der schönsten stadt der welt theo ⚓️\n4                                                                                                                                                 @dushanwegner Amis hätten alles und jeden gewählt...nur Hillary wollten sie nicht und eine Fortsetzung von Obama-Politik erst recht nicht..!\n5                                                                                                                                                     @spdde kein verläßlicher Verhandlungspartner. Nachkarteln nach den Sondierzngsgesprächen - schickt diese Stümper #SPD in die Versenkung.\n6 @Dirki_M Ja, aber wo widersprechen die Zahlen denn denen, die im von uns verlinkten Artikel stehen? In unserem Tweet geht es rein um subs. Geschützte. 2017 ist der gesamte Familiennachzug im Vergleich zu 2016 - die Zahlen, die Hr. Brandner bemüht - übrigens leicht rückläufig gewesen.\n       c1     c2 n_words\n1   OTHER  OTHER      15\n2   OTHER  OTHER      19\n3   OTHER  OTHER      10\n4   OTHER  OTHER      21\n5 OFFENSE INSULT      15\n6   OTHER  OTHER      44"
  },
  {
    "objectID": "posts/count-words01/count-words01.html#qdapwc-1",
    "href": "posts/count-words01/count-words01.html#qdapwc-1",
    "title": "count-words01",
    "section": "qdap::wc",
    "text": "qdap::wc\n\ntic()\nmethod7 &lt;- qdap::wc(germeval_train$text)\ntoc()\n\n1.245 sec elapsed\n\nmethod7 |&gt; head()\n\n[1] 15 20 10 19 15 42\n\n\nDeutlich langsamer als mit tokenizers.\n\nCategories:\n\ntextmining\ntidymodels\ncount\ngermeval\nstring"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-elasticnet/germeval-sent-wordvec-elasticnet.html",
    "href": "posts/germeval-sent-wordvec-elasticnet/germeval-sent-wordvec-elasticnet.html",
    "title": "germeval03-sent-wordvec-elasticnet",
    "section": "",
    "text": "Erstellen Sie ein prädiktives Modell für Textdaten. Nutzen Sie Sentiments und TextFeatures im Rahmen von Feature-Engineering. Nutzen Sie außerdem deutsche Word-Vektoren für das Feature-Engineering.\nAls Lernalgorithmus verwenden Sie ein Elasticnet. Tunen Sie alle Parameter mit insgesamt 100 Kandidaten.\n\n\nVerwenden Sie die GermEval-2018-Daten.\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\nDie Daten sind auch über das R-Paket PradaData zu beziehen.\n\nlibrary(tidyverse)\ndata(\"germeval_train\", package = \"pradadata\")\ndata(\"germeval_test\", package = \"pradadata\")\n\n\n\n\nDie AV lautet c1. Die (einzige) UV lautet: text.\n\n\n\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\nNutzen Sie Tidymodels.\nNutzen Sie das sentiws Lexikon.\n❗ Achten Sie darauf, die Variable c2 zu entfernen bzw. nicht zu verwenden."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-elasticnet/germeval-sent-wordvec-elasticnet.html#daten",
    "href": "posts/germeval-sent-wordvec-elasticnet/germeval-sent-wordvec-elasticnet.html#daten",
    "title": "germeval03-sent-wordvec-elasticnet",
    "section": "",
    "text": "Verwenden Sie die GermEval-2018-Daten.\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\nDie Daten sind auch über das R-Paket PradaData zu beziehen.\n\nlibrary(tidyverse)\ndata(\"germeval_train\", package = \"pradadata\")\ndata(\"germeval_test\", package = \"pradadata\")"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-elasticnet/germeval-sent-wordvec-elasticnet.html#av-und-uv",
    "href": "posts/germeval-sent-wordvec-elasticnet/germeval-sent-wordvec-elasticnet.html#av-und-uv",
    "title": "germeval03-sent-wordvec-elasticnet",
    "section": "",
    "text": "Die AV lautet c1. Die (einzige) UV lautet: text."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-elasticnet/germeval-sent-wordvec-elasticnet.html#hinweise",
    "href": "posts/germeval-sent-wordvec-elasticnet/germeval-sent-wordvec-elasticnet.html#hinweise",
    "title": "germeval03-sent-wordvec-elasticnet",
    "section": "",
    "text": "Orientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\nNutzen Sie Tidymodels.\nNutzen Sie das sentiws Lexikon.\n❗ Achten Sie darauf, die Variable c2 zu entfernen bzw. nicht zu verwenden."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-elasticnet/germeval-sent-wordvec-elasticnet.html#setup",
    "href": "posts/germeval-sent-wordvec-elasticnet/germeval-sent-wordvec-elasticnet.html#setup",
    "title": "germeval03-sent-wordvec-elasticnet",
    "section": "Setup",
    "text": "Setup\nTrain-Datensatz:\n\nd_train &lt;-\n  germeval_train |&gt; \n  select(id, c1, text)\n\nPakete:\n\nlibrary(tictoc)\nlibrary(tidymodels)\nlibrary(beepr)\nlibrary(finetune)  # anova race\n\nEine Vorlage für ein Tidymodels-Pipeline findet sich hier."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-elasticnet/germeval-sent-wordvec-elasticnet.html#learnermodell",
    "href": "posts/germeval-sent-wordvec-elasticnet/germeval-sent-wordvec-elasticnet.html#learnermodell",
    "title": "germeval03-sent-wordvec-elasticnet",
    "section": "Learner/Modell",
    "text": "Learner/Modell\n\nmod &lt;-\n  logistic_reg(mode = \"classification\",\n               penalty = tune(), \n               mixture = tune(),\n               engine = \"glmnet\"\n             )"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-elasticnet/germeval-sent-wordvec-elasticnet.html#gebackenen-datensatz-als-neue-grundlage",
    "href": "posts/germeval-sent-wordvec-elasticnet/germeval-sent-wordvec-elasticnet.html#gebackenen-datensatz-als-neue-grundlage",
    "title": "germeval03-sent-wordvec-elasticnet",
    "section": "Gebackenen Datensatz als neue Grundlage",
    "text": "Gebackenen Datensatz als neue Grundlage\nWir importieren den schon an anderer Stelle aufbereiteten Datensatz. Das hat den Vorteil (hoffentlich), das die Datenvolumina viel kleiner sind. Die Arbeit des Feature Engineering wurde uns schon abgenommen.\n\nd_train_raw &lt;-\n  read_csv(\"https://raw.githubusercontent.com/sebastiansauer/Datenwerk2/main/data/germeval/germeval_train_recipe_wordvec_senti.csv\")\n\nRows: 5009 Columns: 121\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (1): c1\ndbl (120): id, emo_count, schimpf_count, emoji_count, textfeature_text_copy_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nd_test_baked_raw &lt;- read_csv(\"https://raw.githubusercontent.com/sebastiansauer/Datenwerk2/main/data/germeval/germeval_test_recipe_wordvec_senti.csv\")\n\nRows: 3532 Columns: 121\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (1): c1\ndbl (120): id, emo_count, schimpf_count, emoji_count, textfeature_text_copy_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-elasticnet/germeval-sent-wordvec-elasticnet.html#keine-dummysierung-der-av",
    "href": "posts/germeval-sent-wordvec-elasticnet/germeval-sent-wordvec-elasticnet.html#keine-dummysierung-der-av",
    "title": "germeval03-sent-wordvec-elasticnet",
    "section": "Keine Dummysierung der AV",
    "text": "Keine Dummysierung der AV\nLineare Modelle müssen dummysiert sein. Rezepte wollen das nicht so gerne für die AV besorgen.\nABER: Klassifikationsmodelle in Tiymodels (parsnip) benötigen eine factor Variable als AV, sonst werden sie nicht als Klassifikation erkannt.\n\nd_train &lt;-\n  d_train_raw |&gt; \n  mutate(c1 = as.factor(c1)) \n\nlevels(d_train$c1)\n\n[1] \"OFFENSE\" \"OTHER\"  \n\n\nTidymodels modelliert die erste Stufe.\n\nd_test_baked &lt;-\n  d_test_baked_raw |&gt; \n  mutate(c1 = as.factor(c1)) \n\nlevels(d_test_baked$c1)\n\n[1] \"OFFENSE\" \"OTHER\""
  },
  {
    "objectID": "posts/germeval-sent-wordvec-elasticnet/germeval-sent-wordvec-elasticnet.html#dummy-rezept",
    "href": "posts/germeval-sent-wordvec-elasticnet/germeval-sent-wordvec-elasticnet.html#dummy-rezept",
    "title": "germeval03-sent-wordvec-elasticnet",
    "section": "Dummy-Rezept",
    "text": "Dummy-Rezept\nPlain, aber mit Dummyisierung:\n\nrec &lt;- \n  recipe(c1 ~ ., data = d_train)"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-elasticnet/germeval-sent-wordvec-elasticnet.html#workflow",
    "href": "posts/germeval-sent-wordvec-elasticnet/germeval-sent-wordvec-elasticnet.html#workflow",
    "title": "germeval03-sent-wordvec-elasticnet",
    "section": "Workflow",
    "text": "Workflow\n\nwf &lt;-\n  workflow() |&gt; \n  add_recipe(rec) |&gt; \n  add_model(mod)"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-elasticnet/germeval-sent-wordvec-elasticnet.html#parallelisierung-über-mehrere-kerne",
    "href": "posts/germeval-sent-wordvec-elasticnet/germeval-sent-wordvec-elasticnet.html#parallelisierung-über-mehrere-kerne",
    "title": "germeval03-sent-wordvec-elasticnet",
    "section": "Parallelisierung über mehrere Kerne",
    "text": "Parallelisierung über mehrere Kerne\n\nlibrary(parallel)\nall_cores &lt;- detectCores(logical = FALSE)\n\nlibrary(doFuture)\nregisterDoFuture()\ncl &lt;- makeCluster(3)\nplan(cluster, workers = cl)\n\nAchtung: Viele Kerne brauchen auch viel Speicher."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-elasticnet/germeval-sent-wordvec-elasticnet.html#tuneresamplefit",
    "href": "posts/germeval-sent-wordvec-elasticnet/germeval-sent-wordvec-elasticnet.html#tuneresamplefit",
    "title": "germeval03-sent-wordvec-elasticnet",
    "section": "Tune/Resample/Fit",
    "text": "Tune/Resample/Fit\n\ntic()\nfit_wordvec_senti_elasticnet &lt;-\n  tune_race_anova(\n    wf,\n    grid = 100,\n    resamples = vfold_cv(d_train, v = 5),\n    control = control_race(verbose_elim = TRUE))\n\nℹ Racing will maximize the roc_auc metric.\nℹ Resamples are analyzed in a random order.\nℹ Fold4: 19 eliminated; 81 candidates remain.\n\nℹ Fold5: 2 eliminated; 79 candidates remain.\n\ntoc()\n\n164.439 sec elapsed\n\nbeep()"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-elasticnet/germeval-sent-wordvec-elasticnet.html#beste-performance",
    "href": "posts/germeval-sent-wordvec-elasticnet/germeval-sent-wordvec-elasticnet.html#beste-performance",
    "title": "germeval03-sent-wordvec-elasticnet",
    "section": "Beste Performance",
    "text": "Beste Performance\n\nautoplot(fit_wordvec_senti_elasticnet)\n\n\n\n\n\n\n\n\n\nshow_best(fit_wordvec_senti_elasticnet)\n\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n\n\n# A tibble: 5 × 8\n  penalty mixture .metric .estimator  mean     n std_err .config               \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n1 0.00367  0.349  roc_auc binary     0.787     5 0.00703 Preprocessor1_Model032\n2 0.00865  0.204  roc_auc binary     0.787     5 0.00706 Preprocessor1_Model017\n3 0.00423  0.0682 roc_auc binary     0.786     5 0.00682 Preprocessor1_Model002\n4 0.00228  0.305  roc_auc binary     0.786     5 0.00692 Preprocessor1_Model027\n5 0.00174  0.960  roc_auc binary     0.786     5 0.00714 Preprocessor1_Model096\n\nbest_params &lt;- select_best(fit_wordvec_senti_elasticnet)\n\nWarning: No value of `metric` was given; metric 'roc_auc' will be used."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-elasticnet/germeval-sent-wordvec-elasticnet.html#finalisieren",
    "href": "posts/germeval-sent-wordvec-elasticnet/germeval-sent-wordvec-elasticnet.html#finalisieren",
    "title": "germeval03-sent-wordvec-elasticnet",
    "section": "Finalisieren",
    "text": "Finalisieren\n\ntic()\nwf_finalized &lt;- finalize_workflow(wf, best_params)\nlastfit &lt;- fit(wf_finalized, data = d_train)\ntoc()\n\n0.869 sec elapsed"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-elasticnet/germeval-sent-wordvec-elasticnet.html#test-set-güte",
    "href": "posts/germeval-sent-wordvec-elasticnet/germeval-sent-wordvec-elasticnet.html#test-set-güte",
    "title": "germeval03-sent-wordvec-elasticnet",
    "section": "Test-Set-Güte",
    "text": "Test-Set-Güte\n\ntic()\npreds &lt;-\n  predict(lastfit, new_data = d_test_baked)\ntoc()\n\n0.05 sec elapsed\n\n\n\nd_test &lt;-\n  d_test_baked |&gt; \n  bind_cols(preds) |&gt; \n  mutate(c1 = as.factor(c1))\n\n\nmy_metrics &lt;- metric_set(accuracy, f_meas)\nmy_metrics(d_test,\n           truth = c1,\n           estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.717\n2 f_meas   binary         0.502"
  },
  {
    "objectID": "posts/tidymodels2/tidymodels2.html",
    "href": "posts/tidymodels2/tidymodels2.html",
    "title": "tidymodels2",
    "section": "",
    "text": "Aufgabe\nEin merkwürdiger Fehler bzw. eine merkwürdige Fehlermeldung in Tidymodels - das untersuchen wir hier genauer und versuchen das Phänomen zu erklären.\nAufgabe\nErläutern Sie die Ursachen des Fehlers! Schalten Sie den Fehler an und ab, um zu zeigen, dass Sie Ihn verstehen.\n\n\nStartup\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\n\nData import\n\ndata(\"mtcars\")\n\nd_train &lt;- mtcars %&gt;% slice_head(n = 20)\nd_test &lt;- mtcars %&gt;% slice(21:n())\n\n\n\nRecipe\n\npreds_chosen &lt;- c(\"hp\", \"disp\", \"am\")\n\n\nrec1 &lt;- \n  recipe( ~ ., data = d_train) %&gt;% \n  update_role(all_predictors(), new_role = \"id\") %&gt;% \n  update_role(all_of(preds_chosen), new_role = \"predictor\") %&gt;% \n  update_role(mpg, new_role = \"outcome\")\nrec1\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 3\nid:        7\n\n\n\nd_train_baked &lt;-\n  rec1 %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL)\n\nglimpse(d_train_baked)\n\nRows: 20\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1\n\n\n\n\nModel 1\n\nmodel_lm &lt;- linear_reg()\n\n\n\nWorkflow 1\n\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(model_lm) %&gt;% \n  add_recipe(rec1)\n\n\n\nFit\n\nlm_fit1 &lt;-\n  wf1 %&gt;% \n  fit(d_train)\n\n\npreds &lt;-\n  lm_fit1 %&gt;% \n  predict(d_test)\n\nhead(preds)\n\n# A tibble: 6 × 1\n  .pred\n  &lt;dbl&gt;\n1  22.6\n2  17.2\n3  17.4\n4  12.1\n5  14.9\n6  28.2\n\n\nAus Gründen der Reproduzierbarkeit bietet es sich an, eine SessionInfo anzugeben:\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] yardstick_1.2.0    workflowsets_1.0.1 workflows_1.1.3    tune_1.1.2        \n [5] rsample_1.2.0      recipes_1.0.8      parsnip_1.1.1      modeldata_1.2.0   \n [9] infer_1.0.5        dials_1.2.0        scales_1.2.1       broom_1.0.5       \n[13] tidymodels_1.1.1   lubridate_1.9.3    forcats_1.0.0      stringr_1.5.0     \n[17] dplyr_1.1.3        purrr_1.0.2        readr_2.1.4        tidyr_1.3.0       \n[21] tibble_3.2.1       ggplot2_3.4.4      tidyverse_2.0.0    colorout_1.3-0    \n\nloaded via a namespace (and not attached):\n [1] foreach_1.5.2       jsonlite_1.8.7      splines_4.2.1      \n [4] prodlim_2023.03.31  GPfit_1.0-8         yaml_2.3.7         \n [7] globals_0.16.2      ipred_0.9-14        pillar_1.9.0       \n[10] backports_1.4.1     lattice_0.21-8      glue_1.6.2         \n[13] digest_0.6.33       hardhat_1.3.0       colorspace_2.1-0   \n[16] htmltools_0.5.6.1   Matrix_1.5-4.1      timeDate_4022.108  \n[19] pkgconfig_2.0.3     lhs_1.1.6           DiceDesign_1.9     \n[22] listenv_0.9.0       gower_1.0.1         lava_1.7.2.1       \n[25] tzdb_0.4.0          timechange_0.2.0    generics_0.1.3     \n[28] ellipsis_0.3.2      withr_2.5.2         furrr_0.3.1        \n[31] nnet_7.3-19         cli_3.6.1           survival_3.5-5     \n[34] magrittr_2.0.3      evaluate_0.21       fansi_1.0.5        \n[37] future_1.33.0       parallelly_1.36.0   MASS_7.3-60        \n[40] class_7.3-22        tools_4.2.1         data.table_1.14.8  \n[43] hms_1.1.3           lifecycle_1.0.3     munsell_0.5.0      \n[46] compiler_4.2.1      rlang_1.1.1         grid_4.2.1         \n[49] iterators_1.0.14    rstudioapi_0.15.0   htmlwidgets_1.6.2  \n[52] rmarkdown_2.25      gtable_0.3.4        codetools_0.2-19   \n[55] R6_2.5.1            knitr_1.45          fastmap_1.1.1      \n[58] future.apply_1.11.0 utf8_1.2.3          stringi_1.7.12     \n[61] parallel_4.2.1      Rcpp_1.0.11         vctrs_0.6.4        \n[64] rpart_4.1.19        tidyselect_1.2.0    xfun_0.40          \n\n\n         \n\n\nLösung\nDefiniert man das Rezept so:\n\nrec2 &lt;- recipe(mpg ~ hp + disp + am, data = d_train)\n\nDann läuft predict() brav durch.\nAuch dieser Code funktioniert:\n\nrec3 &lt;- \n  recipe(mpg ~ ., data = d_train) %&gt;% \n  update_role(all_predictors(), new_role = \"id\") %&gt;% \n  update_role(all_of(preds_chosen), new_role = \"predictor\") %&gt;% \n  update_role(mpg, new_role = \"outcome\")\n\nDas Problem von rec1 scheint darin zu legen, dass die Rollen der Variablen nicht richtig gelöscht werden, was predict() verwirrt:\n\nrec1 &lt;- \n  recipe(mpg ~ ., data = d_train) %&gt;% \n  update_role(all_predictors(), new_role = \"id\") %&gt;% \n  update_role(all_of(preds_chosen), new_role = \"predictor\") %&gt;% \n  update_role(mpg, new_role = \"outcome\")\nrec1\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 3\nid:        7\n\n\nDaher läuft das Rezept rec3 durch, wenn man zunächst alle Prädiktoren in ID-Variablen umwandelt: Damit sind alle Rollen wieder sauber.\n\nCategories:\n\nds1\ntidymodels\nprediction\nyacsda\nstatlearning\nerror\nstring"
  },
  {
    "objectID": "posts/Rethink2m1/Rethink2m1.html",
    "href": "posts/Rethink2m1/Rethink2m1.html",
    "title": "Rethink2m1",
    "section": "",
    "text": "Aufgabe\nThis question is taken from McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Ed.). Taylor and Francis, CRC Press.\n2M1. Recall the globe tossing model from the chapter. Compute and plot the grid approximate posterior distribution for each of the following sets of observations. In each case, assume a uniform prior for p.\n\nWWW\nWWWL\nLWWLWWW\n\n         \n\n\nLösung\nThe solution is taken from this source.\n\nlibrary(tidyverse)\n\ndist &lt;- \n  tibble(\n    # Gridwerte bestimmen:\n    p_grid = seq(from = 0, to = 1, length.out = 20),\n    # Priori-Wskt bestimmen:\n    prior = rep(1, times = 20)) %&gt;%\n  mutate(\n    # Likelihood berechnen:\n    likelihood_1 = dbinom(3, size = 3, prob = p_grid),  # WWW\n    likelihood_2 = dbinom(3, size = 4, prob = p_grid),  # WWWL\n    likelihood_3 = dbinom(5, size = 7, prob = p_grid),  # LWWLWWW\n    # unstand. Posterior-Wskt:\n    unstand_post_1 = likelihood_1 * prior,\n    unstand_post_2 = likelihood_2 * prior,\n    unstand_post_3 = likelihood_3 * prior,\n    # stand. Post-Wskt:\n    std_post_1 = unstand_post_1 / sum(unstand_post_1),\n    std_post_2 = unstand_post_2 / sum(unstand_post_2),\n    std_post_3 = unstand_post_3 / sum(unstand_post_3)\n    ) \n\nHier ist die Bayes-Box:\n\nknitr::kable(round(dist, 2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np_grid\nprior\nlikelihood_1\nlikelihood_2\nlikelihood_3\nunstand_post_1\nunstand_post_2\nunstand_post_3\nstd_post_1\nstd_post_2\nstd_post_3\n\n\n\n\n0.00\n1\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n0.05\n1\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n0.11\n1\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n0.16\n1\n0.00\n0.01\n0.00\n0.00\n0.01\n0.00\n0.00\n0.00\n0.00\n\n\n0.21\n1\n0.01\n0.03\n0.01\n0.01\n0.03\n0.01\n0.00\n0.01\n0.00\n\n\n0.26\n1\n0.02\n0.05\n0.01\n0.02\n0.05\n0.01\n0.00\n0.01\n0.01\n\n\n0.32\n1\n0.03\n0.09\n0.03\n0.03\n0.09\n0.03\n0.01\n0.02\n0.01\n\n\n0.37\n1\n0.05\n0.13\n0.06\n0.05\n0.13\n0.06\n0.01\n0.03\n0.02\n\n\n0.42\n1\n0.07\n0.17\n0.09\n0.07\n0.17\n0.09\n0.01\n0.05\n0.04\n\n\n0.47\n1\n0.11\n0.22\n0.14\n0.11\n0.22\n0.14\n0.02\n0.06\n0.06\n\n\n0.53\n1\n0.15\n0.28\n0.19\n0.15\n0.28\n0.19\n0.03\n0.07\n0.08\n\n\n0.58\n1\n0.19\n0.33\n0.24\n0.19\n0.33\n0.24\n0.04\n0.09\n0.10\n\n\n0.63\n1\n0.25\n0.37\n0.29\n0.25\n0.37\n0.29\n0.05\n0.10\n0.12\n\n\n0.68\n1\n0.32\n0.40\n0.31\n0.32\n0.40\n0.31\n0.06\n0.11\n0.13\n\n\n0.74\n1\n0.40\n0.42\n0.32\n0.40\n0.42\n0.32\n0.08\n0.11\n0.13\n\n\n0.79\n1\n0.49\n0.41\n0.29\n0.49\n0.41\n0.29\n0.09\n0.11\n0.12\n\n\n0.84\n1\n0.60\n0.38\n0.22\n0.60\n0.38\n0.22\n0.11\n0.10\n0.09\n\n\n0.89\n1\n0.72\n0.30\n0.13\n0.72\n0.30\n0.13\n0.14\n0.08\n0.06\n\n\n0.95\n1\n0.85\n0.18\n0.04\n0.85\n0.18\n0.04\n0.16\n0.05\n0.02\n\n\n1.00\n1\n1.00\n0.00\n0.00\n1.00\n0.00\n0.00\n0.19\n0.00\n0.00\n\n\n\n\n\nJetzt können wir das jeweilige Diagramm zeichnen:\n\nlibrary(ggpubr)\nggline(dist,\n       x = \"p_grid\",\n       y = \"std_post_1\")\n\n\n\n\n\n\n\n\nOder mit ggplot2:\n\nggplot(dist) +\n  aes(x = p_grid, y= std_post_1) +\n  geom_line()+\n  geom_point() +\n  labs(x = \"p(W)\",\n       y = \"Posteriori-Wahrscheinlichkeit\",\n       title = \"Daten: WWW\")\n\n\n\n\n\n\n\nggplot(dist) +\n  aes(x = p_grid, y= std_post_2) +\n  geom_line()+\n  geom_point() +\n  labs(x = \"p(W)\",\n       y = \"Posteriori-Wahrscheinlichkeit\",\n       title = \"Daten: WWWL\")\n\n\n\n\n\n\n\nggplot(dist) +\n  aes(x = p_grid, y= std_post_3) +\n  geom_line()+\n  geom_point() +\n  labs(x = \"p(W)\",\n       y = \"Posteriori-Wahrscheinlichkeit\",\n       title = \"Daten: LWWLWWW\")\n\n\n\n\n\n\n\n\n\nCategories:\n\nprobability\nbayes-grid\nrethink-chap2\nstring"
  },
  {
    "objectID": "posts/sicherheit2/sicherheit2.html",
    "href": "posts/sicherheit2/sicherheit2.html",
    "title": "sicherheit2",
    "section": "",
    "text": "Aufgabe\nEin Betreiber eines komplexen technischen Geräts versucht, Sie zu beruhigen. Die Wahrscheinlichkeit eines Ausfalls (Ereignis \\(A\\)) betrage nur 0.001. Allerdings pro Komponente des Geräts. Das Gerät besteht aus \\(k=10^3\\) Komponenten.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\nUnterstellen Sie Unabhängigkeit der Komponenten.\n\n         \n\n\nLösung\nDen Ausfall der Komponente \\(i\\) bezeichnen wir als \\(A_i\\) und entsprechend \\(Pr(A_i) = 0.001\\).\n\\(Pr(\\neg A_i) = 1- Pr(A_i)\\)\n\nPr_Ai &lt;- 0.001\nPr_negAi &lt;- 1 - Pr_Ai\nPr_negAi\n\n[1] 0.999\n\n\nDie Wahrscheinlichkeit, dass keine der Komponenten ausfällt, ist dann über den Multiplikationssatzu bestimmen:\n\nk &lt;- 10^3\nPr_negA &lt;- Pr_negAi^k\nPr_negA\n\n[1] 0.3676954\n\n\nDie Lösung lautet 0.3676954.\n\nCategories:\n\nR\nprobability\nnum"
  },
  {
    "objectID": "posts/Pupil-size/Pupil-size.html",
    "href": "posts/Pupil-size/Pupil-size.html",
    "title": "Pupil-size",
    "section": "",
    "text": "Aufgabe\nPupillendaten sind ein verbreiteter Analysegegenstand in Bereichen wie Psychologie, Marktforschung und Marketing.\nBetrachten wir dazu ein R-Paket (zum Vorverbarbeitung, preprocessing) und einen Datensatz der Uni Münster.\n\nlibrary(PupilPre)  # installieren, einmalig, nicht vergessen\ndata(\"Pupildat\")\nd &lt;-\n  Pupildat %&gt;% \n  select(size = RIGHT_PUPIL_SIZE,\n         time = TIMESTAMP) %&gt;% \n  mutate(size = size / 100) # in millimeter\n\nMit dem R-Paket eaystats kann man sich bequem typische Statistiken ausgeben lassen. Aber natürlich können Sie auch mit summarise(mw = mean(size)) o.Ä. arbeiten.\n\nlibrary(easystats)\nd %&gt;% \n  describe_distribution()\n\nVariable |     Mean |       SD |      IQR |                Range | Skewness | Kurtosis |     n | n_Missing\n----------------------------------------------------------------------------------------------------------\nsize     |    10.01 |     5.11 |     3.88 |        [1.04, 25.01] |     1.25 |     0.32 | 45343 |      1607\ntime     | 2.99e+06 | 9.87e+05 | 1.95e+06 | [1.44e+06, 4.06e+06] |    -0.41 |    -1.70 | 46950 |         0\n\n\nWir verzichten hier auf eine Aufbereitung der Daten (was eigentlich nötig wäre, aber nicht Gegenstand dieser Übung ist). Stattdessen konzentrieren wir uns auf die Posteriori-Verteilung zur Pupillengröße.\nWir sind also interessiert an einem Modell zur Schätzung der (Verteilung der) Pupillengröße; die Posteriori-Verteilung bildet das ab.\n\nFormulieren Sie ein passendes Modell.\nVerteidigen Sie Ihre Modellspezifikation.\nSimulieren Sie Daten aus der Priori-Verteilung. Kritisieren Sie die Wahl der Priori-Werte.\nBerechnen Sie die Posteriori-Verteilung mit den Pupillendaten d. Geben Sie zentrale Statistiken an.\nGeben Sie ein 95%-Intervall für die mittlere Pupillengröße an auf Basis der Posteriori-Verteilung.\nGeben Sie die Prioris an, die rstanarm verwendet hat.\n\nHinweise:\n\nVerwenden Sie die Defaults von rstanarm für Ihr Modell.\nFalls Sie Teile der Aufgabe nicht lösen können, weil Ihnen der Stoff dazu fehlt: Einfach ignorieren 😄.\n\n         \n\n\nLösung\n\nModelldefinition\n\n\\[\\begin{aligned}\ns_i &\\sim \\mathcal{N}(\\mu, \\sigma)\\qquad \\text{| s wie size }\\\\\n\\mu &\\sim \\mathcal{N}(10, 5)\\\\\n\\sigma &\\sim \\mathcal{E}(.2)\n\\end{aligned}\\]\n\nBegründung der Modellspezifikation\n\n\\(s_i\\): Pupillengrößen sind normalverteilt, da viele Gene additiv auf die Größe hin zusammenwirken\n\\(\\mu\\): Da wir nicht viel wissen über die mittlere Pupillengröße, entscheiden wir uns für Normalverteilung für diesen Parameter, da dies keine weiteren Annahmen (außer dass Mittelwert und Streuung endlich sind) hinzufügt. Ein Modell mit wenig Annahmen nennt man “sparsam” oder konservativ. Es ist wünschenswert, dass Modelle mit so wenig wie möglich Annahmen auskommt (aber so vielen wie nötig).\n\\(\\sigma\\): Die Streuung muss positiv sein, daher kommt keine Normalverteilung in Frage. Eine Exponentialverteilung ist eine von mehreren denkbaren Verteilungen.\nAber welche Werte von lambda kommen in Frage? Probieren wir mal etwas aus:\n\nqexp(p = .5, rate = 1)\n\n[1] 0.6931472\n\n\nMit \\(\\lambda = 1\\) liegt der Median der Streuung der Pupillengrößen (p = .5) bei ca. 0.7 mm. Dieser Wert erscheint mir etwas wenig\n\nqexp(p = .5, rate = 0.2)\n\n[1] 3.465736\n\n\nHm. Eine Streuung von ca. 3.5mm um den Mittelwert herum; das könnte passen.\nDie große Stichprobe wird den Priori-Wert vermutlich überstimmen.\n\nPriori-Prädiktiv-Verteilung\n\n\nn &lt;- 1e4\nsim_prior_pred &lt;-\n  tibble(\n    mu = rnorm(n, mean = 10, sd = 5),\n    sigma = runif(n, min = 0, max = 20),\n    size = rnorm(n, mu, sigma)\n  )\n\nsim_prior_pred %&gt;% \n  ggplot(aes(x = size)) +\n  geom_density()\n\n\n\n\n\n\n\n\nDa es viele negative Pupillengröße-Werte gibt, sieht man deutlich, dass das Modell nicht gut spezifiziert ist. So könnte kleinere Streuungswerte zu einem realistischeren Modell führen. Oder man verwendet Verteilungen, die rein positiv sind (hier nicht weiter ausgeführt).\n\nBerechnen Sie die Posteriori-Verteilung.\n\nDie Modelle wie stan_glm() tun sich leichter, wenn man nur die relevanten Daten, ohne fehlende Werte und schon schön fertig vorverarbeitet, zur Analyse in die Modellberechnung gibt:\n\nd3 &lt;-\n  d %&gt;% \n  select(size) %&gt;% \n  drop_na()\n\nDie Posteriori-Verteilung kann man mit dem Paket {rstanarm} d.h. mit der Funktion stan_glm() berechnen:\n\nlibrary(rstanarm)\nm_pupil &lt;- stan_glm(size ~ 1,\n                    data = d3,\n                    seed = 42,\n                    refresh = 0)\n\nDie Daten sind groß, es kann ein paar Sekunden brauchen…\nHier ist eine nützliche Zusammenfassung der Post-Verteilung.\n\nparameters(m_pupil)\n\nParameter   | Median |        95% CI |   pd |  Rhat |     ESS |                   Prior\n---------------------------------------------------------------------------------------\n(Intercept) |  10.01 | [9.96, 10.05] | 100% | 1.000 | 1992.00 | Normal (10.01 +- 12.77)\n\n\nHier eine Visualisierung der Parameter:\n\nplot(parameters(m_pupil), show_intercept = TRUE)\n\n\n\n\n\n\n\n\nNatürlich kann man auch die Post-Verteilung plotten (z.B: HDI):\n\nm_hdi &lt;- hdi(m_pupil, ci = c(0.5, 0.95))\n\nplot(m_hdi, show_intercept = TRUE)  # Im Default wird der Intercept nicht gezeigt\n\n\n\n\n\n\n\n\nHier zur Info die ersten paar Zeilen des Post-Verteilung:\n\n\n\n\n\n\n\n\n(Intercept)\nsigma\n\n\n\n\n10.04\n5.13\n\n\n10.00\n5.07\n\n\n9.99\n5.08\n\n\n10.00\n5.08\n\n\n9.99\n5.11\n\n\n\n\n\n\n\n\nGeben Sie ein 95%-Intervall für die mittlere Pupillengröße an auf Basis der Posteriori-Verteilung.\n\n\neti(m_pupil)\n\nEqual-Tailed Interval\n\nParameter   |       95% ETI | Effects |   Component\n---------------------------------------------------\n(Intercept) | [9.96, 10.05] |   fixed | conditional\n\n\nUnd dann erstellen wir noch ein 89%-PI, einfach zum Spaß an der Freude:\n\neti(m_pupil, ci = .89)\n\nEqual-Tailed Interval\n\nParameter   |       89% ETI | Effects |   Component\n---------------------------------------------------\n(Intercept) | [9.97, 10.04] |   fixed | conditional\n\n\n\nGeben Sie die Prioris an, die rstanarm verwendet hat.\n\nVoilà:\n\nprior_summary(m_pupil)\n\nPriors for model 'm_pupil' \n------\nIntercept (after predictors centered)\n  Specified prior:\n    ~ normal(location = 10, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 10, scale = 13)\n\nAuxiliary (sigma)\n  Specified prior:\n    ~ exponential(rate = 1)\n  Adjusted prior:\n    ~ exponential(rate = 0.2)\n------\nSee help('prior_summary.stanreg') for more details\n\n\n\nCategories:\n\nprobability\nbayes\nregression\nstring"
  },
  {
    "objectID": "posts/Logikpruefung1/Logikpruefung1.html",
    "href": "posts/Logikpruefung1/Logikpruefung1.html",
    "title": "Logikpruefung1",
    "section": "",
    "text": "Aufgabe\nWir definieren x wie folgt:\n\nx &lt;- 42\n\nGeben Sie die Syntax an, für die Prüfung, ob x kleiner 100 und größer 0 ist.\nGeben Sie keine Leerzeichen in Ihre Lösung ein.\n         \n\n\nLösung\n\nx&lt;100&x&gt;0\n\n[1] TRUE\n\n\nMit Leerzeichen sieht es aber schöner aus:\n\nx &lt; 100 & x &gt; 0\n\n[1] TRUE\n\n\n\nCategories:\n\nR\n‘2023’\nLogikpruefung1"
  },
  {
    "objectID": "posts/kausal06/kausal06.html",
    "href": "posts/kausal06/kausal06.html",
    "title": "kausal06",
    "section": "",
    "text": "Im Rahmen einer Studie soll untersucht werden, ob eine Influenza-Infektion einen (kausalen) Einfluss auf eine Covid19-Infektion hat. Außerdem wird dabei der Nutzen des Medikaments Acetaminophen untersucht.\nIn Wahrheit (aber unbekannt) sei der DAG wie folgt (s.u.).\n\n\n\n\n\n\n\n\n\nIst es sinnvoll, die Einnahme von Fiebersenker (Acetaminophen) zu kontrollieren?\n\n\n\nNein, es ist nicht sinnvoll, da durch eine Kontrolle von Acetaminophen eine Verzerrung erzeugt wird (Kollision)\nJa, nur so ist ein kausaler Effekt identifizierbar\nJa, es ist nicht nötig, aber wird zu exakteren Ergebnissen führen\nNein, es ist nicht sinnvoll,da durch eine Kontrolle von Acetaminophen eine Verzerrung erzeugt wird (Konfundierung)\nNein, es ist nicht sinnvoll, da es nicht nötig ist (aber auch nicht schädlich)"
  },
  {
    "objectID": "posts/kausal06/kausal06.html#answerlist",
    "href": "posts/kausal06/kausal06.html#answerlist",
    "title": "kausal06",
    "section": "",
    "text": "Nein, es ist nicht sinnvoll, da durch eine Kontrolle von Acetaminophen eine Verzerrung erzeugt wird (Kollision)\nJa, nur so ist ein kausaler Effekt identifizierbar\nJa, es ist nicht nötig, aber wird zu exakteren Ergebnissen führen\nNein, es ist nicht sinnvoll,da durch eine Kontrolle von Acetaminophen eine Verzerrung erzeugt wird (Konfundierung)\nNein, es ist nicht sinnvoll, da es nicht nötig ist (aber auch nicht schädlich)"
  },
  {
    "objectID": "posts/kausal06/kausal06.html#answerlist-1",
    "href": "posts/kausal06/kausal06.html#answerlist-1",
    "title": "kausal06",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/Regr-Bayes-interpret03/Regr-Bayes-interpret03.html",
    "href": "posts/Regr-Bayes-interpret03/Regr-Bayes-interpret03.html",
    "title": "Regr-Bayes-interpret03",
    "section": "",
    "text": "Exercise\nBerechnen Sie das Modell und interpretieren Sie die Ausgabe des folgenden Regressionsmodells. Geben Sie für jeden Regressionskoeffizienten an, wie sein Wert zu verstehen ist!\nmpg_z ~ hp_z + am + hp_z:am\nHinweise:\n\nFixieren Sie die Zufallszahlen.\nVerwenden Sie Stan zur Berechnung.\nRunden Sie auf 2 Dezimalstellen.\nDas Suffix _z steht für z-standardisierte Variablen.\n\n         \n\n\nSolution\n\nlibrary(tidyverse)  # Datenjudo\nlibrary(rstanarm)  # Stan, komm her\nlibrary(easystats)  # Komfort\n\ndata(mtcars)\n\nZuerst standardisieren wir die Daten:\n\nmtcars2 &lt;-\n  mtcars %&gt;% \n  standardize(append = TRUE)\n\nmtcars2  %&gt;% \n  describe_distribution()\n\nVariable |      Mean |     SD |    IQR |           Range | Skewness | Kurtosis |  n | n_Missing\n-----------------------------------------------------------------------------------------------\nmpg      |     20.09 |   6.03 |   7.53 |  [10.40, 33.90] |     0.67 |    -0.02 | 32 |         0\ncyl      |      6.19 |   1.79 |   4.00 |    [4.00, 8.00] |    -0.19 |    -1.76 | 32 |         0\ndisp     |    230.72 | 123.94 | 221.53 | [71.10, 472.00] |     0.42 |    -1.07 | 32 |         0\nhp       |    146.69 |  68.56 |  84.50 | [52.00, 335.00] |     0.80 |     0.28 | 32 |         0\ndrat     |      3.60 |   0.53 |   0.84 |    [2.76, 4.93] |     0.29 |    -0.45 | 32 |         0\nwt       |      3.22 |   0.98 |   1.19 |    [1.51, 5.42] |     0.47 |     0.42 | 32 |         0\nqsec     |     17.85 |   1.79 |   2.02 |  [14.50, 22.90] |     0.41 |     0.86 | 32 |         0\nvs       |      0.44 |   0.50 |   1.00 |    [0.00, 1.00] |     0.26 |    -2.06 | 32 |         0\nam       |      0.41 |   0.50 |   1.00 |    [0.00, 1.00] |     0.40 |    -1.97 | 32 |         0\ngear     |      3.69 |   0.74 |   1.00 |    [3.00, 5.00] |     0.58 |    -0.90 | 32 |         0\ncarb     |      2.81 |   1.62 |   2.00 |    [1.00, 8.00] |     1.16 |     2.02 | 32 |         0\nmpg_z    |  7.11e-17 |   1.00 |   1.25 |   [-1.61, 2.29] |     0.67 |    -0.02 | 32 |         0\ncyl_z    | -1.47e-17 |   1.00 |   2.24 |   [-1.22, 1.01] |    -0.19 |    -1.76 | 32 |         0\ndisp_z   | -9.08e-17 |   1.00 |   1.79 |   [-1.29, 1.95] |     0.42 |    -1.07 | 32 |         0\nhp_z     |  1.04e-17 |   1.00 |   1.23 |   [-1.38, 2.75] |     0.80 |     0.28 | 32 |         0\ndrat_z   | -2.92e-16 |   1.00 |   1.57 |   [-1.56, 2.49] |     0.29 |    -0.45 | 32 |         0\nwt_z     |  4.68e-17 |   1.00 |   1.21 |   [-1.74, 2.26] |     0.47 |     0.42 | 32 |         0\nqsec_z   |  5.30e-16 |   1.00 |   1.13 |   [-1.87, 2.83] |     0.41 |     0.86 | 32 |         0\nvs_z     |  6.94e-18 |   1.00 |   1.98 |   [-0.87, 1.12] |     0.26 |    -2.06 | 32 |         0\nam_z     |  4.51e-17 |   1.00 |   2.00 |   [-0.81, 1.19] |     0.40 |    -1.97 | 32 |         0\ngear_z   | -3.47e-18 |   1.00 |   1.36 |   [-0.93, 1.78] |     0.58 |    -0.90 | 32 |         0\ncarb_z   |  3.17e-17 |   1.00 |   1.24 |   [-1.12, 3.21] |     1.16 |     2.02 | 32 |         0\n\n\n\nm1 &lt;- \n  stan_glm(mpg_z ~ hp_z + am + hp_z:am, \n           seed = 42,\n           refresh = 0,\n           data = mtcars2)\n\ncoef(m1)\n\n (Intercept)         hp_z           am      hp_z:am \n-0.357413145 -0.677859338  0.876342434  0.005465839 \n\n\n\nIntercept: Ein Auto mit 0 PS und Automatikantrieb (am=0, s. Hilfe zum Datensatz: help(mtcars)) kann laut Modell mit einer Gallone Sprit ca. -0.36 Meilen fahren. Dieser Wert ist ca. Null, da die AV z-standardisiert ist. Ein Wert von Null in einer z-standardisierten Variablen entspricht dem Mittelwert in den Rohwerten.\nhp: Pro zusätzlichem PS kann ein Auto mit Automatikantrieb pro Gallone Sprit ca. -0.68 Meilen weniger weit fahren.\nam: Ein Auto mit 0 PS und Schaltgetriebe (am=1) kommt pro Gallone Sprit ca. 0.88 Meilen weiter als ein Auto mit Automatikantrieb.\nhp:am: Der Interaktionseffekt ist praktisch Null (-0.36): Der Zusammenhang von PS-Zahl und Spritverbrauch unterscheidet sich nicht (wesentlich) zwischen Autos mit bzw. ohne Automatikantrieb.\n\n\nCategories:\n\nbayes\nregression"
  },
  {
    "objectID": "posts/anim03/anim03.html",
    "href": "posts/anim03/anim03.html",
    "title": "anim03",
    "section": "",
    "text": "Visualisieren Sie in animierter Form die Temperatur in New York im Zeitverlauf der Kontinent soll in der Visualisierung berücksichtigt sein.\nHinweise:\n\nBeziehen Sie sich auf die Daten des Datensatzes airquality.\nNutzen Sie plotly zur Visualisierung.\nNutzen Sie die Monate als “Gruppierungsvariable”.\nVerwenden Sie das Paket gganimate."
  },
  {
    "objectID": "posts/anim03/anim03.html#setup",
    "href": "posts/anim03/anim03.html#setup",
    "title": "anim03",
    "section": "Setup",
    "text": "Setup\n\nlibrary(gapminder)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(gganimate)\ndata(gapminder)"
  },
  {
    "objectID": "posts/anim03/anim03.html#statisches-diagramm",
    "href": "posts/anim03/anim03.html#statisches-diagramm",
    "title": "anim03",
    "section": "Statisches Diagramm",
    "text": "Statisches Diagramm\n\np &lt;- airquality %&gt;% \n  ggplot(aes(x = Day, y = Temp, color = factor(Month))) +\n  geom_line()\np"
  },
  {
    "objectID": "posts/anim03/anim03.html#animiertes-und-interaktives-diagramm",
    "href": "posts/anim03/anim03.html#animiertes-und-interaktives-diagramm",
    "title": "anim03",
    "section": "Animiertes (und interaktives) Diagramm",
    "text": "Animiertes (und interaktives) Diagramm\n\np + transition_reveal(Day)\n\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n\n\n\n\n\n\n\n\n\nDieser Post orientiert sich an dieser Quelle; dort finden sich auch mehr Beispiele.\n\nCategories:\n\n2023\nvis\nanimation\nstring"
  },
  {
    "objectID": "posts/kekse01/kekse01.html",
    "href": "posts/kekse01/kekse01.html",
    "title": "kekse01",
    "section": "",
    "text": "Exercise\nIn Think Bayes stellt Allen Downey folgende Aufgabe:\n“Suppose there are two bowls of cookies.\nBowl 1 contains 30 vanilla cookies and 10 chocolate cookies.\nBowl 2 contains 20 vanilla cookies and 20 chocolate cookies.\nNow suppose you choose one of the bowls at random and, without looking, choose a cookie at random. If the cookie is vanilla, what is the probability that it came from Bowl 1?”\nHinweise:\n\nUntersuchen Sie die Hypothesen \\(\\pi_0 = 0, \\pi_1 = 0.1, \\pi_2 = 0.2, ..., \\pi_{10} = 1\\) für die Trefferwahrscheinlichkeit\nErstellen Sie ein Bayes-Gitter zur Lösung dieser Aufgabe.\nGehen Sie davon aus, dass Sie (apriori) indifferent gegenüber der Hypothesen zu den Parameterwerten sind.\nGeben Sie Prozentzahlen immer als Anteil an und lassen Sie die führende Null weg (z.B. .42).\n\n         \n\n\nSolution\n\n\n\n\n\np_Gitter\nPriori\nLikelihood\nunstd_Post\nPost\n\n\n\n\n1\n1\n0.75\n0.75\n0.6\n\n\n2\n1\n0.50\n0.50\n0.4\n\n\n\n\n\nDie Antwort lautet: .6\n\nCategories:\n\nprobability\nbayes-grid"
  },
  {
    "objectID": "posts/predictioncontest1/predictioncontest1.html",
    "href": "posts/predictioncontest1/predictioncontest1.html",
    "title": "predictioncontest1",
    "section": "",
    "text": "Question\n\nAufgabe\nErstellen Sie eine Analyse, die einem typischen Vorhersageprojekt entspricht!\nNutzen Sie den Datensatz penguins!\nSagen Sie die Variable body_mass_g vorher.\nHinweise:\n\nHalten Sie die Analyse einfach.\nTeilen Sie Test- vs. Train-Set hälftig auf.\nTeilen Sie Analysis vs. Assessment-Set 3:1 auf.\nDen Datensatz penguins können Sie entweder aus dem Paket palmerpenguins beziehen oder z.B. von hier via read_csv() importieren.\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n         \n\n\nLösung\nPakete laden:\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.0.8\n✔ dials        1.2.0     ✔ rsample      1.2.0\n✔ dplyr        1.1.4     ✔ tibble       3.2.1\n✔ ggplot2      3.5.0     ✔ tidyr        1.3.1\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.3.0     ✔ workflows    1.1.3\n✔ parsnip      1.2.0     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.3.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ lubridate 1.9.3     ✔ stringr   1.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.7.0 (red = needs update)\n✔ bayestestR  0.13.2   ✔ correlation 0.8.4 \n✖ datawizard  0.9.1    ✖ effectsize  0.8.6 \n✖ insight     0.19.8   ✔ modelbased  0.8.7 \n✖ performance 0.10.9   ✖ parameters  0.21.5\n✔ report      0.5.8    ✖ see         0.8.2 \n\nRestart the R-Session and update packages with `easystats::easystats_update()`.\n\ndata(\"penguins\", package = \"palmerpenguins\")\n\nMan erinnere sich, dass ein R-Paket erst (einmalig) installiert sein muss, bevor Sie darauf zugreifen können, etwa um Daten - wie den Datensatz penguins - daraus zu beziehen.\nZeilen mischen und Train- vs. Testset aufteilen:\n\npenguins2 &lt;-\n  penguins %&gt;% \n  sample_n(size = nrow(.))\n\nd_train &lt;- penguins2 %&gt;% slice(1:(344/2))\nd_test &lt;- penguins2 %&gt;% slice(173:nrow(penguins))\n\nDas Trainset weiter aufteilen:\n\nd_split &lt;- initial_split(d_train)\n\nd_analysis &lt;- training(d_split)\nd_assessment &lt;- testing(d_split)\n\nRezept definieren:\n\nrec1 &lt;-\n  recipe(body_mass_g ~ ., data = d_analysis) %&gt;% \n  step_impute_knn(all_predictors()) %&gt;% \n  step_normalize(all_numeric(), -all_outcomes())\n\nRezept prüfen:\n\nd_analysis_baked &lt;- \nrec1 %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL)\n\ndescribe_distribution(d_analysis_baked)\n\nVariable          |      Mean |     SD |     IQR |              Range | Skewness | Kurtosis |   n | n_Missing\n-------------------------------------------------------------------------------------------------------------\nbill_length_mm    | -5.75e-16 |   1.00 |    1.63 |      [-2.06, 2.72] |     0.10 |    -0.76 | 129 |         0\nbill_depth_mm     | -1.12e-16 |   1.00 |    1.52 |      [-2.12, 2.30] |    -0.24 |    -0.70 | 129 |         0\nflipper_length_mm |  1.06e-16 |   1.00 |    1.76 |      [-1.62, 2.12] |     0.42 |    -1.15 | 129 |         0\nyear              |  2.18e-15 |   1.00 |    2.51 |      [-1.27, 1.25] |    -0.01 |    -1.42 | 129 |         0\nbody_mass_g       |   4187.30 | 812.31 | 1200.00 | [2700.00, 6050.00] |     0.55 |    -0.56 | 128 |         1\n\n\nWorkflow und CV definieren:\n\nm1 &lt;- \n  linear_reg()\n\nwf1 &lt;-\n  workflow() %&gt;% \n  add_recipe(rec1) %&gt;% \n  add_model(m1)\n\ncv_scheme &lt;- vfold_cv(d_analysis, v = 2)\n\nFitten (hier kein Tuning):\n\nfit1 &lt;-\n  wf1 %&gt;% \n  tune_grid(resamples = cv_scheme)\n\nWarning: No tuning parameters have been detected, performance will be evaluated\nusing the resamples with no tuning. Did you want to [tune()] parameters?\n\n\nFinalisieren:\n\nshow_best(fit1)\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    316.     2    6.86 Preprocessor1_Model1\n\nwf1_final &lt;-\n  wf1 %&gt;% \n  finalize_workflow(show_best(fit1))\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\nwf1_final\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_impute_knn()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nModellgüte:\n\nfit1_final &lt;-\n  wf1_final %&gt;% \n  last_fit(d_split)\n\n\ncollect_metrics(fit1_final)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard     271.    Preprocessor1_Model1\n2 rsq     standard       0.896 Preprocessor1_Model1\n\nfit1_train &lt;-\n  wf1_final %&gt;% \n  fit(d_train)\n\n\nfit1_test &lt;-\n  fit1_train %&gt;% \n  predict(d_test)\n\nhead(fit1_test)\n\n# A tibble: 6 × 1\n  .pred\n  &lt;dbl&gt;\n1 4366.\n2 5444.\n3 4154.\n4 3277.\n5 4164.\n6 4905.\n\n\nVgl https://workflows.tidymodels.org/reference/predict-workflow.html\nSubmitten:\n\nsubm_df &lt;-\n  d_test %&gt;% \n  mutate(id = 173:344) %&gt;% \n  bind_cols(fit1_test) %&gt;% \n  select(id, .pred) %&gt;% \n  rename(pred = .pred)\n\nUnd als CSV-Datei speichern:\n\n#write_csv(subm_df, file = \"submission_blabla.csv\")\n\n\nCategories:\n\nR\nds1\nsose22\nstring"
  },
  {
    "objectID": "posts/interpret-koeff/interpret-koeff.html",
    "href": "posts/interpret-koeff/interpret-koeff.html",
    "title": "interpret-koeff",
    "section": "",
    "text": "Exercise\nBetrachten Sie dieses Modell, das den Zusammenhang von PS-Zahl und Spritverbrauch untersucht (Datensatz mtcars):\n\ndata(mtcars)\nlibrary(rstanarm)\nlibrary(easystats)\nlm1 &lt;- stan_glm(mpg ~ hp, data = mtcars,\n                refresh = 0)\nparameters(lm1)\n\nParameter   | Median |         95% CI |   pd |  Rhat |     ESS |                   Prior\n----------------------------------------------------------------------------------------\n(Intercept) |  30.08 | [26.76, 33.45] | 100% | 1.000 | 3832.00 | Normal (20.09 +- 15.07)\nhp          |  -0.07 | [-0.09, -0.05] | 100% | 1.000 | 3740.00 |   Normal (0.00 +- 0.22)\n\n\n\nWas bedeuten die Koeffizienten?\nWie ist der Effekt von \\(\\beta_1\\) zu interpretieren?\n\n         \n\n\nSolution\n\nIntercept (\\(\\beta_0\\)): Der Achsenabschnitt gibt den geschätzten mittleren Y-Wert (Spritverbrauch) an, wenn \\(x=0\\), also für ein Auto mit 0 PS (was nicht wirklich Sinn macht). hp (\\(\\beta_1\\)) ist der Regressionskoeffizient oder Regressionsgewicht und damit die Steigung der Regressionsgeraden.\nhp (\\(\\beta_1\\)) ist der Regressionskoeffizient oder Regressionsgewicht und gibt den statistischen “Effekt” der PS-Zahl auf den Spritverbrauch an. Vorsicht: Dieser “Effekt” darf nicht vorschnell als kausaler Effekt verstanden werden. Daher muss man vorsichtig sein, wenn man von einem “Effekt” spricht. Vorsichtiger wäre zu sagen: “Ein Auto mit einem PS mehr, kommt im Mittel 0,1 Meilen weniger weit mit einer Gallone Sprit, laut diesem Modell”.\n\n\nCategories:\n\nregression\nlm\nbayes\nstats-nutshell"
  },
  {
    "objectID": "posts/Bayesmod-bestimmen02/Bayesmod-bestimmen02.html",
    "href": "posts/Bayesmod-bestimmen02/Bayesmod-bestimmen02.html",
    "title": "Bayesmod-bestimmen02",
    "section": "",
    "text": "Exercise\nSie möchten, im Rahmen einer Studie, ein einfaches lineare Modell spezifizieren, d.h. den Likelihood und die Priori-Verteilungen benennen.\nFolgende Informationen sind gegeben:\n\nAV: einnahmen\nUV: werbebudget\nAlle empirischen Variablen sind z-standardisiert.\nAlle Variablen sollen als normalverteilt angegeben werden mit Ausnahme der Streuung der AV, diese ist exponenzialverteilt mit Rate 1 zu modellieren.\nStreuungen der Normalverteilung sind mit 2.5 SD anzugeben.\n\nSchreiben Sie in mathematischer Notation folgende Notation auf:\nPriori-Verteilung der Streuung der AV\nHinweise:\n\nVerzichten Sie auf Leerstellen in Ihrer Antwort. \nBenennen Sie \\(\\beta\\) mit b, \\(\\alpha\\) mit a und \\(\\sigma\\) mit s.\nNutzen Sie die Tilde ~ um stochastische Relationen (Verteilungen) anzuzeigen.\nGeben Sie Normalverteilungen als Normal(x;y) und Exponentialverteilung als Exp(x) an (jeweils mit den korrekten Argumenten in der allgemein üblichen Form).\n\n         \n\n\nSolution\ns~Exp(1)\n\nCategories:\n\nregression\nbayes\nprior"
  },
  {
    "objectID": "posts/bayes2/bayes2.html",
    "href": "posts/bayes2/bayes2.html",
    "title": "bayes2",
    "section": "",
    "text": "Aufgabe\nWir haben eine Münze \\(n=10\\) Mal geworfen. Unsere Daten (\\(D\\)) sind: 8 Mal lag “Kopf” oben. Gegeben dieser Datenlage, wie hoch ist die Wahrscheinlichkeit für das Ereignis \\(F\\) (Falschspieler-Münze), dass die Münze also gezinkt ist auf \\(p=.8\\)? Apriori sind wir indifferent, ob die Münze gezinkt ist oder nicht (\\(\\neg F\\), also \\(p=.5\\)). Der Einfachheit halber gehen wir davon aus, dass es nur zwei Zustände für die Münze geben kann, gezinkt (\\(F\\)) oder nicht gezinkt (\\(\\neg F\\)).\nAufgabe: Berechnen Sie die Wahrscheinlichkeit, dass die Münze gezinkt ist (\\(F\\)), gegeben der Datenlage \\(D\\)!\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n         \n\n\nLösung\nGesucht ist die Wahrscheinlichkeit, dass die Münze gezinkt ist, gegeben der beobachteten Daten: \\(Pr(F|D)\\).\n\np1 = .8\np2 = .5\nn = 10\nk = 8\n\nEs gilt:\n\\(Pr(F|D) = \\frac{L \\times Priori}{Evidenz} = \\frac{Pr(D|F) Pr(F)}{Pr(D)} =  \\frac{Pr(D|F) Pr(F)}{Pr(D|F)Pr(F)  + {Pr(D|\\neg F)Pr(\\neg F)}}\\)\nDie Likelihood, L, berechnet sich so:\n\nL &lt;- dbinom(x = k, size = n, prob = p1)\nL\n\n[1] 0.3019899\n\n\nDer Zähler des Bruchs (unstand. Post) berechnet sich so:\n\nPost_unstand &lt;- L * 1/2\nPost_unstand\n\n[1] 0.1509949\n\n\nLikelihood für die Daten, wenn die Münze nicht gezinkt ist:\n\nL2 &lt;- dbinom(x = k, size = n, prob = p2)\nL2\n\n[1] 0.04394531\n\n\nDie unstand. Post-Wahrscheinlichkeit für die Hypothese, dass die Münze nicht gezinkt ist, gegeben der Daten:\n\nPost_unstand2 &lt;- L2 * 1/2\nPost_unstand2\n\n[1] 0.02197266\n\n\nDie Evidenz, E, berechnet sich als Summe aller unstand. Post-Wahrscheinlichkeiten (also über alle möglichen Hypothesen, d.h. \\(F\\) und \\(\\neg F\\), also \\(L\\) plus \\(L_2\\)):\n\nE &lt;- Post_unstand + Post_unstand2\nE\n\n[1] 0.1729676\n\n\nDie standardisierte Post-Wahrscheinlichkeit ist also die unstand. Post-Wahrscheinlichkeit geteilt durch die Evidenz:\n\nPost_std &lt;- Post_unstand / E\nPost_std\n\n[1] 0.8729666\n\n\nAntwort: Die Lösung beträgt 0.87.\n\nCategories:\n\nR\nbayes\nprobability\nnum"
  },
  {
    "objectID": "posts/PCA1/PCA1.html",
    "href": "posts/PCA1/PCA1.html",
    "title": "PCA1",
    "section": "",
    "text": "Principal Component Analysis (PCA) ist ein gängiges Verfahren zur Dimensionsreduktion eines \\(n \\times p\\) Datensatz \\(\\boldsymbol{X}\\). Welche Aussage ist in dem Zusammenhang (am ehesten) korrekt?\n\n\n\nEine z-Transformation ist i.A. empfehlenswert.\nDie Gerade einer Hauptkomponenten und die Regressionsgeraden sind i.A. nicht identisch.\nDie PCA ist ein geleitetes (supervised) Verfahren des statistisches Lernens.\nEin Screeplot zeigt auf der \\(Y\\)-Achse die kumulierte, erklärte Varianz.\nBei einem \\(n \\times p\\) Datensatz \\(\\boldsymbol{X}\\) gibt es max(\\(n-1, p\\)) Komponenten.\nKomponenten können (müssen aber nicht) orthogonal zueinander sein.\nJe stärker korreliert die \\(p\\) Variablen sind, desto weniger sinnvoll ist eine PCA.\nEin Screeplot zeigt auf der \\(X\\)-Achse die kumulierte, erklärte Varianz."
  },
  {
    "objectID": "posts/PCA1/PCA1.html#answerlist",
    "href": "posts/PCA1/PCA1.html#answerlist",
    "title": "PCA1",
    "section": "",
    "text": "Eine z-Transformation ist i.A. empfehlenswert.\nDie Gerade einer Hauptkomponenten und die Regressionsgeraden sind i.A. nicht identisch.\nDie PCA ist ein geleitetes (supervised) Verfahren des statistisches Lernens.\nEin Screeplot zeigt auf der \\(Y\\)-Achse die kumulierte, erklärte Varianz.\nBei einem \\(n \\times p\\) Datensatz \\(\\boldsymbol{X}\\) gibt es max(\\(n-1, p\\)) Komponenten.\nKomponenten können (müssen aber nicht) orthogonal zueinander sein.\nJe stärker korreliert die \\(p\\) Variablen sind, desto weniger sinnvoll ist eine PCA.\nEin Screeplot zeigt auf der \\(X\\)-Achse die kumulierte, erklärte Varianz."
  },
  {
    "objectID": "posts/PCA1/PCA1.html#answerlist-1",
    "href": "posts/PCA1/PCA1.html#answerlist-1",
    "title": "PCA1",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\nschoice"
  },
  {
    "objectID": "posts/OLS-Minimierung/OLS-Minimierung.html",
    "href": "posts/OLS-Minimierung/OLS-Minimierung.html",
    "title": "OLS-Minimierung",
    "section": "",
    "text": "Um die Parameter der Regressiongeraden zu bestimmen, wird ein Ausdruck minimiert. Wählen Sie den korrekten Ausdruck.\n\n\n\n\\(\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\\)\n\\(\\sum_{i=1}^{n}(y_i - \\hat{y}_i)\\)\n\\(\\sum_{i=1}^{n}(|y_i - \\hat{y}_i|)^2\\)\n\\(\\sum_{i=1}^{n}(|y_i + \\hat{y}_i|)\\)\n\\(\\sum_{i=1}^{n}(y_i + \\hat{y}_i)^2\\)"
  },
  {
    "objectID": "posts/OLS-Minimierung/OLS-Minimierung.html#answerlist",
    "href": "posts/OLS-Minimierung/OLS-Minimierung.html#answerlist",
    "title": "OLS-Minimierung",
    "section": "",
    "text": "\\(\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\\)\n\\(\\sum_{i=1}^{n}(y_i - \\hat{y}_i)\\)\n\\(\\sum_{i=1}^{n}(|y_i - \\hat{y}_i|)^2\\)\n\\(\\sum_{i=1}^{n}(|y_i + \\hat{y}_i|)\\)\n\\(\\sum_{i=1}^{n}(y_i + \\hat{y}_i)^2\\)"
  },
  {
    "objectID": "posts/OLS-Minimierung/OLS-Minimierung.html#answerlist-1",
    "href": "posts/OLS-Minimierung/OLS-Minimierung.html#answerlist-1",
    "title": "OLS-Minimierung",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\nschoice"
  },
  {
    "objectID": "posts/ungewiss-arten-regr/ungewiss-arten-regr.html",
    "href": "posts/ungewiss-arten-regr/ungewiss-arten-regr.html",
    "title": "ungewiss-arten-regression",
    "section": "",
    "text": "Exercise\nEine statistische Analyse, wie eine Regression, ist mit mehreren Arten an Ungewissheit konfrontiert. Zum einen gibt es die Ungewissheit in den Modellparametern. Für die Regression bedeutet das: “Liegt die Regressionsgerade in”Wahrheit” (in der Population) genauso wie in der Stichprobe, sind Achsenabschnitt und Steigung in der Stichprobe also identisch zur Popuation?“. Zum anderen die Ungewissheit innerhalb des Modells. Auch wenn wir die”wahre” Regressionsgleichung kennen würden, wären (in aller Regel) die Vorhersagen trotzdem nicht perfekt. Auch wenn wir etwa wüssten, wieviel Klausurpunkte “in Wahrheit” pro Stunde Lernen herausspringen (und wenn wir den wahren Achsenabschnitt kennen würden), so würde das Modell trotzdem keine perfekten Vorhersagen zum Klausurerfolg liefern. Vermutlich fehlen dem Modell wichtige Informationen etwa zur Motivation der Studentis.\nVor diesem Hintergrund, betrachten Sie folgendes statistisches Modell, das mit den Methoden der Bayes-Statistik berechnet wurde. Dazu wurde die Funktion stan_glm() verwendet, die ähnlich zu lm() ein lineare Modell berechnet. Ein wichtiger Unterschied zu lm() ist, dass Ungewissheiten zu den Parameterschätzungen ausgegeben werden.\n\ndata(mtcars) \nlibrary(rstanarm) \nlibrary(easystats)\nlm1 &lt;- stan_glm(mpg ~ hp, data = mtcars,\n                refresh = 0)  # um nicht zu viel R-Ausgabe zu erhalten\n\nparameters(lm1)\n\nParameter   | Median |         95% CI |   pd |  Rhat |     ESS |                   Prior\n----------------------------------------------------------------------------------------\n(Intercept) |  30.08 | [26.70, 33.43] | 100% | 0.999 | 3419.00 | Normal (20.09 +- 15.07)\nhp          |  -0.07 | [-0.09, -0.05] | 100% | 0.999 | 3469.00 |   Normal (0.00 +- 0.22)\n\n\nFür den Prädiktor hp ist das Regressionsgewicht (Punktschätzer) angegeben unter der Spalte Median. Dieser Wert entspricht der Punktschätzung in der Population und ist identisch zum Regressionsgewicht (“b”) der Stichprobe.\nDie Spalte 95% CI gibt das 95%-Konfidenzintervall (CI wie confidence interval) zur Schätzung der Ungewissheit der Koeffizienten (der entsprechenden Zeile) wieder.\n\nWie breit ist das Intervall, in dem mit 95% Gewissheit der Achsenabschnitt liegt (laut diesem Model)?\nWie breit ist das Intervall, in dem mit 95% Gewissheit das Regressionsgewicht liegt (laut diesem Model)?\n\nHinweise:\n\nRunden Sie auf zwei Dezimalstellen.\nIgnorieren Sie die Spalte zu ROPE, pd, Prior und Rhat! Goldene Regel der Statistik: Wenn du eine Information nicht brauchst, dann ignoriere sie erstmal ;-)\n\n         \n\n\nSolution\n\n6.73\n0.04\n\n\nCategories:\n\nqm2\ninference\nlm"
  },
  {
    "objectID": "posts/bike01/bike01.html",
    "href": "posts/bike01/bike01.html",
    "title": "bike01",
    "section": "",
    "text": "Kann man die Anzahl gerade verliehener Fahrräder eines entsprechenden Anbieters anhand der Temperatur vorhersagen?\nIn dieser Übung untersuchen wir diese Frage.\nSie können die Daten von der Webseite der UCI herunterladen.\nWir beziehen uns auf den Datensatz day.\nBerechnen Sie ein lineares Modell mit der Anzahl der aktuell vermieteten Räder als AV und der aktuellen Temperatur als UV!\nGeben Sie den MSE an!\nHinweise"
  },
  {
    "objectID": "posts/bike01/bike01.html#data-split",
    "href": "posts/bike01/bike01.html#data-split",
    "title": "bike01",
    "section": "Data split",
    "text": "Data split\n\nset.seed(42)\nsplit_vec &lt;- initial_split(d, strata = cnt)\n\nd_train &lt;- training(split_vec)\nd_test &lt;- testing(split_vec)"
  },
  {
    "objectID": "posts/bike01/bike01.html#define-recipe",
    "href": "posts/bike01/bike01.html#define-recipe",
    "title": "bike01",
    "section": "Define recipe",
    "text": "Define recipe\n\nrec1 &lt;- \n  recipe(cnt ~ temp, data = d)"
  },
  {
    "objectID": "posts/bike01/bike01.html#define-model",
    "href": "posts/bike01/bike01.html#define-model",
    "title": "bike01",
    "section": "Define model",
    "text": "Define model\n\nm1 &lt;-\n  linear_reg()"
  },
  {
    "objectID": "posts/bike01/bike01.html#workflow",
    "href": "posts/bike01/bike01.html#workflow",
    "title": "bike01",
    "section": "Workflow",
    "text": "Workflow\n\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(m1) %&gt;% \n  add_recipe(rec1)"
  },
  {
    "objectID": "posts/bike01/bike01.html#fit",
    "href": "posts/bike01/bike01.html#fit",
    "title": "bike01",
    "section": "Fit",
    "text": "Fit\n\nfit1 &lt;- last_fit(wf1, split_vec)\nfit1\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits            id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;            &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [547/184]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;"
  },
  {
    "objectID": "posts/bike01/bike01.html#model-performance-metrics-in-test-set",
    "href": "posts/bike01/bike01.html#model-performance-metrics-in-test-set",
    "title": "bike01",
    "section": "Model performance (metrics) in test set",
    "text": "Model performance (metrics) in test set\n\nfit1 %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    1509.    Preprocessor1_Model1\n2 rsq     standard       0.411 Preprocessor1_Model1\n\n\n\nMSE &lt;- fit1 %&gt;% collect_metrics() %&gt;% pluck(3, 1)\nMSE\n\n[1] 1509.477\n\n\nSolution: 1509.4768321\n\nCategories:\n\nstatlearning\ntidymodels\nnum"
  },
  {
    "objectID": "posts/k-coins-k-hits/k-coins-k-hits.html",
    "href": "posts/k-coins-k-hits/k-coins-k-hits.html",
    "title": "k-coins-k-hits",
    "section": "",
    "text": "Aufgabe\nSie werfen eine Münze \\(k = 7\\) Mal; die Trefferchance betrage \\(p = 0.8\\). Die Münzwürfe seien unabhängig voneinander.\nWie hoch ist die Wahrscheinlichkeit für (genau) \\(k = 7\\) Treffer?\nBeachten Sie die Bearbeitungshinweise.\n         \n\n\nLösung\nTrefferchance bei jedem Wurf:\n\n\n[1] 0.8\n\n\nAnzahl Würfe/Münzen:\n\n\n[1] 7\n\n\nAufgrund der Multiplikationsregel der Wahrscheinlichkeitsrechnung sind die Wahrscheinlichkeiten der \\(k\\) Ereignisse (Treffer) zu multiplizieren, da unabhängig. Da es nur eine Möglichkeit gibt, bei \\(k\\) Würfen \\(k\\) Treffer zu erzielen, gibt es nur einen “Pfad”.\n\nsol &lt;- p^k  # \"sol\" wie \"solution\" (Lösung)\nsol\n\n[1] 0.2097152\n\n\n\nAufgaben-ID: k-coins-k-hits\n\nCategories:\n\nprobability\ndyn\nbayes\nnum"
  },
  {
    "objectID": "posts/groesse02/groesse02.html",
    "href": "posts/groesse02/groesse02.html",
    "title": "groesse02",
    "section": "",
    "text": "Aufgabe\nWir interessieren uns für die typische Körpergröße deutscher Studentis. Hier findet sich dazu ein Datensatz.\nAusgehend von der Annahme, dass sich die Körpergröße normalverteilt (innerhalb eines Geschlechts) suchen wir die Parameter der Normalverteilung, also Mittelwert und Streuung.\nGehen wir von folgenden Apriori-Wahrscheinlichkeiten für die Parameter der Normalverteilung aus:\n\nMittelwert: 150cm bis 200 cm, jeder Wert gleich plausibel, alle anderen Werte unmöglich\nSD: 1cm bis 20cm, jeder Wert gleich plausibel, alle anderen Werte unmöglich\n\nJa, das sind ziemlich einfältige Annahmen, aber gut, fangen wir damit an.\nErstellen Sie eine Bayes-Box!\nHinweise:\n\nUntersuchen Sie den angegebenen Parameterbereich in 1cm-Schritten.\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n         \n\n\nLösung\n\nlibrary(pradadata)  # für den Datensatz `wo_men`\nlibrary(prada)  # für bayesbox, alternativ mit `source`\nlibrary(tidyverse)\nlibrary(ggpubr)\n\nDaten importieren:\n\ndata(wo_men)\n\nMittelwert in der Stichprobe:\n\nwo_men |&gt; \n  group_by(sex) |&gt; \n  summarise(height_avg = mean(height, na.rm = TRUE),\n            height_sd = sd(height, na.rm = TRUE))\n\n# A tibble: 3 × 3\n  sex   height_avg height_sd\n  &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n1 man         183.      9.96\n2 woman       161.     42.8 \n3 &lt;NA&gt;        NaN      NA   \n\n\nZur Berechnung der Likelihoods diskretisieren wir die stetige Variable height in Stufen von jeweils 1cm, der Einfachheit halber.\nDie Wahrscheinlichkeit für das 1cm-Intervall um unserem Stichprobenergebnis herem (182.5cm bis 183.5cm), bei z.B. einem Mittelwert von 180cm und einer SD von 10cm, entspricht dann dieser Differenz:\n\nobere_grenze &lt;- pnorm(q = 183 + 0.5, mean = 180, sd = 10)\nuntere_grenze &lt;- pnorm(q = 183 - 0.5, mean = 180, sd = 10)\n\nobere_grenze\n\n[1] 0.6368307\n\nuntere_grenze\n\n[1] 0.5987063\n\nobere_grenze - untere_grenze\n\n[1] 0.03812433\n\n\nVisualisieren wir uns kurz dieses Intervall.\n\nlibrary(mosaic)\nxpnorm(q = c(182.5, 183.5), mean = 180, sd = 10)\n\n[1] 0.5987063 0.6368307\n\n\n\n\n\n\n\n\n\nAls nächstes legen wir die Werte für unsere Bayes-Box fest.\n\nnorm_mean &lt;- seq(from = 150, to = 200, by = 1)\nnorm_sd &lt;- seq(from = 1, to = 20, by = 1)\n\nJetzt bauen wir unsere Bayes-Box.\nWenn wir die Wahrscheinlichkeiten der Parameter für alle Kombinationen aus 51 Mittelwerten und 20 SD-Werten prüfen wollen, wird die Tabelle ganz schön lang:\n\nanzahl_kombinationen &lt;- length(norm_mean) * length(norm_sd)\nanzahl_kombinationen\n\n[1] 1020\n\n\nMit expand_grid kann man sich eine Tabelle erstellen lassen, die alle Kombinationen zweier Variablen aufschreibt:\n\nbayes_box &lt;-\n  expand_grid(norm_mean, norm_sd)\n\nhead(bayes_box)\n\n# A tibble: 6 × 2\n  norm_mean norm_sd\n      &lt;dbl&gt;   &lt;dbl&gt;\n1       150       1\n2       150       2\n3       150       3\n4       150       4\n5       150       5\n6       150       6\n\n\nDas sind unsere Parameterwerte: Jede Kombination eines Mittelwerts und einer Streuung ist eine Hypothese. Insgesamt haben wir also 1020 Parameterwerte.\nSo, bauen wir die Bayes-Box weiter:\n\nL &lt;- pnorm(183.5, mean = bayes_box$norm_mean, sd = bayes_box$norm_sd)\n\nbayes_box2 &lt;-\n  bayes_box |&gt; \n  mutate(hyp = 1:anzahl_kombinationen,\n         lik = L,\n         post_unstand = hyp * lik,\n         post_std = post_unstand / sum(post_unstand))\n\n\nsamples &lt;-\n  bayes_box2 |&gt; \n  slice_sample(\n    n = 1e4,\n    weight_by = post_std,\n    replace = TRUE\n  )\n\nUnd jetzt visualisieren:\n\nsamples |&gt; \n  ggplot() +\n  aes(x = norm_sd,\n      y = norm_mean,\n      fill = post_std) +\n  geom_tile() +\n  scale_fill_viridis_c()\n\n\n\n\n\n\n\n\nDie Stichproben-Postverteilung erlaubt es auch bequem, die einzelnen Parameter der Post-Verteilung jeweils für sich zu visualisieren:\n\ngghistogram(samples,\n            x = \"norm_mean\",\n            bins = 15)\n\n\n\n\n\n\n\n\n\nggdensity(samples,\n          x = \"norm_mean\")\n\n\n\n\n\n\n\n\n\nggdensity(samples,\n          x = \"norm_sd\")\n\n\n\n\n\n\n\n\n\nCategories: Categories:\n\n2023\nbayes\nbayes-box\nstring"
  },
  {
    "objectID": "posts/Wertzuweisen/Wertzuweisen.html",
    "href": "posts/Wertzuweisen/Wertzuweisen.html",
    "title": "Wertzuweisen",
    "section": "",
    "text": "Aufgabe\nWeisen Sie dem Objekt loesung den Wert 42 zu. Geben Sie den korrekten R-Code dafür ein.\nHinweis: Verzichten Sie jegliche Leerzeichen in Ihrer Eingabe, da sonst die Eingabe nicht als korrekt erkannt werden kann.\n         \n\n\nLösung\nloesung&lt;-42\n\nCategories:\n\nR\n‘2023’\nstring"
  },
  {
    "objectID": "posts/Additionssatz1/Additionssatz1.html",
    "href": "posts/Additionssatz1/Additionssatz1.html",
    "title": "Additionssatz1",
    "section": "",
    "text": "Aufgabe\nEin Hersteller überteuerter Mobilfunkgeräte vermutet, dass 80% seiner Kunden auf der Webseite A (Schick-und-Schön) abhängen, und 50% seiner Kunden auf der Webseite B (Cool-but-Useless). Außerdem schätzt er, dass 35% der Kunden auf beiden Seiten abhängen. Auf beiden Webseiten schaltet der Hersteller eine große Werbeanzeige.\nAufgabe: Wie groß ist die Wahrscheinlichkeit, dass ein potenzieller Kunde die Anzeige liest?\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n         \n\n\nLösung\nSei \\(A\\) “Leute, die auf Webseite A abhängen”.\nSei \\(B\\) “Leute, die auf Webseite A abhängen”.\n\\(Pr(A \\cup B) = Pr(A) + Pr(B) - Pr(AB)\\)\n\nAoderB &lt;- .8 + .5 - .35\nAoderB\n\n[1] 0.95\n\n\nDie Lösung lautet 0.95.\n\nCategories:\n\nR\nprobability\nnum"
  },
  {
    "objectID": "posts/Test-MSE1/Test-MSE1.html",
    "href": "posts/Test-MSE1/Test-MSE1.html",
    "title": "Test-MSE1",
    "section": "",
    "text": "Angenommen, Sie arbeiten als Analyst mit folgender Aufgabe:\nEs liegt ein Datensatz mit 600 Beschäftigten (als Beobachtungseinheit) vor. Für jede Person sind folgende Informationen bekannt: Dauer der Betriebszugehörigkeit, Alter, Ausbildung und Ergebnis der letzten Leistungsbeurteilung. Ziel ist es, die Höhe des zu erwartenden Gehalts vorherzusagen.\nWelche Aussage ist richtig?\n\n\n\nEs handelt sich um eine Regression. Ziel ist eine Vorhersage. \\(N=600\\), \\(p=4\\).\nEs handelt sich um eine Klassifikation. Ziel ist eine Vorhersage. \\(N=600\\), \\(p=4\\).\nEs handelt sich um eine Regression. Ziel ist eine Erklärung (inference). \\(N=600\\), \\(p=4\\).\nEs handelt sich um eine Regression. Ziel ist eine Vorhersage. \\(N=600\\), \\(p=5\\).\nEs handelt sich um eine Klassifikation. Ziel ist eine Vorhersage. \\(N=600\\), \\(p=5\\).\nEs handelt sich um eine Regression. Ziel ist eine Vorhersage. \\(N=600\\), \\(p=4\\). Es handelt sich um eine unüberwachte (unsupervised) Analyse."
  },
  {
    "objectID": "posts/Test-MSE1/Test-MSE1.html#answerlist",
    "href": "posts/Test-MSE1/Test-MSE1.html#answerlist",
    "title": "Test-MSE1",
    "section": "",
    "text": "Es handelt sich um eine Regression. Ziel ist eine Vorhersage. \\(N=600\\), \\(p=4\\).\nEs handelt sich um eine Klassifikation. Ziel ist eine Vorhersage. \\(N=600\\), \\(p=4\\).\nEs handelt sich um eine Regression. Ziel ist eine Erklärung (inference). \\(N=600\\), \\(p=4\\).\nEs handelt sich um eine Regression. Ziel ist eine Vorhersage. \\(N=600\\), \\(p=5\\).\nEs handelt sich um eine Klassifikation. Ziel ist eine Vorhersage. \\(N=600\\), \\(p=5\\).\nEs handelt sich um eine Regression. Ziel ist eine Vorhersage. \\(N=600\\), \\(p=4\\). Es handelt sich um eine unüberwachte (unsupervised) Analyse."
  },
  {
    "objectID": "posts/Test-MSE1/Test-MSE1.html#answerlist-1",
    "href": "posts/Test-MSE1/Test-MSE1.html#answerlist-1",
    "title": "Test-MSE1",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nds1\ntidymodels\nstatlearning\nschoice"
  },
  {
    "objectID": "posts/anim02/anim02.html",
    "href": "posts/anim02/anim02.html",
    "title": "anim02",
    "section": "",
    "text": "Visualisieren Sie in animierter Form den Zusammenhang von Lebenserwartung und Bruttosozialprodukt im Verlauf der Jahre (Datensatz gapminder); der Kontinent soll in der Visualisierung berücksichtigt sein.\nHinweise:\n\nNutzen Sie plotly zur Visualisierung."
  },
  {
    "objectID": "posts/anim02/anim02.html#setup",
    "href": "posts/anim02/anim02.html#setup",
    "title": "anim02",
    "section": "Setup",
    "text": "Setup\n\nlibrary(gapminder)\nlibrary(ggplot2)\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\ndata(gapminder)"
  },
  {
    "objectID": "posts/anim02/anim02.html#statisches-diagramm",
    "href": "posts/anim02/anim02.html#statisches-diagramm",
    "title": "anim02",
    "section": "Statisches Diagramm",
    "text": "Statisches Diagramm\n\np &lt;- gapminder %&gt;% \n  ggplot(aes(x = gdpPercap, y = lifeExp, color = continent, frame = year)) +\n  geom_point()+\n  scale_x_log10()\np"
  },
  {
    "objectID": "posts/anim02/anim02.html#animiertes-und-interaktives-diagramm",
    "href": "posts/anim02/anim02.html#animiertes-und-interaktives-diagramm",
    "title": "anim02",
    "section": "Animiertes (und interaktives) Diagramm",
    "text": "Animiertes (und interaktives) Diagramm\n\nggplotly(p)\n\n\n\n\n\nDieser Post orientiert sich an dieser Quelle; dort finden sich auch mehr Beispiele.\n\nCategories:\n\n2023\nvis\nanimation\nstring"
  },
  {
    "objectID": "posts/max-corr2/max-corr2.html",
    "href": "posts/max-corr2/max-corr2.html",
    "title": "max-corr2",
    "section": "",
    "text": "Aufgabe\nWelches Diagramm zeigt den stärksten (absoluten) linearen negativen Zusammenhang (Korrelation)?\nGeben Sie die Nummer ein, die in der Kopfzeile jedes Teildiagramms angezeigt wird.\n         \n\n\nLösung\n\n\n\n\n\n\n\n\n\n\nCategories:\n\nvis\n‘2023’\nnum"
  },
  {
    "objectID": "posts/kausal07/kausal07.html",
    "href": "posts/kausal07/kausal07.html",
    "title": "kausal07",
    "section": "",
    "text": "Eine Forscherin untersucht den Zusammenhang von Rauchen smo (smoking, UV, exposure) und Herzstillstand ca (cardiac arrest, AV, outcome). Sie hegt die Hypothese, dass Rauchen einen Einfluss auf den Cholesterolspiegel cho (cholestorol) hat, was wiederum Herzstillstand auslösen könnte.\n\n\n\n\n\n\n\n\n\nHier sehen Sie die Definition des DAGs:\n\n\ndag {\nca [outcome]\ncho\nsmo [exposure]\nunh\nwei\ncho -&gt; ca\nsmo -&gt; cho\nunh -&gt; smo\nunh -&gt; wei\nwei -&gt; cho\n}\n\n\nDie Forscherin überlegt, Cholestorol zu kontrollieren. Ist diese Idee sinnvoll?\n\n\n\nNein, da die Assoziation zwischen UV und AV unterbrochen wird.\nJa, so wird der kausale Effekt identifiziert.\nJa, nur so wird der kausale Effekt identifiziert.\nEs schadet nicht, aber es ist auch nicht nötig.\nNein, da eine Kollision erzeugt wird."
  },
  {
    "objectID": "posts/kausal07/kausal07.html#answerlist",
    "href": "posts/kausal07/kausal07.html#answerlist",
    "title": "kausal07",
    "section": "",
    "text": "Nein, da die Assoziation zwischen UV und AV unterbrochen wird.\nJa, so wird der kausale Effekt identifiziert.\nJa, nur so wird der kausale Effekt identifiziert.\nEs schadet nicht, aber es ist auch nicht nötig.\nNein, da eine Kollision erzeugt wird."
  },
  {
    "objectID": "posts/kausal07/kausal07.html#answerlist-1",
    "href": "posts/kausal07/kausal07.html#answerlist-1",
    "title": "kausal07",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/Regr-Bayes-interpret02/Regr-Bayes-interpret02.html",
    "href": "posts/Regr-Bayes-interpret02/Regr-Bayes-interpret02.html",
    "title": "Regr-Bayes-interpret02",
    "section": "",
    "text": "Exercise\nBerechnen Sie das Modell und interpretieren Sie die Ausgabe des folgenden Regressionsmodells. Geben Sie für jeden Regressionskoeffizienten an, wie sein Wert zu verstehen ist!\nmpg ~ hp_z + am + hp_z:am\nHinweise:\n\nFixieren Sie die Zufallszahlen.\nVerwenden Sie Stan zur Berechnung.\nRunden Sie auf 2 Dezimalstellen.\nDas Suffix _z steht für z-standardisierte Variablen.\n\n         \n\n\nSolution\n\nlibrary(tidyverse)  # Datenjudo\nlibrary(rstanarm)  # Stan, komm her\nlibrary(easystats)  # Komfort\n\ndata(mtcars)\n\nZuerst standardisieren wir die Daten:\n\nmtcars2 &lt;-\n  mtcars %&gt;% \n  standardize(append = TRUE)\n\nmtcars2  %&gt;% \n  describe_distribution()\n\nVariable |      Mean |     SD |    IQR |           Range | Skewness | Kurtosis |  n | n_Missing\n-----------------------------------------------------------------------------------------------\nmpg      |     20.09 |   6.03 |   7.53 |  [10.40, 33.90] |     0.67 |    -0.02 | 32 |         0\ncyl      |      6.19 |   1.79 |   4.00 |    [4.00, 8.00] |    -0.19 |    -1.76 | 32 |         0\ndisp     |    230.72 | 123.94 | 221.53 | [71.10, 472.00] |     0.42 |    -1.07 | 32 |         0\nhp       |    146.69 |  68.56 |  84.50 | [52.00, 335.00] |     0.80 |     0.28 | 32 |         0\ndrat     |      3.60 |   0.53 |   0.84 |    [2.76, 4.93] |     0.29 |    -0.45 | 32 |         0\nwt       |      3.22 |   0.98 |   1.19 |    [1.51, 5.42] |     0.47 |     0.42 | 32 |         0\nqsec     |     17.85 |   1.79 |   2.02 |  [14.50, 22.90] |     0.41 |     0.86 | 32 |         0\nvs       |      0.44 |   0.50 |   1.00 |    [0.00, 1.00] |     0.26 |    -2.06 | 32 |         0\nam       |      0.41 |   0.50 |   1.00 |    [0.00, 1.00] |     0.40 |    -1.97 | 32 |         0\ngear     |      3.69 |   0.74 |   1.00 |    [3.00, 5.00] |     0.58 |    -0.90 | 32 |         0\ncarb     |      2.81 |   1.62 |   2.00 |    [1.00, 8.00] |     1.16 |     2.02 | 32 |         0\nmpg_z    |  7.11e-17 |   1.00 |   1.25 |   [-1.61, 2.29] |     0.67 |    -0.02 | 32 |         0\ncyl_z    | -1.47e-17 |   1.00 |   2.24 |   [-1.22, 1.01] |    -0.19 |    -1.76 | 32 |         0\ndisp_z   | -9.08e-17 |   1.00 |   1.79 |   [-1.29, 1.95] |     0.42 |    -1.07 | 32 |         0\nhp_z     |  1.04e-17 |   1.00 |   1.23 |   [-1.38, 2.75] |     0.80 |     0.28 | 32 |         0\ndrat_z   | -2.92e-16 |   1.00 |   1.57 |   [-1.56, 2.49] |     0.29 |    -0.45 | 32 |         0\nwt_z     |  4.68e-17 |   1.00 |   1.21 |   [-1.74, 2.26] |     0.47 |     0.42 | 32 |         0\nqsec_z   |  5.30e-16 |   1.00 |   1.13 |   [-1.87, 2.83] |     0.41 |     0.86 | 32 |         0\nvs_z     |  6.94e-18 |   1.00 |   1.98 |   [-0.87, 1.12] |     0.26 |    -2.06 | 32 |         0\nam_z     |  4.51e-17 |   1.00 |   2.00 |   [-0.81, 1.19] |     0.40 |    -1.97 | 32 |         0\ngear_z   | -3.47e-18 |   1.00 |   1.36 |   [-0.93, 1.78] |     0.58 |    -0.90 | 32 |         0\ncarb_z   |  3.17e-17 |   1.00 |   1.24 |   [-1.12, 3.21] |     1.16 |     2.02 | 32 |         0\n\n\n\nm1 &lt;- \n  stan_glm(mpg ~ hp_z + am + hp_z:am, \n           seed = 42,\n           refresh = 0,\n           data = mtcars2)\n\ncoef(m1)\n\n(Intercept)        hp_z          am     hp_z:am \n17.95232456 -4.07311028  5.26504242  0.06037871 \n\n\n\nIntercept: Ein Auto mit 0 PS und Automatikantrieb (am=0, s. Hilfe zum Datensatz: help(mtcars)) kann laut Modell mit einer Gallone Sprit ca. 17.95 Meilen fahren.\nhp: Pro zusätzlichem PS kann ein Auto mit Automatikantrieb pro Gallone Sprit ca. -4.07 Meilen weniger weit fahren.\nam: Ein Auto mit 0 PS und Schaltgetriebe (am=1) kommt pro Gallone Sprit ca. 5.27 Meilen weiter als ein Auto mit Automatikantrieb.\nhp:am: Der Interaktionseffekt ist praktisch Null (17.95): Der Zusammenhang von PS-Zahl und Spritverbrauch unterscheidet sich nicht (wesentlich) zwischen Autos mit bzw. ohne Automatikantrieb.\n\n\nCategories:\n\nbayes\nregression"
  },
  {
    "objectID": "posts/tidymodels-tree2/tidymodels-tree2.html",
    "href": "posts/tidymodels-tree2/tidymodels-tree2.html",
    "title": "tidymodels-tree2",
    "section": "",
    "text": "Berechnen Sie folgendes einfache Modell:\n\nEntscheidungsbaum\n\nModellformel: am ~ . (Datensatz mtcars)\nHier geht es darum, die Geschwindigkeit (und den Ressourcenverbrauch) beim Fitten zu verringern. Benutzen Sie dazu folgende Methoden\n\nVerwenden mehrerer Prozesskerne\n\nHinweise:\n\nTunen Sie alle Parameter (die der Engine anbietet).\nVerwenden Sie Defaults, wo nicht anders angegeben.\nFühren Sie eine \\(v=2\\)-fache Kreuzvalidierung durch (weil die Stichprobe so klein ist).\nBeachten Sie die üblichen Hinweise."
  },
  {
    "objectID": "posts/tidymodels-tree2/tidymodels-tree2.html#setup",
    "href": "posts/tidymodels-tree2/tidymodels-tree2.html#setup",
    "title": "tidymodels-tree2",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.0.8\n✔ dials        1.2.0     ✔ rsample      1.2.0\n✔ dplyr        1.1.3     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\ndata(mtcars)\nlibrary(tictoc)  # Zeitmessung\nlibrary(doParallel)  # Nutzen mehrerer Kerne\n\nLoading required package: foreach\n\n\n\nAttaching package: 'foreach'\n\n\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n\n\nLoading required package: iterators\n\n\nLoading required package: parallel\n\n\nFür Klassifikation verlangt Tidymodels eine nominale AV, keine numerische:\n\nmtcars &lt;-\n  mtcars %&gt;% \n  mutate(am = factor(am))"
  },
  {
    "objectID": "posts/tidymodels-tree2/tidymodels-tree2.html#daten-teilen",
    "href": "posts/tidymodels-tree2/tidymodels-tree2.html#daten-teilen",
    "title": "tidymodels-tree2",
    "section": "Daten teilen",
    "text": "Daten teilen\n\nset.seed(42)\nd_split &lt;- initial_split(mtcars)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)"
  },
  {
    "objectID": "posts/tidymodels-tree2/tidymodels-tree2.html#modelle",
    "href": "posts/tidymodels-tree2/tidymodels-tree2.html#modelle",
    "title": "tidymodels-tree2",
    "section": "Modell(e)",
    "text": "Modell(e)\n\nmod_tree &lt;-\n  decision_tree(mode = \"classification\",\n                cost_complexity = tune(),\n                tree_depth = tune(),\n                min_n = tune())"
  },
  {
    "objectID": "posts/tidymodels-tree2/tidymodels-tree2.html#rezepte",
    "href": "posts/tidymodels-tree2/tidymodels-tree2.html#rezepte",
    "title": "tidymodels-tree2",
    "section": "Rezept(e)",
    "text": "Rezept(e)\n\nrec_plain &lt;- \n  recipe(am ~ ., data = d_train)"
  },
  {
    "objectID": "posts/tidymodels-tree2/tidymodels-tree2.html#resampling",
    "href": "posts/tidymodels-tree2/tidymodels-tree2.html#resampling",
    "title": "tidymodels-tree2",
    "section": "Resampling",
    "text": "Resampling\n\nset.seed(42)\nrsmpl &lt;- vfold_cv(d_train, v = 2)"
  },
  {
    "objectID": "posts/tidymodels-tree2/tidymodels-tree2.html#workflows",
    "href": "posts/tidymodels-tree2/tidymodels-tree2.html#workflows",
    "title": "tidymodels-tree2",
    "section": "Workflows",
    "text": "Workflows\n\nwf_tree &lt;-\n  workflow() %&gt;%  \n  add_recipe(rec_plain) %&gt;% \n  add_model(mod_tree)"
  },
  {
    "objectID": "posts/tidymodels-tree2/tidymodels-tree2.html#tuningfitting",
    "href": "posts/tidymodels-tree2/tidymodels-tree2.html#tuningfitting",
    "title": "tidymodels-tree2",
    "section": "Tuning/Fitting",
    "text": "Tuning/Fitting\nTuninggrid:\n\ntune_grid &lt;- grid_regular(extract_parameter_set_dials(mod_tree), levels = 5)\ntune_grid\n\n# A tibble: 125 × 3\n   cost_complexity tree_depth min_n\n             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt;\n 1    0.0000000001          1     2\n 2    0.0000000178          1     2\n 3    0.00000316            1     2\n 4    0.000562              1     2\n 5    0.1                   1     2\n 6    0.0000000001          4     2\n 7    0.0000000178          4     2\n 8    0.00000316            4     2\n 9    0.000562              4     2\n10    0.1                   4     2\n# ℹ 115 more rows"
  },
  {
    "objectID": "posts/tidymodels-tree2/tidymodels-tree2.html#ohne-parallelisierung",
    "href": "posts/tidymodels-tree2/tidymodels-tree2.html#ohne-parallelisierung",
    "title": "tidymodels-tree2",
    "section": "Ohne Parallelisierung",
    "text": "Ohne Parallelisierung\n\ntic()\nfit_tree &lt;-\n  tune_grid(object = wf_tree,\n            grid = tune_grid,\n            metrics = metric_set(roc_auc),\n            resamples = rsmpl)\n\n→ A | warning: 21 samples were requested but there were 12 rows in the data. 12 will be used.\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x3\n\n\n→ B | warning: 30 samples were requested but there were 12 rows in the data. 12 will be used.\n\n\nThere were issues with some computations   A: x3\nThere were issues with some computations   A: x25   B: x7\n→ C | warning: 40 samples were requested but there were 12 rows in the data. 12 will be used.\nThere were issues with some computations   A: x25   B: x7\nThere were issues with some computations   A: x25   B: x25   C: x10\nThere were issues with some computations   A: x26   B: x25   C: x25\nThere were issues with some computations   A: x35   B: x25   C: x25\nThere were issues with some computations   A: x50   B: x38   C: x25\nThere were issues with some computations   A: x50   B: x50   C: x39\nThere were issues with some computations   A: x50   B: x50   C: x50\n\ntoc()\n\n23.317 sec elapsed\n\n\nca. 45 sec. auf meinem Rechner (4-Kerne-MacBook Pro 2020)."
  },
  {
    "objectID": "posts/tidymodels-tree2/tidymodels-tree2.html#mit-parallelisierung",
    "href": "posts/tidymodels-tree2/tidymodels-tree2.html#mit-parallelisierung",
    "title": "tidymodels-tree2",
    "section": "Mit Parallelisierung",
    "text": "Mit Parallelisierung\nWie viele CPUs hat mein Computer?\n\nparallel::detectCores(logical = FALSE)\n\n[1] 4\n\n\nParallele Verarbeitung starten:\n\ncl &lt;- makePSOCKcluster(4)  # Create 4 clusters\nregisterDoParallel(cl)\n\n\ntic()\nfit_tree2 &lt;-\n  tune_grid(object = wf_tree,\n            grid = tune_grid,\n            metrics = metric_set(roc_auc),\n            resamples = rsmpl)\ntoc()\n\n12.936 sec elapsed\n\n\nca. 17 Sekunden - deutlich schneller!\n\nCategories:\n\nstatlearning\ntrees\ntidymodels\nspeed\nstring"
  },
  {
    "objectID": "posts/iq09/iq09.html",
    "href": "posts/iq09/iq09.html",
    "title": "iq09",
    "section": "",
    "text": "Aufgabe\nZwei Forscherinnen, Prof. Weiss-Ois und Prof. Blitz-Chegga, streiten sich über den Effekt von Cannabis auf die Intelligenz.\nDazu untersuchen Sie die Intelligenz langjähriger Konsumentis.\nProf. Weiss-Ois hat (apriori) folgende Hypothese: \\(IQ \\sim N(90, 10)\\). Prof. Blitz-Chegga hat (apriori) folgende Hypothese: \\(IQ \\sim N(95, 5)\\).\nMit großer Spannung wurden die Messdaten zur Intelligenz erwartet (die erst nach langem Streit über die zu verwendenden Intelligenztests erhoben werden konnten). Insgesamt wurden \\(N=541\\) Personen untersucht.\nTatsächlich sei die wahre IQ-Verteilung jener Cannabis-Konsumentis wie folgt: \\(IQ \\sim N(92.5, 7.5)\\). Natürlich kennen die Forschis diese Verteilung nicht.\nWessen Hypothese unterstützen die Daten mehr? Die von Prof. Weiss-Ois oder von Prof. Blitz-Chegga?\nHinweise:\n\nNutzen Sie Simulationsmethoden.\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\nSimulieren Sie \\(n=10^4\\) Stichproben.\nNutzen Sie die Zahl 42 als Startwert für Ihre Zufallszahlen (um die Reproduzierbarkeit zu gewährleisten).\n\n         \n\n\nLösung\n\nlibrary(tidyverse)\n\n\nCategories:\n\nprobability\nsimulation\nnormal-distribution\nnum"
  },
  {
    "objectID": "posts/tidymodels-tree5/tidymodels-tree5.html",
    "href": "posts/tidymodels-tree5/tidymodels-tree5.html",
    "title": "tidymodels-tree5",
    "section": "",
    "text": "Berechnen Sie folgendes einfache Modell:\n\nRandom Forest mit trees=50\n\nModellformel: body_mass_g ~ . (Datensatz palmerpenguins::penguins)\nHier geht es darum, die Geschwindigkeit (und den Ressourcenverbrauch) beim Fitten zu verringern. Benutzen Sie dazu folgende Methoden\n\nAuslassen gering performanter Tuningparameterwerte\nVerwenden Sie ein Anova-Grid-Search!\nParallelisieren Sie auf mehrere Kerne (wenn möglich).\n\nHinweise:\n\nTunen Sie alle Parameter (die der Engine anbietet).\nVerwenden Sie Defaults, wo nicht anders angegeben.\nBeachten Sie die üblichen Hinweise."
  },
  {
    "objectID": "posts/tidymodels-tree5/tidymodels-tree5.html#setup",
    "href": "posts/tidymodels-tree5/tidymodels-tree5.html#setup",
    "title": "tidymodels-tree5",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.0.8\n✔ dials        1.2.0     ✔ rsample      1.2.0\n✔ dplyr        1.1.3     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\ndata(\"penguins\", package = \"palmerpenguins\")\nlibrary(tictoc)  # Zeitmessung\nlibrary(finetune)  # tune_race_anova\nlibrary(doParallel)  # mehrere CPUs nutzen \n\nLoading required package: foreach\n\n\n\nAttaching package: 'foreach'\n\n\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n\n\nLoading required package: iterators\n\n\nLoading required package: parallel\n\nset.seed(42)\n\nEntfernen wir Fälle mit fehlenden Werten:\n\nd &lt;-\n  penguins %&gt;% \n  drop_na()"
  },
  {
    "objectID": "posts/tidymodels-tree5/tidymodels-tree5.html#daten-teilen",
    "href": "posts/tidymodels-tree5/tidymodels-tree5.html#daten-teilen",
    "title": "tidymodels-tree5",
    "section": "Daten teilen",
    "text": "Daten teilen\n\nset.seed(42)\nd_split &lt;- initial_split(d)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)"
  },
  {
    "objectID": "posts/tidymodels-tree5/tidymodels-tree5.html#modelle",
    "href": "posts/tidymodels-tree5/tidymodels-tree5.html#modelle",
    "title": "tidymodels-tree5",
    "section": "Modell(e)",
    "text": "Modell(e)\n\nmod_rf &lt;-\n  rand_forest(mode = \"regression\",\n              mtry = tune())\nmod_rf\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n\nComputational engine: ranger"
  },
  {
    "objectID": "posts/tidymodels-tree5/tidymodels-tree5.html#rezepte",
    "href": "posts/tidymodels-tree5/tidymodels-tree5.html#rezepte",
    "title": "tidymodels-tree5",
    "section": "Rezept(e)",
    "text": "Rezept(e)\n\nrec_plain &lt;- \n  recipe(body_mass_g ~ ., data = d_train) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_impute_knn(all_predictors())\n\n\nd_train_baked &lt;-\n  bake(prep(rec_plain, d_train), new_data = NULL)\n\nhead(d_train_baked)\n\n# A tibble: 6 × 10\n  bill_length_mm bill_depth_mm flipper_length_mm  year body_mass_g\n           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt; &lt;int&gt;       &lt;int&gt;\n1           34.5          18.1               187  2008        2900\n2           52.2          18.8               197  2009        3450\n3           45.4          14.6               211  2007        4800\n4           42.1          19.1               195  2008        4000\n5           50            15.9               224  2009        5350\n6           41.5          18.5               201  2009        4000\n# ℹ 5 more variables: species_Chinstrap &lt;dbl&gt;, species_Gentoo &lt;dbl&gt;,\n#   island_Dream &lt;dbl&gt;, island_Torgersen &lt;dbl&gt;, sex_male &lt;dbl&gt;\n\n\nKeine fehlenden Werte mehr?\n\nsum(is.na(d_train_baked))\n\n[1] 0"
  },
  {
    "objectID": "posts/tidymodels-tree5/tidymodels-tree5.html#resampling",
    "href": "posts/tidymodels-tree5/tidymodels-tree5.html#resampling",
    "title": "tidymodels-tree5",
    "section": "Resampling",
    "text": "Resampling\n\nset.seed(42)\nrsmpl &lt;- vfold_cv(d_train)"
  },
  {
    "objectID": "posts/tidymodels-tree5/tidymodels-tree5.html#workflows",
    "href": "posts/tidymodels-tree5/tidymodels-tree5.html#workflows",
    "title": "tidymodels-tree5",
    "section": "Workflows",
    "text": "Workflows\n\nwf_rf &lt;-\n  workflow() %&gt;%  \n  add_recipe(rec_plain) %&gt;% \n  add_model(mod_rf)\n\nwf_rf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_dummy()\n• step_impute_knn()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n\nComputational engine: ranger"
  },
  {
    "objectID": "posts/tidymodels-tree5/tidymodels-tree5.html#ohne-speed-up",
    "href": "posts/tidymodels-tree5/tidymodels-tree5.html#ohne-speed-up",
    "title": "tidymodels-tree5",
    "section": "Ohne Speed-up",
    "text": "Ohne Speed-up\n\ntic()\nfit_rf &lt;-\n  tune_grid(\n    object = wf_rf,\n    resamples = rsmpl)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\ntoc()\n\n13.061 sec elapsed\n\n\nDie angegebene Rechenzeit bezieht sich auf einen 4-Kerne-MacBook Pro (2020)."
  },
  {
    "objectID": "posts/tidymodels-tree5/tidymodels-tree5.html#mit-speeed-up-1",
    "href": "posts/tidymodels-tree5/tidymodels-tree5.html#mit-speeed-up-1",
    "title": "tidymodels-tree5",
    "section": "Mit Speeed-up 1",
    "text": "Mit Speeed-up 1\n\ntic()\nfit_rf2 &lt;-\n  tune_race_anova(\n    object = wf_rf,\n    resamples = rsmpl)\ntoc()\n\n17.925 sec elapsed"
  },
  {
    "objectID": "posts/tidymodels-tree5/tidymodels-tree5.html#mit-speeed-up-2",
    "href": "posts/tidymodels-tree5/tidymodels-tree5.html#mit-speeed-up-2",
    "title": "tidymodels-tree5",
    "section": "Mit Speeed-up 2",
    "text": "Mit Speeed-up 2\n\ndoParallel::registerDoParallel()\n\ntic()\nfit_tree2 &lt;-\n  tune_race_anova(\n    object = wf_rf,\n    metrics = metric_set(rmse),\n    control = control_race(verbose = FALSE,\n                           pkgs = c(\"tidymodels\"),\n                           save_pred = TRUE),\n            resamples = rsmpl)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\ntoc()\n\n12.339 sec elapsed"
  },
  {
    "objectID": "posts/tidymodels-tree5/tidymodels-tree5.html#mit-speeed-up-3",
    "href": "posts/tidymodels-tree5/tidymodels-tree5.html#mit-speeed-up-3",
    "title": "tidymodels-tree5",
    "section": "Mit Speeed-up 3",
    "text": "Mit Speeed-up 3\n\ndoParallel::registerDoParallel()\n\ntic()\nfit_tree2 &lt;-\n  tune_grid(object = wf_rf,\n            metrics = metric_set(rmse),\n            control = control_grid(verbose = FALSE,\n                                   save_pred = TRUE),\n            resamples = rsmpl)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\ntoc()\n\n4.53 sec elapsed"
  },
  {
    "objectID": "posts/tidymodels-tree5/tidymodels-tree5.html#fazit",
    "href": "posts/tidymodels-tree5/tidymodels-tree5.html#fazit",
    "title": "tidymodels-tree5",
    "section": "Fazit",
    "text": "Fazit\nMit Speed-up ist schneller also ohne. Ein Random-Forest ist ein Modelltyp, der von Parallelisierung gut profitiert.\n\nCategories:\n\nstatlearning\ntrees\ntidymodels\nspeed\nstring"
  },
  {
    "objectID": "posts/mse/mse.html",
    "href": "posts/mse/mse.html",
    "title": "mse",
    "section": "",
    "text": "Wie ist der MSE definiert? Entscheiden Sie sich für eine der folgenden Definitionen:\n\n\n\n\\(MSE= \\frac{1}{n}\\sum^n_{i=1}(y_i -\\hat{f}(x_i))^2\\)\n\\(MSE= \\frac{1}{n}\\sum^n_{i=1}(y_i -\\hat{f}(x_i))\\)\n\\(MSE= \\frac{1}{n}\\sum^n_{i=1}(|y_i -\\hat{f}(x_i)|)\\)\n\\(MSE= \\sum^n_{i=1}(y_i -\\hat{f}(x_i))^2\\)\n\\(MSE= \\frac{1}{n}\\sum^n_{i=1}(y_i + \\hat{f}(x_i))^2\\)"
  },
  {
    "objectID": "posts/mse/mse.html#answerlist",
    "href": "posts/mse/mse.html#answerlist",
    "title": "mse",
    "section": "",
    "text": "\\(MSE= \\frac{1}{n}\\sum^n_{i=1}(y_i -\\hat{f}(x_i))^2\\)\n\\(MSE= \\frac{1}{n}\\sum^n_{i=1}(y_i -\\hat{f}(x_i))\\)\n\\(MSE= \\frac{1}{n}\\sum^n_{i=1}(|y_i -\\hat{f}(x_i)|)\\)\n\\(MSE= \\sum^n_{i=1}(y_i -\\hat{f}(x_i))^2\\)\n\\(MSE= \\frac{1}{n}\\sum^n_{i=1}(y_i + \\hat{f}(x_i))^2\\)"
  },
  {
    "objectID": "posts/mse/mse.html#answerlist-1",
    "href": "posts/mse/mse.html#answerlist-1",
    "title": "mse",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nds1\ntidymodels\nstatlearning\nschoice"
  },
  {
    "objectID": "posts/regression1b/regression1b.html",
    "href": "posts/regression1b/regression1b.html",
    "title": "regression1b",
    "section": "",
    "text": "Die folgende Frage bezieht sich auf dieses Ergebnis einer Regressionsanalyse:\n\n\n\nCall:\nlm(formula = y ~ x, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.06052 -0.31526 -0.02412  0.28884  1.05826 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.08282    0.04901   -1.69   0.0947 .  \nx            0.66228    0.04540   14.59   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4559 on 85 degrees of freedom\nMultiple R-squared:  0.7146,    Adjusted R-squared:  0.7112 \nF-statistic: 212.8 on 1 and 85 DF,  p-value: &lt; 2.2e-16\n\n\nWelche der folgenden Aussagen ist korrekt?\n\n\n\nWenn x=2, dann ist ein Mittelwert von y in Höhe von ca. 1.24 zu erwarten.\nDer Mittelwert der abhängigen Variaben y sinkt mit zunehmenden x.\nWenn x um 1 Einheit steigt, dann kann eine Veränderung um etwa -0.08 Einheiten in y erwartet werden.\nWenn x=1, dann ist ein Mittelwert von y in Höhe von ca. -0.08 zu erwarten.\nDas (nicht-adjustierte) \\(R^2\\) liegt im Modell bei 0.66."
  },
  {
    "objectID": "posts/regression1b/regression1b.html#answerlist",
    "href": "posts/regression1b/regression1b.html#answerlist",
    "title": "regression1b",
    "section": "",
    "text": "Wenn x=2, dann ist ein Mittelwert von y in Höhe von ca. 1.24 zu erwarten.\nDer Mittelwert der abhängigen Variaben y sinkt mit zunehmenden x.\nWenn x um 1 Einheit steigt, dann kann eine Veränderung um etwa -0.08 Einheiten in y erwartet werden.\nWenn x=1, dann ist ein Mittelwert von y in Höhe von ca. -0.08 zu erwarten.\nDas (nicht-adjustierte) \\(R^2\\) liegt im Modell bei 0.66."
  },
  {
    "objectID": "posts/regression1b/regression1b.html#answerlist-1",
    "href": "posts/regression1b/regression1b.html#answerlist-1",
    "title": "regression1b",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nregression\nR\nlm\nschoice"
  },
  {
    "objectID": "posts/mariokart-max1/mariokart-max1.html",
    "href": "posts/mariokart-max1/mariokart-max1.html",
    "title": "mariokart-max1",
    "section": "",
    "text": "Aufgabe\nImportieren Sie den Datensatz mariokart in R. Berechnen Sie die maximale Verkaufspreise (total_pr) für Spiele, die mit 0, 1, 2, … Lenkräder (wheels) gekauft werden. Bilden Sie davon den Mittelwert und geben Sie diesen an.\nHinweise:\n\nRunden Sie auf 1 Dezimalstelle.\n\n         \n\n\nLösung\nPakete starten:\n\nlibrary(tidyverse)\nlibrary(easystats)\n\nDaten importieren:\n\nd_url &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\"\nd &lt;- data_read(d_url)\n\n\nsolution &lt;-\nd  %&gt;% \n  group_by(wheels) %&gt;% \n  summarise(pr_max = max(total_pr)) %&gt;% \n  summarise(pr_max_mean = mean(pr_max))\n\nsolution\n\n# A tibble: 1 × 1\n  pr_max_mean\n        &lt;dbl&gt;\n1        128.\n\n\nLösung: 127.95.\n\nCategories:\n\ndatawrangling\ndplyr\neda\nnum"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-01/Verteilungen-Quiz-01.html",
    "href": "posts/Verteilungen-Quiz-01/Verteilungen-Quiz-01.html",
    "title": "Verteilungen-Quiz-01",
    "section": "",
    "text": "Beziehen Sie sich auf den Standard-Globusversuch mit \\(N=9\\) Würfen und \\(W=6\\) Wassertreffern (binomialverteilt), vgl. hier.\nDie Stichproben-Postverteilung sieht so aus:\n\n\n\n\n\n\n\n\n\nIst sich das Modell auf Basis dieser Post-Verteilung sicher sein, dass der Wasseranteil \\(\\pi=.7\\) beträgt?\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-01/Verteilungen-Quiz-01.html#answerlist",
    "href": "posts/Verteilungen-Quiz-01/Verteilungen-Quiz-01.html#answerlist",
    "title": "Verteilungen-Quiz-01",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-01/Verteilungen-Quiz-01.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-01/Verteilungen-Quiz-01.html#answerlist-1",
    "title": "Verteilungen-Quiz-01",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/scikit-llm-zeroshot/scikit-llm-zeroshot.html",
    "href": "posts/scikit-llm-zeroshot/scikit-llm-zeroshot.html",
    "title": "Scikit-Learn-LLM Zero Shot Learners",
    "section": "",
    "text": "Aufgabe\nFragen Sie ChatGPT via Scikit-Learn-LLM zum Sentiment der ersten 7 Texte (=Tweets) aus dem Germeval-2018-Datensatz (Test). Nutzen Sie die gleiche Zahl an Tweets aus dem Train-Datensatz zum Finetuning Ihres Modells. Nutzen Sie den Endpoint ZeroShotGPTClassifier.\nHinweise:\n\nBeachten Sie die Standardhinweise des Datenwerks.\nNutzen Sie Python, nicht R.\nDas Verwenden der OpenAI-API kostet Geld. 💸 Informieren Sie sich vorab. Um auf die API zugreifen zu können, müssen Sie sich ein Konto angelegt haben und über ein Guthaben verfügen. Werfen Sie hin und wieder einen Blick auf Ihr OpenAI-Guthaben-Konto.\n\n\n\n\n\n\n\nCaution\n\n\n\nAktuell sind scikit-llm und openai in den aktuellsten Versionen inkompatibel.\n\nERROR: pip’s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. scikit-llm 0.4.2 requires openai&lt;1.0,&gt;=0.27.9, but you have openai 1.3.5 which is incompatible.\n\nDie einfachste Lösung ist, beide Pakete in verschiedenen venvs zu lagern. \\(\\square\\)\n\n\n\n\n\nsci-llm\n\n\n         \n\n\nLösung\nWir legen ggf. eine neue venv an:\n\nlibrary(reticulate)\n\n\n#virtualenv_create(\"scikit-llm\")\n\nUnd nutzen diese:\n\nuse_virtualenv(\"scikit-llm\")\n\nCheck:\n\npy_config()\n\npython:         /Users/sebastiansaueruser/.virtualenvs/scikit-llm/bin/python\nlibpython:      /Users/sebastiansaueruser/.pyenv/versions/3.11.1/lib/libpython3.11.dylib\npythonhome:     /Users/sebastiansaueruser/.virtualenvs/scikit-llm:/Users/sebastiansaueruser/.virtualenvs/scikit-llm\nversion:        3.11.1 (main, Oct  4 2023, 18:12:06) [Clang 15.0.0 (clang-1500.0.40.1)]\nnumpy:          /Users/sebastiansaueruser/.virtualenvs/scikit-llm/lib/python3.11/site-packages/numpy\nnumpy_version:  1.26.2\n\nNOTE: Python version was forced by use_python() function\n\n\n\npy_list_packages() |&gt; head()\n\n          package version            requirement\n1         absl-py   1.4.0         absl-py==1.4.0\n2         aiohttp   3.9.0         aiohttp==3.9.0\n3       aiosignal   1.3.1       aiosignal==1.3.1\n4 annotated-types   0.6.0 annotated-types==0.6.0\n5           anyio   3.7.1           anyio==3.7.1\n6    array-record   0.4.0    array-record==0.4.0\n\n\nGgf. müssen Sie zunächst die nötigen Module installieren, z.B. so: reticulate::py_install(\"scikit-llm\").\n\n#py_install(\"scikit-llm\")\n\nModule importieren:\n\nfrom skllm import ZeroShotGPTClassifier   \nfrom skllm.config import SKLLMConfig  # Anmeldung\nimport pandas as pd\nimport time \nimport os\n\nTrain-Daten importieren:\n\ncsv_file_path_train = 'https://github.com/sebastiansauer/pradadata/raw/master/data-raw/germeval_train.csv'\ngermeval_train = pd.read_csv(csv_file_path_train)\n\nTest-Daten importieren:\n\ncsv_file_path_test = 'https://github.com/sebastiansauer/pradadata/raw/master/data-raw/germeval_test.csv'\ngermeval_test = pd.read_csv(csv_file_path_test)\n\nDie ersten paar Texte aus dem Train-Datensatz herausziehen:\n\nn_tweets = 7\nX_train = germeval_train[\"text\"].head(n_tweets).tolist()\nX_train\n\n['@corinnamilborn Liebe Corinna, wir würden dich gerne als Moderatorin für uns gewinnen! Wärst du begeisterbar?', '@Martin28a Sie haben ja auch Recht. Unser Tweet war etwas missverständlich. Dass das BVerfG Sachleistungen nicht ausschließt, kritisieren wir.', '@ahrens_theo fröhlicher gruß aus der schönsten stadt der welt theo ⚓️', '@dushanwegner Amis hätten alles und jeden gewählt...nur Hillary wollten sie nicht und eine Fortsetzung von Obama-Politik erst recht nicht..!', '@spdde kein verläßlicher Verhandlungspartner. Nachkarteln nach den Sondierzngsgesprächen - schickt diese Stümper #SPD in die Versenkung.', '@Dirki_M Ja, aber wo widersprechen die Zahlen denn denen, die im von uns verlinkten Artikel stehen? In unserem Tweet geht es rein um subs. Geschützte. 2017 ist der gesamte Familiennachzug im Vergleich zu 2016 - die Zahlen, die Hr. Brandner bemüht - übrigens leicht rückläufig gewesen.', '@milenahanm 33 bis 45 habe ich noch gar nicht gelebt und es geht mir am Arsch vorbei was in dieser Zeit geschehen ist. Ich lebe im heute und jetzt und nicht in der Vergangenheit.']\n\n\nUnd hier sind die Labels dazu:\n\ny_train = germeval_train[\"c1\"].head(n_tweets).tolist()\ny_train\n\n['OTHER', 'OTHER', 'OTHER', 'OTHER', 'OFFENSE', 'OTHER', 'OFFENSE']\n\n\nUnd analog für den Test-Datensatz:\n\nX_test = germeval_test[\"text\"].head(n_tweets).tolist()\n\nAnmelden bei OpenAI:\n\nOPENAI_SECRET_KEY = os.environ.get(\"OPENAI_API_KEY\")\nOPENAI_ORG_ID = os.environ.get(\"OPENAI_ORG_ID\")\n\nSKLLMConfig.set_openai_key(OPENAI_SECRET_KEY)\nSKLLMConfig.set_openai_org(OPENAI_ORG_ID)\n\nModel definieren:\n\nclf = ZeroShotGPTClassifier(openai_model=\"gpt-3.5-turbo\")\n\nModel fitten:\n\nclf.fit(X = X_train, y = y_train)  \n\nZeroShotGPTClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ZeroShotGPTClassifierZeroShotGPTClassifier()\n\n\nVorhersagen:\n\ny_pred = clf.predict(X = X_test)  \n\n  0%|          | 0/7 [00:00&lt;?, ?it/s]\n 14%|█▍        | 1/7 [00:38&lt;03:48, 38.03s/it]\n 29%|██▊       | 2/7 [00:48&lt;01:48, 21.62s/it]\n 43%|████▎     | 3/7 [01:16&lt;01:38, 24.65s/it]\n 57%|█████▋    | 4/7 [01:18&lt;00:47, 15.81s/it]\n 71%|███████▏  | 5/7 [01:28&lt;00:27, 13.69s/it]\n 86%|████████▌ | 6/7 [01:31&lt;00:09,  9.87s/it]\n100%|██████████| 7/7 [01:33&lt;00:00, 13.32s/it]\nVoilà:\n\nfor tweet, sentiment in zip(X_test, y_pred):\n    print(f\"Review: {tweet}\\nPredicted Sentiment: {sentiment}\\n\\n\")\n\nReview: Meine Mutter hat mir erzählt, dass mein Vater einen Wahlkreiskandidaten nicht gewählt hat, weil der gegen die Homo-Ehe ist ☺\nPredicted Sentiment: OFFENSE\n\n\nReview: @Tom174_ @davidbest95 Meine Reaktion; |LBR| Nicht jeder Moslem ist ein Terrorist. Aber jeder Moslem glaubt an Überlieferungen, die Gewalt und Terror begünstigen.\nPredicted Sentiment: OFFENSE\n\n\nReview: #Merkel rollt dem Emir von #Katar, der islamistischen Terror unterstützt, den roten Teppich aus.Wir brauchen einen sofortigen #Waffenstopp!\nPredicted Sentiment: OFFENSE\n\n\nReview: „Merle ist kein junges unschuldiges Mädchen“ Kch....... 😱 #tatort\nPredicted Sentiment: OFFENSE\n\n\nReview: @umweltundaktiv Asylantenflut bringt eben nur negatives für Deutschland. Drum Asylanenstop und Rückführung der Mehrzahl.\nPredicted Sentiment: OFFENSE\n\n\nReview: @_StultaMundi Die Bibel enthält ebenfalls Gesetze des Zivil- und Strafrechts.\nPredicted Sentiment: OTHER\n\n\nReview: @Thueringen_ @Miquwarchar @Pontifex_de Man munkelt, Franziskus ist großer \"Kiss\"- und \"Black Sabbath\"-Fan! #RockOn\nPredicted Sentiment: OTHER"
  },
  {
    "objectID": "posts/Lose-Nieten-Binomial-Grid/Lose-Nieten-Binomial-Grid.html",
    "href": "posts/Lose-Nieten-Binomial-Grid/Lose-Nieten-Binomial-Grid.html",
    "title": "Lose-Nieten-Binomial-Grid",
    "section": "",
    "text": "Exercise\nIn einer Lostrommel befinden sich “sehr viele” Lose, davon ein Anteil \\(p\\) Treffer (und \\(1-p\\) Nieten), mit zunächst \\(p=0.01\\).\nSie kaufen \\(n=10\\) Lose.\n\nWie groß ist die Wahrscheinlichkeit für genau \\(k=0,1,...,10\\) Treffer?\nSagen wir, Sie haben 3 Treffer in den 10 Losen. Yeah! Jetzt sei \\(p\\) unbekannt und Sie sind indifferent zu den einzelnen Werten von \\(p\\). Visualisieren Sie die Posteriori-Wahrscheinlichkeitsverteilung mit ca. 100 Gridwerten. Was beobachten Sie?\nVariieren Sie \\(n\\), aber halten Sie die Trefferquote bei 1/3. Was beobachten Sie?\n\nNutzen Sie die Gittermethode. Treffen Sie Annahmen, wo nötig.\n         \n\n\nSolution\n\nWie groß ist die Wahrscheinlichkeit für genau \\(k=0,1,...,10\\) Treffer?\n\n\nd_a &lt;- \n  tibble(\n    k = 0:10,\n    wskt = dbinom(k, size = 10, prob = .01))\n\nd_a %&gt;% \n  ggplot() +\n  aes(x = k, y = wskt) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = 1:10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nk\nwskt\n\n\n\n\n0\n9.04 × 10−1\n\n\n1\n9.14 × 10−2\n\n\n2\n4.15 × 10−3\n\n\n3\n1.12 × 10−4\n\n\n4\n1.98 × 10−6\n\n\n5\n2.40 × 10−8\n\n\n6\n2.02 × 10−10\n\n\n7\n1.16 × 10−12\n\n\n8\n4.41 × 10−15\n\n\n9\n9.90 × 10−18\n\n\n10\n1.00 × 10−20\n\n\n\n\n\n\n\n\nSagen wir, Sie haben 3 Treffer in den 10 Losen. Yeah! Jetzt sei \\(p\\) unbekannt und Sie sind indifferent zu den einzelnen Werten von \\(p\\). Visualisieren Sie die Posteriori-Wahrscheinlichkeitsverteilung mit ca. 100 Gridwerten. Was beobachten Sie?\n\n\nd2 &lt;-\n  tibble(\n    p_grid = seq(0, 1, by = 0.01),\n    prior = 1,\n    Likelihood = dbinom(x = 3, size = 10, prob = p_grid),\n    unstand_post = prior * Likelihood,\n    std_post = unstand_post / sum(unstand_post)\n  )\n\nd2 %&gt;% \n  ggplot() +\n  aes(x = p_grid, y = std_post) +\n  geom_point() +\n  geom_line()\n\n\n\n\n\n\n\n\nDer Modus liegt bei ca 1/3. Der Bereich plausibler Werte für \\(p\\) liegt ca. zwischen 0.1 und und 0.7, grob visuell geschätzt. Mehr dazu später.\n\nVariieren Sie \\(n\\), aber halten Sie die Trefferquote bei 1/3. Was beobachten Sie?\n\n\n# n = 2\nd3 &lt;-\n  tibble(\n    p_grid = seq(0,1, by = 0.01),\n    prior = 1,\n    Likelihood = dbinom(x = 2, size = 6, prob = p_grid),\n    unstand_post = prior * Likelihood,\n    std_post = unstand_post / sum(unstand_post)\n  )\n\nd3 %&gt;% \n  ggplot() +\n  aes(x = p_grid, y = std_post) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"n=20\")\n\n\n# n = 20\nd4 &lt;-\n  tibble(\n    p_grid = seq(0,1, by = 0.01),\n    prior = 1,\n    Likelihood = dbinom(x = 20, size = 60, prob = p_grid),\n    unstand_post = prior * Likelihood,\n    std_post = unstand_post / sum(unstand_post)\n  )\n\nd4 %&gt;% \n  ggplot() +\n  aes(x = p_grid, y = std_post) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"n = 20\")\n\n# n = 200\nd5 &lt;-\n  tibble(\n    p_grid = seq(0,1, by = 0.01),\n    prior = 1,\n    Likelihood = dbinom(x = 200, size = 600, prob = p_grid),\n    unstand_post = prior * Likelihood,\n    std_post = unstand_post / sum(unstand_post)\n  )\n\nd5 %&gt;% \n  ggplot() +\n  aes(x = p_grid, y = std_post) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"n = 20\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDer Modus und andere Maße der zentralen Tendenz bleiben gleich; die Streuung wird geringer.\n\nCategories:\n\nprobability\nbinomial"
  },
  {
    "objectID": "posts/Gem-Wskt1/Gem-Wskt1.html",
    "href": "posts/Gem-Wskt1/Gem-Wskt1.html",
    "title": "Gem-Wskt1",
    "section": "",
    "text": "Exercise\nProf. Bitter untersucht eine seiner Lieblingsfragen: Wie viel bringt das Lernen auf eine Klausur? Dabei konzentriert er sich auf das Fach Statistik (es gefällt ihm gut). In einer aktuellen Untersuchung hat er \\(n=100\\) Studierende untersucht (s. Tabelle und Diagramm) und jeweils erfasst, ob die Person die Klausur bestanden (\\(B\\)) hat oder durchgefallen (\\(\\neg B\\)) ist. Im Hinblick auf das Lernen, \\(L\\) (wie viel die Person gelernt hat) hat er zwei Gruppen unterschieden: Die “Viel-Lerner” (VL) und die “Wenig-Lerner” (WL).\nBerechnen Sie die folgende: gemeinsame Wahrscheinlichkeit: p(Bestehen UND Viellerner).\nBeispiel: Wenn Sie ausrechnen, dass die Wahrscheinlichkeit bei 42 Prozentpunkten liegt, so geben Sie ein: 0,42 bzw. 0.42 (das Dezimalzeichen ist abhängig von Ihren Spracheinstellungen).\n\nGeben Sie nur eine Zahl ein (ohne Prozentzeichen o.Ä.), z.B. 0,42.\nAndere Angaben können u.U. nicht gewertet werden.\nRunden Sie auf zwei Dezimalstellen.\nAchten Sie darauf, das korrekte Dezimaltrennzeichen einzugeben; auf Geräten mit deutscher Spracheinstellung ist dies oft ein Komma.\n\nDas folgende Diagramm zeigt die Häufigkeiten pro Gruppe:\n\n\n\n\n\n\n\n\n\nHier ist die Kontingenztabelle mit den Häufigkeiten pro Gruppe:\n\n\n\n\n\n\n\nLerntyp\nBestehen\nDurchfallen\n\n\n\n\nViellerner\n29\n11\n\n\nWeniglerner\n42\n18\n\n\n\n\n\n\n         \n\n\nSolution\nDie gemeinsame Wahrscheinlichkeit beträgt 0.29.\n\n\n\n\n\n\n\n\nLerntyp\nKlausurergebnis\nn\nn_group\nprop_conditional_group\njoint_prob\n\n\n\n\nViellerner\nBestehen\n29\n40\n0.725\n0.29\n\n\n\n\n\n\n\nDie gemeinsame Wahrscheinlichkeit berechnet sich hier als der Quotient der Zellenhäufigkeit und der Gesamthäufigkeit.\nMan kann auch die Formel für gemeinsame Wahrscheinlichkeiten anwenden: \\(Pr(A) \\cdot \\Pr(B)\\).\nDazu berechnet man die Anteile für jedes der beiden Ereignisse und multipliziert diese beiden Anteile:\n`Klausurergebnis_selected * Lerntyp_selected*\n\nCategories:\n\nprobability"
  },
  {
    "objectID": "posts/wskt-quiz13/wskt-quiz13.html",
    "href": "posts/wskt-quiz13/wskt-quiz13.html",
    "title": "wskt-quiz13",
    "section": "",
    "text": "Behauptung: Die Post-Verteilung gibt die Wahrscheinlichkeit der Daten an, gegeben der Hypothese.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n\n\n\nFalsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz13/wskt-quiz13.html#answerlist",
    "href": "posts/wskt-quiz13/wskt-quiz13.html#answerlist",
    "title": "wskt-quiz13",
    "section": "",
    "text": "Falsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz13/wskt-quiz13.html#answerlist-1",
    "href": "posts/wskt-quiz13/wskt-quiz13.html#answerlist-1",
    "title": "wskt-quiz13",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\nquiz\nprobability\nbayes\nquiz1-qm2-ws23\nschoice"
  },
  {
    "objectID": "posts/Typ-Fehler-R-02/Typ-Fehler-R-02.html",
    "href": "posts/Typ-Fehler-R-02/Typ-Fehler-R-02.html",
    "title": "Typ-Fehler-R-02",
    "section": "",
    "text": "R gibt folgende Fehlermeldung aus:\n(Fehler in library(XXX): es gibt kein Paket namens 'XXX'),\nwobei für XXX ein Paketname wie tidyverse angeführt wird.\nWählen Sie die plausibelste Ursache aus!\n\n\n\nDas Paket XXX ist nicht installiert auf dem aktuellen Rechner.\nDas Paket XXX ist nicht verfügbar genau für dieses Betriebssystem.\nEs existiert kein Paket mit Namen XXX.\nDas Paket XXX ist nicht geladen.\nDas Paket XXX ist defekt."
  },
  {
    "objectID": "posts/Typ-Fehler-R-02/Typ-Fehler-R-02.html#answerlist",
    "href": "posts/Typ-Fehler-R-02/Typ-Fehler-R-02.html#answerlist",
    "title": "Typ-Fehler-R-02",
    "section": "",
    "text": "Das Paket XXX ist nicht installiert auf dem aktuellen Rechner.\nDas Paket XXX ist nicht verfügbar genau für dieses Betriebssystem.\nEs existiert kein Paket mit Namen XXX.\nDas Paket XXX ist nicht geladen.\nDas Paket XXX ist defekt."
  },
  {
    "objectID": "posts/Typ-Fehler-R-02/Typ-Fehler-R-02.html#answerlist-1",
    "href": "posts/Typ-Fehler-R-02/Typ-Fehler-R-02.html#answerlist-1",
    "title": "Typ-Fehler-R-02",
    "section": "Answerlist",
    "text": "Answerlist\n\nRichtig.\nFalsch.\nFalsch.\nFalsch.\nFalsch.\n\n\nCategories:\n\nR\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/Schiefe1/Schiefe1.html",
    "href": "posts/Schiefe1/Schiefe1.html",
    "title": "Schiefe1",
    "section": "",
    "text": "Welche der Abbildungen zeigt am deutlichsten eine rechtsschiefe Verteilung?\n\n\n\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD"
  },
  {
    "objectID": "posts/Schiefe1/Schiefe1.html#answerlist",
    "href": "posts/Schiefe1/Schiefe1.html#answerlist",
    "title": "Schiefe1",
    "section": "",
    "text": "A\nB\nC\nD"
  },
  {
    "objectID": "posts/Schiefe1/Schiefe1.html#answerlist-1",
    "href": "posts/Schiefe1/Schiefe1.html#answerlist-1",
    "title": "Schiefe1",
    "section": "Answerlist",
    "text": "Answerlist\n\nRichtig\nFalsch\nFalsch\nFalsch\n\n\nCategories:\nschoice"
  },
  {
    "objectID": "posts/Kennwert-robust2/Kennwert-robust2.html",
    "href": "posts/Kennwert-robust2/Kennwert-robust2.html",
    "title": "Kennwert-robust2",
    "section": "",
    "text": "Welcher der folgenden Kennwerte ist robust (im statistischen Sinn)?\n\n\n\nVarianz\nSchiefe\nSumme\nInterquartilsabstand\nKorrelation\nRegressionsgewicht"
  },
  {
    "objectID": "posts/Kennwert-robust2/Kennwert-robust2.html#answerlist",
    "href": "posts/Kennwert-robust2/Kennwert-robust2.html#answerlist",
    "title": "Kennwert-robust2",
    "section": "",
    "text": "Varianz\nSchiefe\nSumme\nInterquartilsabstand\nKorrelation\nRegressionsgewicht"
  },
  {
    "objectID": "posts/Kennwert-robust2/Kennwert-robust2.html#answerlist-1",
    "href": "posts/Kennwert-robust2/Kennwert-robust2.html#answerlist-1",
    "title": "Kennwert-robust2",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nWahr\nFalsch\nFalsch\n\n\nCategories:\n\neda\nstreuungsmaß\nvariability\nschoice"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-08/Verteilungen-Quiz-08.html",
    "href": "posts/Verteilungen-Quiz-08/Verteilungen-Quiz-08.html",
    "title": "Verteilungen-Quiz-08",
    "section": "",
    "text": "Ist folgende Aussage wahr?\nSei \\(X \\sim N(100,15)\\), dann ist \\(Pr(X \\ge 115) \\approx 0.16\\).\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-08/Verteilungen-Quiz-08.html#answerlist",
    "href": "posts/Verteilungen-Quiz-08/Verteilungen-Quiz-08.html#answerlist",
    "title": "Verteilungen-Quiz-08",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-08/Verteilungen-Quiz-08.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-08/Verteilungen-Quiz-08.html#answerlist-1",
    "title": "Verteilungen-Quiz-08",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/Var-vs-Stufe/Var-vs-Stufe.html",
    "href": "posts/Var-vs-Stufe/Var-vs-Stufe.html",
    "title": "Var-vs-Stufe",
    "section": "",
    "text": "Für ein Forschungsprojekt hat ein Forschungsteam die Frage getestet, ob Personen, die einen animierten Graphen zu Auswirkungen von Stress gesehen haben danach eine höhere Motivation haben ihr Stresspensum anzugehen, als Personen, die einen statischen Graph gesehen haben. Dazu wurde jeweils in einem Fragebogen die Veränderungsbereitschaft auf das Stressniveau angepasst abgefragt, dann den jeweiligen Graphen gezeigt und danach dieselben Fragen wie davor nochmals gestellt.\nZur Auswertung wurde nun zu jeder der Fragen zur Veränderungsbereitschaft die Mittelwerte der Vor-sehen-des-Graphen-Gruppe von der Nach-sehen-des-Graphen-Gruppe abgezogen und diese Werte dann verglichen von dem animierten und dem statischen Graphen. Dabei konnte der gewünschten Effekt deutlich erkannt werden, hypothesenkonform.\nNun kommt dem Forschungsteam folgender Zweifel auf:\nOder wäre die animierte und die statische Graphdarstellung jeweils als einzelne Variable zu betrachten?\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n\n\n\nJa, es wäre richtig gewesen, die animierte und die statische Graphdarstellung jeweils als einzelne Variable zu betrachten.\nNein, es wäre falsch gewesen, die animierte und die statische Graphdarstellung jeweils als einzelne Variable zu betrachten.\nBeide Sichtweisen sind möglich (entweder jeweils als einzelne Variable oder als ingesamt eine einzelne Variable)\nAuf Basis des dargestellten Forschungsdesigns ist diese Frage nicht zu beantworten"
  },
  {
    "objectID": "posts/Var-vs-Stufe/Var-vs-Stufe.html#answerlist",
    "href": "posts/Var-vs-Stufe/Var-vs-Stufe.html#answerlist",
    "title": "Var-vs-Stufe",
    "section": "",
    "text": "Ja, es wäre richtig gewesen, die animierte und die statische Graphdarstellung jeweils als einzelne Variable zu betrachten.\nNein, es wäre falsch gewesen, die animierte und die statische Graphdarstellung jeweils als einzelne Variable zu betrachten.\nBeide Sichtweisen sind möglich (entweder jeweils als einzelne Variable oder als ingesamt eine einzelne Variable)\nAuf Basis des dargestellten Forschungsdesigns ist diese Frage nicht zu beantworten"
  },
  {
    "objectID": "posts/Var-vs-Stufe/Var-vs-Stufe.html#answerlist-1",
    "href": "posts/Var-vs-Stufe/Var-vs-Stufe.html#answerlist-1",
    "title": "Var-vs-Stufe",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\nFalsch\nFalsch\n\n\nCategories:\n\nfopro\nresearchdesign\nschoice"
  },
  {
    "objectID": "posts/germeval01-textfeatures/germeval01-textfeatures.html",
    "href": "posts/germeval01-textfeatures/germeval01-textfeatures.html",
    "title": "germeval01-textfeatures",
    "section": "",
    "text": "Aufgabe\nExtrahieren Sie typisches Text-Features aus einem Text.\nNutzen Sie das Paket textfeatures.\nNutzen Sie die GermEval-2018-Daten.\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\nDie Daten sind auch über das R-Paket PradaData zu beziehen.\n\nlibrary(tidyverse)\nlibrary(easystats)\ndata(\"germeval_train\", package = \"pradadata\")\n\nNutzen Sie diesen Text-Datensatz, bevor Sie den größeren germeval-Datensatz verwenden:\n\n\nDaten\nTeststring:\n\ntext &lt;- c(\"Abbau, Abbruch ist jetzt\", \n          \"Test   🧑‍🎓 😄 heute!!\", \n          \"Abbruch #morgen #perfekt\", \n          \"Abmachung... LORE IPSUM\", \n          \"boese ja\", \"böse nein\", \n          \"hallo ?! www.google.de\", \n          \"gut schlecht I am you are he she it is\")\n\nn_emo &lt;- c(2, 0, 2, 1, 1, 1, 0, 2)\n\ntest_text &lt;-\n  data.frame(id = 1:length(text),\n         text = text,\n         n_emo = n_emo)\n\ntest_text\n\n  id                     text n_emo\n1  1 Abbau, Abbruch ist jetzt     2\n2  2   Test   🧑‍🎓 😄 heute!!     0\n3  3 Abbruch #morgen #perfekt     2\n [ reached 'max' / getOption(\"max.print\") -- omitted 5 rows ]\n\n\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n         \n\n\nLösung\nDas Paket textfeatures ist aktuelle nicht auf CRAN, aber über Github zu bekommen (oder im CRAN-Archiv).\n\nlibrary(tidyverse)\nlibrary(tictoc)\nlibrary(textfeatures)\n\n\n\nTest 1\nHier ein Test vom Autor des Pakets:\n\nx &lt;- c(\n  \"this is A!\\t sEntence https://github.com about #rstats @github\",\n  \"and another sentence here\", \"THe following list:\\n- one\\n- two\\n- three\\nOkay!?!\"\n)\n\n## get text features\ntextfeatures::textfeatures(x, verbose = FALSE)\n\n# A tibble: 3 × 36\n  n_urls n_uq_urls n_hashtags n_uq_hashtags n_mentions n_uq_mentions n_chars\n   &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n1  1.15      1.15       1.15          1.15       1.15          1.15    0.243\n2 -0.577    -0.577     -0.577        -0.577     -0.577        -0.577  -1.10 \n3 -0.577    -0.577     -0.577        -0.577     -0.577        -0.577   0.856\n# ℹ 29 more variables: n_uq_chars &lt;dbl&gt;, n_commas &lt;dbl&gt;, n_digits &lt;dbl&gt;,\n#   n_exclaims &lt;dbl&gt;, n_extraspaces &lt;dbl&gt;, n_lowers &lt;dbl&gt;, n_lowersp &lt;dbl&gt;,\n#   n_periods &lt;dbl&gt;, n_words &lt;dbl&gt;, n_uq_words &lt;dbl&gt;, n_caps &lt;dbl&gt;,\n#   n_nonasciis &lt;dbl&gt;, n_puncts &lt;dbl&gt;, n_capsp &lt;dbl&gt;, n_charsperword &lt;dbl&gt;,\n#   sent_afinn &lt;dbl&gt;, sent_bing &lt;dbl&gt;, sent_syuzhet &lt;dbl&gt;, sent_vader &lt;dbl&gt;,\n#   n_polite &lt;dbl&gt;, n_first_person &lt;dbl&gt;, n_first_personp &lt;dbl&gt;,\n#   n_second_person &lt;dbl&gt;, n_second_personp &lt;dbl&gt;, n_third_person &lt;dbl&gt;, …\n\n\n\n\nTest 2\n\ntextfeatures::textfeatures(test_text$text,\n                           sentiment = FALSE,\n                           word_dims = FALSE)\n\n\u001b[32m↪\u001b[39m \u001b[38;5;244mCounting features in text...\u001b[39m\n\u001b[32m↪\u001b[39m \u001b[38;5;244mParts of speech...\u001b[39m\n\u001b[32m↪\u001b[39m \u001b[38;5;244mWord dimensions started\u001b[39m\n\u001b[32m↪\u001b[39m \u001b[38;5;244mNormalizing data\u001b[39m\n\u001b[32m✔\u001b[39m Job's done!\n\n\n# A tibble: 8 × 29\n  n_urls n_uq_urls n_hashtags n_uq_hashtags n_mentions n_uq_mentions n_chars\n   &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;\n1      0         0     -0.354        -0.354          0             0  0.532 \n2      0         0     -0.354        -0.354          0             0  0.0800\n3      0         0      2.47          2.47           0             0  0.589 \n4      0         0     -0.354        -0.354          0             0  0.532 \n5      0         0     -0.354        -0.354          0             0 -1.86  \n6      0         0     -0.354        -0.354          0             0 -1.25  \n7      0         0     -0.354        -0.354          0             0  0.471 \n8      0         0     -0.354        -0.354          0             0  0.910 \n# ℹ 22 more variables: n_uq_chars &lt;dbl&gt;, n_commas &lt;dbl&gt;, n_digits &lt;dbl&gt;,\n#   n_exclaims &lt;dbl&gt;, n_extraspaces &lt;dbl&gt;, n_lowers &lt;dbl&gt;, n_lowersp &lt;dbl&gt;,\n#   n_periods &lt;dbl&gt;, n_words &lt;dbl&gt;, n_uq_words &lt;dbl&gt;, n_caps &lt;dbl&gt;,\n#   n_nonasciis &lt;dbl&gt;, n_puncts &lt;dbl&gt;, n_capsp &lt;dbl&gt;, n_charsperword &lt;dbl&gt;,\n#   n_first_person &lt;dbl&gt;, n_first_personp &lt;dbl&gt;, n_second_person &lt;dbl&gt;,\n#   n_second_personp &lt;dbl&gt;, n_third_person &lt;dbl&gt;, n_tobe &lt;dbl&gt;,\n#   n_prepositions &lt;dbl&gt;\n\n\n\nCategories:\n\n2023\ntextmining\ndatawrangling\ngermeval\nstring"
  },
  {
    "objectID": "posts/iq01/iq01.html",
    "href": "posts/iq01/iq01.html",
    "title": "iq01",
    "section": "",
    "text": "Aufgabe\nIntelligenz wird häufig mittels einem IQ-Test ermittelt. Ab einem Testwert von 130 Punkten nennt man die getestete Person hochbegabt.\nWie groß ist die Wahrscheinlichkeit, dass die nächste Person, die Sie treffen, hochbetagthochbegabt ist? Geben Sie die Wahrscheinlichkeit (als Anteil) an.\nHinweise:\n\nNutzen Sie Simulationsmethoden.\nGehen Sie von folgender IQ-Verteilung aus: \\(IQ \\sim N(100,15)\\).\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\nSimulieren Sie \\(n=10^3\\) Stichproben.\nNutzen Sie die Zahl 42 als Startwert für Ihre Zufallszahlen (um die Reproduzierbarkeit zu gewährleisten).\nWir wollen hier keine Post-Verteilung berechnen, sondern lediglich Werte simulieren.\nGeben Sie keine Prozentzahlen, sondern stets Anteile an.\n\n         \n\n\nLösung\n\nlibrary(tidyverse)\n\nWir simulieren die Daten:\n\nset.seed(42)  # Reproduzierbarkeit\nd &lt;- tibble(\n  id = 1:10^3,  # Der Doppelpunkt heißt \"bis\", also \"von 1 bis 10 hoch 3\". Diese Spalte ist nicht so wichtig.\n  iq = rnorm(n = 10^3, mean = 100, sd = 15))\n\nhead(d)  # Die ersten paar Zeilen\n\n# A tibble: 6 × 2\n     id    iq\n  &lt;int&gt; &lt;dbl&gt;\n1     1 121. \n2     2  91.5\n3     3 105. \n4     4 109. \n5     5 106. \n6     6  98.4\n\n\nWir filtern wie in der Angabe gewünscht:\n\nd %&gt;% \n  count(iq &gt;= 130)\n\n# A tibble: 2 × 2\n  `iq &gt;= 130`     n\n  &lt;lgl&gt;       &lt;int&gt;\n1 FALSE         979\n2 TRUE           21\n\n\nCa. 20 von 1000 Personen erfüllen diese Bedingung (IQ &gt;= 130).\nGenau genommen:\n\n21/1000\n\n[1] 0.021\n\n\nLösung: Die gesuchte Wahrscheinlichkeit beträgt ca. 2% bzw. 0.02.\n\nCategories:\n\nprobability\nsimulation\nnormal-distribution\nexam-22\nnum"
  },
  {
    "objectID": "posts/tidymodels-error1/tidymodels-error1.html",
    "href": "posts/tidymodels-error1/tidymodels-error1.html",
    "title": "tidymodels-error1introd",
    "section": "",
    "text": "Aufgabe\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.0.8\n✔ dials        1.2.0     ✔ rsample      1.2.0\n✔ dplyr        1.1.4     ✔ tibble       3.2.1\n✔ ggplot2      3.5.0     ✔ tidyr        1.3.1\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.3.0     ✔ workflows    1.1.3\n✔ parsnip      1.2.0     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.3.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(tictoc)\n\n# Data:\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\"\nd &lt;- read.csv(d_path)\n\nDie folgende Pipeline hat einen Fehler. Welcher ist das?\n\nset.seed(42)\nd_split &lt;- initial_split(d)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\n\n# model:\nmod1 &lt;-\n  rand_forest(mode = \"regression\")\n\n\n# cv:\nset.seed(42)\nrsmpl &lt;- vfold_cv(d_train)\n\n\n# recipe:\nrec1 &lt;- recipe(body_mass_g ~  ., data = d_train) |&gt; \n  #step_unknown(all_nominal_predictors(), new_level = \"NA\") |&gt; \n  #step_novel(all_nominal_predictors()) |&gt; \n  step_naomit(all_predictors()) |&gt; \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_nzv(all_predictors()) |&gt; \n  step_normalize(all_predictors()) \n\n\n\n# workflow:\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(mod1) %&gt;% \n  add_recipe(rec1)\n\n\n# fitting:\ntic()\nwf1_fit &lt;-\n  wf1 %&gt;% \n  fit(data = d_train)\ntoc()\n\n0.256 sec elapsed\n\npreds &lt;- predict(wf1_fit, new_data = d_test) \n\nError: Missing data in columns: bill_length_mm, bill_depth_mm, flipper_length_mm.\n\n\nAls Check: Das gepreppte/bebackene Rezept:\n\nrec1_prepped &lt;- prep(rec1)\nd_train_baked &lt;- bake(rec1_prepped, new_data = NULL)\n\n\nd_train_baked |&gt; \n  head()\n\n# A tibble: 6 × 12\n  rownames bill_length_mm bill_depth_mm flipper_length_mm    year body_mass_g\n     &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;   &lt;dbl&gt;       &lt;int&gt;\n1   -1.24          -1.53          0.386            -0.794 -1.29          3450\n2    1.45           1.32          0.386            -0.365  1.14          3675\n3   -0.212          0.401        -1.97              0.707 -1.29          4500\n4   -0.993          0.343         0.887            -0.294 -0.0757        4150\n5    0.530          0.879        -0.566             2.07  -0.0757        5800\n6   -0.281         -0.957         0.787            -1.15   1.14          3650\n# ℹ 6 more variables: species_Chinstrap &lt;dbl&gt;, species_Gentoo &lt;dbl&gt;,\n#   island_Dream &lt;dbl&gt;, island_Torgersen &lt;dbl&gt;, sex_female &lt;dbl&gt;,\n#   sex_male &lt;dbl&gt;\n\n\n\nd_train_baked |&gt; \n  map_int(~ sum(is.na(.)))\n\n         rownames    bill_length_mm     bill_depth_mm flipper_length_mm \n                0                 0                 0                 0 \n             year       body_mass_g species_Chinstrap    species_Gentoo \n                0                 0                 0                 0 \n     island_Dream  island_Torgersen        sex_female          sex_male \n                0                 0                 0                 0 \n\n\n         \n\n\nLösung\nDer Fehler liegt darin, dass das Rezept keine Änderungen an der AV ausführt. In der AV gibt es aber fehlende Werte (NA) im Test-Set.\n\ncolSums(is.na(d_test))\n\n         rownames           species            island    bill_length_mm \n                0                 0                 0                 1 \n    bill_depth_mm flipper_length_mm       body_mass_g               sex \n                1                 1                 1                 0 \n             year \n                0 \n\n\nEinen fehlenden Wert, um genau zu sein. Dieser eine fehlende Wert versalzt uns die Suppe:\n\nd_test_nona &lt;-\n  d_test |&gt; \n  na.omit()\n\nUnd schon geht’s.\n\npreds &lt;- predict(wf1_fit, new_data = d_test_nona) \npreds |&gt; \n  head()\n\n# A tibble: 6 × 1\n  .pred\n  &lt;dbl&gt;\n1 3952.\n2 3675.\n3 3615.\n4 3806.\n5 3490.\n6 3390.\n\n\nDieser SO-Post handelt von einem vergleichbarem Problem.\n\nCategories:\n\ntidymodels\nstatlearning\nerror\nNA\nstring"
  },
  {
    "objectID": "posts/tidymodels-tree4/tidymodels-tree4.html",
    "href": "posts/tidymodels-tree4/tidymodels-tree4.html",
    "title": "tidymodels-tree4",
    "section": "",
    "text": "Berechnen Sie folgendes einfache Modell:\n\nEntscheidungsbaum\n\nModellformel: body_mass_g ~ . (Datensatz palmerpenguins::penguins)\nHier geht es darum, die Geschwindigkeit (und den Ressourcenverbrauch) beim Fitten zu verringern. Benutzen Sie dazu folgende Methoden\n\nAuslassen gering performanter Tuningparameterwerte\nVerwenden Sie ein Anova-Grid-Search!\nParallelisieren Sie auf mehrere Kerne (wenn möglich).\n\nHinweise:\n\nTunen Sie alle Parameter (die der Engine anbietet).\nVerwenden Sie Defaults, wo nicht anders angegeben.\nBeachten Sie die üblichen Hinweise."
  },
  {
    "objectID": "posts/tidymodels-tree4/tidymodels-tree4.html#setup",
    "href": "posts/tidymodels-tree4/tidymodels-tree4.html#setup",
    "title": "tidymodels-tree4",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.0.8\n✔ dials        1.2.0     ✔ rsample      1.2.0\n✔ dplyr        1.1.3     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\ndata(\"penguins\", package = \"palmerpenguins\")\nlibrary(tictoc)  # Zeitmessung\nlibrary(finetune)  # tune_race_anova\nlibrary(doParallel)  # mehrere CPUs nutzen \n\nLoading required package: foreach\n\n\n\nAttaching package: 'foreach'\n\n\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n\n\nLoading required package: iterators\n\n\nLoading required package: parallel\n\nset.seed(42)\n\nEntfernen wir Fälle ohne y-Wert:\n\nd &lt;-\n  penguins %&gt;% \n  drop_na(body_mass_g)"
  },
  {
    "objectID": "posts/tidymodels-tree4/tidymodels-tree4.html#daten-teilen",
    "href": "posts/tidymodels-tree4/tidymodels-tree4.html#daten-teilen",
    "title": "tidymodels-tree4",
    "section": "Daten teilen",
    "text": "Daten teilen\n\nset.seed(42)\nd_split &lt;- initial_split(d)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)"
  },
  {
    "objectID": "posts/tidymodels-tree4/tidymodels-tree4.html#modelle",
    "href": "posts/tidymodels-tree4/tidymodels-tree4.html#modelle",
    "title": "tidymodels-tree4",
    "section": "Modell(e)",
    "text": "Modell(e)\n\nmod_tree &lt;-\n  decision_tree(mode = \"regression\",\n                cost_complexity = tune(),\n                tree_depth = tune(),\n                min_n = tune())"
  },
  {
    "objectID": "posts/tidymodels-tree4/tidymodels-tree4.html#rezepte",
    "href": "posts/tidymodels-tree4/tidymodels-tree4.html#rezepte",
    "title": "tidymodels-tree4",
    "section": "Rezept(e)",
    "text": "Rezept(e)\n\nrec_plain &lt;- \n  recipe(body_mass_g ~ ., data = d_train)"
  },
  {
    "objectID": "posts/tidymodels-tree4/tidymodels-tree4.html#resampling",
    "href": "posts/tidymodels-tree4/tidymodels-tree4.html#resampling",
    "title": "tidymodels-tree4",
    "section": "Resampling",
    "text": "Resampling\n\nset.seed(42)\nrsmpl &lt;- vfold_cv(d_train)"
  },
  {
    "objectID": "posts/tidymodels-tree4/tidymodels-tree4.html#workflows",
    "href": "posts/tidymodels-tree4/tidymodels-tree4.html#workflows",
    "title": "tidymodels-tree4",
    "section": "Workflows",
    "text": "Workflows\n\nwf_tree &lt;-\n  workflow() %&gt;%  \n  add_recipe(rec_plain) %&gt;% \n  add_model(mod_tree)"
  },
  {
    "objectID": "posts/tidymodels-tree4/tidymodels-tree4.html#tuning-grid",
    "href": "posts/tidymodels-tree4/tidymodels-tree4.html#tuning-grid",
    "title": "tidymodels-tree4",
    "section": "Tuning-Grid",
    "text": "Tuning-Grid\nTuninggrid:\n\ntune_grid &lt;- grid_regular(extract_parameter_set_dials(mod_tree), levels = 5)\n\nHinweis: Andere Arten von Tuning-Grids sind sinnvoller, hier ist nur zum Vergleich mit anderen Aufgaben diese Form des Tuning-Grids gewählt.\nDie Zeilen im Tuninggrid zeigen uns, für wie viele Modellparameter ein Modell berechnet wird. Natürlich üblicherweise jedes Modell mit Resampling. Da kommt in Summe ein mitunter sehr große Menge an Modellberechnungen zusammen."
  },
  {
    "objectID": "posts/tidymodels-tree4/tidymodels-tree4.html#ohne-speed-up",
    "href": "posts/tidymodels-tree4/tidymodels-tree4.html#ohne-speed-up",
    "title": "tidymodels-tree4",
    "section": "Ohne Speed-up",
    "text": "Ohne Speed-up\n\ntic()\nfit_tree &lt;-\n  tune_grid(object = wf_tree,\n            grid = tune_grid,\n            metrics = metric_set(rmse),\n            resamples = rsmpl)\ntoc()\n\n78.464 sec elapsed\n\n\nDie angegebene Rechenzeit bezieht sich auf einen 4-Kerne-MacBook Pro (2020)."
  },
  {
    "objectID": "posts/tidymodels-tree4/tidymodels-tree4.html#mit-speeed-up-1",
    "href": "posts/tidymodels-tree4/tidymodels-tree4.html#mit-speeed-up-1",
    "title": "tidymodels-tree4",
    "section": "Mit Speeed-up 1",
    "text": "Mit Speeed-up 1\n\ntic()\nfit_tree2 &lt;-\n  tune_race_anova(object = wf_tree,\n            grid = tune_grid,\n            metrics = metric_set(rmse),\n            control = control_race(verbose = FALSE,\n                                   pkgs = c(\"tidymodels\"),\n                                   save_pred = TRUE),\n            resamples = rsmpl)\ntoc()\n\n72.066 sec elapsed"
  },
  {
    "objectID": "posts/tidymodels-tree4/tidymodels-tree4.html#mit-speeed-up-2",
    "href": "posts/tidymodels-tree4/tidymodels-tree4.html#mit-speeed-up-2",
    "title": "tidymodels-tree4",
    "section": "Mit Speeed-up 2",
    "text": "Mit Speeed-up 2\n\ndoParallel::registerDoParallel()\n\ntic()\nfit_tree2 &lt;-\n  tune_race_anova(\n    object = wf_tree,\n    grid = tune_grid,\n    metrics = metric_set(rmse),\n    control = control_race(verbose = FALSE,\n                           pkgs = c(\"tidymodels\"),\n                           save_pred = TRUE),\n            resamples = rsmpl)\ntoc()\n\n27.524 sec elapsed"
  },
  {
    "objectID": "posts/tidymodels-tree4/tidymodels-tree4.html#mit-speeed-up-3",
    "href": "posts/tidymodels-tree4/tidymodels-tree4.html#mit-speeed-up-3",
    "title": "tidymodels-tree4",
    "section": "Mit Speeed-up 3",
    "text": "Mit Speeed-up 3\n\ndoParallel::registerDoParallel()\n\ntic()\nfit_tree2 &lt;-\n  tune_grid(object = wf_tree,\n            grid = tune_grid,\n            metrics = metric_set(rmse),\n            control = control_grid(verbose = FALSE,\n                                   save_pred = TRUE),\n            resamples = rsmpl)\ntoc()\n\n27.312 sec elapsed"
  },
  {
    "objectID": "posts/tidymodels-tree4/tidymodels-tree4.html#fazit",
    "href": "posts/tidymodels-tree4/tidymodels-tree4.html#fazit",
    "title": "tidymodels-tree4",
    "section": "Fazit",
    "text": "Fazit\nMit Speed-up ist schneller also ohne. Hier haben wir einen Entscheidungsbaum berechnet, der ist nicht so sehr parallelisierbar. Bei einem “Wald-Modell”, wie Random Forests, sollte der Vorteil der Parallisierung viel deutlich sein.\n\nCategories:\n\nstatlearning\ntrees\ntidymodels\nspeed\nstring"
  },
  {
    "objectID": "posts/tidymodels-tree3/tidymodels-tree3.html",
    "href": "posts/tidymodels-tree3/tidymodels-tree3.html",
    "title": "tidymodels-tree3",
    "section": "",
    "text": "Berechnen Sie folgendes einfache Modell:\n\nEntscheidungsbaum\n\nModellformel: body_mass_g ~ . (Datensatz palmerpenguins::penguins)\nHier geht es darum, die Geschwindigkeit (und den Ressourcenverbrauch) beim Fitten zu verringern. Benutzen Sie dazu folgende Methoden\n\nAuslassen gering performanter Tuningparameterwerte\n\nHinweise:\n\nTunen Sie alle Parameter (die der Engine anbietet).\nVerwenden Sie Defaults, wo nicht anders angegeben.\nBeachten Sie die üblichen Hinweise."
  },
  {
    "objectID": "posts/tidymodels-tree3/tidymodels-tree3.html#setup",
    "href": "posts/tidymodels-tree3/tidymodels-tree3.html#setup",
    "title": "tidymodels-tree3",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.0.8\n✔ dials        1.2.0     ✔ rsample      1.2.0\n✔ dplyr        1.1.3     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\ndata(\"penguins\", package = \"palmerpenguins\")\nlibrary(tictoc)  # Zeitmessung\nlibrary(finetune)  # tune_race_anova\nset.seed(42)\n\nEntfernen wir Fälle ohne y-Wert:\n\nd &lt;-\n  penguins %&gt;% \n  drop_na(body_mass_g)"
  },
  {
    "objectID": "posts/tidymodels-tree3/tidymodels-tree3.html#daten-teilen",
    "href": "posts/tidymodels-tree3/tidymodels-tree3.html#daten-teilen",
    "title": "tidymodels-tree3",
    "section": "Daten teilen",
    "text": "Daten teilen\n\nset.seed(42)\nd_split &lt;- initial_split(d)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)"
  },
  {
    "objectID": "posts/tidymodels-tree3/tidymodels-tree3.html#modelle",
    "href": "posts/tidymodels-tree3/tidymodels-tree3.html#modelle",
    "title": "tidymodels-tree3",
    "section": "Modell(e)",
    "text": "Modell(e)\n\nmod_tree &lt;-\n  decision_tree(mode = \"regression\",\n                cost_complexity = tune(),\n                tree_depth = tune(),\n                min_n = tune())"
  },
  {
    "objectID": "posts/tidymodels-tree3/tidymodels-tree3.html#rezepte",
    "href": "posts/tidymodels-tree3/tidymodels-tree3.html#rezepte",
    "title": "tidymodels-tree3",
    "section": "Rezept(e)",
    "text": "Rezept(e)\n\nrec_plain &lt;- \n  recipe(body_mass_g ~ ., data = d_train)"
  },
  {
    "objectID": "posts/tidymodels-tree3/tidymodels-tree3.html#resampling",
    "href": "posts/tidymodels-tree3/tidymodels-tree3.html#resampling",
    "title": "tidymodels-tree3",
    "section": "Resampling",
    "text": "Resampling\n\nset.seed(42)\nrsmpl &lt;- vfold_cv(d_train)"
  },
  {
    "objectID": "posts/tidymodels-tree3/tidymodels-tree3.html#workflows",
    "href": "posts/tidymodels-tree3/tidymodels-tree3.html#workflows",
    "title": "tidymodels-tree3",
    "section": "Workflows",
    "text": "Workflows\n\nwf_tree &lt;-\n  workflow() %&gt;%  \n  add_recipe(rec_plain) %&gt;% \n  add_model(mod_tree)"
  },
  {
    "objectID": "posts/tidymodels-tree3/tidymodels-tree3.html#tuning-grid",
    "href": "posts/tidymodels-tree3/tidymodels-tree3.html#tuning-grid",
    "title": "tidymodels-tree3",
    "section": "Tuning-Grid",
    "text": "Tuning-Grid\nTuninggrid:\n\ntune_grid &lt;- grid_regular(extract_parameter_set_dials(mod_tree), levels = 5)\n\nDie Zeilen im Tuninggrid zeigen uns, für wie viele Modellparameter ein Modell berechnet wird. Natürlich üblicherweise jedes Modell mit Resampling. Da kommt in Summe ein mitunter sehr große Menge an Modellberechnungen zusammen."
  },
  {
    "objectID": "posts/tidymodels-tree3/tidymodels-tree3.html#ohne-speed-up",
    "href": "posts/tidymodels-tree3/tidymodels-tree3.html#ohne-speed-up",
    "title": "tidymodels-tree3",
    "section": "Ohne Speed-up",
    "text": "Ohne Speed-up\n\ntic()\nfit_tree &lt;-\n  tune_grid(object = wf_tree,\n            grid = tune_grid,\n            metrics = metric_set(rmse),\n            resamples = rsmpl)\ntoc()\n\n77.274 sec elapsed\n\n\nca. auf meinem Rechner (4-Kerne-MacBook Pro 2020)."
  },
  {
    "objectID": "posts/tidymodels-tree3/tidymodels-tree3.html#mit-geschicktem-weglassen-von-tuningparametern",
    "href": "posts/tidymodels-tree3/tidymodels-tree3.html#mit-geschicktem-weglassen-von-tuningparametern",
    "title": "tidymodels-tree3",
    "section": "Mit geschicktem Weglassen von Tuningparametern",
    "text": "Mit geschicktem Weglassen von Tuningparametern\n\ntic()\nfit_tree2 &lt;-\n  tune_race_anova(object = wf_tree,\n            grid = tune_grid,\n            metrics = metric_set(rmse),\n            resamples = rsmpl)\ntoc()\n\n68.768 sec elapsed\n\n\nca. - schneller!\n\nCategories:\n\nstatlearning\ntrees\ntidymodels\nspeed\nstring"
  },
  {
    "objectID": "posts/pigs2/pigs2.html",
    "href": "posts/pigs2/pigs2.html",
    "title": "pigs2",
    "section": "",
    "text": "Aufgabe\n\nlibrary(tidyverse)  # Datenjudo\nlibrary(rstanarm)  # Bayes-Inferenz\nlibrary(easystats)  # Komfort\n\nLaden Sie den Datensatz toothgrowth, z.B. von dieser Quelle. In dem Datensatz sind die Daten eines einfaches Experiments berichtet.\nIn dem Experiment wird der (kausale) Effekt verschiedener Darreichungsformen und Konzentrationen von Vitamin C auf das Zahnwachstum von Meerschweinchen untersucht.\nForschungsfrage:\nHat die Dosis (dose) einen (kausalen) Effekt auf die AV, Zahnlänge (len)?\nWir gehen mal einfach davon aus, dass der Faktor experimentell (also randomisiert und auf Störeffekte hin kontrollliert) veraeicht wurde. Sonst wäre eine Kausalinterpretation nicht (ohne Weiteres) möglich.\nAufgabe: Berechnen Sie die Breite eines 95%-HDI für den Effekt!\nHinweise\n         \n\n\nLösung\n\nSchritt: Modell berechnen\n\n\nlm2 &lt;- stan_glm(len ~ dose, data = d,\n                seed = 42,\n                refresh = 0)\n\nZur Erinnerung: Bei der Regressionsformel gilt immer av ~  uv.\n\nSchritt: Posteriori-Verteilung betrachten\n\nMit parameters() kriegt man einen guten Überblick über die Modellparameter:\n\nparameters(lm2)\n\nParameter   | Median |        95% CI |   pd |  Rhat |     ESS |                   Prior\n---------------------------------------------------------------------------------------\n(Intercept) |   7.40 | [4.84,  9.96] | 100% | 0.999 | 3881.00 | Normal (18.81 +- 19.12)\ndose        |   9.76 | [7.81, 11.76] | 100% | 0.999 | 4144.00 |  Normal (0.00 +- 30.41)\n\n\nDas Modell zeigt einen positiven Effekt für dose:\nPro Einheit von dose steigt die Zahnlänge (len) um ca. 8-12 mm im Schnitt (laut unserem Modell).\nNull ist nicht im Intervall enthalten; die Nullhypothese ist demnach auszuschließen (falls das jemanden interessiert). Man sagt, man verwirft die Nullhypothese (oder weist sie zurück).\nDas können wir auch plotten:\n\nplot(parameters(lm2))\n\n\n\n\n\n\n\n\nMan kann sich auch ein HDI direkt ausgeben, ohne die sonstigen Informationen, die parameters() ausgibt:\n\nhdi(lm2, ci = .95)\n\nHighest Density Interval\n\nParameter   |       95% HDI\n---------------------------\n(Intercept) | [4.96, 10.06]\ndose        | [7.77, 11.70]\n\n\nWie wir sehen, wird im Standard ein 95%-Intervall berichtet, wie in der Aufgabenstellung verlangt.\nWieder sehen wir, die Null ist nicht im Intervall enthalten. Null ist also kein plausibler Wert für den gesuchten Effekt (laut unserem Modell).\nSchauen wir uns mal zum Vergleich die Stichproben-Daten an:\n\nd %&gt;% \n  ggplot(aes(y = len, x = dose)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nMan sieht deutlich einen positiven Effekt: Die Regressionsgerade steigt.\nDie Breite des Schätzintervalls für den Effekt beträgt also:\n\nsol &lt;- 11.70 - 7.77\nsol\n\n[1] 3.93\n\n\n\nCategories:\n\nbayes\n~\nregression\nexam-22"
  },
  {
    "objectID": "posts/Kausal/Kausal.html",
    "href": "posts/Kausal/Kausal.html",
    "title": "Kausal",
    "section": "",
    "text": "Die kausale Abhängigkeit zwischen drei Variablen A, B, C sieht wie folgt aus:\n\nB hängt ab von A.\nC hängt ab von A.\nC hängt ab von B.\n\nHinweise:\n\nAbhängigkeit ist kausal zu verstehen.\n\nAufgabe: Wie lautet die R-Formel, um den totalen kausalen Effekt von A auf C zu bestimmen?\n\n\n\nC ~ A\nC ~ B\nC ~ A + B\nA ~ C\nA ~ C + B\nA ~ B\nDer totale kausale Effekt kann nicht bestimmt werden."
  },
  {
    "objectID": "posts/Kausal/Kausal.html#answerlist",
    "href": "posts/Kausal/Kausal.html#answerlist",
    "title": "Kausal",
    "section": "",
    "text": "C ~ A\nC ~ B\nC ~ A + B\nA ~ C\nA ~ C + B\nA ~ B\nDer totale kausale Effekt kann nicht bestimmt werden."
  },
  {
    "objectID": "posts/Kausal/Kausal.html#answerlist-1",
    "href": "posts/Kausal/Kausal.html#answerlist-1",
    "title": "Kausal",
    "section": "Answerlist",
    "text": "Answerlist\n\nRichtig\nFalsch\nFalsch\nFalsch\nFalsch\nFalsch\nFalsch"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-09/Verteilungen-Quiz-09.html",
    "href": "posts/Verteilungen-Quiz-09/Verteilungen-Quiz-09.html",
    "title": "Verteilungen-Quiz-09",
    "section": "",
    "text": "Ist folgende Aussage wahr?\nSei \\(X \\sim N(100,15)\\), dann ist \\(Pr(X \\ge \\bar{x} + \\sigma) \\approx 0.84\\).\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-09/Verteilungen-Quiz-09.html#answerlist",
    "href": "posts/Verteilungen-Quiz-09/Verteilungen-Quiz-09.html#answerlist",
    "title": "Verteilungen-Quiz-09",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-09/Verteilungen-Quiz-09.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-09/Verteilungen-Quiz-09.html#answerlist-1",
    "title": "Verteilungen-Quiz-09",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/germeval06/germeval06.html",
    "href": "posts/germeval06/germeval06.html",
    "title": "germeval06",
    "section": "",
    "text": "Erstellen Sie ein prädiktives Modell für Textdaten. Nutzen Sie Glove6b Word-Vektoren für das Feature-Engineering.\nNutzen Sie die GermEval-2018-Daten.\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\nDie Daten sind auch über das R-Paket PradaData zu beziehen.\n\nlibrary(tidyverse)\ndata(\"germeval_train\", package = \"pradadata\")\ndata(\"germeval_test\", package = \"pradadata\")\n\nDie AV lautet c1. Die (einzige) UV lautet: text.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\nNutzen Sie Tidymodels."
  },
  {
    "objectID": "posts/germeval06/germeval06.html#textvektoren-importieren",
    "href": "posts/germeval06/germeval06.html#textvektoren-importieren",
    "title": "germeval06",
    "section": "Textvektoren importieren",
    "text": "Textvektoren importieren\n\nlibrary(textdata)\n\nglove_embedding &lt;- embedding_glove6b(\n  dir = \"/Users/sebastiansaueruser/datasets\",\n  return_path = TRUE,\n  manual_download = TRUE\n)\n\nhead(glove_embedding)\n\n# A tibble: 6 × 51\n  token     d1      d2     d3      d4    d5      d6     d7     d8        d9\n  &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1 the   0.418   0.250  -0.412  0.122  0.345 -0.0445 -0.497 -0.179 -0.000660\n2 ,     0.0134  0.237  -0.169  0.410  0.638  0.477  -0.429 -0.556 -0.364   \n3 .     0.152   0.302  -0.168  0.177  0.317  0.340  -0.435 -0.311 -0.450   \n4 of    0.709   0.571  -0.472  0.180  0.544  0.726   0.182 -0.524  0.104   \n5 to    0.680  -0.0393  0.302 -0.178  0.430  0.0322 -0.414  0.132 -0.298   \n6 and   0.268   0.143  -0.279  0.0163 0.114  0.699  -0.513 -0.474 -0.331   \n# ℹ 41 more variables: d10 &lt;dbl&gt;, d11 &lt;dbl&gt;, d12 &lt;dbl&gt;, d13 &lt;dbl&gt;, d14 &lt;dbl&gt;,\n#   d15 &lt;dbl&gt;, d16 &lt;dbl&gt;, d17 &lt;dbl&gt;, d18 &lt;dbl&gt;, d19 &lt;dbl&gt;, d20 &lt;dbl&gt;,\n#   d21 &lt;dbl&gt;, d22 &lt;dbl&gt;, d23 &lt;dbl&gt;, d24 &lt;dbl&gt;, d25 &lt;dbl&gt;, d26 &lt;dbl&gt;,\n#   d27 &lt;dbl&gt;, d28 &lt;dbl&gt;, d29 &lt;dbl&gt;, d30 &lt;dbl&gt;, d31 &lt;dbl&gt;, d32 &lt;dbl&gt;,\n#   d33 &lt;dbl&gt;, d34 &lt;dbl&gt;, d35 &lt;dbl&gt;, d36 &lt;dbl&gt;, d37 &lt;dbl&gt;, d38 &lt;dbl&gt;,\n#   d39 &lt;dbl&gt;, d40 &lt;dbl&gt;, d41 &lt;dbl&gt;, d42 &lt;dbl&gt;, d43 &lt;dbl&gt;, d44 &lt;dbl&gt;,\n#   d45 &lt;dbl&gt;, d46 &lt;dbl&gt;, d47 &lt;dbl&gt;, d48 &lt;dbl&gt;, d49 &lt;dbl&gt;, d50 &lt;dbl&gt;\n\n\n\n# model:\nmod1 &lt;-\n  logistic_reg()\n\n\n# cv:\nset.seed(42)\nrsmpl &lt;- vfold_cv(d_train, v = 5)\n\n\n# recipe:\nrec1 &lt;-\n  recipe(c1 ~ ., data = d_train) |&gt; \n  update_role(id, new_role = \"id\")  |&gt; \n  #update_role(c2, new_role = \"ignore\") |&gt; \n  step_tokenize(text) %&gt;%\n  step_stopwords(text, keep = FALSE) %&gt;%\n  step_word_embeddings(text,\n                       embeddings = glove_embedding,\n                       aggregation = \"mean\") |&gt; \n  step_normalize(all_numeric_predictors()) \n\n\n# workflow:\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(mod1) %&gt;% \n  add_recipe(rec1)"
  },
  {
    "objectID": "posts/germeval06/germeval06.html#tuning",
    "href": "posts/germeval06/germeval06.html#tuning",
    "title": "germeval06",
    "section": "Tuning",
    "text": "Tuning\n\ntic()\nwf1_fit &lt;-\n  wf1 %&gt;% \n  fit_resamples(\n    resamples = rsmpl,\n    metrics = metric_set(accuracy, f_meas, roc_auc),\n    control = control_grid(verbose = TRUE))\ntoc()\n\n25.238 sec elapsed\n\nbeep()\n\n\nwf1_fit |&gt; collect_metrics()\n\n# A tibble: 3 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.656     5 0.00726 Preprocessor1_Model1\n2 f_meas   binary     0.129     5 0.0156  Preprocessor1_Model1\n3 roc_auc  binary     0.593     5 0.00947 Preprocessor1_Model1\n\n\nBester Fold:\n\nshow_best(wf1_fit)\n\n# A tibble: 1 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.656     5 0.00726 Preprocessor1_Model1"
  },
  {
    "objectID": "posts/germeval06/germeval06.html#fit",
    "href": "posts/germeval06/germeval06.html#fit",
    "title": "germeval06",
    "section": "Fit",
    "text": "Fit\n\ntic()\nfit1 &lt;- \n  wf1 |&gt; \n  fit(data = d_train)\ntoc()\n\n5.413 sec elapsed"
  },
  {
    "objectID": "posts/germeval06/germeval06.html#test-set-güte",
    "href": "posts/germeval06/germeval06.html#test-set-güte",
    "title": "germeval06",
    "section": "Test-Set-Güte",
    "text": "Test-Set-Güte\nVorhersagen im Test-Set:\n\ntic()\npreds &lt;-\n  predict(fit1, new_data = germeval_test)\ntoc()\n\n2.356 sec elapsed\n\n\nUnd die Vorhersagen zum Test-Set hinzufügen, damit man TRUTH und ESTIMATE vergleichen kann:\n\nd_test &lt;-\n  germeval_test |&gt; \n  bind_cols(preds) |&gt; \n  mutate(c1 = as.factor(c1))\n\n\nmy_metrics &lt;- metric_set(accuracy, f_meas)\nmy_metrics(d_test,\n           truth = c1,\n           estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.652\n2 f_meas   binary         0.138"
  },
  {
    "objectID": "posts/germeval06/germeval06.html#fazit",
    "href": "posts/germeval06/germeval06.html#fazit",
    "title": "germeval06",
    "section": "Fazit",
    "text": "Fazit\nglove6b ist für die englische Sprache vorgekocht. Das macht wenig Sinn für einen deutschsprachigen Corpus.\n\nCategories:\n\ntextmining\ndatawrangling\ngermeval\nprediction\ntidymodels\nwordvec\nstring"
  },
  {
    "objectID": "posts/Typ-Fehler-R-03/Typ-Fehler-R-03.html",
    "href": "posts/Typ-Fehler-R-03/Typ-Fehler-R-03.html",
    "title": "Typ-Fehler-R-03",
    "section": "",
    "text": "Aufgabe\nGegeben sei diese Syntax:\n\nx &lt;- 42\nY &lt;- 1\n\nLässt man folgende Syntax laufen, so kommt eine Fehlermeldung:\n\nX + Y\n\nError in eval(expr, envir, enclos): object 'X' not found\n\n\nGeben Sie die korrekte Syntax ein (zur Berechnung der Summe), die nicht zu einer Fehlermeldung führt!\nBitte verwenden Sie keine Leerzeichen bei Ihrer Eingabe.\n         \n\n\nLösung\n\nx+Y\n\n[1] 43\n\n\nDie Antwort lautet: x+Y.\n\nCategories:\nstring"
  },
  {
    "objectID": "posts/mtcars-rope1/mtcars-rope1.html",
    "href": "posts/mtcars-rope1/mtcars-rope1.html",
    "title": "mtcars-rope1",
    "section": "",
    "text": "Exercise\nIm Datensatz mtcars: Ist der (mittlere) Unterschied im Spritverbrauch zwischen den beiden Stufen von vs vernachlässigbar klein?\nDefinieren Sie “vernachlässigbar klein” mit “höchstens eine Meile”.\n\nGeben Sie die Breite des 95% PI an (im Bezug zur gesuchten Größe).\nGeben Sie das 95% HDI an (im Bezug zur gesuchten Größe).\nIm Hinblick auf die Rope-Methode: Ist der Unterschied vernachlässigbar klein? (ja/nein/unentschieden)\n\nHinweise:\n\nVerwenden Sie ansonsten die Standardwerte (Defaults) der typischen (im Unterricht verwendeten) R-Funktionen.\nRunden Sie auf 2 Dezimalstellen.\nVerwenden Sie Methoden der Bayes-Statistik.\n\n         \n\n\nSolution\nSetup:\n\ndata(mtcars)\nlibrary(tidyverse)\nlibrary(rstanarm)\nlibrary(bayesplot)  # Histogramm-Plots für Post-Vert.\nlibrary(bayestestR)  # rope\n\nModell berechnen:\n\nm1 &lt;- stan_glm(mpg ~ vs, data = mtcars,\n               refresh = 0)\n\n\ncoef(m1)\n\n(Intercept)          vs \n  16.595116    7.992921 \n\n\nzu a)\n95%-PI:\n\npost_m1_vs &lt;- posterior_interval(m1, prob = .95,\n                   pars = \"vs\")\npost_m1_vs[1]\n\n[1] 4.684142\n\npost_m1_vs[2]\n\n[1] 11.49721\n\n\nBreite des Intervalls:\n\nbreite &lt;- post_m1_vs[2] - post_m1_vs[1]\nbreite &lt;- breite %&gt;% round(2)\nbreite\n\n[1] 6.81\n\n\nDie Antwort für a) lautet also 6.81.\n\nmcmc_areas(m1)\n\n\n\n\n\n\n\n\nzu b)\nWir nutzen den Befehl hdi() aus {bayestestR}.\n\nhdi(m1)\n\nHighest Density Interval\n\nParameter   |        95% HDI\n----------------------------\n(Intercept) | [14.30, 18.81]\nvs          | [ 4.84, 11.59]\n\n\nMit dem Schalter ci = .89 bekäme man bspw. ein 89%-Intervall (s. Hilfe für den Befehl).\n“hdi” und “hdpi” und “hpdi” sind synonym.\n\nggplot(mtcars) +\n  aes(x = vs, y = mpg) +\n  geom_point()+\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nzu c)\n\nrope(m1,range = c(-1,1))\n\n# Proportion of samples inside the ROPE [-1.00, 1.00]:\n\nParameter   | inside ROPE\n-------------------------\n(Intercept) |      0.00 %\nvs          |      0.00 %\n\n\n\nplot(rope(m1, range = c(-1,1)))\n\n\n\n\n\n\n\n\nWir verwerfen also die H0-Rope.\n\nCategories:\n\nbayes\nlm"
  },
  {
    "objectID": "posts/wskt-quiz12/wskt-quiz12.html",
    "href": "posts/wskt-quiz12/wskt-quiz12.html",
    "title": "wskt-quiz12",
    "section": "",
    "text": "Behauptung: Der Likelihood \\(L\\) gibt die Wahrscheinlichkeit eines Ereignisses (einer Hypothese) an, gegeben der Daten.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n\n\n\nFalsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz12/wskt-quiz12.html#answerlist",
    "href": "posts/wskt-quiz12/wskt-quiz12.html#answerlist",
    "title": "wskt-quiz12",
    "section": "",
    "text": "Falsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz12/wskt-quiz12.html#answerlist-1",
    "href": "posts/wskt-quiz12/wskt-quiz12.html#answerlist-1",
    "title": "wskt-quiz12",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\nquiz\nprobability\nbayes\nquiz1-qm2-ws23\nschoice"
  },
  {
    "objectID": "posts/germeval-textfeatures01/germeval-textfeatures01.html",
    "href": "posts/germeval-textfeatures01/germeval-textfeatures01.html",
    "title": "germeval-textfeatures01",
    "section": "",
    "text": "Extrahieren Sie gängige Textfeatures - mit Hilfe des gleichnamigen R-Pakets - als Teil des Feature Engineering im Rahmen eines Tidymodels-Klassifikationsmodells.\nModellieren Sie dann mit einem einfachen linearen Modell die abhängige Variable.\nVerwenden Sie diesen Datensatz:\nDie AV ist c1.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks."
  },
  {
    "objectID": "posts/germeval-textfeatures01/germeval-textfeatures01.html#setup",
    "href": "posts/germeval-textfeatures01/germeval-textfeatures01.html#setup",
    "title": "germeval-textfeatures01",
    "section": "Setup",
    "text": "Setup"
  },
  {
    "objectID": "posts/germeval-textfeatures01/germeval-textfeatures01.html#daten",
    "href": "posts/germeval-textfeatures01/germeval-textfeatures01.html#daten",
    "title": "germeval-textfeatures01",
    "section": "Daten",
    "text": "Daten\nc2 brauchen wir hier nicht:"
  },
  {
    "objectID": "posts/germeval-textfeatures01/germeval-textfeatures01.html#rezept",
    "href": "posts/germeval-textfeatures01/germeval-textfeatures01.html#rezept",
    "title": "germeval-textfeatures01",
    "section": "Rezept",
    "text": "Rezept\nRezept definieren:\nstep_mutate ergänzt für die erzeugte (mutierte) Variable automatisch eine Rolle im Rezept, nimmt sie also als Prädiktor auf.\nMal schauen:\n\n\n# A tibble: 1 × 6\n  number operation type        trained skip  id               \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;       &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;            \n1      1 step      textfeature FALSE   FALSE textfeature_OUeIy\n\n\nPreppen und backen:\n\n\n6.321 sec elapsed\n\n\n\n\n# A tibble: 6 × 29\n     id c1      textfeature_text_n_words textfeature_text_n_uq_words\n  &lt;int&gt; &lt;fct&gt;                      &lt;int&gt;                       &lt;int&gt;\n1     1 OTHER                         15                          15\n2     2 OTHER                         19                          19\n3     3 OTHER                         11                          10\n4     4 OTHER                         19                          18\n5     5 OFFENSE                       16                          16\n6     6 OTHER                         44                          39\n# ℹ 25 more variables: textfeature_text_n_charS &lt;int&gt;,\n#   textfeature_text_n_uq_charS &lt;int&gt;, textfeature_text_n_digits &lt;int&gt;,\n#   textfeature_text_n_hashtags &lt;int&gt;, textfeature_text_n_uq_hashtags &lt;int&gt;,\n#   textfeature_text_n_mentions &lt;int&gt;, textfeature_text_n_uq_mentions &lt;int&gt;,\n#   textfeature_text_n_commas &lt;int&gt;, textfeature_text_n_periods &lt;int&gt;,\n#   textfeature_text_n_exclaims &lt;int&gt;, textfeature_text_n_extraspaces &lt;int&gt;,\n#   textfeature_text_n_caps &lt;int&gt;, textfeature_text_n_lowers &lt;int&gt;, …\n\n\nFolgende Spalten/Features hat step_textfeatures extrahiert:\n\n\n [1] \"id\"                              \"c1\"                             \n [3] \"textfeature_text_n_words\"        \"textfeature_text_n_uq_words\"    \n [5] \"textfeature_text_n_charS\"        \"textfeature_text_n_uq_charS\"    \n [7] \"textfeature_text_n_digits\"       \"textfeature_text_n_hashtags\"    \n [9] \"textfeature_text_n_uq_hashtags\"  \"textfeature_text_n_mentions\"    \n[11] \"textfeature_text_n_uq_mentions\"  \"textfeature_text_n_commas\"      \n[13] \"textfeature_text_n_periods\"      \"textfeature_text_n_exclaims\"    \n[15] \"textfeature_text_n_extraspaces\"  \"textfeature_text_n_caps\"        \n[17] \"textfeature_text_n_lowers\"       \"textfeature_text_n_urls\"        \n[19] \"textfeature_text_n_uq_urls\"      \"textfeature_text_n_nonasciis\"   \n[21] \"textfeature_text_n_puncts\"       \"textfeature_text_politeness\"    \n[23] \"textfeature_text_first_person\"   \"textfeature_text_first_personp\" \n[25] \"textfeature_text_second_person\"  \"textfeature_text_second_personp\"\n[27] \"textfeature_text_third_person\"   \"textfeature_text_to_be\"         \n[29] \"textfeature_text_prepositions\""
  },
  {
    "objectID": "posts/germeval-textfeatures01/germeval-textfeatures01.html#model",
    "href": "posts/germeval-textfeatures01/germeval-textfeatures01.html#model",
    "title": "germeval-textfeatures01",
    "section": "Model",
    "text": "Model"
  },
  {
    "objectID": "posts/germeval-textfeatures01/germeval-textfeatures01.html#workflow",
    "href": "posts/germeval-textfeatures01/germeval-textfeatures01.html#workflow",
    "title": "germeval-textfeatures01",
    "section": "Workflow",
    "text": "Workflow"
  },
  {
    "objectID": "posts/germeval-textfeatures01/germeval-textfeatures01.html#fit",
    "href": "posts/germeval-textfeatures01/germeval-textfeatures01.html#fit",
    "title": "germeval-textfeatures01",
    "section": "Fit",
    "text": "Fit\n\n\n5.78 sec elapsed\n\n\n\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_textfeature()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n                    (Intercept)         textfeature_text_n_words  \n                        1.50724                          0.05024  \n    textfeature_text_n_uq_words         textfeature_text_n_charS  \n                       -0.05456                         -1.08311  \n    textfeature_text_n_uq_charS        textfeature_text_n_digits  \n                       -0.01885                          1.12019  \n    textfeature_text_n_hashtags   textfeature_text_n_uq_hashtags  \n                        0.43821                         -0.30226  \n    textfeature_text_n_mentions   textfeature_text_n_uq_mentions  \n                       -0.08228                          0.14038  \n      textfeature_text_n_commas       textfeature_text_n_periods  \n                        1.23295                          1.07770  \n    textfeature_text_n_exclaims   textfeature_text_n_extraspaces  \n                        0.79465                         -0.20735  \n        textfeature_text_n_caps        textfeature_text_n_lowers  \n                        1.04501                          1.08349  \n        textfeature_text_n_urls       textfeature_text_n_uq_urls  \n                             NA                               NA  \n   textfeature_text_n_nonasciis        textfeature_text_n_puncts  \n                             NA                          1.09470  \n    textfeature_text_politeness    textfeature_text_first_person  \n                             NA                               NA  \n textfeature_text_first_personp   textfeature_text_second_person  \n                             NA                               NA  \ntextfeature_text_second_personp    textfeature_text_third_person  \n                             NA                               NA  \n         textfeature_text_to_be    textfeature_text_prepositions  \n                             NA                               NA  \n\nDegrees of Freedom: 5008 Total (i.e. Null);  4992 Residual\nNull Deviance:      6402 \nResidual Deviance: 6100     AIC: 6134"
  },
  {
    "objectID": "posts/germeval-textfeatures01/germeval-textfeatures01.html#test-set-güte",
    "href": "posts/germeval-textfeatures01/germeval-textfeatures01.html#test-set-güte",
    "title": "germeval-textfeatures01",
    "section": "Test-Set-Güte",
    "text": "Test-Set-Güte\nVorhersagen im Test-Set:\n\n\n2.28 sec elapsed\n\n\nUnd die Vorhersagen zum Test-Set hinzufügen, damit man TRUTH und ESTIMATE vergleichen kann:\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary        0.673 \n2 kap      binary        0.0800"
  },
  {
    "objectID": "posts/germeval-textfeatures01/germeval-textfeatures01.html#baseline",
    "href": "posts/germeval-textfeatures01/germeval-textfeatures01.html#baseline",
    "title": "germeval-textfeatures01",
    "section": "Baseline",
    "text": "Baseline\nEin einfaches Referenzmodell ist, einfach die häufigste Kategorie vorherzusagen:\n\n\n# A tibble: 2 × 2\n  c1          n\n  &lt;chr&gt;   &lt;int&gt;\n1 OFFENSE  1688\n2 OTHER    3321\n\n\n\nCategories:\n\ntidymodels\ntextmining\nprediction\nsentimentanalysis\ngermeval\nstring"
  },
  {
    "objectID": "posts/stan_glm_prioriwerte/stan_glm_prioriwerte.html",
    "href": "posts/stan_glm_prioriwerte/stan_glm_prioriwerte.html",
    "title": "stan_glm_prioriwerte",
    "section": "",
    "text": "Berechnet man eine Posteriori-Verteilung mit stan_glm(), so kann man entweder die schwach informativen Prioriwerte der Standardeinstellung verwenden, oder selber Prioriwerte definieren.\nBetrachten Sie dazu dieses Modell:\nstan_glm(price ~ cut, data = diamonds, \n                   prior = normal(location = c(100, 100, 100, 100),\n                                  scale = c(10, 10, 10, 10)),\n                   prior_intercept = normal(3000, 500))\nBeziehen Sie sich auf den Datensatz diamonds.\nHinweise:\n\nGehen Sie davon aus, dass die Post-Verteilung von Intercept und Gruppeneffekte normalverteilt sind.\n\nWelche Aussage dazu passt (am besten)?\n\n\n\nEs wird für (genau) einen Parameter eine Priori-Verteilung definiert.\nFür das Regressionsgewicht \\(\\beta_1\\) sind negative Werte apriori plausibel.\nMit prior = normal() werden Gruppenmittelwerte definiert.\nAlle Parameter des Modells sind normalverteilt."
  },
  {
    "objectID": "posts/stan_glm_prioriwerte/stan_glm_prioriwerte.html#answerlist",
    "href": "posts/stan_glm_prioriwerte/stan_glm_prioriwerte.html#answerlist",
    "title": "stan_glm_prioriwerte",
    "section": "",
    "text": "Es wird für (genau) einen Parameter eine Priori-Verteilung definiert.\nFür das Regressionsgewicht \\(\\beta_1\\) sind negative Werte apriori plausibel.\nMit prior = normal() werden Gruppenmittelwerte definiert.\nAlle Parameter des Modells sind normalverteilt."
  },
  {
    "objectID": "posts/stan_glm_prioriwerte/stan_glm_prioriwerte.html#answerlist-1",
    "href": "posts/stan_glm_prioriwerte/stan_glm_prioriwerte.html#answerlist-1",
    "title": "stan_glm_prioriwerte",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch. Es gibt mehrere Parameter im Modell (Achsenabschnitt, 4 Prädiktoren, sigma)\nWahr. Für cutGood sind negative Werte plausibel.\nFalsch. prior = normal() werden Regressionskoeffizienten in ihren Prioris definiert.\nFalsch. sigma ist in Stans Voreinstellung exponentialverteilt.\n\n\nCategories:\n\nbayes\nregression"
  },
  {
    "objectID": "posts/step-dummy/step-dummy.html",
    "href": "posts/step-dummy/step-dummy.html",
    "title": "step-dummy",
    "section": "",
    "text": "Viele Lernalgorithmen können nicht mit nominalen Variablen umgehen; daher muss man sie dummifizieren, um sie einer Verarbeitung zugänglich zu machen. In Tidymodels gibt es dafür step_dummy().\nAber bezieht step_dummy() nur Variablen vom Typ factor ein oder auch Variablen vom Typ character? Oder vielleicht weder noch?\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n\n\n\nNur Variablen vom Typ factor\nNur Variablen vom Typ character\nSowohl Variablen vom Typ factor als auch vom Typ character\nWeder Variablen vom Typ factor noch vom Typ character"
  },
  {
    "objectID": "posts/step-dummy/step-dummy.html#answerlist",
    "href": "posts/step-dummy/step-dummy.html#answerlist",
    "title": "step-dummy",
    "section": "",
    "text": "Nur Variablen vom Typ factor\nNur Variablen vom Typ character\nSowohl Variablen vom Typ factor als auch vom Typ character\nWeder Variablen vom Typ factor noch vom Typ character"
  },
  {
    "objectID": "posts/step-dummy/step-dummy.html#setup",
    "href": "posts/step-dummy/step-dummy.html#setup",
    "title": "step-dummy",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidymodels)\n\nDaten:\n\nd &lt;-\n  data.frame(\n    y = c(1,2,3,4,5),\n    x = c(\"A\", \"B\", \"B\", \"C\", \"A\")\n  )\n\nstr(d)\n\n'data.frame':   5 obs. of  2 variables:\n $ y: num  1 2 3 4 5\n $ x: chr  \"A\" \"B\" \"B\" \"C\" ..."
  },
  {
    "objectID": "posts/step-dummy/step-dummy.html#rezept-1",
    "href": "posts/step-dummy/step-dummy.html#rezept-1",
    "title": "step-dummy",
    "section": "Rezept 1",
    "text": "Rezept 1\nRezept 1, mit Variable vom Typ character:\n\nrec &lt;-\n  recipe(y ~ x, data = d) %&gt;% \n  step_dummy(x)\n\nd_baked &lt;- rec %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL)\n\nstr(d_baked)\n\ntibble [5 × 3] (S3: tbl_df/tbl/data.frame)\n $ y  : num [1:5] 1 2 3 4 5\n $ x_B: num [1:5] 0 1 1 0 0\n $ x_C: num [1:5] 0 0 0 1 0"
  },
  {
    "objectID": "posts/step-dummy/step-dummy.html#rezept-2",
    "href": "posts/step-dummy/step-dummy.html#rezept-2",
    "title": "step-dummy",
    "section": "Rezept 2",
    "text": "Rezept 2\nRezept 2, mit Variable vom Typ factor:\nDaten:\n\nd2 &lt;-\n  data.frame(\n    y = c(1,2,3,4,5),\n    x = factor(c(\"A\", \"B\", \"B\", \"C\", \"A\"))\n  )\n\nstr(d2)\n\n'data.frame':   5 obs. of  2 variables:\n $ y: num  1 2 3 4 5\n $ x: Factor w/ 3 levels \"A\",\"B\",\"C\": 1 2 2 3 1\n\n\n\nrec2 &lt;-\n  recipe(y ~ x, data = d2) %&gt;% \n  step_dummy(x)\n\nd_baked2 &lt;- rec2 %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL)\n\nstr(d_baked2)\n\ntibble [5 × 3] (S3: tbl_df/tbl/data.frame)\n $ y  : num [1:5] 1 2 3 4 5\n $ x_B: num [1:5] 0 1 1 0 0\n $ x_C: num [1:5] 0 0 0 1 0"
  },
  {
    "objectID": "posts/step-dummy/step-dummy.html#answerlist-1",
    "href": "posts/step-dummy/step-dummy.html#answerlist-1",
    "title": "step-dummy",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nWahr. step_dummy transformiert beide Arten von Variablen\nFalsch\n\n\nCategories:\n\ntidymodels\nstatlearning\nschoice"
  },
  {
    "objectID": "posts/Pfad/Pfad.html",
    "href": "posts/Pfad/Pfad.html",
    "title": "Pfad",
    "section": "",
    "text": "Aufgabe\nRecherchieren Sie den Datensatz “Palmer Penguins” als CSV-Datei im Internet.\n\nImportieren Sie die Datendatei in R von einer geeigneten Online-Quelle.\nLaden Sie die Datendatei herunter, speichern Sie Sie in den Ordner Ihres aktuellen RStudio-Projekts. Dann importieren Sie die Datendatei in R von diesem Ort.\n\n         \n\n\nLösung\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nAd 1)\n\npenguins_url &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\"\n\nd &lt;- read_csv(penguins_url)\n\n\n\n\n\n\n\n\n\nrownames\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n1\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\n2\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\n4\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n2007\n\n\n5\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\n6\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007\n\n\n\n\n\n\n\nAlternativ (hier aber nicht verlangt) können Sie den Datensatz penguins auch über ein R-Paket beziehen:\n\ndata(penguins, package = \"palmerpenguins\")\n\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nSynonym:\n\nlibrary(palmerpenguins)\ndata(penguins)\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nAchtung:\nWenn Sie das Paket palmerpenguins nicht mit library() gestartet haben, dann wird data(penguins) nicht funktionieren.\nAd 2)\nWenn Sie die Datei heruntergeladen haben und in Ihrem (aktuellen) RStudio-Projektordner abgespeichert haben, dann (und nur dann) können Sie sie ohne Angabe eines Pfades in R importieren:\n\nd &lt;- read_csv(\"penguins.csv\")  # die Datei muss im aktuellen Verzeichnis liegen\n\n\nCategories:\n\nR\npath\ndatawrangling\nqm1\nqm2\nstring"
  },
  {
    "objectID": "posts/wskt-quiz08/wskt-quiz08.html",
    "href": "posts/wskt-quiz08/wskt-quiz08.html",
    "title": "wskt-quiz08",
    "section": "",
    "text": "Mehrere Proben werden zu einem unbekannten Planeten geschossen. Die Forschungsfrage lautet: Ist es die Erde (70% Wasseranteil) oder der Planet “Bath42” mit 90% Wasseranteil?\nWir sind indifferent (apriori) zu den Parameterwerten.\nDaten: 6 Treffer (Wasser) von 9 Versuchen (Proben).\nBehauptung: “Das ist fast sicher Bath42!”.\nIst die Wahrscheinlichkeit höher für Bath42 (als für die Erde)?\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n\n\n\nFalsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz08/wskt-quiz08.html#answerlist",
    "href": "posts/wskt-quiz08/wskt-quiz08.html#answerlist",
    "title": "wskt-quiz08",
    "section": "",
    "text": "Falsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz08/wskt-quiz08.html#answerlist-1",
    "href": "posts/wskt-quiz08/wskt-quiz08.html#answerlist-1",
    "title": "wskt-quiz08",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\nquiz\nprobability\nbayes\nquiz1-qm2-ws23\nschoice"
  },
  {
    "objectID": "posts/rf-finalize2/rf-finalize2.html",
    "href": "posts/rf-finalize2/rf-finalize2.html",
    "title": "rf-finalize2",
    "section": "",
    "text": "Aufgabe\n\nBerechnen Sie ein prädiktives Modell (Random Forest) mit dieser Modellgleichung:\nbody_mass_g ~ . (Datensatz: palmerpenguins::penguins).\nZeigen Sie, welche Werte für mtry im Default von Tidymodels gesetzt werden!\nHinweise: - Tunen Sie mtry - Verwenden Sie Kreuzvalidierung - Verwenden Sie Standardwerte, wo nicht anders angegeben. - Fixieren Sie Zufallszahlen auf den Startwert 42.\n         \n\n\nLösung\nZuererst der Standardablauf:\n\n# Setup:\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.0.8\n✔ dials        1.2.0     ✔ rsample      1.2.0\n✔ dplyr        1.1.3     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.4\n✔ lubridate 1.9.3     ✔ stringr   1.5.0\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tictoc)  # Zeitmessung\n\n\n# Data:\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\"\nd &lt;- read_csv(d_path)\n\nRows: 344 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): species, island, sex\ndbl (6): rownames, bill_length_mm, bill_depth_mm, flipper_length_mm, body_ma...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# rm NA in the dependent variable:\nd &lt;- d %&gt;% \n  drop_na(body_mass_g)\n\n\nset.seed(42)\nd_split &lt;- initial_split(d)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\n\n# model:\nmod_rf &lt;-\n  rand_forest(mode = \"regression\",\n           mtry = tune())\n\n\n# cv:\nset.seed(42)\nrsmpl &lt;- vfold_cv(d_train)\n\n\n# recipe:\nrec_plain &lt;- \n  recipe(body_mass_g ~  ., data = d_train) %&gt;% \n  step_impute_bag(all_predictors())\n\n\n# workflow:\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(mod_rf) %&gt;% \n  add_recipe(rec_plain)\n\n\n# tuning:\ntic()\nwf1_fit &lt;-\n  wf1 %&gt;% \n  tune_grid(\n    resamples = rsmpl)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\ntoc()\n\n22.142 sec elapsed\n\n\nDann schauen wir uns das Ergebnisobjekt vom Tuning an.\n\nwf1_fit %&gt;% \n  collect_metrics() %&gt;% \n  filter(.metric == \"rmse\") %&gt;% \n  arrange(mtry)\n\n# A tibble: 8 × 7\n   mtry .metric .estimator  mean     n std_err .config             \n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1     1 rmse    standard    309.    10   12.8  Preprocessor1_Model1\n2     2 rmse    standard    282.    10   11.1  Preprocessor1_Model5\n3     3 rmse    standard    282.    10   10.6  Preprocessor1_Model7\n4     4 rmse    standard    283.    10    9.95 Preprocessor1_Model4\n5     5 rmse    standard    283.    10    9.41 Preprocessor1_Model3\n6     6 rmse    standard    284.    10    9.95 Preprocessor1_Model6\n7     7 rmse    standard    283.    10    9.79 Preprocessor1_Model8\n8     8 rmse    standard    282.    10    9.84 Preprocessor1_Model2\n\n\nIn der Hilfe ist zu lesen:\n\nIf no tuning grid is provided, a semi-random grid (via dials::grid_latin_hypercube()) is created with 10 candidate parameter combinations.\n\nAus irgendwelchen Gründen wurden hier nur 10 Kandidatenwerte berechnet.\nWeiter steht dort:\n\nIn some cases, the tuning parameter values depend on the dimensions of the data. For example, mtry in random forest models depends on the number of predictors. In this case, the default tuning parameter object requires an upper range. dials::finalize() can be used to derive the data-dependent parameters. Otherwise, a parameter set can be created (via dials::parameters()) and the dials update() function can be used to change the values. This updated parameter set can be passed to the function via the param_info argument.\n\nAchtung: step_impute_knn scheint Probleme zu haben, wenn es Charakter-Variablen gibt.\n\nCategories:\n\ntidymodels\nstatlearning\ntemplate\nstring"
  },
  {
    "objectID": "posts/nasa04/nasa04.html",
    "href": "posts/nasa04/nasa04.html",
    "title": "nasa04",
    "section": "",
    "text": "Viele Quellen berichten Klimadaten unserer Erde, z.B. auch National Aeronautics and Space Administration - Goddard Institute for Space Studies.\nVon dieser Quelle beziehen wir diesen Datensatz.\nDie Datensatz sind auf der Webseite wie folgt beschrieben:\nTables of Global and Hemispheric Monthly Means and Zonal Annual Means\nCombined Land-Surface Air and Sea-Surface Water Temperature Anomalies (Land-Ocean Temperature Index, L-OTI)\nThe following are plain-text files in tabular format of temperature anomalies, i.e. deviations from the corresponding 1951-1980 means.\n\nGlobal-mean monthly, seasonal, and annual means, 1880-present, updated through most recent month: TXT, CSV\n\nStarten Sie zunächst das R-Paket tidyverse falls noch nicht geschehen.\n\nlibrary(tidyverse)\n\nZum Animieren verwenden wir diese Pakete:\n\nlibrary(gganimate)\nlibrary(plotly)\n\nImportieren Sie dann die Daten:\n\ndata_path &lt;- \"https://data.giss.nasa.gov/gistemp/tabledata_v4/GLB.Ts+dSST.csv\"\nd &lt;- read_csv(data_path, skip = 1)\n\nWir lassen die 1. Zeile des Datensatzes aus (Argument skip), da dort Metadaten stehen, also keine Daten, sondern Informationen (Daten) zu den eigentlichen Daten.\nAufgaben\n\nVisualisieren Sie Temperatur pro Jahr und Dekade.\nBONUSAUFGABE: Animieren Sie Ihre Diagramme mittels gganimate.\n\nHinweise:\n\nSie müssen zuerst die Dekade als neue Spalte berechnen."
  },
  {
    "objectID": "posts/nasa04/nasa04.html#daten-aufbereiten",
    "href": "posts/nasa04/nasa04.html#daten-aufbereiten",
    "title": "nasa04",
    "section": "Daten aufbereiten",
    "text": "Daten aufbereiten\nDekade berechnen:\n\nd &lt;-\n  d %&gt;% \n  mutate(decade = round(Year/10))\n\nDie Temperaturdaten der Monate April bis Dezember sind als Textdaten (character) formatiert. Aber wir brauchen Zahlen zum Rechne. Daher müssen wir noch in Zahlen umwandeln:\n\nd2 &lt;-\n  d %&gt;% \n  select(Year:Dec) %&gt;% \n  mutate(across(Apr:Dec, as.numeric))\n\nIns lange Format umwandeln:\n\nd3 &lt;- \n  d2 %&gt;% \n  pivot_longer(-Year, \n               values_to = \"temp\", \n               names_to = \"month\")\n\nDie Monate sind wie folgt bezeichnet in den Daten:\n\nmonth_vec &lt;- c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\",\n               \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\",\n               \"Nov\", \"Dec\")\n\n\nmonths &lt;-\n  tibble(\n    month = month_vec,\n    month_num = 1:12\n  )\n\nUnd dann die Monatsnummer (1-12) zu den Daten (d3) hinzufügen. Das geht komfortabel mit einem Join:\n\nd4 &lt;- \n  d3 %&gt;% \n  full_join(months)"
  },
  {
    "objectID": "posts/nasa04/nasa04.html#daten-zusammenfassen",
    "href": "posts/nasa04/nasa04.html#daten-zusammenfassen",
    "title": "nasa04",
    "section": "Daten zusammenfassen",
    "text": "Daten zusammenfassen\nStatistiken pro Dekade für Januar:\n\nd_summarized &lt;- \n  d %&gt;% \n  group_by(decade) %&gt;% \n  summarise(temp_mean = mean(Jan),\n            temp_sd = sd(Jan))\n\nd_summarized"
  },
  {
    "objectID": "posts/nasa04/nasa04.html#statisches-diagramm",
    "href": "posts/nasa04/nasa04.html#statisches-diagramm",
    "title": "nasa04",
    "section": "Statisches Diagramm",
    "text": "Statisches Diagramm\nZur Veranschaulichung visualisieren wir die Ergebnisse:\n\nd_summarized %&gt;% \n  pivot_longer(-decade) %&gt;% \n  ggplot(aes(x = decade, y = value)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~ name)\n\n\n\n\n\n\n\n\nAlternativ können Sie zum Visualisieren der Daten z.B. das Paket ggpubr nutzen:\n\nlibrary(ggpubr)\nggscatter(d_summarized, x = \"decade\", y = \"temp_mean\", add = \"reg.line\")"
  },
  {
    "objectID": "posts/nasa04/nasa04.html#animiertes-diagramm-pro-dekade",
    "href": "posts/nasa04/nasa04.html#animiertes-diagramm-pro-dekade",
    "title": "nasa04",
    "section": "Animiertes Diagramm pro Dekade",
    "text": "Animiertes Diagramm pro Dekade\nMit Punken:\n\np1 &lt;- \n  d_summarized %&gt;% \n  ggplot(aes(x = decade, y = temp_mean)) +\n  geom_point(aes(group = seq_along(decade))) \n\np1 + transition_reveal(decade) \n\n\n\n\n\n\n\n\nMit Linie:\n\np2 &lt;- ggplot(d_summarized,\n            aes(x = decade, temp_mean)) +\n  geom_line()\np2 + transition_reveal(decade)"
  },
  {
    "objectID": "posts/nasa04/nasa04.html#animiertes-diagramm-pro-jahr",
    "href": "posts/nasa04/nasa04.html#animiertes-diagramm-pro-jahr",
    "title": "nasa04",
    "section": "Animiertes Diagramm pro Jahr",
    "text": "Animiertes Diagramm pro Jahr\n\nd3 %&gt;% \n  ggplot(aes(x = Year, y = temp)) +\n  geom_line() + \n  transition_reveal(Year)"
  },
  {
    "objectID": "posts/nasa04/nasa04.html#statisches-diagramm-für-alle-monate",
    "href": "posts/nasa04/nasa04.html#statisches-diagramm-für-alle-monate",
    "title": "nasa04",
    "section": "Statisches Diagramm für alle Monate",
    "text": "Statisches Diagramm für alle Monate\n\np3 &lt;- d3 %&gt;% \n  ggplot(aes(x = Year, y = temp, color = month, group = month)) +\n  geom_line()"
  },
  {
    "objectID": "posts/nasa04/nasa04.html#animiertes-diagramm-für-alle-monate",
    "href": "posts/nasa04/nasa04.html#animiertes-diagramm-für-alle-monate",
    "title": "nasa04",
    "section": "Animiertes Diagramm für alle Monate",
    "text": "Animiertes Diagramm für alle Monate\n\np3 + transition_reveal(Year)"
  },
  {
    "objectID": "posts/nasa04/nasa04.html#fazit",
    "href": "posts/nasa04/nasa04.html#fazit",
    "title": "nasa04",
    "section": "Fazit",
    "text": "Fazit\nFalls Sie Teile der R-Syntax nicht kennen: Machen Sie sich nichts daraus. Be happy 😄\n\nCategories:\n\ndata\neda\nlagemaße\nvis\nanimation\nstring"
  },
  {
    "objectID": "posts/Regression6/Regression6.html",
    "href": "posts/Regression6/Regression6.html",
    "title": "Regression6",
    "section": "",
    "text": "Gegeben sei ein Datensatz mit folgenden Prädiktoren, wobei Studierende die Beobachtungseinheit darstellen:\n\n\\(X_1\\): Muttersprachler (0: nein, 1: ja)\n\\(X_2\\): Letzte Mathenote (z-Wert)\n\\(X_3\\): Matheanteil im Studium (z-Wert)\n\\(X_4\\): Interaktion von \\(X1\\) und \\(X2\\)\n\nDie vorherzusagende Variable (\\(Y\\); Kriterium) ist Gehalt nach Studienabschluss.\nFolgende Modellparameter einer Regression (Least Squares, mit lm()) seien gegeben:\n\n\\(\\beta_0: 30\\)\n\\(\\beta_1: 10\\)\n\\(\\beta_2: 5\\)\n\\(\\beta_3: 1\\)\n\\(\\beta_4: 1\\)\n\nWelche der Aussagen ist korrekt?\n\n\n\nFür einen bestimmten (festen) Wert von \\(X_2=\\) Letzte Mathenote (z-Wert) und \\(X_3=\\) Matheanteil im Studium (z-Wert) gilt, dass das erwartete Gehalt im Mittel höher ist bei \\(X_1=1\\) im Vergleich zu \\(X_1=0\\), laut dem Modell.\nFür einen bestimmten (festen) Wert von \\(X_2\\)= Letzte Mathenote (z-Wert) und \\(X_3\\)= Matheanteil im Studium (z-Wert) gilt, dass das erwartete Gehalt im Mittel höher ist bei \\(X_1=0\\) im Vergleich zu \\(X_1=1\\), laut dem Modell.\nDer mittlere erwartete Gehaltsunterschied \\(Y\\) zweier Personen \\(a\\) und \\(b\\), wobei bei Person \\(a\\) gilt \\(X_1=0\\) und bei Person \\(b\\) gilt \\(X_1=1\\), beträgt stets \\(30\\), laut dem Modell.\nDer mittlere erwartete Gehaltsunterschied \\(Y\\) zweier Personen \\(a\\) und \\(b\\), wobei bei Person \\(a\\) gilt \\(X_2=0\\) und bei Person \\(b\\) gilt \\(X_2=1\\), beträgt stets \\(30\\), laut dem Modell.\nDer mittlere erwartete Gehaltsunterschied von Menschen ist eine Wirkung von genau drei Ursachen: Muttersprachler (0: nein, 1: ja), Letzte Mathenote (z-Wert), Matheanteil im Studium (z-Wert), laut dem Modell."
  },
  {
    "objectID": "posts/Regression6/Regression6.html#answerlist",
    "href": "posts/Regression6/Regression6.html#answerlist",
    "title": "Regression6",
    "section": "",
    "text": "Für einen bestimmten (festen) Wert von \\(X_2=\\) Letzte Mathenote (z-Wert) und \\(X_3=\\) Matheanteil im Studium (z-Wert) gilt, dass das erwartete Gehalt im Mittel höher ist bei \\(X_1=1\\) im Vergleich zu \\(X_1=0\\), laut dem Modell.\nFür einen bestimmten (festen) Wert von \\(X_2\\)= Letzte Mathenote (z-Wert) und \\(X_3\\)= Matheanteil im Studium (z-Wert) gilt, dass das erwartete Gehalt im Mittel höher ist bei \\(X_1=0\\) im Vergleich zu \\(X_1=1\\), laut dem Modell.\nDer mittlere erwartete Gehaltsunterschied \\(Y\\) zweier Personen \\(a\\) und \\(b\\), wobei bei Person \\(a\\) gilt \\(X_1=0\\) und bei Person \\(b\\) gilt \\(X_1=1\\), beträgt stets \\(30\\), laut dem Modell.\nDer mittlere erwartete Gehaltsunterschied \\(Y\\) zweier Personen \\(a\\) und \\(b\\), wobei bei Person \\(a\\) gilt \\(X_2=0\\) und bei Person \\(b\\) gilt \\(X_2=1\\), beträgt stets \\(30\\), laut dem Modell.\nDer mittlere erwartete Gehaltsunterschied von Menschen ist eine Wirkung von genau drei Ursachen: Muttersprachler (0: nein, 1: ja), Letzte Mathenote (z-Wert), Matheanteil im Studium (z-Wert), laut dem Modell."
  },
  {
    "objectID": "posts/Regression6/Regression6.html#answerlist-1",
    "href": "posts/Regression6/Regression6.html#answerlist-1",
    "title": "Regression6",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr. \\(X_1\\) ist positiv. Daher hat Gruppe \\(X_1=1\\) höhere erwartete Werte als \\(X_1=0\\).\nFalsch. Diese Option sagt das Gegenteil wie die fast gleich lautende (aber richtige) Antwortoption.\nFalsch. Der Unterschied in der AV ist von mehreren UV abhängig. Bei Kenntnis des Wertes nur einer UV kann nicht sicher auf den erwarteten Wert der AV geschlossen werden.\nFalsch. Der Unterschied in der AV ist von mehreren UV abhängig. Bei Kenntnis des Wertes nur einer UV kann nicht sicher auf den erwarteten Wert der AV geschlossen werden.\nFalsch. Ein Regressionsmodell ist nicht automatisch ein Kausalmodell.\n\n\nCategories:\n\ndyn\nregression\nexam-22\nschoice"
  },
  {
    "objectID": "posts/wfsets1/wfsets1.html",
    "href": "posts/wfsets1/wfsets1.html",
    "title": "wfsets1",
    "section": "",
    "text": "Berechnen Sie die Vorhersagegüte (RMSE) für folgende Lernalgorithmen mittesl tidymodels:\n\nlineares Modell\n\nModellgleichung: body_mass_g ~ bill_length_mm, data = d_train.\nNutzen Sie minimale Vorverarbeitung im Rahmen zweier Rezepte.\nNutzen Sie ein Workflowset."
  },
  {
    "objectID": "posts/wfsets1/wfsets1.html#setup",
    "href": "posts/wfsets1/wfsets1.html#setup",
    "title": "wfsets1",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidymodels)\nlibrary(tictoc)  # Zeitmessung\ndata(penguins, package = \"palmerpenguins\")"
  },
  {
    "objectID": "posts/wfsets1/wfsets1.html#daten",
    "href": "posts/wfsets1/wfsets1.html#daten",
    "title": "wfsets1",
    "section": "Daten",
    "text": "Daten\n\nd &lt;-\n  penguins %&gt;% \n  drop_na()\n\n\nd_split &lt;- initial_split(d)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)"
  },
  {
    "objectID": "posts/wfsets1/wfsets1.html#modelle",
    "href": "posts/wfsets1/wfsets1.html#modelle",
    "title": "wfsets1",
    "section": "Modelle",
    "text": "Modelle\nLineares Modell:\n\nmod_lin &lt;- linear_reg()\n\nmod_knn &lt;- nearest_neighbor(mode = \"regression\",\n                                  neighbors = tune())"
  },
  {
    "objectID": "posts/wfsets1/wfsets1.html#rezepte",
    "href": "posts/wfsets1/wfsets1.html#rezepte",
    "title": "wfsets1",
    "section": "Rezepte",
    "text": "Rezepte\n\nrec_basic &lt;- recipe(body_mass_g ~ bill_length_mm, data = d_train) %&gt;% \n         step_normalize(all_predictors())\n\nrec_basic\n\n\nrec_plain &lt;- recipe(body_mass_g ~ bill_length_mm, data = d_train)"
  },
  {
    "objectID": "posts/wfsets1/wfsets1.html#resampling",
    "href": "posts/wfsets1/wfsets1.html#resampling",
    "title": "wfsets1",
    "section": "Resampling",
    "text": "Resampling\n\nrsmpls &lt;- vfold_cv(d_train, v = 5)"
  },
  {
    "objectID": "posts/wfsets1/wfsets1.html#workflow-set",
    "href": "posts/wfsets1/wfsets1.html#workflow-set",
    "title": "wfsets1",
    "section": "Workflow Set",
    "text": "Workflow Set\n\nwf_set &lt;-\n  workflow_set(\n    preproc = list(rec_simple = rec_basic,\n                   rec_plain = rec_plain),\n    models = list(mod_lm = mod_lin)\n  )\n\nwf_set\n\n# A workflow set/tibble: 2 × 4\n  wflow_id          info             option    result    \n  &lt;chr&gt;             &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 rec_simple_mod_lm &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 rec_plain_mod_lm  &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;"
  },
  {
    "objectID": "posts/wfsets1/wfsets1.html#fitten",
    "href": "posts/wfsets1/wfsets1.html#fitten",
    "title": "wfsets1",
    "section": "Fitten",
    "text": "Fitten\n\ntic()\nwf_fit &lt;-\n  wf_set %&gt;% \n  workflow_map(resamples = rsmpls)\ntoc()\n\n1.261 sec elapsed\n\nwf_fit\n\n# A workflow set/tibble: 2 × 4\n  wflow_id          info             option    result   \n  &lt;chr&gt;             &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n1 rec_simple_mod_lm &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n2 rec_plain_mod_lm  &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n\n\nCheck:\n\nwf_fit %&gt;% pluck(\"result\")\n\n[[1]]\n# Resampling results\n# 5-fold cross-validation \n# A tibble: 5 × 4\n  splits           id    .metrics         .notes          \n  &lt;list&gt;           &lt;chr&gt; &lt;list&gt;           &lt;list&gt;          \n1 &lt;split [199/50]&gt; Fold1 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n2 &lt;split [199/50]&gt; Fold2 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n3 &lt;split [199/50]&gt; Fold3 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n4 &lt;split [199/50]&gt; Fold4 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n5 &lt;split [200/49]&gt; Fold5 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n\n[[2]]\n# Resampling results\n# 5-fold cross-validation \n# A tibble: 5 × 4\n  splits           id    .metrics         .notes          \n  &lt;list&gt;           &lt;chr&gt; &lt;list&gt;           &lt;list&gt;          \n1 &lt;split [199/50]&gt; Fold1 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n2 &lt;split [199/50]&gt; Fold2 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n3 &lt;split [199/50]&gt; Fold3 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n4 &lt;split [199/50]&gt; Fold4 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n5 &lt;split [200/49]&gt; Fold5 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;"
  },
  {
    "objectID": "posts/wfsets1/wfsets1.html#bester-kandidat",
    "href": "posts/wfsets1/wfsets1.html#bester-kandidat",
    "title": "wfsets1",
    "section": "Bester Kandidat",
    "text": "Bester Kandidat\n\nautoplot(wf_fit)\n\n\n\n\n\n\n\n\n\nautoplot(wf_fit, select_best = TRUE)\n\n\n\n\n\n\n\n\n\ncollect_metrics(wf_fit)\n\n# A tibble: 4 × 9\n  wflow_id        .config preproc model .metric .estimator    mean     n std_err\n  &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 rec_simple_mod… Prepro… recipe  line… rmse    standard   655.        5 23.0   \n2 rec_simple_mod… Prepro… recipe  line… rsq     standard     0.357     5  0.0336\n3 rec_plain_mod_… Prepro… recipe  line… rmse    standard   655.        5 23.0   \n4 rec_plain_mod_… Prepro… recipe  line… rsq     standard     0.357     5  0.0336\n\n\n\nrank_results(wf_fit, rank_metric = \"rmse\") %&gt;% \n  filter(.metric == \"rmse\")\n\n# A tibble: 2 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 rec_simple_mod_lm Prepro… rmse     655.    23.0     5 recipe       line…     1\n2 rec_plain_mod_lm  Prepro… rmse     655.    23.0     5 recipe       line…     2"
  },
  {
    "objectID": "posts/wfsets1/wfsets1.html#last-fit",
    "href": "posts/wfsets1/wfsets1.html#last-fit",
    "title": "wfsets1",
    "section": "Last Fit",
    "text": "Last Fit\n\nbest_wf &lt;-\n  wf_fit %&gt;% \n  extract_workflow(\"rec_simple_mod_lm\")\n\nFinalisieren müssen wir diesen Workflow nicht, da er keine Tuningparameter hatte.\n\nfit_final &lt;-\n  best_wf %&gt;% \n  last_fit(d_split)"
  },
  {
    "objectID": "posts/wfsets1/wfsets1.html#modellgüte-im-test-set",
    "href": "posts/wfsets1/wfsets1.html#modellgüte-im-test-set",
    "title": "wfsets1",
    "section": "Modellgüte im Test-Set",
    "text": "Modellgüte im Test-Set\n\ncollect_metrics(fit_final)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard     653.    Preprocessor1_Model1\n2 rsq     standard       0.341 Preprocessor1_Model1"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-13/Verteilungen-Quiz-13.html",
    "href": "posts/Verteilungen-Quiz-13/Verteilungen-Quiz-13.html",
    "title": "Verteilungen-Quiz-13",
    "section": "",
    "text": "Ist folgende Aussage wahr?\nBei rechtsschiefen Verteilungen gilt \\(\\bar{x} \\gt Md\\).\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-13/Verteilungen-Quiz-13.html#answerlist",
    "href": "posts/Verteilungen-Quiz-13/Verteilungen-Quiz-13.html#answerlist",
    "title": "Verteilungen-Quiz-13",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-13/Verteilungen-Quiz-13.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-13/Verteilungen-Quiz-13.html#answerlist-1",
    "title": "Verteilungen-Quiz-13",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/wskt-quiz06/wskt-quiz06.html",
    "href": "posts/wskt-quiz06/wskt-quiz06.html",
    "title": "wskt-quiz06",
    "section": "",
    "text": "Jemand geht zum Krebstest. Der Test habe eine Sicherheit von 95%. Die Grundrate des Krebs liege bei 0.001. Leider zeigt der Test einen positiven Befund, also Krebs.\nGilt: \\(Pr(K|T) = .95\\)?\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n\n\n\nFalsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz06/wskt-quiz06.html#answerlist",
    "href": "posts/wskt-quiz06/wskt-quiz06.html#answerlist",
    "title": "wskt-quiz06",
    "section": "",
    "text": "Falsch\nWahr"
  },
  {
    "objectID": "posts/wskt-quiz06/wskt-quiz06.html#answerlist-1",
    "href": "posts/wskt-quiz06/wskt-quiz06.html#answerlist-1",
    "title": "wskt-quiz06",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\nquiz\nprobability\nbayes\nquiz1-qm2-ws23\nschoice"
  },
  {
    "objectID": "posts/saratoga-cor2/saratoga-cor2.html",
    "href": "posts/saratoga-cor2/saratoga-cor2.html",
    "title": "saratoga-cor2",
    "section": "",
    "text": "Importieren Sie den Datensatz saratoga.\nBerechnen Sie dann den Zusammenhang zwischen price und livingArea pro Stufe von bedrooms.\nHinweise:\n\nBeachten Sie die Standardhinweise des Datenwerks."
  },
  {
    "objectID": "posts/saratoga-cor2/saratoga-cor2.html#setup",
    "href": "posts/saratoga-cor2/saratoga-cor2.html#setup",
    "title": "saratoga-cor2",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(ggpubr)\n\n\ndata(\"SaratogaHouses\", package = \"mosaicData\")"
  },
  {
    "objectID": "posts/saratoga-cor2/saratoga-cor2.html#gruppieren",
    "href": "posts/saratoga-cor2/saratoga-cor2.html#gruppieren",
    "title": "saratoga-cor2",
    "section": "Gruppieren",
    "text": "Gruppieren\n\nd2 &lt;-\n  SaratogaHouses |&gt; \n  group_by(bedrooms)"
  },
  {
    "objectID": "posts/saratoga-cor2/saratoga-cor2.html#statistiken",
    "href": "posts/saratoga-cor2/saratoga-cor2.html#statistiken",
    "title": "saratoga-cor2",
    "section": "Statistiken",
    "text": "Statistiken\n\nd2 |&gt; \n  summarise(korrelation = cor(livingArea, price))\n\n# A tibble: 7 × 2\n  bedrooms korrelation\n     &lt;int&gt;       &lt;dbl&gt;\n1        1       0.115\n2        2       0.510\n3        3       0.636\n4        4       0.687\n5        5       0.721\n6        6       0.882\n7        7       0.791"
  },
  {
    "objectID": "posts/saratoga-cor2/saratoga-cor2.html#visualisierung",
    "href": "posts/saratoga-cor2/saratoga-cor2.html#visualisierung",
    "title": "saratoga-cor2",
    "section": "Visualisierung",
    "text": "Visualisierung\n\nggscatter(d2, \n          x = \"livingArea\",\n          y = \"price\",\n          facet.by = \"bedrooms\",\n          add = \"reg.line\")"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-14/Verteilungen-Quiz-14.html",
    "href": "posts/Verteilungen-Quiz-14/Verteilungen-Quiz-14.html",
    "title": "Verteilungen-Quiz-14",
    "section": "",
    "text": "Ist folgende Aussage wahr?\nBeim Perzentilintervall (PI) werden “links” und “rechts” die gleiche Wahrscheinlichkeitsmasse von einer Verteilung “abgeschnitten”.\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-14/Verteilungen-Quiz-14.html#answerlist",
    "href": "posts/Verteilungen-Quiz-14/Verteilungen-Quiz-14.html#answerlist",
    "title": "Verteilungen-Quiz-14",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-14/Verteilungen-Quiz-14.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-14/Verteilungen-Quiz-14.html#answerlist-1",
    "title": "Verteilungen-Quiz-14",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/durch3-durch5/durch3-durch5.html",
    "href": "posts/durch3-durch5/durch3-durch5.html",
    "title": "durch3-durch5",
    "section": "",
    "text": "Aufgabe\nGegeben sei ein Vektor x:\n\nx &lt;- 1:50\n\nSchreiben Sie R-Code, der “durch3” ausgibt, wenn x durch 3 teilbar ist und (auch) “durch5” ausgibt, wenn x durch5 teilbar ist.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\n\n         \n\n\nLösung\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nd &lt;-\n  tibble(\n    x = 1:50,\n    txt = \"\"\n  ) %&gt;% \n  mutate(txt = case_when(\n    (x %% 3) == 0 ~ str_c(txt, \"_durch_3\"),\n    TRUE ~ txt)\n  ) %&gt;% \n  mutate(txt = case_when(\n    (x %% 5) == 0 ~ str_c(txt, \"_durch_5\"),\n    TRUE ~ txt\n  ))\n\nSo sieht die Tabelle aus:\n\nd\n\n# A tibble: 50 × 2\n       x txt       \n   &lt;int&gt; &lt;chr&gt;     \n 1     1 \"\"        \n 2     2 \"\"        \n 3     3 \"_durch_3\"\n 4     4 \"\"        \n 5     5 \"_durch_5\"\n 6     6 \"_durch_3\"\n 7     7 \"\"        \n 8     8 \"\"        \n 9     9 \"_durch_3\"\n10    10 \"_durch_5\"\n# ℹ 40 more rows\n\n\nSo kann man sich dann den Text ausgeben lassen:\n\nd$txt\n\n [1] \"\"                 \"\"                 \"_durch_3\"         \"\"                \n [5] \"_durch_5\"         \"_durch_3\"         \"\"                 \"\"                \n [9] \"_durch_3\"         \"_durch_5\"         \"\"                 \"_durch_3\"        \n[13] \"\"                 \"\"                 \"_durch_3_durch_5\" \"\"                \n[17] \"\"                 \"_durch_3\"         \"\"                 \"_durch_5\"        \n[21] \"_durch_3\"         \"\"                 \"\"                 \"_durch_3\"        \n[25] \"_durch_5\"         \"\"                 \"_durch_3\"         \"\"                \n[29] \"\"                 \"_durch_3_durch_5\" \"\"                 \"\"                \n[33] \"_durch_3\"         \"\"                 \"_durch_5\"         \"_durch_3\"        \n[37] \"\"                 \"\"                 \"_durch_3\"         \"_durch_5\"        \n[41] \"\"                 \"_durch_3\"         \"\"                 \"\"                \n[45] \"_durch_3_durch_5\" \"\"                 \"\"                 \"_durch_3\"        \n[49] \"\"                 \"_durch_5\"        \n\n\nOder so:\n\nd$txt %&gt;% discard(\\(x) x == \"\")\n\n [1] \"_durch_3\"         \"_durch_5\"         \"_durch_3\"         \"_durch_3\"        \n [5] \"_durch_5\"         \"_durch_3\"         \"_durch_3_durch_5\" \"_durch_3\"        \n [9] \"_durch_5\"         \"_durch_3\"         \"_durch_3\"         \"_durch_5\"        \n[13] \"_durch_3\"         \"_durch_3_durch_5\" \"_durch_3\"         \"_durch_5\"        \n[17] \"_durch_3\"         \"_durch_3\"         \"_durch_5\"         \"_durch_3\"        \n[21] \"_durch_3_durch_5\" \"_durch_3\"         \"_durch_5\"        \n\n\n\nCategories:\n\nr\nchallenge\nstring"
  },
  {
    "objectID": "posts/ReThink3m5/ReThink3m5.html",
    "href": "posts/ReThink3m5/ReThink3m5.html",
    "title": "ReThink3m5",
    "section": "",
    "text": "Aufgabe\nNehmen wir an, wir haben 8 (Wasser-)“Treffer” (\\(W=8\\)) bei 15 Würfen (\\(N=15\\)) erhalten (wieder im Globusversuch).\nNehmen Sie dieses Mal keine gleichverteilte Priori-Verteilung an. Stattdessen verwenden Sie einen Priori-Wert von Null solange \\(p &lt; 0.5\\) und einen konstanten Wert für \\(p \\ge 0.5\\). Diese Priori-Verteilung kodiert die Information, dass mindestens die Hälfte der Erdoberfläche mit Sicherheit aus Wasser besteht.\nFür alle folgenden Berechnungen, vergleichen Sie Ihre Ergebnisse zu der analogen Analyse mit einem konstanten (gleichverteilten) Priori-Wert!\n\nBerechnen Sie die Posteriori-Verteilung und visualisieren Sie sie. Nutzen Sie die Gittermethode.\nZiehen Sie \\(10^4\\) Stichproben aus der Posteriori-Verteilung, die Sie mit der Gittermethode erhalten haben. Berechnen Sie auf dieser Grundlage das 90%-HDI.\nBerechnen Sie die PPV für dieses Modell. Was ist die Wahrscheinlichkeit 8 von 15 Treffer zu erzielen laut dieser PPV?\nAuf Basis der aktuellen Posteriori-Wahrscheinlichkeit: Was ist die Wahrscheinlichkeit für 6 Wasser bei 9 Würfen?\n\nHinweise:\n\nBerechnen Sie eine Bayes-Box (Gittermethode).\nVerwenden Sie 1000 Gitterwerte.\nFixieren Sie die Zufallszahlen mit dem Startwert 42, d.h. set.seed(42).\nGehen Sie von einem gleichverteilten Prior aus.\n\nQuelle: McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Aufl.). Taylor and Francis, CRC Press.\n         \n\n\nLösung\n\nBerechnen Sie die Posteriori-Verteilung und visualisieren Sie sie. Nutzen Sie die Gittermethode.\n\n\nset.seed(42)\np_grid &lt;- seq(from = 0, to = 1, length.out = 1000)\nprior &lt;- case_when(\n  p_grid &lt;  0.5 ~ 0,\n  p_grid &gt;= 0.5 ~ 1)\nlikelihood &lt;- dbinom(8, size = 15, prob = p_grid)\nunstand_posterior &lt;- likelihood * prior\nposterior &lt;- unstand_posterior / sum(unstand_posterior)\n\n\ntibble(p = p_grid, \n       posterior = posterior) %&gt;%\n  ggplot(aes(x = p, y = posterior)) +\n # geom_point() +\n  geom_line() +\n  labs(x = \"Proportion Water (p)\", y = \"Posterior Density\")\n\n\n\n\n\n\n\n\nAlternativ können Sie mit ggpubr::ggline() visualisieren.\n\nZiehen Sie \\(10^4\\) Stichproben aus der Posteriori-Verteilung, die Sie mit der Gittermethode erhalten haben. Berechnen Sie auf dieser Grundlage das 90%-HDI.\n\n\nlibrary(easystats)\n# Stichproben (samples) aus der Posteriori-Verteilung:\nsamples &lt;- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE)\nhdi(samples, prob = 0.9)\n\n95% HDI: [0.50, 0.75]\n\n\n\nBerechnen Sie die PPV für dieses Modell. Was ist die Wahrscheinlichkeit 8 von 15 Treffer zu erzielen laut dieser PPV?\n\n\nPPV &lt;-\n  tibble(w = rbinom(1e4, size = 15, prob = samples))  # w wie Wasser\n\nPPV %&gt;% \n  count(w == 8) %&gt;% \n  mutate(prop = n/sum(n))\n\n# A tibble: 2 × 3\n  `w == 8`     n  prop\n  &lt;lgl&gt;    &lt;int&gt; &lt;dbl&gt;\n1 FALSE     8508 0.851\n2 TRUE      1492 0.149\n\n\n\nAuf Basis der aktuellen Posteriori-Wahrscheinlichkeit: Was ist die Wahrscheinlichkeit für 6 Wasser bei 9 Würfen?\n\n\nPPV &lt;-\n  PPV %&gt;% \n  mutate(w2 = rbinom(1e4, size = 9, prob = samples))\n\nPPV %&gt;% \n  count(w2 == 6) %&gt;% \n  mutate(prop = n/sum(n))\n\n# A tibble: 2 × 3\n  `w2 == 6`     n  prop\n  &lt;lgl&gt;     &lt;int&gt; &lt;dbl&gt;\n1 FALSE      7738 0.774\n2 TRUE       2262 0.226\n\n\n\nCategories:\n\nbayes\nppv\nprobability\nstring"
  },
  {
    "objectID": "posts/korr01/korr01.html",
    "href": "posts/korr01/korr01.html",
    "title": "korr01",
    "section": "",
    "text": "Welcher Korrelationswert (Pearson) beschreibt die Korrelation in den Daten am besten?\n\n\n\n\\(r = 1\\)\n\\(r = -1\\)\n\\(r = 0\\)\n\\(r = .8\\)\n\\(r = -.8\\)"
  },
  {
    "objectID": "posts/korr01/korr01.html#answerlist",
    "href": "posts/korr01/korr01.html#answerlist",
    "title": "korr01",
    "section": "",
    "text": "\\(r = 1\\)\n\\(r = -1\\)\n\\(r = 0\\)\n\\(r = .8\\)\n\\(r = -.8\\)"
  },
  {
    "objectID": "posts/korr01/korr01.html#answerlist-1",
    "href": "posts/korr01/korr01.html#answerlist-1",
    "title": "korr01",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nWahr\nFalsch\nFalsch\n\n\nCategories:\n\ndyn\neda\nassociation\nschoice"
  },
  {
    "objectID": "posts/hintertuer/hintertuer.html",
    "href": "posts/hintertuer/hintertuer.html",
    "title": "hintertuer",
    "section": "",
    "text": "Aufgabe\nWir wollen hier den (kausalen) Einfluss der Eltern E und Großeltern G auf den Bildungserfolg der Kinder K untersuchen.\nWir nehmen folgende Effekte an:\n\nindirekter Effekt von G auf K: \\(G \\rightarrow E \\rightarrow K\\)\ndirekter Effekt von E auf K: \\(E \\rightarrow K\\)\ndirekter Effekt von G auf K: \\(G \\rightarrow K\\)\n\nWir sind v.a. interessiert an \\(G \\rightarrow K\\), dem direkten kausalen Effekt von Großeltern auf ihre Enkel, s. Figure 1, \\(G \\rightarrow K\\).\n\n\n\n\n\n\n\n\nFigure 1: Der kausale Effekt von Großeltern auf Enkel hinsichtlich Bildung.\n\n\n\n\n\nAufgabe: Geben Sie das minimale Adjustierungsset an, um den direkten kausalen Effekt von G auf K zu identifizieren.\nHinweise:\n\nBeachten Sie die Standardhinweise des Datenwerks.\n\n         \n\n\nLösung\nEs ist keine Adjustierung nötig, um nicht-kausale Pfade zu schließen. Alle nicht-kausalen Pfade sind bereits geschlossen.\nIst man am direkten Effekt von G auf K interessiert, so muss man folgende Variablen kontrollieren:\n\nE und U (beide)\n\nFalls man U nicht kontrollieren kann (was der Namen U andeutet), so ist der gesuchte kausale Effekt nicht idenfizierbar."
  },
  {
    "objectID": "posts/gini-plot/gini-plot.html",
    "href": "posts/gini-plot/gini-plot.html",
    "title": "gini-plot",
    "section": "",
    "text": "Aufgabe\nVisualisieren Sie die Gini-Funktion!\n         \n\n\nLösung\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\ngranularity &lt;- .1\nx1 = seq(from = 0, to = 1, by = granularity)\nx2 = seq(from = 1, to = 0, by = -granularity)\n#x2 &lt;- 1 - x1\n  \nd &lt;- expand_grid(x1, x2)\n\nGini-Loss:\n\ngini_loss &lt;- function(x1, x2) {1 - (x1^2 + x2^2)}\n\nFunktion berechnen:\n\nd2 &lt;-\n  d %&gt;% \n  rowwise() %&gt;% \n  mutate(y = gini_loss(x1, x2))\n\n\n# d2 &lt;-\n#   outer(x1, x3, FUN = gini_loss) %&gt;% \n#   as_tibble() %&gt;% \n#   pivot_longer(cols = everything())\n\n\n# d &lt;-\n#   d %&gt;% \n#   mutate(\n#     x3 = 1 - x1,\n#     y = 1 - (x1^2 + x3^2))\n\n\nd2 %&gt;% \n  ggplot(aes(x1, x2, fill = y)) +\n  geom_tile() +\n  scale_x_continuous(limits = c(-2, 2)) +\n  scale_y_continuous(limits = c(-2, 2))\n\n\n\n\n\n\n\n\nSo sieht der Funktionsgraph in Geogebra aus.\n\nCategories:\n\n2023\nvis\nstatlearning\ntree\nstring"
  },
  {
    "objectID": "posts/Bsp-Binomial/Bsp-Binomial.html",
    "href": "posts/Bsp-Binomial/Bsp-Binomial.html",
    "title": "Bsp-Binomial",
    "section": "",
    "text": "Aufgabe\nDie Binomialverteilung wird in Lehrbüchern häufig mit Münzwürfen motiviert. Im Buch Statistical Rethinking muss etwa ein Globus herhalten (also ein Zufallsexperiment mit den Ergebnissen Wasser und Land unter dem Zeigefinger). Dahinter steht als theoretisches Konzept die Binomialverteilung.\nDie Beispiele sind ja gut und schön. Aber was hat das mit der Praxis zu tun? Gute Frage. Nennen Sie Beispiele aus Berufsfeldern der Sozialwissenschaften, für die die Binomialverteilung relevant ist.\nSie müssen nichts rechnen, nur Beispiele nennen.\n         \n\n\nLösung\nZur Erinnerung: Die Inferenzstatistik macht Aussagen bzgl. einer Population, nicht einer Stichprobe. Solche Aussagen sind ungewiss, also mit einer Unsicherheit behaftet, da wir nicht die ganze Population kennen. Aber die Daten der Stichprobe werden als Grundlage der Schätzung herangezogen.\n\nAuswahl geeigneter Kandidatis in einem Assessment-Verfahren. Man hat \\(n=40\\) Bewerbis, und die Wahrscheinlichkeit geeigneter Kandidatis liege bei \\(p=10%\\). Welche Spannweite an geeigneten Bewerbis kann man erwarten?\nSocial Influencing. Sie posten 100 Videoclips; davon werden 9 viral. Welche Spannweite plausibler Werte für eine Erfolgsquote kann man zugrunde legen?\nApp-Wartung. Sie prüfen eine Anzahl (\\(n=42\\)) alter Apps, aus einer früheren Kampagne. Sie finden, dass \\(k=19\\) noch funktionieren. Welche Quote an “technisch veraltet” muss man in der Population erwarten, und in welchem Bereich könnte sich diese Quote bewegen?\nSchulungsprogramm. Sie entwickeln ein Schulungsprogramm, das im großen Stil in einer Firma eingesetzt werden soll; mehrere Tausend Personen sollen das Programm durchlaufen. In einer Pilotstudie mit \\(n=90\\) Personen erreichen \\(k=42\\) nicht das Lernziel. Welche Parameterwerte für \\(p\\) (Lernziel erreicht) sind plausibel?\n\n\nCategories:\n\nprobability\nbinomial\nexample\nstring"
  },
  {
    "objectID": "posts/randomdag1/randomdag1.html",
    "href": "posts/randomdag1/randomdag1.html",
    "title": "randomdag1",
    "section": "",
    "text": "Gegeben sei der DAG g (s. u.). Der DAG verfügt über \\(n = 7\\) Variablen, die als Knoten im Graph dargestellt sind und mit \\(x_1, x_2, \\ldots x_n\\) bezeichnet sind.\nWelche minimale Variablenmenge muss kontrolliert werden, um den kausalen Effekt von der UV zur AV zu identifizieren?\nUV: x6.\nAV: x7.\nHinweise:\n\nMengen sind mittels geschweifter Klammern gekennzeichnet, z.B. {x1, x2} meint die Menge mit den zwei Elementen x1 und x2.\nDie leere Menge { } bedeutet, dass keine Variable kontrolliert werden muss, um den kausalen Effekt zu identifizieren.\nAlle Variablen werden als gemessen vorausgesetzt.\nEs ist möglich, dass es keine Lösung gibt, dass es also keine Adjustierungsmenge gibt, um den kausalen Effekt zu identifizieren. Wenn dies der Fall sein sollte, wählen Sie “/”.\nEs ist möglich, dass einzelne Variablen keine Kanten besitzen, also keine Verbindung zu anderen Variablen (Knoten) haben. Diese Variablen sind dann kausal unabhängig von den übrigen Variablen.\n\n\n\n\n\n\n\n\n\n\n\n\n\n{ x6 }\n{ x2 , x5 }\n{ x2, x4 }\n{ x4, x5 }\n{ x1, x6 }"
  },
  {
    "objectID": "posts/randomdag1/randomdag1.html#answerlist",
    "href": "posts/randomdag1/randomdag1.html#answerlist",
    "title": "randomdag1",
    "section": "",
    "text": "{ x6 }\n{ x2 , x5 }\n{ x2, x4 }\n{ x4, x5 }\n{ x1, x6 }"
  },
  {
    "objectID": "posts/randomdag1/randomdag1.html#answerlist-1",
    "href": "posts/randomdag1/randomdag1.html#answerlist-1",
    "title": "randomdag1",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nRichtig\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ncausal\ndag"
  },
  {
    "objectID": "posts/vis-gapminder/vis-gapminder.html",
    "href": "posts/vis-gapminder/vis-gapminder.html",
    "title": "vis-gapminder",
    "section": "",
    "text": "In dieser Fallstudie (YACSDA: Yet another Case Study on Data Analysis) untersuchen wir den Datensatz gapminder.\nSie können den Datensatz so beziehen:\n\n#install.packages(\"gapminder\")\nlibrary(gapminder)\ndata(\"gapminder\")\nd &lt;- gapminder \n\nOder so:\n\nd &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/gapminder/gapminder.csv\")\n\nEin Codebook finden Sie hier.\nDie Forschungsfrage lautet:\nWas ist der Einfluss des Kontinents und des Bruttosozialprodukts auf die Lebenswartung?\n\nAbhängige Variable (metrisch), y: Lebenserwartung\nUnabhängige Variable 1 (nominal), x1: Kontinent\nUnabhängige Variable 2 (metrisch), x2: Bruttosozialprodukt\n\nVisualisieren Sie dazu folgende Aspekte der Forschungsfrage!"
  },
  {
    "objectID": "posts/vis-gapminder/vis-gapminder.html#umbenennen",
    "href": "posts/vis-gapminder/vis-gapminder.html#umbenennen",
    "title": "vis-gapminder",
    "section": "Umbenennen",
    "text": "Umbenennen\nZur einfacheren Verarbeitung nenne ich die Variablen um:\n\nd &lt;-\n  d |&gt; \n  rename(y = lifeExp, x1 = continent, x2 = gdpPercap)"
  },
  {
    "objectID": "posts/vis-gapminder/vis-gapminder.html#visualisieren-sie-die-verteilung-von-y-auf-zwei-verschiedene-arten.",
    "href": "posts/vis-gapminder/vis-gapminder.html#visualisieren-sie-die-verteilung-von-y-auf-zwei-verschiedene-arten.",
    "title": "vis-gapminder",
    "section": "Visualisieren Sie die Verteilung von y auf zwei verschiedene Arten.",
    "text": "Visualisieren Sie die Verteilung von y auf zwei verschiedene Arten.\nDas R-Paket ggpubr erstellt schöne Diagramme (basierend auf ggplot) auf einfache Art. Nehmen wir ein Dichtediagramm; die Variable y soll auf der X-Achse stehen:\n\nggdensity(d, x = \"y\")\n\n\n\n\n\n\n\n\nBeachten Sie, dass die Variable in Anführungsstriche gesetzt werden muss: x = \"y\".\nOder ein Histogramm:\n\ngghistogram(d, x = \"y\")\n\nWarning: Using `bins = 30` by default. Pick better value with the argument\n`bins`."
  },
  {
    "objectID": "posts/vis-gapminder/vis-gapminder.html#fügen-sie-relevante-kennzahlen-zur-letzten-visualisierung-hinzu.",
    "href": "posts/vis-gapminder/vis-gapminder.html#fügen-sie-relevante-kennzahlen-zur-letzten-visualisierung-hinzu.",
    "title": "vis-gapminder",
    "section": "Fügen Sie relevante Kennzahlen zur letzten Visualisierung hinzu.",
    "text": "Fügen Sie relevante Kennzahlen zur letzten Visualisierung hinzu.\nUm Diagramme mit Statistiken anzureichen, bietet sich das Paket ggstatsplot an:\n\ngghistostats(d, x = y)\n\n\n\n\n\n\n\n\nBeachten Sie, dass die Variable nicht in Anführungsstriche gesetzt werden darf: x = y."
  },
  {
    "objectID": "posts/vis-gapminder/vis-gapminder.html#visualisieren-sie-die-verteilung-von-x1-und-x2.",
    "href": "posts/vis-gapminder/vis-gapminder.html#visualisieren-sie-die-verteilung-von-x1-und-x2.",
    "title": "vis-gapminder",
    "section": "Visualisieren Sie die Verteilung von x1 und x2.",
    "text": "Visualisieren Sie die Verteilung von x1 und x2.\n\nx1\n\nd_counted &lt;- \n  d |&gt; \n  count(x1) \n\n\nggbarplot(data = d_counted, y = \"n\", x = \"x1\", label = TRUE)\n\n\n\n\n\n\n\n\n\n\nx2\n\ngghistostats(d, x = x2)"
  },
  {
    "objectID": "posts/vis-gapminder/vis-gapminder.html#visualisieren-sie-die-verteilung-von-y-bedingt-auf-x1",
    "href": "posts/vis-gapminder/vis-gapminder.html#visualisieren-sie-die-verteilung-von-y-bedingt-auf-x1",
    "title": "vis-gapminder",
    "section": "Visualisieren Sie die Verteilung von y bedingt auf x1",
    "text": "Visualisieren Sie die Verteilung von y bedingt auf x1\n\ngghistogram(d, x = \"y\", fill = \"x1\")\n\nWarning: Using `bins = 30` by default. Pick better value with the argument\n`bins`.\n\n\n\n\n\n\n\n\n\nOder so:\n\ngghistogram(d, x = \"y\", facet.by = \"x1\")\n\nWarning: Using `bins = 30` by default. Pick better value with the argument\n`bins`."
  },
  {
    "objectID": "posts/vis-gapminder/vis-gapminder.html#fügen-sie-relevante-kennzahlen-zur-letzten-visualisierung-hinzu",
    "href": "posts/vis-gapminder/vis-gapminder.html#fügen-sie-relevante-kennzahlen-zur-letzten-visualisierung-hinzu",
    "title": "vis-gapminder",
    "section": "Fügen Sie relevante Kennzahlen zur letzten Visualisierung hinzu",
    "text": "Fügen Sie relevante Kennzahlen zur letzten Visualisierung hinzu\n\ngrouped_gghistostats(d, x = y, grouping.var = x1)"
  },
  {
    "objectID": "posts/vis-gapminder/vis-gapminder.html#visualisieren-sie-den-zusammenhang-von-y-und-x2",
    "href": "posts/vis-gapminder/vis-gapminder.html#visualisieren-sie-den-zusammenhang-von-y-und-x2",
    "title": "vis-gapminder",
    "section": "Visualisieren Sie den Zusammenhang von y und x2",
    "text": "Visualisieren Sie den Zusammenhang von y und x2\n\nggscatter(d, x = \"x2\", y = \"y\")"
  },
  {
    "objectID": "posts/vis-gapminder/vis-gapminder.html#verbessern-sie-das-letzte-diagramm-so-dass-es-übersichtlicher-wird",
    "href": "posts/vis-gapminder/vis-gapminder.html#verbessern-sie-das-letzte-diagramm-so-dass-es-übersichtlicher-wird",
    "title": "vis-gapminder",
    "section": "Verbessern Sie das letzte Diagramm, so dass es übersichtlicher wird",
    "text": "Verbessern Sie das letzte Diagramm, so dass es übersichtlicher wird\nEs gibt mehrere Wege, das Diagramm übersichtlicher zu machen. Logarithmieren ist ein Weg.\n\nd |&gt; \n  mutate(x2 = log(x2)) |&gt; \n  ggscatter(x = \"x2\", y = \"y\")\n\n\n\n\n\n\n\n\nSynonym könnten wir schreiben:\n\nd_logged &lt;- \n  d |&gt; \n  mutate(x2 = log(x2))\n  \n\nggscatter(d_logged, x = \"x2\", y = \"y\")"
  },
  {
    "objectID": "posts/vis-gapminder/vis-gapminder.html#fügen-sie-dem-letzten-diagramm-relevante-kennzahlen-hinzu",
    "href": "posts/vis-gapminder/vis-gapminder.html#fügen-sie-dem-letzten-diagramm-relevante-kennzahlen-hinzu",
    "title": "vis-gapminder",
    "section": "Fügen Sie dem letzten Diagramm relevante Kennzahlen hinzu",
    "text": "Fügen Sie dem letzten Diagramm relevante Kennzahlen hinzu\n\nggscatterstats(d_logged, x = \"x2\", y = \"y\")"
  },
  {
    "objectID": "posts/vis-gapminder/vis-gapminder.html#fügen-sie-dem-diagramm-zum-zusammenhang-von-y-und-x2-eine-regressionsgerade-hinzu",
    "href": "posts/vis-gapminder/vis-gapminder.html#fügen-sie-dem-diagramm-zum-zusammenhang-von-y-und-x2-eine-regressionsgerade-hinzu",
    "title": "vis-gapminder",
    "section": "Fügen Sie dem Diagramm zum Zusammenhang von y und x2 eine Regressionsgerade hinzu",
    "text": "Fügen Sie dem Diagramm zum Zusammenhang von y und x2 eine Regressionsgerade hinzu\n\nggscatter(d_logged, x = \"x2\", y = \"y\", add = \"reg.line\", \n             add.params = list(color = \"blue\"))"
  },
  {
    "objectID": "posts/vis-gapminder/vis-gapminder.html#ersetzen-sie-die-regressionsgerade-durch-eine-loess-gerade",
    "href": "posts/vis-gapminder/vis-gapminder.html#ersetzen-sie-die-regressionsgerade-durch-eine-loess-gerade",
    "title": "vis-gapminder",
    "section": "Ersetzen Sie die Regressionsgerade durch eine LOESS-Gerade",
    "text": "Ersetzen Sie die Regressionsgerade durch eine LOESS-Gerade\n\nggscatter(d_logged, x = \"x2\", y = \"y\", add = \"loess\", \n             add.params = list(color = \"blue\"))"
  },
  {
    "objectID": "posts/vis-gapminder/vis-gapminder.html#gruppieren-sie-das-letzte-diagramm-nach-x1",
    "href": "posts/vis-gapminder/vis-gapminder.html#gruppieren-sie-das-letzte-diagramm-nach-x1",
    "title": "vis-gapminder",
    "section": "Gruppieren Sie das letzte Diagramm nach x1",
    "text": "Gruppieren Sie das letzte Diagramm nach x1\n\nggscatter(d_logged, x = \"x2\", y = \"y\", add = \"loess\", \n             add.params = list(color = \"blue\"),\n          facet.by = \"x1\")"
  },
  {
    "objectID": "posts/vis-gapminder/vis-gapminder.html#dichotomisieren-sie-y-und-zählen-sie-die-häufigkeiten",
    "href": "posts/vis-gapminder/vis-gapminder.html#dichotomisieren-sie-y-und-zählen-sie-die-häufigkeiten",
    "title": "vis-gapminder",
    "section": "Dichotomisieren Sie y und zählen Sie die Häufigkeiten",
    "text": "Dichotomisieren Sie y und zählen Sie die Häufigkeiten\nNehmen wir einen Mediansplit, um zu dichotomisieren.\n\nd &lt;-\n  d |&gt; \n  mutate(y_dicho = ifelse(y &gt; median(y), \"high\", \"low\"))\n\n\nd |&gt; \n  count(y_dicho) |&gt; \n  ggbarplot(x = \"y_dicho\", y = \"n\")\n\n\n\n\n\n\n\n\nGleich viele! Das sollte nicht verwundern."
  },
  {
    "objectID": "posts/vis-gapminder/vis-gapminder.html#gruppieren-sie-das-letzte-diagramm-nach-den-stufen-von-x1",
    "href": "posts/vis-gapminder/vis-gapminder.html#gruppieren-sie-das-letzte-diagramm-nach-den-stufen-von-x1",
    "title": "vis-gapminder",
    "section": "Gruppieren Sie das letzte Diagramm nach den Stufen von x1",
    "text": "Gruppieren Sie das letzte Diagramm nach den Stufen von x1\n\nd_count &lt;- \nd |&gt; \n  count(y_dicho, x1) \n\nd_count\n\n# A tibble: 9 × 3\n  y_dicho x1           n\n  &lt;chr&gt;   &lt;fct&gt;    &lt;int&gt;\n1 high    Africa      64\n2 high    Americas   211\n3 high    Asia       207\n4 high    Europe     346\n5 high    Oceania     24\n6 low     Africa     560\n7 low     Americas    89\n8 low     Asia       189\n9 low     Europe      14\n\n\n\nggbarplot(d_count, x = \"y_dicho\", y = \"n\", facet.by = \"x1\")"
  },
  {
    "objectID": "posts/vis-gapminder/vis-gapminder.html#variieren-sie-das-letzte-diagramm-so-dass-anteile-relative-häufigkeiten-statt-absoluter-häufigkeiten-gezeigt-werden",
    "href": "posts/vis-gapminder/vis-gapminder.html#variieren-sie-das-letzte-diagramm-so-dass-anteile-relative-häufigkeiten-statt-absoluter-häufigkeiten-gezeigt-werden",
    "title": "vis-gapminder",
    "section": "Variieren Sie das letzte Diagramm so, dass Anteile (relative Häufigkeiten) statt absoluter Häufigkeiten gezeigt werden",
    "text": "Variieren Sie das letzte Diagramm so, dass Anteile (relative Häufigkeiten) statt absoluter Häufigkeiten gezeigt werden\n\nd_count &lt;-\n  d_count |&gt; \n  mutate(prop = n / sum(n)) |&gt; \n  mutate(prop = round(prop, 2))\n\nd_count\n\n# A tibble: 9 × 4\n  y_dicho x1           n  prop\n  &lt;chr&gt;   &lt;fct&gt;    &lt;int&gt; &lt;dbl&gt;\n1 high    Africa      64  0.04\n2 high    Americas   211  0.12\n3 high    Asia       207  0.12\n4 high    Europe     346  0.2 \n5 high    Oceania     24  0.01\n6 low     Africa     560  0.33\n7 low     Americas    89  0.05\n8 low     Asia       189  0.11\n9 low     Europe      14  0.01\n\n\nCheck:\n\nd_count |&gt; \n  summarise(sum(prop))\n\n# A tibble: 1 × 1\n  `sum(prop)`\n        &lt;dbl&gt;\n1        0.99\n\n\nGut! Die Anteile summieren sich zu ca. 1 (100 Prozent).\n\nggbarplot(d_count, x = \"y_dicho\", y = \"prop\", facet.by = \"x1\", label = TRUE)\n\n\n\n\n\n\n\n\nMan beachten, dass sich die Anteile auf das “Gesamt-N” beziehen.\nVielleicht möchten wir die Anteile lieber pro Stufe von x1 beziehen. Dazu gruppieren wir nach (und pro Stufe von) x1.\n\nd_count &lt;-\n  d_count |&gt; \n  group_by(x1) |&gt; \n  mutate(prop = n / sum(n)) |&gt; \n  mutate(prop = round(prop, 2))\n\nd_count\n\n# A tibble: 9 × 4\n# Groups:   x1 [5]\n  y_dicho x1           n  prop\n  &lt;chr&gt;   &lt;fct&gt;    &lt;int&gt; &lt;dbl&gt;\n1 high    Africa      64  0.1 \n2 high    Americas   211  0.7 \n3 high    Asia       207  0.52\n4 high    Europe     346  0.96\n5 high    Oceania     24  1   \n6 low     Africa     560  0.9 \n7 low     Americas    89  0.3 \n8 low     Asia       189  0.48\n9 low     Europe      14  0.04\n\n\n\nggbarplot(d_count, x = \"y_dicho\", y = \"prop\", facet.by = \"x1\", label = TRUE)\n\n\n\n\n\n\n\n\n\nCategories:\n\nvis\nyacsda\nggquick\ngapminder\nstring"
  },
  {
    "objectID": "posts/ReThink3e1-7/ReThink3e1-7.html",
    "href": "posts/ReThink3e1-7/ReThink3e1-7.html",
    "title": "ReThink3e1-7",
    "section": "",
    "text": "Exercise\nErstellen Sie die Posteriori-Verteilung für den Globusversuch. Nutzen Sie dafür diese Syntax:\n\np_grid &lt;- seq( from=0 , to=1 , length.out=1000 )  # Gitterwerte\n\nprior &lt;- rep( 1 , 1000 )  # Priori-Gewichte\n\nlikelihood &lt;- dbinom( 6 , size=9 , prob=p_grid ) \n\nunstandardisierte_posterior &lt;- likelihood * prior \n\nposterior &lt;- unstandardisierte_posterior / sum(unstandardisierte_posterior)\n\n# um die Zufallszahlen festzulegen, damit wir alle die gleichen Zahlen bekommen zum Schnluss: \nset.seed(42) \n\n# Stichproben ziehen aus der Posteriori-Verteilung\nsamples &lt;- \n  tibble(\n    p = sample( p_grid , prob=posterior, size=1e4, replace=TRUE)) \n\n\nWie viel Wahrscheinlichkeitsmasse liegt unter \\(p=0.2\\)?\nWie viel Wahrscheinlichkeitsmasse liegt über \\(p=0.8\\)?\nWelcher Anteil der Posteriori-Verteilung liegt zwischen \\(p=0.2\\) und \\(p=0.8\\)?\nUnter welchem Wasseranteil \\(p\\) liegen 10% der Posteriori-Verteilung?\nÜber welchem Wasseranteil \\(p\\) liegen 10% der Posteriori-Verteilung?\nWelches schmälstes Intervall von \\(p\\) enthält 66% der Posteriori-Wahrscheinlichkeit?\nWelcher Wertebereich (synonym: Welches Intervall) von \\(p\\) enthält 66% der Posteriori-Wahrscheinlichkeit (hier wird Posteriori-Wahrscheinlichkeit synonym gebraucht zu Posteriori-Verteilung)? Wie nennt man diese Arten von Intervall?\n\nQuelle: McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Aufl.). Taylor and Francis, CRC Press.\n         \n\n\nSolution\nEs finden sich auch Lösungsvorschläge online, z.B. hier\n\nWie viel Wahrscheinlichkeitsmasse liegt unter \\(p=0.2\\)?\n\n\nsamples %&gt;% \n  count(p &lt; 0.2)\n\n# A tibble: 2 × 2\n  `p &lt; 0.2`     n\n  &lt;lgl&gt;     &lt;int&gt;\n1 FALSE      9993\n2 TRUE          7\n\n\nFast nix!\n\nWie viel Wahrscheinlichkeitsmasse liegt über \\(p=0.8\\)?\n\n\nsamples %&gt;% \n  count(p &gt; 0.8)\n\n# A tibble: 2 × 2\n  `p &gt; 0.8`     n\n  &lt;lgl&gt;     &lt;int&gt;\n1 FALSE      8842\n2 TRUE       1158\n\n\nNaja, so gut 10%!\n\nWelcher Anteil der Posteriori-Verteilung liegt zwischen \\(p=0.2\\) und \\(p=0.8\\)?\n\n\nsamples %&gt;% \n  count(p &gt; 0.2 & p &lt; 0.8) \n\n# A tibble: 2 × 2\n  `p &gt; 0.2 & p &lt; 0.8`     n\n  &lt;lgl&gt;               &lt;int&gt;\n1 FALSE                1165\n2 TRUE                 8835\n\n\nKnapp 90%!\n\nUnter welchem Wasseranteil \\(p\\) liegen 20% der Posteriori-Verteilung?\n\nEine Möglichkeit: Wir sortieren \\(p\\) der Größe nach (aufsteigend), filtern dann so, dass wir nur die ersten 20% der Zeilen behalten und schauen dann, was der größte Wert ist.\n\nsamples %&gt;% \n  arrange(p) %&gt;% \n  slice_head(prop = 0.2) %&gt;% \n  summarise(quantil_20 = max(p))\n\n# A tibble: 1 × 1\n  quantil_20\n       &lt;dbl&gt;\n1      0.517\n\n\nAndererseits: Das, was wir gerade gemacht haben, nennt man auch ein Quantil berechnen, s. auch hier. Dafür gibt’s fertige Funktionen in R, wie quantile():\n\nsamples %&gt;% \n  summarise(q_20 = quantile(p, 0.2))\n\n# A tibble: 1 × 1\n   q_20\n  &lt;dbl&gt;\n1 0.517\n\n\n\nÜber welchem Wasseranteil \\(p\\) liegen 10% der Posteriori-Verteilung?\n\n\nsamples %&gt;% \n  summarise(quantile(p, 0.9))\n\n# A tibble: 1 × 1\n  `quantile(p, 0.9)`\n               &lt;dbl&gt;\n1              0.810\n\n\nMit 90% Wahrscheinlichkeit ist der Wasseranteil höchstns bei 81%.\n\nWelches schmälstes Intervall von \\(p\\) enthält 66% der Posteriori-Wahrscheinlichkeit?\n\n\nlibrary(easystats)\nhdi(samples, ci = 0.66)\n\nHighest Density Interval\n\nParameter |      66% HDI\n------------------------\np         | [0.52, 0.79]\n\n\n\nWelcher Wertebereich von \\(p\\) enthält 66% der Posteriori-Wahrscheinlichkeit (hier wird Posteriori-Wahrscheinlichkeit syonyom gebraucht zu Posteriori-Verteilung)?\n\nWir nutzen hier die Equal-Tail-Intervall (oder Perzentilintervall genannt), da die Aufgabe keine genauen Angaben macht.\n\neti(samples, ci = 0.66)\n\nEqual-Tailed Interval\n\nParameter |      66% ETI\n------------------------\np         | [0.50, 0.77]\n\n\nEin “mittleres” 2/3-Intervall lässt 1/3 der Wahrscheinlichkeitsmasse außen vor, und zwar gleichmäßig in zwei Hälften links und rechts, also jeweils 1/6 (17%). So ein Intervall heißt Perzentilintervall. Daher synonym:\n\nsamples %&gt;% \n  summarise(PI_66 = quantile(p, prob = c(0.17, .84)))\n\n# A tibble: 2 × 1\n  PI_66\n  &lt;dbl&gt;\n1 0.501\n2 0.779\n\n\n\nCategories:\n\nbayes\nprobability\npost"
  },
  {
    "objectID": "posts/nasa05/nasa05.html",
    "href": "posts/nasa05/nasa05.html",
    "title": "nasa05",
    "section": "",
    "text": "Viele Quellen berichten Klimadaten unserer Erde, z.B. auch National Aeronautics and Space Administration - Goddard Institute for Space Studies.\nVon dieser Quelle beziehen wir diesen Datensatz.\nDie Datensatz sind auf der Webseite wie folgt beschrieben:\nTables of Global and Hemispheric Monthly Means and Zonal Annual Means\nCombined Land-Surface Air and Sea-Surface Water Temperature Anomalies (Land-Ocean Temperature Index, L-OTI)\nThe following are plain-text files in tabular format of temperature anomalies, i.e. deviations from the corresponding 1951-1980 means.\n\nGlobal-mean monthly, seasonal, and annual means, 1880-present, updated through most recent month: TXT, CSV\n\nStarten Sie zunächst das R-Paket tidyverse falls noch nicht geschehen.\n\nlibrary(tidyverse)\n\nZum Animieren verwenden wir diese Pakete:\n\nlibrary(gganimate)\nlibrary(plotly)\n\nImportieren Sie dann die Daten:\n\ndata_path &lt;- \"https://data.giss.nasa.gov/gistemp/tabledata_v4/GLB.Ts+dSST.csv\"\nd &lt;- read_csv(data_path, skip = 1)\n\nWir lassen die 1. Zeile des Datensatzes aus (Argument skip), da dort Metadaten stehen, also keine Daten, sondern Informationen (Daten) zu den eigentlichen Daten.\nAufgabe\n\nVisualisieren Sie Temperatur pro Jahr und Dekade; animieren Sie Ihre Diagramme mittels plotly.\n\nHinweise:\n\nSie müssen zuerst die Dekade als neue Spalte berechnen."
  },
  {
    "objectID": "posts/nasa05/nasa05.html#daten-aufbereiten",
    "href": "posts/nasa05/nasa05.html#daten-aufbereiten",
    "title": "nasa05",
    "section": "Daten aufbereiten",
    "text": "Daten aufbereiten\nCharacter in Zahlen umwandeln:\n\nd2 &lt;-\n  d %&gt;% \n  select(Year:Dec) %&gt;% \n  mutate(across(Apr:Dec, as.numeric))\n\n\nd3 &lt;- \n  d2 %&gt;% \n  pivot_longer(-Year, values_to = \"temp\", names_to = \"month\")\n\n\nmonths &lt;-\n  tibble(\n    month = d3$month[1:12],\n    month_num = 1:12\n  )\n\n\nd3 &lt;- \n  d3 %&gt;% \n  full_join(months)"
  },
  {
    "objectID": "posts/nasa05/nasa05.html#daten-zusammenfassen",
    "href": "posts/nasa05/nasa05.html#daten-zusammenfassen",
    "title": "nasa05",
    "section": "Daten zusammenfassen",
    "text": "Daten zusammenfassen"
  },
  {
    "objectID": "posts/nasa05/nasa05.html#animation-mit-plotly",
    "href": "posts/nasa05/nasa05.html#animation-mit-plotly",
    "title": "nasa05",
    "section": "Animation mit plotly",
    "text": "Animation mit plotly\n\nd3 %&gt;% \nplot_ly(\n  x = ~Year,\n  y = ~temp,\n  #color = ~month,\n  frame = ~Year,\n  hoverinfo = \"text\",\n  text = ~Year,\n  type = \"scatter\",\n  mode = \"markers\"\n)\n\n\n\n\n\n\nd3 %&gt;% \nplot_ly(\n  x = ~Year,\n  y = ~temp,\n  color = ~month_num,\n  frame = ~Year,\n  hoverinfo = \"text\",\n  text = ~Year,\n  type = \"scatter\",\n  mode = \"markers\"\n)"
  },
  {
    "objectID": "posts/nasa05/nasa05.html#fazit",
    "href": "posts/nasa05/nasa05.html#fazit",
    "title": "nasa05",
    "section": "Fazit",
    "text": "Fazit\nFalls Sie Teile der R-Syntax nicht kennen: Machen Sie sich nichts daraus. Be happy 😄\n\nCategories:\n\ndata\neda\nlagemaße\nvis\nanimation\nstring"
  },
  {
    "objectID": "posts/min-corr1/min-corr1.html",
    "href": "posts/min-corr1/min-corr1.html",
    "title": "min-corr1",
    "section": "",
    "text": "Aufgabe\nWelches Diagramm zeigt den schwächsten (absoluten) linearen Zusammenhang (Korrelation)?\nGeben Sie die Nummer ein, die in der Kopfzeile jedes Teildiagramms angezeigt wird.\n         \n\n\nLösung\n\n\n\n\n\n\n\n\n\n\nCategories:\n\nvis\n‘2023’\nnum"
  },
  {
    "objectID": "posts/nasa02/nasa02.html",
    "href": "posts/nasa02/nasa02.html",
    "title": "nasa02",
    "section": "",
    "text": "Aufgabe\nViele Quellen berichten Klimadaten unserer Erde, z.B. auch National Aeronautics and Space Administration - Goddard Institute for Space Studies.\nVon dieser Quelle beziehen wir diesen Datensatz.\nDie Datensatz sind auf der Webseite wie folgt beschrieben:\nTables of Global and Hemispheric Monthly Means and Zonal Annual Means\nCombined Land-Surface Air and Sea-Surface Water Temperature Anomalies (Land-Ocean Temperature Index, L-OTI)\nThe following are plain-text files in tabular format of temperature anomalies, i.e. deviations from the corresponding 1951-1980 means.\n\nGlobal-mean monthly, seasonal, and annual means, 1880-present, updated through most recent month: TXT, CSV\n\nStarten Sie zunächst das R-Paket tidyverse falls noch nicht geschehen.\n\nlibrary(tidyverse)\n\nImportieren Sie dann die Daten:\n\ndata_path &lt;- \"https://data.giss.nasa.gov/gistemp/tabledata_v4/GLB.Ts+dSST.csv\"\nd &lt;- read_csv(data_path, skip = 1)\n\nOder von der eigenen Festplatte, wenn schon heruntergeladen:\n\nd &lt;- read_csv(\"/Users/sebastiansaueruser/datasets/nasa02.csv\")\n\nWir lassen die 1. Zeile des Datensatzes aus (Argument skip), da dort Metadaten stehen, also keine Daten, sondern Informationen (Daten) zu den eigentlichen Daten.\nAufgaben\n\nBerechnen Sie die die Korrelation der Temperatur von Januar und Februar\nBerechnen Sie die die Korrelation der Temperatur von Januar und Februar pro Dekade\n\nHinweise:\n\nSie müssen zuerst die Dekade als neue Spalte berechnen.\n\n         \n\n\nLösung\nDekade berechnen:\n\nd &lt;-\n  d %&gt;% \n  mutate(decade = round(Year/10))\n\nKorrelation:\n\nd %&gt;% \n  summarise(temp_cor = cor(Jan, Feb))\n\n# A tibble: 1 × 1\n  temp_cor\n     &lt;dbl&gt;\n1    0.942\n\n\nKorrelation pro Dekade:\n\nd_summarized &lt;- \n  d %&gt;% \n  group_by(decade) %&gt;% \n  summarise(temp_cor = cor(Jan, Feb))\n\n\n\n\n\n\n\n\n\ndecade\ntemp_cor\n\n\n\n\n188\n0.88\n\n\n189\n0.79\n\n\n190\n0.53\n\n\n191\n0.84\n\n\n192\n0.82\n\n\n193\n0.72\n\n\n194\n0.50\n\n\n195\n0.78\n\n\n196\n0.54\n\n\n197\n0.79\n\n\n198\n0.80\n\n\n199\n0.52\n\n\n200\n0.58\n\n\n201\n0.66\n\n\n202\n0.93\n\n\n\n\n\n\n\nZum Visualisieren gibt es viele Möglichkeiten. Wer kein Experte sein will, dem reicht es, eine Möglichkeit zu kennen.\nHier ist eine Visualisierung mit Hilfe des R-Pakets ggplot2:\n\nd_summarized %&gt;% \n  ggplot(aes(x = decade, y = temp_cor)) +\n  geom_point(color = \"darkblue\") +\n  geom_line(alpha = .7) +\n  scale_y_continuous(limits = c(0,1))\n\n\n\n\n\n\n\n\nAlternativ können Sie zum Visualisieren der Daten z.B. das Paket ggpubr nutzen:\n\nlibrary(ggpubr)\nggscatter(d_summarized, x = \"decade\", y = \"temp_cor\", add = \"reg.line\")\n\n\n\n\n\n\n\n\nOder mit dem R-Paket DataExplorer:\n\nlibrary(DataExplorer)\nd_summarized |&gt; \n  plot_scatterplot(by = \"temp_cor\")\n\n\n\n\n\n\n\n\nDie Korrelation der Temperaturen und damit die Ähnlichkeit der Muster hat im Laufe der Dekaden immer mal wieder geschwankt.\nFalls Sie Teile der R-Syntax nicht kennen: Machen Sie sich nichts daraus. 😄\n\nCategories:\n\ndata\neda\nlagemaße\nstring"
  },
  {
    "objectID": "posts/Likelihood-identifizieren/Likelihood-identifizieren.html",
    "href": "posts/Likelihood-identifizieren/Likelihood-identifizieren.html",
    "title": "Likelihood-identifizieren",
    "section": "",
    "text": "Welche Zeile der folgenden Modellspezifikation zeigt den Likelihood?\n\\[\n\\begin{align}\n\\text{height}_i &\\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta \\cdot  \\text{weight}_i\\\\\n\\alpha &\\sim \\operatorname{Normal}(178, 20)\\\\\n\\beta &\\sim \\operatorname{Normal}(5,3)\\\\\n\\sigma &\\sim \\operatorname{Exp}(0.1)\n\\end{align}\n\\]\nZeile …\n\n\n\n1\n2\n3\n4\n5"
  },
  {
    "objectID": "posts/Likelihood-identifizieren/Likelihood-identifizieren.html#answerlist",
    "href": "posts/Likelihood-identifizieren/Likelihood-identifizieren.html#answerlist",
    "title": "Likelihood-identifizieren",
    "section": "",
    "text": "1\n2\n3\n4\n5"
  },
  {
    "objectID": "posts/Likelihood-identifizieren/Likelihood-identifizieren.html#answerlist-1",
    "href": "posts/Likelihood-identifizieren/Likelihood-identifizieren.html#answerlist-1",
    "title": "Likelihood-identifizieren",
    "section": "Answerlist",
    "text": "Answerlist\n\nRichtig\nFalsch. Lineares Modell.\nFalsch. Prior Achsenabschnitt.\nFalsch. Prior Regressiongewicht.\nFalsch. Prior Streuung der AV.\n\n\nCategories:\n\nregression\nbayes\nlikelihood"
  },
  {
    "objectID": "posts/tidymodels-ames-01/tidymodels-ames-01.html",
    "href": "posts/tidymodels-ames-01/tidymodels-ames-01.html",
    "title": "tidymodels-ames-01",
    "section": "",
    "text": "Aufgabe\nBerechnen Sie ein lineares Modell mit tidymodels und zwar anhand des ames Datensatzes.\nModellgleichung: Sale_Price ~ Gr_Liv_Area, data = ames.\nBerechnen Sie ein multiplikatives (exponenzielles) Modell.\nGesucht ist R-Quadrat als Maß für die Modellgüte im Train-Sample.\nHinweise:\n\nFixieren Sie die Zufallszahlen auf den Startwert 42.\n\n         \n\n\nLösung\n\nlibrary(tidymodels)\ndata(ames)\n\nMultiplikatives Modell:\n\names &lt;- \n  ames %&gt;% \n  mutate(Sale_Price = log10(Sale_Price))\n\nDatensatz aufteilen:\n\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\nModell definieren:\n\nm1 &lt;-\n  linear_reg() # engine ist \"lm\" im Default\n\nModell fitten:\n\nfit1 &lt;-\n  m1 %&gt;% \n  fit(Sale_Price ~ Gr_Liv_Area, data = ames)\n\n\nfit1 %&gt;% pluck(\"fit\") \n\n\nCall:\nstats::lm(formula = Sale_Price ~ Gr_Liv_Area, data = data)\n\nCoefficients:\n(Intercept)  Gr_Liv_Area  \n  4.8552133    0.0002437  \n\n\nModellgüte im Train-Sample:\n\nfit1_performance &lt;-\n  fit1 %&gt;% \n  extract_fit_engine()  # identisch zu pluck(\"fit\")\n\nModellgüte:\n\nfit1_performance %&gt;% summary()\n\n\nCall:\nstats::lm(formula = Sale_Price ~ Gr_Liv_Area, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.02587 -0.06577  0.01342  0.07202  0.39231 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 4.855e+00  7.355e-03  660.12   &lt;2e-16 ***\nGr_Liv_Area 2.437e-04  4.648e-06   52.43   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1271 on 2928 degrees of freedom\nMultiple R-squared:  0.4842,    Adjusted R-squared:  0.484 \nF-statistic:  2749 on 1 and 2928 DF,  p-value: &lt; 2.2e-16\n\n\nR-Quadrat via easystats:\n\nlibrary(easystats)\nfit1_performance %&gt;% r2()  # rmse()\n\n# R2 for Linear Regression\n       R2: 0.484\n  adj. R2: 0.484\n\n\n\ntidy(fit1_performance)  # ähnlich zu parameters()\n\n# A tibble: 2 × 5\n  term        estimate  std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) 4.86     0.00736        660.        0\n2 Gr_Liv_Area 0.000244 0.00000465      52.4       0\n\n\n\nsol &lt;- 0.484\n\n\nCategories:\n\nds1\ntidymodels\nprediction\nyacsda\nstatlearning\nnum"
  },
  {
    "objectID": "posts/wozu-balkendiagramm/wozu-balkendiagramm.html",
    "href": "posts/wozu-balkendiagramm/wozu-balkendiagramm.html",
    "title": "wozu-balkendiagramm",
    "section": "",
    "text": "Zu welchem Zweck ist ein Balkendiagramm am besten geeignet?\n\n\n\nUm Mittelwerte zwischen zwei oder mehr Gruppen darzustellen.\nUm Häufigkeiten darzustellen.\nUm metrische Verteilungen darzustellen.\nUm metrische Zusammenhänge darzustellen."
  },
  {
    "objectID": "posts/wozu-balkendiagramm/wozu-balkendiagramm.html#answerlist",
    "href": "posts/wozu-balkendiagramm/wozu-balkendiagramm.html#answerlist",
    "title": "wozu-balkendiagramm",
    "section": "",
    "text": "Um Mittelwerte zwischen zwei oder mehr Gruppen darzustellen.\nUm Häufigkeiten darzustellen.\nUm metrische Verteilungen darzustellen.\nUm metrische Zusammenhänge darzustellen."
  },
  {
    "objectID": "posts/wozu-balkendiagramm/wozu-balkendiagramm.html#answerlist-1",
    "href": "posts/wozu-balkendiagramm/wozu-balkendiagramm.html#answerlist-1",
    "title": "wozu-balkendiagramm",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\nFalsch\nFalsch\n\n\nCategories:\n\nvis\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/Kung-height/Kung-height.html",
    "href": "posts/Kung-height/Kung-height.html",
    "title": "Kung-height",
    "section": "",
    "text": "Exercise\nBetrachten Sie den Datensatz zur Größe der !Kung:\n\nlibrary(tidyverse)\nurl_kung &lt;- \"https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/Howell1.csv\"\nd &lt;-\n  read_delim(url_kung, delim = \";\")  # Strichpunkt als Trennzeichen in der CSV-Datei\n\n\nUntersuchen Sie mit Hilfe eines Diagramms, ob bzw. inwieweit sich die Größe der erwachsenen Personen normalverteilt.\nKennzahlen, die angegeben, inwieweit sich eine Größe normalverteilt, sind Schiefe und Kurtosis. Die Schiefe gibt an, wie symmetrische eine Verteilung ist.\n\nNormalverteilungen sind symmetrisch und haben daher einen Wert von 0 für Schiefe. Kurtosis gibt die “Wölbung”, also wie “spitz” oder “plattgedrückt” eine Verteilung ist. Eine Normalverteilung hat eine Wert von 3 für Kurtosis.\nEntsprechende R-Funktionen finden Sie z.B. im Paket moments. Berechnen Sie die beiden Kennzahlen für die Gruppe der Erwachsenen sowie aufgeteilt nach dem Geschlecht. Interpretieren Sie das Ergebnis.\n\nDiskutieren Sie, inwieweit man aus biologisch fundierten Sachverhalten (also ontologisch) eine Normalverteilung der Körpergröße annehmen kann.\n\n         \n\n\nSolution\n\nVisuelle Prüfung der Normalverteilung\n\n\nd2 &lt;- d %&gt;% \n  filter(age &gt;= 18)\n\nd3 &lt;- d2 %&gt;% \n  select(-male)\n\nggplot(d2, aes(x = height)) +\n  geom_density()\n\nggplot(d2, aes(x = height )) +\n  facet_wrap(~ male) +\n    geom_density()\n\nggplot(d2, aes(x = height)) +\n  facet_wrap(~ male) +\n  geom_histogram(data = d3, fill = \"grey60\", alpha = .6) +\n    geom_histogram() +\n  labs(caption = \"Grau hinterlegt ist das Histogramm für die Daten über beide Geschlechter\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSchiefe und Kurtosis\n\n\nlibrary(easystats)\nd2 %&gt;%  skewness()\n\nParameter | Skewness |    SE\n----------------------------\nheight    |    0.151 | 0.129\nweight    |    0.132 | 0.129\nage       |    0.665 | 0.129\nmale      |    0.126 | 0.129\n\nd2 %&gt;% kurtosis()\n\nParameter | Kurtosis |    SE\n----------------------------\nheight    |   -0.483 | 0.256\nweight    |   -0.506 | 0.256\nage       |   -0.213 | 0.256\nmale      |   -1.996 | 0.256\n\n\n\nNormalverteilung, Begründung\n\nEs ist plausibel anzunehmen, dass der Phänotyp Körpergröße das Resultat des (kausalen) Einflusses vieler Gene ist, vieler Gene, die über einen vergleichbar starken Einfluss verfügen.\nEine besondere Situation stellt das X- bzw. Y-Chromosom dar, das Gene zum Geschlecht bereitstellt. Das Geschlecht ist ein einzelner Faktor, der (erfahrungsgemäß) einen relativ großen Einfluss auf die Körpergröße hat (in Anbetracht, dass vielleicht Tausende Gene additiv die Größe bestimmen). Insofern ist eine klarere Annäherung an die Normalverteilung zu erwarten, wenn man die Geschlechter einzeln betrachtet.\n\nCategories:\n\nbayes\nppv\nprobability"
  },
  {
    "objectID": "posts/movies-vis1/movies-vis1.html",
    "href": "posts/movies-vis1/movies-vis1.html",
    "title": "movies-vis1",
    "section": "",
    "text": "Aufgabe\nImportieren Sie bitte für diese Aufgabe den Datensatz movies (aus dem R-Paket ggplot2movies). Ein Data-Dictionary findet sich hier.\nErstellen Sie folgende Visualisierung:\n\nStreudiagramme mit rating als Y-Variable, und alle übrigen metrischen Variablen als X-Variable.\nLassen Sie aber folgende Variablen außen vor: etwaige ID-Variablen, die Variablen, die die Perzentile der Bewertungen angeben (rX, mit X von 1 bis 10)\nBerücksichtigen Sie nur Actionfilme ab 2000\nVerzichten Sie auf Filme mit einer unterdurchschnittlichen Zahl an Bewertungen (votes; gemessen an allen Filmen, gerundet zur nächsten ganzen Zahl)\n\n         \n\n\nLösung\nPakete starten:\n\nlibrary(tidyverse)\nlibrary(DataExplorer)\n\nDaten importieren:\n\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/ggplot2movies/movies.csv\"\nd &lt;- read.csv(d_path)\n\nDurchschnittliche Zahl an Bewertungen:\n\nd %&gt;% \n  summarise(votes_mean = mean(votes))\n\n  votes_mean\n1   632.1304\n\n\nDie durchschnittliche Zahl an Bewertungen beträgt also 632.\n\nd %&gt;% \n  select(length, budget, rating, year, votes, Action) %&gt;% \n  filter(year &gt;= 2000) %&gt;% \n  filter(Action == 1) %&gt;% \n  filter(votes &gt;= 632) %&gt;% \n  select(-Action) %&gt;% \n  plot_scatterplot(by = \"rating\")\n\nWarning: Removed 66 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nCategories:\n\nvis\neda\nstring"
  },
  {
    "objectID": "posts/chatgpt-sentiment-loop/chatgpt-sentiment-loop.html",
    "href": "posts/chatgpt-sentiment-loop/chatgpt-sentiment-loop.html",
    "title": "chatgpt-sentiment-loop",
    "section": "",
    "text": "Aufgabe\nFragen Sie ChatGPT via API zum Sentiment der ersten zwei Texte aus dem Germeval-2018-Datensatz (Train).\n\n\n\n\n\nHinweise:\n\nBeachten Sie die Standardhinweise des Datenwerks.\nNutzen Sie Python, nicht R.\nDas Verwenden der OpenAI-API kostet Geld. 💸 Informieren Sie sich vorab. Um auf die API zugreifen zu können, müssen Sie sich ein Konto angelegt haben und über ein Guthaben verfügen.\n\n         \n\n\nLösung\n\nOpenAI hat eine neue API (Stand: 2023-11-23), V1.3.5. Der Code der alten API bricht. 💔 \\(\\square\\)\n\nDie richtige venv nutzen:\n\nlibrary(reticulate)\n#virtualenv_create(\"chatgpt\")\nuse_virtualenv(\"chatgpt\")\n\nCheck zu Python:\n\nreticulate::py_config()\n\npython:         /Users/sebastiansaueruser/.virtualenvs/chatgpt/bin/python\nlibpython:      /Users/sebastiansaueruser/.pyenv/versions/3.11.1/lib/libpython3.11.dylib\npythonhome:     /Users/sebastiansaueruser/.virtualenvs/chatgpt:/Users/sebastiansaueruser/.virtualenvs/chatgpt\nversion:        3.11.1 (main, Oct  4 2023, 18:12:06) [Clang 15.0.0 (clang-1500.0.40.1)]\nnumpy:          /Users/sebastiansaueruser/.virtualenvs/chatgpt/lib/python3.11/site-packages/numpy\nnumpy_version:  1.26.2\n\nNOTE: Python version was forced by use_python() function\n\n\nGgf. noch Module installieren:\n\n#reticulate::py_install(\"pandas\")\n\nModule importieren:\n\nfrom openai import OpenAI\nimport pandas as pd\nimport time \nimport tiktoken  # Token zählen\n\nVersionen der importierten Module:\n\npd.__version__\n\n'2.1.3'\n\n\n\n```{zsh}\npip list | grep openai\n```\n\n\n[notice] A new release of pip is available: 23.3.1 -&gt; 23.3.2\n[notice] To update, run: pip install --upgrade pip\nopenai             1.3.5\n\n\nWir brauchen &gt;= 1.35.\nDaten importieren:\n\ncsv_file_path_train = 'https://github.com/sebastiansauer/pradadata/raw/master/data-raw/germeval_train.csv'\n\ngermeval_train = pd.read_csv(csv_file_path_train)\n\nDie ersten paar Texte herausziehen:\n\nn_tweets = 2\ntweets_first_few = germeval_train[\"text\"].head(n_tweets).tolist()\ntweets_first_few\n\n['@corinnamilborn Liebe Corinna, wir würden dich gerne als Moderatorin für uns gewinnen! Wärst du begeisterbar?', '@Martin28a Sie haben ja auch Recht. Unser Tweet war etwas missverständlich. Dass das BVerfG Sachleistungen nicht ausschließt, kritisieren wir.']\n\n\nPrompt definieren:\n\nprompt_stem  = \"Als KI mit Exertise in natürlicher Sprache und Emotionserkennung ist es Ihre Aufgabe, das Sentiment des folgenden Textes zu erkennen. Bitte antworten Sie nur mit einem Wort, entweder 'positiv', 'neutral' oder 'negativ'. Dieses Wort soll die Insgesamt-Einschätzung des Sentiments des Textes zusammenfassen. Nach dem Doppelpunkt folt der Text, dessen Sentiment Sie einschätzen sollen: \\n\"\n\nMit “List Comprehension” können wir die Tweets jeweils mit dem Prompt verknüpfen:\n\nprompts = [prompt_stem + tweet for tweet in tweets_first_few]\nprompts[0]\n\n\"Als KI mit Exertise in natürlicher Sprache und Emotionserkennung ist es Ihre Aufgabe, das Sentiment des folgenden Textes zu erkennen. Bitte antworten Sie nur mit einem Wort, entweder 'positiv', 'neutral' oder 'negativ'. Dieses Wort soll die Insgesamt-Einschätzung des Sentiments des Textes zusammenfassen. Nach dem Doppelpunkt folt der Text, dessen Sentiment Sie einschätzen sollen: \\n@corinnamilborn Liebe Corinna, wir würden dich gerne als Moderatorin für uns gewinnen! Wärst du begeisterbar?\"\n\n\nCheck: Wie viele Elemente hat die Liste prompts?\n\nlen(prompts)\n\n2\n\n\nCheck: Wie viele Tokens hat jeder String (jeder Prompt)?\nWir definieren eine Helper-Funktion:\n\ndef count_tokens(string: str, encoding_name: str) -&gt; int:\n    encoding = tiktoken.get_encoding(encoding_name)\n    num_tokens = len(encoding.encode(string))\n    return num_tokens\n\nUnd zählen:\n\nencoding_name = \"cl100k_base\"\n\nnum_tokens_list = [count_tokens(prompt, encoding_name) for prompt in prompts]\n\nfor i, num_tokens in enumerate(num_tokens_list):\n    print(f\"The number of tokens in Prompt {[i]} is {num_tokens}.\")\n\nThe number of tokens in Prompt [0] is 142.\nThe number of tokens in Prompt [1] is 153.\n\n\nMehr Infos zum Encoding bei ChatGPT finden sich hier.\nLaut OpenAI kostet 1k Token für das Modell gpt-3.5-turbo-1106 $0.001.\nAnmelden bei OpenAI:\n\nclient = OpenAI()\n\n\n\n\n\n\n\nNote\n\n\n\nDieses Anmeldeverfahren setzt voraus, dass in .Renviron die Variable OPENAI_API_KEY hinterlegt ist. \\(\\square\\)\n\n\nAnfrage an die API, in eine Funktion gepackt:\n\ndef get_completion(prompt, client_instance, model=\"gpt-3.5-turbo\"):\n  messages = [{\"role\": \"user\", \"content\": prompt}]\n  response = client_instance.chat.completions.create(\n    model=model,\n    messages=messages,\n    max_tokens=50,\n    temperature=0,\n  )\n  return response.choices[0].message.content\n\nUnd jetzt als Schleife. Ergebnisliste anlegen, am Anfang noch leer:\n\nresults = []\n\n\nstart_time = time.time()\n\nfor prompt in prompts:\n  result = get_completion(prompt, client) \n  results.append(result)\n\nend_time = time.time()\nend_time - start_time\n\n2.179842948913574\n\n\nVoilà:\n\nprint(results)\n\n['positiv', 'neutral']"
  },
  {
    "objectID": "posts/ReThink3m3/ReThink3m3.html",
    "href": "posts/ReThink3m3/ReThink3m3.html",
    "title": "ReThink3m3",
    "section": "",
    "text": "Exercise\nNehmen wir an, wir haben 8 (Wasser-)“Treffer” (\\(W=8\\)) bei 15 Würfen (\\(N=15\\)) erhalten (wieder im Globusversuch).\n\nFühren Sie einen Posteriori-Prädiktiv-Check durch: Erstellen Sie also eine Posteriori-Prädiktiv-Verteilung (PPV). Mit anderen Worten: Erstellen Sie die Stichprobenverteilung, gemittelt über die Posteriori-Wahrscheinlichkeiten des Wasseranteils \\(p\\)!\nVisualisieren Sie die PPV!\nWas ist die Wahrscheinlichkeit laut PPV 8 von 15 Treffer zu erzielen (also 8 Wasser in 15 Würfen)?\n\nHinweise:\n\nBerechnen Sie eine Bayes-Box (Gittermethode).\nVerwenden Sie 1000 Gitterwerte.\nGehen Sie von einem gleichverteilten Prior aus.\nFixieren Sie die Zufallszahlen mit dem Startwert 42, d.h. set.seed(42).\n\nQuelle: McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Aufl.). Taylor and Francis, CRC Press.\n         \n\n\nSolution\n\nlibrary(tidyverse)\n\n\n\n\nErstellen wir zuerst wieder die Posteriori-Verteilung für den Globusversuch.\n\np_grid &lt;- seq( from=0 , to=1 , length.out=1000 )  # Gitterwerte\n\nprior &lt;- rep(1, 1000 )  # Priori-Gewichte\n\nlikelihood &lt;- dbinom(8 , size= 15, prob=p_grid ) \n\nunstandardisierte_posterior &lt;- likelihood * prior \n\nposterior &lt;- unstandardisierte_posterior / sum(unstandardisierte_posterior)\n\nDann ziehen wir unsere Stichproben daraus:\n\n# um die Zufallszahlen festzulegen, damit alle die gleichen Zufallswerte bekommen: \nset.seed(42) \n\n# Stichproben ziehen aus der Posteriori-Verteilung\nsamples &lt;- \n  tibble(\n    p = sample(p_grid , prob=posterior, size=1e4, replace=TRUE))\n\n\nPPV &lt;- \n  samples %&gt;% \n  mutate( anzahl_wasser = rbinom(1e4, size = 15, prob = p))\n\nDurch prob = p gewichten wir die Wahrscheinlichkeit an den Werten der Posteriori-Verteilung.\nSo sehen die ersten paar Zeilen von PPV aus:\n\n\n\n\n\n\n\n\np\nanzahl_wasser\n\n\n\n\n0.4304304\n4\n\n\n0.5575576\n11\n\n\n0.6516517\n4\n\n\n0.6156156\n9\n\n\n0.6716717\n6\n\n\n\n\n\n\n\n\n\n\n\nPPV %&gt;% \n  ggplot() +\n  aes(x = anzahl_wasser) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n\nPPV %&gt;% \n  count(anzahl_wasser == 8)\n\n# A tibble: 2 × 2\n  `anzahl_wasser == 8`     n\n  &lt;lgl&gt;                &lt;int&gt;\n1 FALSE                 8536\n2 TRUE                  1464\n\n\nAlternativer R-Code:\n\nw &lt;- rbinom(1e4, size = 15, prob = samples$p)\nmean(w == 8)\n\n[1] 0.1504\n\n\nQuelle\n\nCategories:\n\nbayes\nppv\nprobability"
  },
  {
    "objectID": "posts/kausal25/kausal25.html",
    "href": "posts/kausal25/kausal25.html",
    "title": "kausal25",
    "section": "",
    "text": "Gegeben sei der DAG g (s. u.). Der DAG verfügt über 6 Variablen, die als Knoten im Graph dargestellt sind und mit \\(x_1, x_2, \\ldots x_n\\) bezeichnet sind.\nWelche minimale Variablenmenge muss kontrolliert werden, um den kausalen Effekt von der UV zur AV zu identifizieren?\nUV: x4.\nAV: x5.\nHinweise:\n\nMengen sind mittels geschweifter Klammern gekennzeichnet, z.B. {x8, x9} meint die Menge mit den zwei Elementen x8 und x9.\nDie leere Menge { } bedeutet, dass keine Variable kontrolliert werden muss, um den kausalen Effekt zu identifizieren.\nAlle Variablen werden als gemessen vorausgesetzt.\nEs ist möglich, dass es keine Lösung gibt, dass es also keine Adjustierungsmenge gibt, um den kausalen Effekt zu identifizieren. Wenn dies der Fall sein sollte, wählen Sie “/”.\nEs ist möglich, dass einzelne Variablen keine Kanten besitzen, also keine Verbindung zu anderen Variablen (Knoten) haben.\n\n\n\n\n\n\n\n\n\n\n\n\n\n{ x4 }\n{ }\n{ x1, x5 }\n{ x4, x6 }\n{ x3, x4 }"
  },
  {
    "objectID": "posts/kausal25/kausal25.html#answerlist",
    "href": "posts/kausal25/kausal25.html#answerlist",
    "title": "kausal25",
    "section": "",
    "text": "{ x4 }\n{ }\n{ x1, x5 }\n{ x4, x6 }\n{ x3, x4 }"
  },
  {
    "objectID": "posts/kausal25/kausal25.html#answerlist-1",
    "href": "posts/kausal25/kausal25.html#answerlist-1",
    "title": "kausal25",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nRichtig\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/mariokart-sd2/mariokart-sd2.html",
    "href": "posts/mariokart-sd2/mariokart-sd2.html",
    "title": "mariokart-sd2",
    "section": "",
    "text": "Aufgabe\nImportieren Sie den Datensatz mariokart in R. Berechnen Sie die SD des Verkaufspreis (total_pr)!\nHinweise:\n\nRunden Sie auf 1 Dezimalstelle.\n\n         \n\n\nLösung\nPakete starten:\n\nlibrary(tidyverse)\nlibrary(easystats)\n\nDaten importieren:\n\nd_url &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\"\nmariokart &lt;- data_read(d_url)\n\n\nmariokart %&gt;% \n  summarise(pr_sd = sd(total_pr))\n\n     pr_sd\n1 25.68856\n\n\nLösung: 25.7.\n\nCategories:\n\ndatawrangling\ndplyr\neda\nvariability\nnum"
  },
  {
    "objectID": "posts/Skalenniveau1b/Skalenniveau1b.html",
    "href": "posts/Skalenniveau1b/Skalenniveau1b.html",
    "title": "Skalenniveau1b",
    "section": "",
    "text": "Variable\n\n\n\n\nErgebnis bei Talent-Show (1. Platz Mr. Cool, 2. Platz Mr. Bright, 3. Platz Mr. Right)\n\n\nGewicht eines Tieres\n\n\nAugenfarbe\n\n\nAlter\n\n\n\n\n\n\n\nGeben Sie für jede der folgenden vier Variable an, ob sie über ein metrisches Skalenniveau verfügt!"
  },
  {
    "objectID": "posts/Skalenniveau1b/Skalenniveau1b.html#answerlist",
    "href": "posts/Skalenniveau1b/Skalenniveau1b.html#answerlist",
    "title": "Skalenniveau1b",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nRichtig\nFalsch\nRichtig\n\n\nCategories:\n\ndyn\nvariable-levels\nvariable-levels\nmchoice"
  },
  {
    "objectID": "posts/Priorwahl2/Priorwahl2.html",
    "href": "posts/Priorwahl2/Priorwahl2.html",
    "title": "Priorwahl2",
    "section": "",
    "text": "Betrachten wir den biologisch fundierten Zusammenhang von Gewicht (UV) und Körpergröße (AV).\nWelche der folgenden Priori-Verteilungen passt am besten für \\(\\beta\\)?\nGehen Sie von z-standardisierten Variablen aus.\n\n\n\n\\(N(0,1)\\)\n\\(N(0,100)\\)\n\\(N(1,0)\\)\n\\(N(0,0)\\)\n\\(N(-1,1)\\)"
  },
  {
    "objectID": "posts/Priorwahl2/Priorwahl2.html#answerlist",
    "href": "posts/Priorwahl2/Priorwahl2.html#answerlist",
    "title": "Priorwahl2",
    "section": "",
    "text": "\\(N(0,1)\\)\n\\(N(0,100)\\)\n\\(N(1,0)\\)\n\\(N(0,0)\\)\n\\(N(-1,1)\\)"
  },
  {
    "objectID": "posts/Priorwahl2/Priorwahl2.html#answerlist-1",
    "href": "posts/Priorwahl2/Priorwahl2.html#answerlist-1",
    "title": "Priorwahl2",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr. Plausibler Prior. Bei z-standardisierten Werten sind die Koeffizienten meist kleiner 1. Noch sinnvoller wäre vermutlich, wenn \\(\\mu &gt; 0\\) und nicht \\(\\mu=0\\).\nFalsch. Zu weit.\nFalsch. Keine Streuung.\nFalsch. Keine Streuung.\nFalsch. Negativer Mittelwert ist nicht sehr plausibel.\n\n\nCategories:\n\nregression\nbayes\ndistribution"
  },
  {
    "objectID": "posts/knn-ames01/knn-ames01.html",
    "href": "posts/knn-ames01/knn-ames01.html",
    "title": "knn-ames01",
    "section": "",
    "text": "Aufgabe\nBerechnen Sie ein knn-Modell für den Datensatz ames!\nNutzen Sie diese Modellformel: Sale_Price ~ Lot_Area + Fireplaces + Longitude + Latitude.\nBerichten Sie die Modellgüte.\nHinweise:\n\nTunen Sie \\(k\\) mit den Werten 1 bis 10.\nTeilen Sie in Train- und Test-Sample auf.\nVerwenden Sie Defaults der Funktionen, wo nicht anders angegeben.\nz-Transformieren Sie die Prädiktoren.\nVerwenden Sie den RSME als Kennzahl der Modellgüte.\n\n         \n\n\nLösung\n\nlibrary(tidymodels)\ndata(ames)\n\nDaten aufteilen:\n\nd_split &lt;- initial_split(ames)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\nModell definieren:\n\nmod1 &lt;-\n  nearest_neighbor(\n    mode = \"regression\",\n    neighbors = tune())  # k-Wert zum Tunen taggen\n\nRezept definieren:\n\nrec1 &lt;-\n  recipe(Sale_Price ~ Lot_Area + Fireplaces + Longitude + Latitude, data = d_split) %&gt;% \n  step_normalize(all_predictors())\n\nWorkflow definieren:\n\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(mod1) %&gt;% \n  add_recipe(rec1)\n\nResampling definieren:\n\ncv1 &lt;- vfold_cv(d_train)\n\nTuning definieren:\n\nk_grid &lt;-\n  tibble(neighbors = 1:10)\n\nFitting:\n\nfit1 &lt;-\n  tune_grid(wf1,\n            resamples = vfold_cv(d_train),\n            metrics = metric_set(rmse),  # nur RMSE als Modellgüte, Default ist RMSE und R2\n            grid = k_grid,\n            control = control_grid(save_workflow = TRUE)  # nur nötig für \"fit_best\", s.u.\n            )\n\nMetriken im Train-Sample (genauer: im Assessment-Sample):\n\nshow_best(fit1)\n\n# A tibble: 5 × 7\n  neighbors .metric .estimator   mean     n std_err .config              \n      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1        10 rmse    standard   43067.    10   1798. Preprocessor1_Model10\n2         9 rmse    standard   43089.    10   1774. Preprocessor1_Model09\n3         8 rmse    standard   43115.    10   1738. Preprocessor1_Model08\n4         7 rmse    standard   43181.    10   1708. Preprocessor1_Model07\n5         6 rmse    standard   43317.    10   1682. Preprocessor1_Model06\n\n\n(Komplettes) Train-Sample mit bestem Tuning-Kandidat fitten:\n\ntune1_best &lt;- fit_best(fit1)\n\nIm Test-Sample predicten:\n\nfit_test &lt;- last_fit(tune1_best, d_split)\n\nMetriken einsammeln:\n\ncollect_metrics(fit_test)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   42554.    Preprocessor1_Model1\n2 rsq     standard       0.717 Preprocessor1_Model1\n\n\nDie Lösung lautet 4.2554397^{4}.\n\nCategories:\n\nstatlearning\ntidymodels\nnum"
  },
  {
    "objectID": "posts/Boosting2/Boosting2.html",
    "href": "posts/Boosting2/Boosting2.html",
    "title": "Boosting2",
    "section": "",
    "text": "Boosting ist eine beliebte Methode des statistischen Lernens, da sie sich in vielen prädiktiven Fragestellungen als hoch prädiktiv herausgestellt hat. Die Modellfunktion von Boosting kann man so darstellen:\n\\[\\hat{f}(x)=\\sum_{b=1}^B \\lambda \\hat{f}^b(x)\\]\n(Dabei stellt \\(B\\) die Anzahl der Bäume im Modell dar und \\(\\lambda\\) einen Tuningparamter zur Penalisierung/Regularisierung bzw. die Lernrate des Modells.)\nWelche Aussage ist in diesem Zusammenhang korrekt?\n\n\n\nBoosting gleicht einem Random-Forest-Modell, nur dass die Bäume sequenzielle Modelle darstellen und nicht parallel (gleichzeitig) in ein Modell einfließen.\nBoosting-Modelle bestehen aus einer Sequenz von Bäumen mit jeweils nur einer Inputvariablen (Gabelung/Split; internal nodes).\nAlle Boosting-Modelle erfüllen obige Funktionsgleichung und sind daher immer linear.\nBoosting-Modelle analysieren im Allgemeinen in jedem der \\(B\\) Durchläufe einen anderen Datensatz.\nDer Parameter \\(B\\) sollte nicht über Kreuzvalidierungsmethoden bestimmt werden."
  },
  {
    "objectID": "posts/Boosting2/Boosting2.html#answerlist",
    "href": "posts/Boosting2/Boosting2.html#answerlist",
    "title": "Boosting2",
    "section": "",
    "text": "Boosting gleicht einem Random-Forest-Modell, nur dass die Bäume sequenzielle Modelle darstellen und nicht parallel (gleichzeitig) in ein Modell einfließen.\nBoosting-Modelle bestehen aus einer Sequenz von Bäumen mit jeweils nur einer Inputvariablen (Gabelung/Split; internal nodes).\nAlle Boosting-Modelle erfüllen obige Funktionsgleichung und sind daher immer linear.\nBoosting-Modelle analysieren im Allgemeinen in jedem der \\(B\\) Durchläufe einen anderen Datensatz.\nDer Parameter \\(B\\) sollte nicht über Kreuzvalidierungsmethoden bestimmt werden."
  },
  {
    "objectID": "posts/Boosting2/Boosting2.html#answerlist-1",
    "href": "posts/Boosting2/Boosting2.html#answerlist-1",
    "title": "Boosting2",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch. Im Gegensatz zu Random-Forest-Modellen wird im Boosting u.a. kein Resampling verwendet.\nFalsch. Boosting-Modelle können mehr als eine Gabelung enthalten.\nFalsch. Boosting-Modelle sind nur dann linear, wenn Sie nur eine Gabelung enthalten.\nRichtig. In jedem Durchlauf wird der analysierte Datensatz verändert, insofern als das jeweils die Residuen des vorherigen Durchlaufs den Datensatz des nächsten Durchlaufs bilden.\nFalsch. Es macht Sinn, den Paramter \\(B\\) über Kreuzvalidierungsmethoden zu bestimmen. Allerdings ist die Überfittingsgefahr relativ gering.\n\n\nCategories:\nschoice"
  },
  {
    "objectID": "posts/Szenario-charakterisieren1/Szenario-charakterisieren1.html",
    "href": "posts/Szenario-charakterisieren1/Szenario-charakterisieren1.html",
    "title": "Szenario-charakterisieren1",
    "section": "",
    "text": "Angenommen, Sie arbeiten als Analyst mit folgender Aufgabe:\nEs liegt ein Datensatz mit 600 Beschäftigten (als Beobachtungseinheit) vor. Für jede Person sind folgende Informationen bekannt: Dauer der Betriebszugehörigkeit, Alter, Ausbildung und Ergebnis der letzten Leistungsbeurteilung. Ziel ist es, die Höhe des zu erwartenden Gehalts vorherzusagen.\n\n\n\nEs handelt sich um eine Regression. Ziel ist eine Vorhersage. \\(N=600\\), \\(p=4\\).\nEs handelt sich um eine Klassifikation. Ziel ist eine Vorhersage. \\(N=600\\), \\(p=4\\).\nEs handelt sich um eine Regression. Ziel ist eine Erklärung (inference) \\(N=600\\), \\(p=4\\).\nEs handelt sich um eine Regression. Ziel ist eine Vorhersage. \\(N=600\\), \\(p=5\\).\nEs handelt sich um eine Klassifikation. Ziel ist eine Vorhersage. \\(N=600\\), \\(p=4\\).\nEs handelt sich um eine Regression. Ziel ist eine Vorhersage. \\(N=600\\), \\(p=4\\). Es handelt sich um eine unüberwachte (unsupervised) Analyse."
  },
  {
    "objectID": "posts/Szenario-charakterisieren1/Szenario-charakterisieren1.html#answerlist",
    "href": "posts/Szenario-charakterisieren1/Szenario-charakterisieren1.html#answerlist",
    "title": "Szenario-charakterisieren1",
    "section": "",
    "text": "Es handelt sich um eine Regression. Ziel ist eine Vorhersage. \\(N=600\\), \\(p=4\\).\nEs handelt sich um eine Klassifikation. Ziel ist eine Vorhersage. \\(N=600\\), \\(p=4\\).\nEs handelt sich um eine Regression. Ziel ist eine Erklärung (inference) \\(N=600\\), \\(p=4\\).\nEs handelt sich um eine Regression. Ziel ist eine Vorhersage. \\(N=600\\), \\(p=5\\).\nEs handelt sich um eine Klassifikation. Ziel ist eine Vorhersage. \\(N=600\\), \\(p=4\\).\nEs handelt sich um eine Regression. Ziel ist eine Vorhersage. \\(N=600\\), \\(p=4\\). Es handelt sich um eine unüberwachte (unsupervised) Analyse."
  },
  {
    "objectID": "posts/Szenario-charakterisieren1/Szenario-charakterisieren1.html#answerlist-1",
    "href": "posts/Szenario-charakterisieren1/Szenario-charakterisieren1.html#answerlist-1",
    "title": "Szenario-charakterisieren1",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\nschoice"
  },
  {
    "objectID": "posts/regr-tree03/regr-tree03.html",
    "href": "posts/regr-tree03/regr-tree03.html",
    "title": "regr-tree03",
    "section": "",
    "text": "library(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.0.8\n✔ dials        1.2.0     ✔ rsample      1.2.0\n✔ dplyr        1.1.3     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/"
  },
  {
    "objectID": "posts/regr-tree03/regr-tree03.html#setup",
    "href": "posts/regr-tree03/regr-tree03.html#setup",
    "title": "regr-tree03",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidymodels)\ndata(mtcars)\nlibrary(tictoc)  # Zeitmessung\n\nFür Klassifikation verlangt Tidymodels eine nominale AV, keine numerische:\n\nmtcars &lt;-\n  mtcars %&gt;% \n  mutate(am = factor(am))"
  },
  {
    "objectID": "posts/regr-tree03/regr-tree03.html#daten-teilen",
    "href": "posts/regr-tree03/regr-tree03.html#daten-teilen",
    "title": "regr-tree03",
    "section": "Daten teilen",
    "text": "Daten teilen\n\nd_split &lt;- initial_split(mtcars)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)"
  },
  {
    "objectID": "posts/regr-tree03/regr-tree03.html#modelle",
    "href": "posts/regr-tree03/regr-tree03.html#modelle",
    "title": "regr-tree03",
    "section": "Modell(e)",
    "text": "Modell(e)\n\nmod_tree &lt;-\n  decision_tree(mode = \"classification\",\n                cost_complexity = tune(),\n                tree_depth = tune(),\n                min_n = tune())"
  },
  {
    "objectID": "posts/regr-tree03/regr-tree03.html#rezepte",
    "href": "posts/regr-tree03/regr-tree03.html#rezepte",
    "title": "regr-tree03",
    "section": "Rezept(e)",
    "text": "Rezept(e)\n\nrec1 &lt;- \n  recipe(am ~ ., data = d_train)"
  },
  {
    "objectID": "posts/regr-tree03/regr-tree03.html#resampling",
    "href": "posts/regr-tree03/regr-tree03.html#resampling",
    "title": "regr-tree03",
    "section": "Resampling",
    "text": "Resampling\n\nrsmpl &lt;- vfold_cv(d_train, v = 2)"
  },
  {
    "objectID": "posts/regr-tree03/regr-tree03.html#workflow",
    "href": "posts/regr-tree03/regr-tree03.html#workflow",
    "title": "regr-tree03",
    "section": "Workflow",
    "text": "Workflow\n\nwf1 &lt;-\n  workflow() %&gt;%  \n  add_recipe(rec1) %&gt;% \n  add_model(mod_tree)"
  },
  {
    "objectID": "posts/regr-tree03/regr-tree03.html#tuningfitting",
    "href": "posts/regr-tree03/regr-tree03.html#tuningfitting",
    "title": "regr-tree03",
    "section": "Tuning/Fitting",
    "text": "Tuning/Fitting\nTuninggrid:\n\ntune_grid &lt;- grid_regular(extract_parameter_set_dials(mod_tree), levels = 5)\ntune_grid\n\n# A tibble: 125 × 3\n   cost_complexity tree_depth min_n\n             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt;\n 1    0.0000000001          1     2\n 2    0.0000000178          1     2\n 3    0.00000316            1     2\n 4    0.000562              1     2\n 5    0.1                   1     2\n 6    0.0000000001          4     2\n 7    0.0000000178          4     2\n 8    0.00000316            4     2\n 9    0.000562              4     2\n10    0.1                   4     2\n# ℹ 115 more rows\n\n\n\ntic()\nfit1 &lt;-\n  tune_grid(object = wf1,\n            grid = tune_grid,\n            metrics = metric_set(roc_auc),\n            resamples = rsmpl)\n\n→ A | warning: 21 samples were requested but there were 12 rows in the data. 12 will be used.\n\n\nThere were issues with some computations   A: x1\n\n\n→ B | warning: 30 samples were requested but there were 12 rows in the data. 12 will be used.\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x25   B: x1\nThere were issues with some computations   A: x25   B: x23\n→ C | warning: 40 samples were requested but there were 12 rows in the data. 12 will be used.\nThere were issues with some computations   A: x25   B: x23\nThere were issues with some computations   A: x25   B: x25   C: x18\nThere were issues with some computations   A: x26   B: x25   C: x25\nThere were issues with some computations   A: x27   B: x25   C: x25\nThere were issues with some computations   A: x50   B: x25   C: x25\nThere were issues with some computations   A: x50   B: x48   C: x25\nThere were issues with some computations   A: x50   B: x50   C: x45\nThere were issues with some computations   A: x50   B: x50   C: x50\n\ntoc()\n\n29.277 sec elapsed"
  },
  {
    "objectID": "posts/regr-tree03/regr-tree03.html#bester-kandidat",
    "href": "posts/regr-tree03/regr-tree03.html#bester-kandidat",
    "title": "regr-tree03",
    "section": "Bester Kandidat",
    "text": "Bester Kandidat\n\nautoplot(fit1)\n\n\n\n\n\n\n\n\n\nshow_best(fit1)\n\n# A tibble: 5 × 9\n  cost_complexity tree_depth min_n .metric .estimator  mean     n std_err\n            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1    0.0000000001          1     2 roc_auc binary     0.825     2   0.075\n2    0.0000000178          1     2 roc_auc binary     0.825     2   0.075\n3    0.00000316            1     2 roc_auc binary     0.825     2   0.075\n4    0.000562              1     2 roc_auc binary     0.825     2   0.075\n5    0.1                   1     2 roc_auc binary     0.825     2   0.075\n# ℹ 1 more variable: .config &lt;chr&gt;"
  },
  {
    "objectID": "posts/regr-tree03/regr-tree03.html#finalisieren",
    "href": "posts/regr-tree03/regr-tree03.html#finalisieren",
    "title": "regr-tree03",
    "section": "Finalisieren",
    "text": "Finalisieren\n\nwf1_finalized &lt;-\n  wf1 %&gt;% \n  finalize_workflow(select_best(fit1))"
  },
  {
    "objectID": "posts/regr-tree03/regr-tree03.html#last-fit",
    "href": "posts/regr-tree03/regr-tree03.html#last-fit",
    "title": "regr-tree03",
    "section": "Last Fit",
    "text": "Last Fit\n\nfinal_fit &lt;- \n  last_fit(object = wf1_finalized, d_split)\n\ncollect_metrics(final_fit)\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.75  Preprocessor1_Model1\n2 roc_auc  binary         0.833 Preprocessor1_Model1\n\n\n\nCategories:\n\nstatlearning\ntrees\ntidymodels\nstring"
  },
  {
    "objectID": "posts/mtcars-simple2/mtcars-simple2.html",
    "href": "posts/mtcars-simple2/mtcars-simple2.html",
    "title": "mtcars-simple2",
    "section": "",
    "text": "Exercise\nWe will use the dataset mtcars in this exercise.\nAssume your causal model of your research dictates that fuel economy is a linear function of horse power, cylinder count and displacement of the engine.\nCompute the explained variance (point estimate) for the above model!\nNotes:\n\nUse can either use frequentist or bayesian modeling.\nUse R for all computations.\nThere are multiple ways to find a solution.\n\n         \n\n\nSolution\nCompute Model:\n\nlm1_freq &lt;- lm(mpg ~ hp + cyl + disp, data = mtcars)\n\nlibrary(rstanarm)\nlm1_bayes &lt;- stan_glm(mpg ~ hp + cyl + disp, data = mtcars, refresh = 0)\n\nGet R2:\n\nlibrary(easystats)\n\n\nr2(lm1_freq)\n\n# R2 for Linear Regression\n       R2: 0.768\n  adj. R2: 0.743\n\n\n\nr2(lm1_bayes)\n\n# Bayesian R2 with Compatibility Interval\n\n  Conditional R2: 0.749 (95% CI [0.599, 0.849])\n\n\nThe coefficient is estimated as about 0.77.\n\nCategories:\n\nregression\nen\nbayes\nfrequentist\nqm1\nstats-nutshell"
  },
  {
    "objectID": "posts/Streudiagramm/Streudiagramm.html",
    "href": "posts/Streudiagramm/Streudiagramm.html",
    "title": "Streudiagramm",
    "section": "",
    "text": "-.90\n0\n+.90\n1\n-1"
  },
  {
    "objectID": "posts/Streudiagramm/Streudiagramm.html#answerlist",
    "href": "posts/Streudiagramm/Streudiagramm.html#answerlist",
    "title": "Streudiagramm",
    "section": "",
    "text": "-.90\n0\n+.90\n1\n-1"
  },
  {
    "objectID": "posts/Streudiagramm/Streudiagramm.html#answerlist-1",
    "href": "posts/Streudiagramm/Streudiagramm.html#answerlist-1",
    "title": "Streudiagramm",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nvis\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/penguins-vis-bodymass2/index.html",
    "href": "posts/penguins-vis-bodymass2/index.html",
    "title": "penguins-vis-bodymass2",
    "section": "",
    "text": "Aufgabe\nIm Datensatz palmerpenguins: Ist der Zusammenhang zwischen Körpergewicht und Schnabelhöhe (bill depth) (vgl. Schemazeichnung hier) größer, wenn man den Zusammenhang getrennt für jede Spezies betrachtet?\nBeantworten Sie diese Frage mit Hilfe einer Visualisierung!\nSie können den Datensatz so beziehen:\n\n#install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\ndata(\"penguins\")\nd &lt;- penguins \n\nOder so:\n\nd &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\")\n\nEin Codebook finden Sie hier.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\nNutzen Sie das R-Paket ggpubr zur Visualisierung. Dort finden Sie einen Befehl namens ggscatter(datensatz, x-variable, y_variable, facet_by), mit dem Sie Streudiagramme aufgeteilt nach (“facettiert nach”) einer (nominal skalierten) Gruppierungsvariablen erstellen können.\n\n\n\nLösung\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggpubr)\n\n\nd &lt;- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\")\n\n\nd |&gt; \n  ggscatter(x = \"bill_depth_mm\", y = \"body_mass_g\", facet.by = \"species\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nUnd jetzt erstellen wir das Streudiagramm ohne Aufteilung in die Gruppen von species:\n\nggscatter(d, x = \"bill_depth_mm\", y = \"body_mass_g\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nWie man sieht, tritt der Zusammenhang klarer hervor, wenn man die Daten in die von species definierten Gruppen aufteilt."
  },
  {
    "objectID": "posts/filter-na5/filter-na5.html",
    "href": "posts/filter-na5/filter-na5.html",
    "title": "filter-na5",
    "section": "",
    "text": "Zählen Sie fehlende Werte im Datensatz penguins zeilenweise."
  },
  {
    "objectID": "posts/filter-na5/filter-na5.html#setup",
    "href": "posts/filter-na5/filter-na5.html#setup",
    "title": "filter-na5",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\"\nd &lt;- read_csv(d_path)\nnrow(d)\n\n[1] 344"
  },
  {
    "objectID": "posts/filter-na5/filter-na5.html#weg-1",
    "href": "posts/filter-na5/filter-na5.html#weg-1",
    "title": "filter-na5",
    "section": "Weg 1",
    "text": "Weg 1\n\napply(d, 1, function(x) sum(is.na(x)))\n\n  [1] 0 0 0 5 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[149] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n[186] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n[223] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n[260] 0 0 0 0 0 0 0 0 0 1 0 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[297] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[334] 0 0 0 0 0 0 0 0 0 0 0"
  },
  {
    "objectID": "posts/filter-na5/filter-na5.html#weg-2",
    "href": "posts/filter-na5/filter-na5.html#weg-2",
    "title": "filter-na5",
    "section": "Weg 2",
    "text": "Weg 2\n\nd_na_only &lt;- \n  d %&gt;% \n  rowwise() %&gt;% \n  mutate(na_n = sum(is.na(cur_data()))) %&gt;%  # \"na_n\" für \"Anzahl (n) NA\"\n  ungroup()\n\nd_na_only %&gt;% \n  pull(na_n)\n\n  [1] 0 0 0 5 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[149] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n[186] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n[223] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n[260] 0 0 0 0 0 0 0 0 0 1 0 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[297] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[334] 0 0 0 0 0 0 0 0 0 0 0"
  },
  {
    "objectID": "posts/filter-na5/filter-na5.html#weg-3",
    "href": "posts/filter-na5/filter-na5.html#weg-3",
    "title": "filter-na5",
    "section": "Weg 3",
    "text": "Weg 3\n\nrowSums(is.na(d))\n\n  [1] 0 0 0 5 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[149] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n[186] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n[223] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n[260] 0 0 0 0 0 0 0 0 0 1 0 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[297] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[334] 0 0 0 0 0 0 0 0 0 0 0"
  },
  {
    "objectID": "posts/filter-na5/filter-na5.html#weg-4",
    "href": "posts/filter-na5/filter-na5.html#weg-4",
    "title": "filter-na5",
    "section": "Weg 4",
    "text": "Weg 4\nDer folgende Weg funktioniert nur, wenn alle Variablen vom Typ numeric sind.\n\nd %&gt;% \n  rowwise() %&gt;% \n  mutate(na_n = sum(is.na(c_across(everything()))))\n\nError in `mutate()`:\nℹ In argument: `na_n = sum(is.na(c_across(everything())))`.\nℹ In row 1.\nCaused by error in `vec_c()`:\n! Can't combine `rownames` &lt;double&gt; and `species` &lt;character&gt;."
  },
  {
    "objectID": "posts/filter-na5/filter-na5.html#weg-5",
    "href": "posts/filter-na5/filter-na5.html#weg-5",
    "title": "filter-na5",
    "section": "Weg 5",
    "text": "Weg 5\nWir definieren eine Funktion, die die NAs des Vektors x zählt:\n\nsum_na &lt;- function(x) {sum(is.na(x))}\n\nDiese Funktion “mappen” wir wir auf jede Spalte (und lassen uns einen numerischen Vektor dbl (“double”) zurückgeben):\n\nd |&gt; \n  map_dbl(sum_na)\n\n         rownames           species            island    bill_length_mm \n                0                 0                 0                 2 \n    bill_depth_mm flipper_length_mm       body_mass_g               sex \n                2                 2                 2                11 \n             year \n                0 \n\n\n\nCategories:\n\n2023\neda\nna\nstring"
  },
  {
    "objectID": "posts/anova-skalenniveau/anova-skalenniveau.html",
    "href": "posts/anova-skalenniveau/anova-skalenniveau.html",
    "title": "anova-skalenniveau",
    "section": "",
    "text": "Die Varianzanalyse (ANOVA) ist ein inferenzstatistisches Verfahren des Frequentismus. Welches Skalenniveau passt zu diesem Verfahren?\n\n\n\nUV: nominal (mehrstufig), AV: metrisch\nUV: metrisch, AV: nominal (zweistufig)\nUV: nominal (mehrstufig), AV: nominal (mehrstufig)\nUV: metrisch, AV: nominal (zweistufig)\nUV: nominal (zweistufig), AV: metrisch"
  },
  {
    "objectID": "posts/anova-skalenniveau/anova-skalenniveau.html#answerlist",
    "href": "posts/anova-skalenniveau/anova-skalenniveau.html#answerlist",
    "title": "anova-skalenniveau",
    "section": "",
    "text": "UV: nominal (mehrstufig), AV: metrisch\nUV: metrisch, AV: nominal (zweistufig)\nUV: nominal (mehrstufig), AV: nominal (mehrstufig)\nUV: metrisch, AV: nominal (zweistufig)\nUV: nominal (zweistufig), AV: metrisch"
  },
  {
    "objectID": "posts/anova-skalenniveau/anova-skalenniveau.html#answerlist-1",
    "href": "posts/anova-skalenniveau/anova-skalenniveau.html#answerlist-1",
    "title": "anova-skalenniveau",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nvariable-levels\nanova"
  },
  {
    "objectID": "posts/purrr-map02/purrr-map02.html",
    "href": "posts/purrr-map02/purrr-map02.html",
    "title": "purrr-map02",
    "section": "",
    "text": "Exercise\nBestimmen Sie die häufigsten Worte im Grundatzprogramm der Partei AfD (in der aktuellsten Version).\n         \n\n\nSolution\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nText aus PDF-Dateien kann man mit dem Paket pdftools einlesen:\n\nlibrary(pdftools)\n\nUsing poppler version 22.02.0\n\nd_path &lt;- \"~/Literatur/_Div/Politik/afd-grundsatzprogramm-2022.pdf\"\n\nd &lt;- tibble(text = pdf_text(d_path))\n\nDann erstellen wir eine Tidy-Version und tokenisieren nach Wörtern:\n\nlibrary(tidytext)\nd2 &lt;-\n  d %&gt;% \n  unnest_tokens(output = word, input = text)\n\nhead(d2)\n\n# A tibble: 6 × 1\n  word             \n  &lt;chr&gt;            \n1 programm         \n2 für              \n3 deutschland      \n4 das              \n5 grundsatzprogramm\n6 der              \n\n\nDann zählen wir die Wörter:\n\nd2 %&gt;% \n  count(word, sort = TRUE) %&gt;% \n  head(20)\n\n# A tibble: 20 × 2\n   word            n\n   &lt;chr&gt;       &lt;int&gt;\n 1 die          1151\n 2 und          1147\n 3 der           870\n 4 zu            435\n 5 für           392\n 6 in            392\n 7 den           271\n 8 von           257\n 9 ist           251\n10 das           225\n11 werden        214\n12 eine          211\n13 nicht         196\n14 ein           191\n15 deutschland   190\n16 sind          187\n17 wir           176\n18 afd           171\n19 des           169\n20 sich          158\n\n\n\nCategories:\n\nR\nmap\ntidyverse"
  },
  {
    "objectID": "posts/diamonds-histogram/diamonds-histogram.html",
    "href": "posts/diamonds-histogram/diamonds-histogram.html",
    "title": "diamonds-histogram",
    "section": "",
    "text": "Die Daten beziehen sich auf den Datensatz diamonds und sind hier einzusehen bzw. können bei Interesse dort heruntergeladen werden.\n\n\n\n\n\n\n\n\n\nAuf der X-Achse ist eine nominalskalierte Variable abgetragen.\nDer vertikale Strich in jedem Bild passt gut zur Position des insgesamten Medians.\nDie Variable cut ist eine intervallskalierte Variable.\nAuf der x-Achse werden Häufigkeiten abgetragen.\nDie Gruppierungsvariable cut wird hier als ordinale Variable, also mit Ordnungsstruktur, verwendet."
  },
  {
    "objectID": "posts/diamonds-histogram/diamonds-histogram.html#answerlist",
    "href": "posts/diamonds-histogram/diamonds-histogram.html#answerlist",
    "title": "diamonds-histogram",
    "section": "",
    "text": "Auf der X-Achse ist eine nominalskalierte Variable abgetragen.\nDer vertikale Strich in jedem Bild passt gut zur Position des insgesamten Medians.\nDie Variable cut ist eine intervallskalierte Variable.\nAuf der x-Achse werden Häufigkeiten abgetragen.\nDie Gruppierungsvariable cut wird hier als ordinale Variable, also mit Ordnungsstruktur, verwendet."
  },
  {
    "objectID": "posts/diamonds-histogram/diamonds-histogram.html#answerlist-1",
    "href": "posts/diamonds-histogram/diamonds-histogram.html#answerlist-1",
    "title": "diamonds-histogram",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nvis\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/tmdb05/tmdb05.html",
    "href": "posts/tmdb05/tmdb05.html",
    "title": "tmdb05",
    "section": "",
    "text": "Melden Sie sich an für die Kaggle Competition TMDB Box Office Prediction - Can you predict a movie’s worldwide box office revenue?.\nSie benötigen dazu ein Konto; es ist auch möglich, sich mit seinem Google-Konto anzumelden.\nBei diesem Prognosewettbewerb geht es darum, vorherzusagen, wieviel Umsatz wohl einige Filme machen werden. Als Prädiktoren stehen einige Infos wie Budget, Genre, Titel etc. zur Verfügung. Eine klassische “predictive Competition” also :-) Allerdings können immer ein paar Schwierigkeiten auftreten ;-)\nAufgabe\nErstellen Sie ein Boosting-Modell mit Tidymodels!\nHinweise\n\n\nFür den Start empfehle ich, etwaige Vorverarbeitung erstmal klein zu halten. Nach dem Motto: Erstmal das Modell zum Laufen kriegen, dann erst verbessern.\nTunen Sie die typischen Parameter.\nReichen Sie das Modell bei Kaggle ein und berichten Sie Ihren Score.\nIm Übrigen sind Sie frei in Ihrem Vorgehen."
  },
  {
    "objectID": "posts/tmdb05/tmdb05.html#rezept-checken",
    "href": "posts/tmdb05/tmdb05.html#rezept-checken",
    "title": "tmdb05",
    "section": "Rezept checken",
    "text": "Rezept checken\n\nd_train_baked &lt;- prep(rec1) %&gt;% bake(new_data = NULL)\nd_train_baked\n\n# A tibble: 3,000 × 4\n   budget popularity runtime  revenue\n    &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1 -0.230    -0.156   -0.673 12314651\n 2  0.472    -0.0177   0.233 95149435\n 3 -0.519     4.61    -0.129 13092000\n 4 -0.576    -0.437    0.640 16000000\n 5 -0.609    -0.604    0.459  3923970\n 6 -0.392    -0.638   -1.13   3261638\n 7 -0.230    -0.0972  -0.718 85446075\n 8 -0.609    -0.538   -1.08   2586511\n 9 -0.609    -0.129   -0.356 34327391\n10 -0.446    -0.313   -0.763 18750246\n# ℹ 2,990 more rows\n\n\nViele Modelle können nicht arbeiten mit nominalen Prädiktoren oder mit fehlenden Werten. Daher sollte man im Rezept diese Fehler vorab abfangen.\nEin letzter Blick:\n\ndescribe_distribution(d_train_baked)\n\nVariable   |      Mean |       SD |      IQR |            Range | Skewness | Kurtosis |    n | n_Missing\n--------------------------------------------------------------------------------------------------------\nbudget     | -1.33e-18 |     1.00 |     0.78 |    [-0.61, 9.65] |     3.10 |    13.23 | 3000 |         0\npopularity | -6.08e-17 |     1.00 |     0.57 |   [-0.70, 23.62] |    14.38 |   280.10 | 3000 |         0\nruntime    |  3.63e-17 |     1.00 |     1.09 |   [-4.88, 10.42] |     1.02 |     8.19 | 2998 |         2\nrevenue    |  6.67e+07 | 1.38e+08 | 6.66e+07 | [1.00, 1.52e+09] |     4.54 |    27.78 | 3000 |         0\n\n\nSieht ok aus."
  },
  {
    "objectID": "posts/tidymodels-vorlage3/tidymodels-vorlage3.html",
    "href": "posts/tidymodels-vorlage3/tidymodels-vorlage3.html",
    "title": "tidymodels-vorlage3",
    "section": "",
    "text": "Aufgabe\n\nSchreiben Sie eine prototypische Analyse für ein Vorhersagemodell, das sich als Vorlage für Analysen dieser Art eignet!\nVerzichten Sie auf Resampling und Tuning.\nHinweise:\n\nBerechnen Sie ein Modell\nTunen Sie keinen Parameter des Modells\nVerwenden Sie keine Kreuzvalidierung.\nVerwenden Sie Standardwerte, wo nicht anders angegeben.\nFixieren Sie Zufallszahlen auf den Startwert 42.\n\n         \n\n\nLösung\n\n# Setup:\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.0.8\n✔ dials        1.2.0     ✔ rsample      1.2.0\n✔ dplyr        1.1.3     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.4\n✔ lubridate 1.9.3     ✔ stringr   1.5.0\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tictoc)  # Zeitmessung\nlibrary(easystats)   # NAs zählen\n\n# Attaching packages: easystats 0.6.0 (red = needs update)\n✔ bayestestR  0.13.1   ✔ correlation 0.8.4 \n✔ datawizard  0.9.0    ✔ effectsize  0.8.6 \n✔ insight     0.19.6   ✔ modelbased  0.8.6 \n✔ performance 0.10.8   ✔ parameters  0.21.3\n✔ report      0.5.7    ✖ see         0.8.0 \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`.\n\n# Data:\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\"\nd &lt;- read_csv(d_path)\n\nRows: 344 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): species, island, sex\ndbl (6): rownames, bill_length_mm, bill_depth_mm, flipper_length_mm, body_ma...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nset.seed(42)\nd_split &lt;- initial_split(d)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\n\n# model:\nmod1 &lt;-\n  rand_forest(mode = \"regression\")\n\n\n# cv:\nset.seed(42)\nrsmpl &lt;- vfold_cv(d_train)\n\n\n# recipe:\nrec1 &lt;- recipe(body_mass_g ~  ., data = d_train) |&gt; \n  step_unknown(all_nominal_predictors(), new_level = \"NA\") |&gt; \n  step_naomit(all_predictors()) |&gt; \n  step_dummy(all_nominal_predictors()) |&gt; \n  step_zv(all_predictors()) |&gt; \n  step_normalize(all_predictors()) \n\n\n\n# workflow:\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(mod1) %&gt;% \n  add_recipe(rec1)\n\n\n# tuning:\ntic()\nwf1_fit &lt;-\n  wf1 %&gt;% \n  last_fit(split = d_split)\n\n→ A | error:   Missing data in columns: bill_length_mm, bill_depth_mm, flipper_length_mm.\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1\n\n\nWarning: All models failed. Run `show_notes(.Last.tune.result)` for more\ninformation.\n\ntoc()\n\n0.594 sec elapsed\n\ncollect_metrics(wf1_fit)\n\nNULL\n\n\nAls Check: Das gepreppte/bebackene Rezept:\n\nrec1_prepped &lt;- prep(rec1)\nd_train_baked &lt;- bake(rec1_prepped, new_data = NULL)\n\n\nd_train_baked |&gt; \n  head()\n\n# A tibble: 6 × 12\n  rownames bill_length_mm bill_depth_mm flipper_length_mm    year body_mass_g\n     &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n1   -1.24          -1.53          0.386            -0.794 -1.29          3450\n2    1.45           1.32          0.386            -0.365  1.14          3675\n3   -0.212          0.401        -1.97              0.707 -1.29          4500\n4   -0.993          0.343         0.887            -0.294 -0.0757        4150\n5    0.530          0.879        -0.566             2.07  -0.0757        5800\n6   -0.281         -0.957         0.787            -1.15   1.14          3650\n# ℹ 6 more variables: species_Chinstrap &lt;dbl&gt;, species_Gentoo &lt;dbl&gt;,\n#   island_Dream &lt;dbl&gt;, island_Torgersen &lt;dbl&gt;, sex_male &lt;dbl&gt;, sex_NA. &lt;dbl&gt;\n\n\n\ndescribe_distribution(d_train_baked)\n\nVariable          |      Mean |     SD |     IQR |              Range | Skewness | Kurtosis |   n | n_Missing\n-------------------------------------------------------------------------------------------------------------\nrownames          | -5.63e-17 |   1.00 |    1.70 |      [-1.72, 1.68] |    -0.01 |    -1.21 | 257 |         0\nbill_length_mm    | -2.97e-16 |   1.00 |    1.68 |      [-2.28, 2.98] |     0.01 |    -0.79 | 257 |         0\nbill_depth_mm     |  2.71e-16 |   1.00 |    1.60 |      [-2.02, 2.19] |    -0.11 |    -0.87 | 257 |         0\nflipper_length_mm | -9.83e-16 |   1.00 |    1.64 |      [-1.94, 2.07] |     0.32 |    -1.02 | 257 |         0\nyear              | -6.89e-14 |   1.00 |    2.43 |      [-1.29, 1.14] |    -0.12 |    -1.51 | 257 |         0\nbody_mass_g       |   4200.97 | 792.54 | 1212.50 | [2700.00, 6300.00] |     0.49 |    -0.69 | 257 |         0\nspecies_Chinstrap | -2.24e-17 |   1.00 |    0.00 |      [-0.50, 1.98] |     1.49 |     0.22 | 257 |         0\nspecies_Gentoo    |  1.64e-17 |   1.00 |    2.07 |      [-0.76, 1.31] |     0.56 |    -1.70 | 257 |         0\nisland_Dream      | -5.50e-17 |   1.00 |    2.08 |      [-0.75, 1.34] |     0.60 |    -1.66 | 257 |         0\nisland_Torgersen  |  1.72e-17 |   1.00 |    0.00 |      [-0.41, 2.43] |     2.04 |     2.18 | 257 |         0\nsex_male          | -5.86e-17 |   1.00 |    2.00 |      [-0.96, 1.03] |     0.07 |    -2.01 | 257 |         0\nsex_NA.           |  1.45e-17 |   1.00 |    0.00 |      [-0.15, 6.46] |     6.35 |    38.63 | 257 |         0\n\n\n\nCategories:\n\ntidymodels\nstatlearning\ntemplate\nstring"
  },
  {
    "objectID": "posts/twitter03/twitter03.html",
    "href": "posts/twitter03/twitter03.html",
    "title": "twitter03",
    "section": "",
    "text": "Exercise\nLaden Sie die neuesten Tweets an karl_lauterbach herunter, die mindestens 100 Likes oder 100 Retweets haben.\n         \n\n\nSolution\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rtweet)\n\n\nAttaching package: 'rtweet'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\n\nEinloggen bei Twitter; zuerst die Credentials bereithalten:\n\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\n\nDann anmelden:\n\nauth &lt;- rtweet_app(bearer_token = Bearer_Token)\n\nTweets an Karl Lauterbach suchen:\n\nkarl1 &lt;- search_tweets(\"@karl_lauterbach min_faves:100 OR min_retweets:100\", n = 10)\n\n\nkarl1 %&gt;% \n  select(retweet_count, favorite_count)\n\n# A tibble: 10 × 2\n   retweet_count favorite_count\n           &lt;int&gt;          &lt;int&gt;\n 1            56            210\n 2            56            229\n 3            44           1626\n 4            60            225\n 5            30            494\n 6             5            148\n 7            27            435\n 8            12            178\n 9            13            162\n10            46            375\n\nCategories:\n\ntextmining\ntwitter"
  },
  {
    "objectID": "posts/mariokart-mean2/mariokart-mean2.html",
    "href": "posts/mariokart-mean2/mariokart-mean2.html",
    "title": "mariokart-mean2",
    "section": "",
    "text": "Aufgabe\nImportieren Sie den Datensatz mariokart in R. Berechnen Sie den mittleren Verkaufspreis (total_pr) für neue Spiele.\nHinweise:\n\nRunden Sie auf 1 Dezimalstelle.\n\n         \n\n\nLösung\nPakete starten:\n\nlibrary(tidyverse)\nlibrary(easystats)\n\nDaten importieren:\n\nd_url &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\"\nd &lt;- data_read(d_url)\n\n\nsolution &lt;-\nd  %&gt;% \n  filter(cond == \"new\") %&gt;% \n  summarise(pr_mean = mean(total_pr))\n\nsolution\n\n   pr_mean\n1 53.77068\n\n\nLösung: 53.77.\n\nCategories:\n\ndatawrangling\ndplyr\neda\nnum"
  },
  {
    "objectID": "posts/PPV1a-mtcars/PPV1a-mtcars.html",
    "href": "posts/PPV1a-mtcars/PPV1a-mtcars.html",
    "title": "PPV1a-mtcars",
    "section": "",
    "text": "Im Folgenden ist der Datensatz mtcars zu analysieren.\nEine Möglichkeit, den Datensatz zu beziehen, ist diese Sammlung an Datensätzen. Suchen Sie dort nach dem Namen des Datensatzes. Importieren Sie dann die Daten in R.\nHilfe zum Datensatz ist auf dieser Webseite abrufbar.\nBerechnen Sie das folgende lineare Modell:\nAV: mpg.\nUVs: hp, am.\nAufgabe: Was ist der Wert des Punktschätzers für eine Beobachtung, bei der alle Prädiktoren den Wert 0 aufweisen?\nHinweise\nWählen Sie die am besten passende Antwortoption!\n\n\n\n-27\n-17\n-7\n17\n27"
  },
  {
    "objectID": "posts/PPV1a-mtcars/PPV1a-mtcars.html#answerlist",
    "href": "posts/PPV1a-mtcars/PPV1a-mtcars.html#answerlist",
    "title": "PPV1a-mtcars",
    "section": "",
    "text": "-27\n-17\n-7\n17\n27"
  },
  {
    "objectID": "posts/PPV1a-mtcars/PPV1a-mtcars.html#answerlist-1",
    "href": "posts/PPV1a-mtcars/PPV1a-mtcars.html#answerlist-1",
    "title": "PPV1a-mtcars",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nFalsch\nWahr\n\n\nCategories:\n\nbayes\nregression\nexam-22"
  },
  {
    "objectID": "posts/mtcars-simple3/mtcars-simple3.html",
    "href": "posts/mtcars-simple3/mtcars-simple3.html",
    "title": "mtcars-simple3",
    "section": "",
    "text": "We will use the dataset mtcars in this exercise.\nAssume your causal model of your research dictates that fuel economy is a linear function of horse power, cylinder count and displacement of the engine.\nWhich of the predictors in the above model has the weakest causal impact on the output variable?\nNotes:\n\nUse can either use frequentist or bayesian modeling.\nUse R for all computations.\nThere are multiple ways to find a solution.\n\n\n\n\ncyl\nhp\ndisp\nAll are equally strong\nnone of the above"
  },
  {
    "objectID": "posts/mtcars-simple3/mtcars-simple3.html#answerlist",
    "href": "posts/mtcars-simple3/mtcars-simple3.html#answerlist",
    "title": "mtcars-simple3",
    "section": "",
    "text": "cyl\nhp\ndisp\nAll are equally strong\nnone of the above"
  },
  {
    "objectID": "posts/mtcars-simple3/mtcars-simple3.html#answerlist-1",
    "href": "posts/mtcars-simple3/mtcars-simple3.html#answerlist-1",
    "title": "mtcars-simple3",
    "section": "Answerlist",
    "text": "Answerlist\n\nwrong\ncorrect\nwrong\nwrong\nwrong\n\n\nCategories:\n\nregression\nen\nbayes\nfrequentist\nqm1\nstats-nutshell"
  },
  {
    "objectID": "posts/mtcars-post/mtcars-post.html",
    "href": "posts/mtcars-post/mtcars-post.html",
    "title": "mtcars-post",
    "section": "",
    "text": "Aufgabe\nIm Datensatz mtcars: Berichten Sie die Breite eines Schätzintervalls (89%, HDI) zum mittleren Spritverbrauch! Nutzen Sie Methoden der Bayes-Statistik.\nHinweise\n         \n\n\nLösung\nSetup:\n\ndata(mtcars)\nlibrary(tidyverse)\nlibrary(rstanarm)\nlibrary(easystats)\n\nModell berechnen:\n\nm1 &lt;- stan_glm(mpg ~ 1, \n               data = mtcars,\n               seed = 42,\n               refresh = 0)\n\nModellparameter auslesen, wobei wir als CI-Methode ein HDI auswählen, und als CI-Größe 89%:\n\nparameters(m1, ci = .89, ci_method = \"hdi\")\n\nParameter   | Median |         89% CI |   pd |  Rhat |     ESS |                   Prior\n----------------------------------------------------------------------------------------\n(Intercept) |  20.10 | [18.26, 21.64] | 100% | 1.001 | 2838.00 | Normal (20.09 +- 15.07)\n\n\nIm Standard wird ein 95%-Perzentilintervall berechnet, s. die Dokumentation zur Funktion hier.\nDie Lösung lautet also:\n\nsolution &lt;- 21.64 - 18.26\nsolution\n\n[1] 3.38\n\n\n\nCategories:\n\nbayes\npost\nestimation\nexam-22"
  },
  {
    "objectID": "posts/regr-tree02/regr-tree02.html",
    "href": "posts/regr-tree02/regr-tree02.html",
    "title": "regr-tree02",
    "section": "",
    "text": "library(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ recipes      1.0.8\n✔ dials        1.2.0     ✔ rsample      1.2.0\n✔ dplyr        1.1.3     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts."
  },
  {
    "objectID": "posts/regr-tree02/regr-tree02.html#setup",
    "href": "posts/regr-tree02/regr-tree02.html#setup",
    "title": "regr-tree02",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidymodels)\ndata(mtcars)\nlibrary(tictoc)  # Zeitmessung\n\nFür Klassifikation verlangt Tidymodels eine nominale AV, keine numerische:\n\nmtcars &lt;-\n  mtcars %&gt;% \n  mutate(am = factor(am))"
  },
  {
    "objectID": "posts/regr-tree02/regr-tree02.html#daten-teilen",
    "href": "posts/regr-tree02/regr-tree02.html#daten-teilen",
    "title": "regr-tree02",
    "section": "Daten teilen",
    "text": "Daten teilen\n\nd_split &lt;- initial_split(mtcars)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)"
  },
  {
    "objectID": "posts/regr-tree02/regr-tree02.html#modelle",
    "href": "posts/regr-tree02/regr-tree02.html#modelle",
    "title": "regr-tree02",
    "section": "Modell(e)",
    "text": "Modell(e)\n\nmod_tree &lt;-\n  decision_tree(mode = \"classification\",\n                cost_complexity = tune(),\n                tree_depth = tune(),\n                min_n = tune())"
  },
  {
    "objectID": "posts/regr-tree02/regr-tree02.html#rezepte",
    "href": "posts/regr-tree02/regr-tree02.html#rezepte",
    "title": "regr-tree02",
    "section": "Rezept(e)",
    "text": "Rezept(e)\n\nrec1 &lt;- \n  recipe(am ~ ., data = d_train)"
  },
  {
    "objectID": "posts/regr-tree02/regr-tree02.html#resampling",
    "href": "posts/regr-tree02/regr-tree02.html#resampling",
    "title": "regr-tree02",
    "section": "Resampling",
    "text": "Resampling\n\nrsmpl &lt;- vfold_cv(d_train, v = 2)"
  },
  {
    "objectID": "posts/regr-tree02/regr-tree02.html#workflow",
    "href": "posts/regr-tree02/regr-tree02.html#workflow",
    "title": "regr-tree02",
    "section": "Workflow",
    "text": "Workflow\n\nwf1 &lt;-\n  workflow() %&gt;%  \n  add_recipe(rec1) %&gt;% \n  add_model(mod_tree)"
  },
  {
    "objectID": "posts/regr-tree02/regr-tree02.html#tuningfitting",
    "href": "posts/regr-tree02/regr-tree02.html#tuningfitting",
    "title": "regr-tree02",
    "section": "Tuning/Fitting",
    "text": "Tuning/Fitting\n\nfit1 &lt;-\n  tune_grid(object = wf1,\n            metrics = metric_set(roc_auc),\n            resamples = rsmpl)\n\n→ A | warning: 30 samples were requested but there were 12 rows in the data. 12 will be used.\n\n\nThere were issues with some computations   A: x1\n\n\n→ B | warning: 18 samples were requested but there were 12 rows in the data. 12 will be used.\n\n\nThere were issues with some computations   A: x1\n→ C | warning: 27 samples were requested but there were 12 rows in the data. 12 will be used.\nThere were issues with some computations   A: x1\n→ D | warning: 17 samples were requested but there were 12 rows in the data. 12 will be used.\nThere were issues with some computations   A: x1\n→ E | warning: 33 samples were requested but there were 12 rows in the data. 12 will be used.\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1   B: x1   C: x1   D: x1   E: x1\n→ F | warning: 22 samples were requested but there were 12 rows in the data. 12 will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1   D: x1   E: x1\n→ G | warning: 37 samples were requested but there were 12 rows in the data. 12 will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1   D: x1   E: x1\nThere were issues with some computations   A: x2   B: x2   C: x2   D: x2   E: x…"
  },
  {
    "objectID": "posts/regr-tree02/regr-tree02.html#bester-kandidat",
    "href": "posts/regr-tree02/regr-tree02.html#bester-kandidat",
    "title": "regr-tree02",
    "section": "Bester Kandidat",
    "text": "Bester Kandidat\n\nautoplot(fit1)\n\n\n\n\n\n\n\n\n\nshow_best(fit1)\n\n# A tibble: 5 × 9\n  cost_complexity tree_depth min_n .metric .estimator  mean     n std_err\n            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1        5.46e- 2          8     3 roc_auc binary     0.879     2  0.121 \n2        4.23e- 5         13    30 roc_auc binary     0.816     2  0.0589\n3        1.06e- 7          2    18 roc_auc binary     0.816     2  0.0589\n4        2.41e- 5         15     8 roc_auc binary     0.816     2  0.0589\n5        9.18e-10         10    11 roc_auc binary     0.816     2  0.0589\n# ℹ 1 more variable: .config &lt;chr&gt;"
  },
  {
    "objectID": "posts/regr-tree02/regr-tree02.html#finalisieren",
    "href": "posts/regr-tree02/regr-tree02.html#finalisieren",
    "title": "regr-tree02",
    "section": "Finalisieren",
    "text": "Finalisieren\n\nwf1_finalized &lt;-\n  wf1 %&gt;% \n  finalize_workflow(select_best(fit1))"
  },
  {
    "objectID": "posts/regr-tree02/regr-tree02.html#last-fit",
    "href": "posts/regr-tree02/regr-tree02.html#last-fit",
    "title": "regr-tree02",
    "section": "Last Fit",
    "text": "Last Fit\n\nfinal_fit &lt;- \n  last_fit(object = wf1_finalized, d_split)\n\ncollect_metrics(final_fit)\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary             1 Preprocessor1_Model1\n2 roc_auc  binary             1 Preprocessor1_Model1\n\n\n\nCategories:\n\nstatlearning\ntrees\ntidymodels\nstring"
  },
  {
    "objectID": "posts/subjektiv-Bayes/subjektiv-Bayes.html",
    "href": "posts/subjektiv-Bayes/subjektiv-Bayes.html",
    "title": "subjektiv-Bayes",
    "section": "",
    "text": "Exercise\nNennen Sie einen Aspekte der bayesianischen Analyse, der (in Teilen) subjektiv ist - abgesehen von der Wahl der Priori-Verteilung.\n         \n\n\nSolution\n\nLinearitätsannahme in (linearen) Modellen\nWahl des Likelihoods\nWahl der Daten\nMethoden der Modellprüfung\nGeneralisierung des Modells auf andere Situationen\nWahl der Prädiktoren\n\n\nCategories:\n~"
  },
  {
    "objectID": "posts/Tengku-Hanis01/Tengku-Hanis01.html",
    "href": "posts/Tengku-Hanis01/Tengku-Hanis01.html",
    "title": "Tengku-Hanis01",
    "section": "",
    "text": "Aufgabe\nBearbeiten Sie diese Fallstudie von Tengku Hanis!\n         \n\n\nLösung\nDie folgende Lösung basiert auf der oben angegebenen Fallstudie.\nPakete laden:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n✔ broom        1.0.5     ✔ rsample      1.2.0\n✔ dials        1.2.0     ✔ tune         1.1.2\n✔ infer        1.0.5     ✔ workflows    1.1.3\n✔ modeldata    1.2.0     ✔ workflowsets 1.0.1\n✔ parsnip      1.1.1     ✔ yardstick    1.2.0\n✔ recipes      1.0.8     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nlibrary(finetune)\n\nDaten importieren:\n\ndata(income, package = \"kernlab\")\n\nDatensatz vereinfachen:\n\nset.seed(2021)\nincome2 &lt;- \n  income %&gt;% \n  filter(INCOME == \"[75.000-\" | INCOME == \"[50.000-75.000)\") %&gt;% \n  slice_sample(n = 600) %&gt;% \n  mutate(INCOME = fct_drop(INCOME), \n         INCOME = fct_recode(INCOME, \n                             rich = \"[75.000-\",\n                             less_rich = \"[50.000-75.000)\"), \n         INCOME = factor(INCOME, ordered = F)) %&gt;% \n  mutate(across(-INCOME, fct_drop))\n\nCheck:\n\nDataExplorer::plot_missing(income)\n\n\n\n\n\n\n\n\n{DataExplorer} sieht nach einem nützlichen Paket aus. Check it out hier!\nDaten aufteilen (“Spending our data budget”):\n\nset.seed(2021)\ndat_index &lt;- initial_split(income2, strata = INCOME)\ndat_train &lt;- training(dat_index)\ndat_test &lt;- testing(dat_index)\n\nKreuzvalidierung:\n\nset.seed(2021)\ndat_cv &lt;- vfold_cv(dat_train, v = 10, repeats = 1, strata = INCOME)\n\nRezept:\n\ndat_rec &lt;- \n  recipe(INCOME ~ ., data = dat_train) %&gt;% \n  step_impute_mode(all_predictors()) %&gt;% \n  step_ordinalscore(AGE, EDUCATION, AREA, HOUSEHOLD.SIZE, UNDER18)\n\nAls Modell (im engeren Sinne) nutzen wir ein Random-Forest-Modell:\n\nrf_mod &lt;- \n  rand_forest(mtry = tune(),\n              trees = tune(),\n              min_n = tune()) %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_engine(\"ranger\")\n\nWie man sieht, geben wir 3 Tuningparameter an.\nModell und Rezept zum Workflow zusammenfassen:\n\nrf_wf &lt;- \n  workflow() %&gt;% \n  add_recipe(dat_rec) %&gt;% \n  add_model(rf_mod)\n\nTuning Grids definieren:\nWichtig ist, dass wir genau die Parameter angeben im Grid, die wir auch zum Tunen getaggt haben. Das kann man händisch erledigen:\n\n# Regular grid:\nreg_grid &lt;- grid_regular(mtry(c(1, 13)), \n                         trees(), \n                         min_n(), \n                         levels = 3)\n\n# Random grid mit 100 Kandidaten:\nrand_grid &lt;- grid_random(mtry(c(1, 13)), \n                         trees(), \n                         min_n(), \n                         size = 100)\n\nWir speichern die Vorhersagen aller Folds im Train-Sample, um die Modellgüte im Train- bzw. Validierungssample anschauen zu können:\n\nctrl &lt;- control_grid(save_pred = T,\n                     extract = extract_model)\nmeasure &lt;- metric_set(roc_auc)\n\nAußerdem haben wir als Gütemaß roc_auc definiert.\nIn der Fallstudie wurde noch extract = extract_model bei control_grid() ergänzt. Das lassen wir der Einfachheit halber mal weg.\nParallelisieren auf mehreren Kernen, um Rechenzeit zu sparen:\n\nlibrary(doParallel)\n\nLoading required package: foreach\n\n\n\nAttaching package: 'foreach'\n\n\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n\n\nLoading required package: iterators\n\n\nLoading required package: parallel\n\n# Create a cluster object and then register: \ncl &lt;- makePSOCKcluster(4)\nregisterDoParallel(cl)\n\nWie viele CPUs hat mein Computer?\n\ndetectCores(logical = FALSE)\n\n[1] 4\n\n\nJetzt geht’s ab: Tuning und Fitting!\nHier das “reguläre Gitter” an Tuningkandidaten:\n\nset.seed(2021)\ntune_regular &lt;- \n  rf_wf %&gt;% \n  tune_grid(\n    resamples = dat_cv, \n    grid = reg_grid,         \n    control = ctrl, \n    metrics = measure)\n\nstopCluster(cl)\n\nDie Modellgüte im Vergleich zwischen den Tuning-Kandidaten kann man sich schön ausgeben lassen:\n\nautoplot(tune_regular)\n\n\n\n\n\n\n\n\nGeht aber nur, wenn man oben gesagt hat, dass man die Predictions speichern möchte.\nWelche Kandidatin war am besten:\n\nshow_best(tune_regular)\n\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1     7  2000     2 roc_auc binary     0.690    10  0.0178 Preprocessor1_Model08\n2     7  2000    40 roc_auc binary     0.689    10  0.0184 Preprocessor1_Model26\n3     7  1000     2 roc_auc binary     0.688    10  0.0162 Preprocessor1_Model05\n4    13  1000    21 roc_auc binary     0.687    10  0.0155 Preprocessor1_Model15\n5     7  2000    21 roc_auc binary     0.687    10  0.0161 Preprocessor1_Model17\n\n\nSo kann man sich die beste Kandidatin anschauen:\n\nshow_best(tune_regular) %&gt;% \n  arrange(-mean) %&gt;% \n  slice(1)\n\n# A tibble: 1 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1     7  2000     2 roc_auc binary     0.690    10  0.0178 Preprocessor1_Model08\n\n\nAber man kann sich auch von Tidymodels einfach die beste Kandidatin sagen lassen:\n\nbest_rf &lt;-\n  select_best(tune_regular, \"roc_auc\")\n\nAuf dieser Basis können wir jetzt den Workflow finalisieren, also die Tuningparameter einfüllen:\n\nfinal_wf &lt;- \n  rf_wf %&gt;% \n  finalize_workflow(best_rf)\nfinal_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_impute_mode()\n• step_ordinalscore()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = 7\n  trees = 2000\n  min_n = 2\n\nComputational engine: ranger \n\n\nUnd mit diesen Werten den ganzen Train-Datensatz fitten:\n\ntest_fit &lt;- \n  final_wf %&gt;%\n  last_fit(dat_index) \n\nWie gut ist das jetzt?\n\ntest_fit %&gt;%\n  collect_metrics()\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.576 Preprocessor1_Model1\n2 roc_auc  binary         0.599 Preprocessor1_Model1\n\n\n\nCategories:\n\ntidymodels\nprediction\nyacsda\nstatlearning\ntrees\nspeed\nstring"
  },
  {
    "objectID": "posts/kausal23/kausal23.html",
    "href": "posts/kausal23/kausal23.html",
    "title": "kausal23",
    "section": "",
    "text": "Gegeben sei der DAG g (s. u.). Der DAG verfügt über mehrere Variablen, die als Knoten im Graph dargestellt sind und mit \\(x_1, x_2, \\ldots x_n\\) bezeichnet sind.\nWelche minimale Variablenmenge muss kontrolliert werden, um den kausalen Effekt von der UV zur AV zu identifizieren?\nUV: x6.\nAV: x5.\nHinweise:\n\nMengen sind mittels geschweifter Klammern gekennzeichnet, z.B. {x8, x9} meint die Menge mit den zwei Elementen x8 und x9.\nDie leere Menge { } bedeutet, dass keine Variable kontrolliert werden muss, um den kausalen Effekt zu identifizieren.\nAlle Variablen werden als gemessen vorausgesetzt.\nEs ist möglich, dass es keine Lösung gibt, dass es also keine Adjustierungsmenge gibt, um den kausalen Effekt zu identifizieren. Wenn dies der Fall sein sollte, wählen Sie “/”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n{ x4 }\n{ x2, x5 }\n/\n{ x1 }\n{ x1, x6 }"
  },
  {
    "objectID": "posts/kausal23/kausal23.html#answerlist",
    "href": "posts/kausal23/kausal23.html#answerlist",
    "title": "kausal23",
    "section": "",
    "text": "{ x4 }\n{ x2, x5 }\n/\n{ x1 }\n{ x1, x6 }"
  },
  {
    "objectID": "posts/kausal23/kausal23.html#answerlist-1",
    "href": "posts/kausal23/kausal23.html#answerlist-1",
    "title": "kausal23",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nRichtig\nFalsch\nFalsch\n\nDieser DAG ist nicht ganz ein Gespenster-DAG. Es stimmt fast, dass man alle nicht-kausalen Pfade zumachen (blockieren) kann. Aber nicht ganz: Der Pfad von AV zu UV muss offen bleiben. Und dieser Pfad ist NICHT kausal in diesem DAG, da er in die falsche Richtung zeigt (von AV zu UV, was die falsche Richtung ist). Darum ist es ein “biasing path”, ein “böser Pfad”. Aber man kann ihn nicht zumachen, da man keine Pfade mit UV oder AV zumachen kann. Insofern ist dieser DAG ein verlorener Fall. 👻\nEs ist also ein bisschen ein Spezialfall. Wie gesagt, der entscheidende Punkt ist, dass der Pfad “x5 (AV) -&gt; x6 (UV)” nicht kausal ist.\nNatürlich ist das Beispiel extrem; niemand würde so einen DAG spezifizieren (normalerweise). Es macht keinen Sinn, eine Theorie, die sagt “Ich glaube, dass meine Ursache eigentliche die Wirkung ist”. 🤪\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/Griech-Buchstaben-Inferenz/Griech-Buchstaben-Inferenz.html",
    "href": "posts/Griech-Buchstaben-Inferenz/Griech-Buchstaben-Inferenz.html",
    "title": "Griech-Buchstaben-Inferenz",
    "section": "",
    "text": "Exercise\nFür Statistiken (Stichprobe) verwendet man meist lateinische Buchstaben; für Parameter (Population) verwendet man meist (die entsprechenden) griechischen Buchstaben.\nVervollständigen Sie folgende Tabelle entsprechend!\n\n\n\n\n\nKennwert\nStatistik\nParameter\n\n\n\n\nMittelwert\n\\(\\bar{X}\\)\nNA\n\n\nMittelwertsdifferenz\n\\(\\bar{X}_1-\\bar{X}_2\\)\nNA\n\n\nStreuung\nsd\nNA\n\n\nAnteil\np\nNA\n\n\nKorrelation\nr\nNA\n\n\nRegressionsgewicht\nb\nNA\n\n\n\n\n\n         \n\n\nSolution\n\n\n\n\n\nKennwert\nStatistik\nParameter\n\n\n\n\nMittelwert\n\\[\\bar{X}\\]\n\\[\\mu\\]\n\n\nMittelwertsdifferenz\n\\[d=\\bar{X}_1-\\bar{X}_2\\]\n\\[\\mu_1\\]- \\[\\mu_2\\]\n\n\nStreuung\nsd\n\\[\\sigma\\]\n\n\nAnteil\np\n\\[\\pi\\]\n\n\nKorrelation\nr\n\\[\\rho\\]\n\n\nRegressionsgewicht\nb\n\\[\\beta\\]\n\n\n\n\n\n\nCategories:\n\nqm2\ninference\nparameters"
  },
  {
    "objectID": "posts/kausal24/kausal28.html",
    "href": "posts/kausal24/kausal28.html",
    "title": "kausal24",
    "section": "",
    "text": "library(dagitty)\nlibrary(ggdag)\nlibrary(ggplot2)\n\nGegeben sei der DAG (Graph) g (s. u.). Der DAG verfügt über mehrere Variablen, die als Knoten im Graph dargestellt sind.\n\ng &lt;-\n  dagify(\n    y ~ z + m,\n    m ~ x + z,\n    exposure = \"x\",\n    outcome = \"y\"\n  )\n\nHier ist die Definition des DAGs:\n\n\ndag {\nm\nx [exposure]\ny [outcome]\nz\nm -&gt; y\nx -&gt; m\nz -&gt; m\nz -&gt; y\n}\n\n\nUnd so sieht er aus:\n\nggdag(g) + theme_dag_blank()\n\n\n\n\n\n\n\n\nWelche minimale Variablenmenge muss kontrolliert werden, um den kausalen Effekt von der UV zur AV zu identifizieren?\nUV: x\nAV: y\nHinweise:\n\nMengen sind mittels geschweifter Klammern gekennzeichnet, z.B. {x8, x9} meint die Menge mit den zwei Elementen x8 und x9.\nDie leere Menge { } bedeutet, dass keine Variable kontrolliert werden muss, um den kausalen Effekt zu identifizieren.\nAlle Variablen werden als gemessen vorausgesetzt.\nEs ist möglich, dass es keine Lösung gibt, dass es also keine Adjustierungsmenge gibt, um den kausalen Effekt zu identifizieren. Wenn dies der Fall sein sollte, wählen Sie “keine Lösung”.\n\n\n\n\n{m}\n{z}\n{m, z}\n{ }\nkeine Lösung"
  },
  {
    "objectID": "posts/kausal24/kausal28.html#answerlist",
    "href": "posts/kausal24/kausal28.html#answerlist",
    "title": "kausal24",
    "section": "",
    "text": "{m}\n{z}\n{m, z}\n{ }\nkeine Lösung"
  },
  {
    "objectID": "posts/kausal24/kausal28.html#answerlist-1",
    "href": "posts/kausal24/kausal28.html#answerlist-1",
    "title": "kausal24",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nRichtig\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb/germeval-sent-wordvec-xgb.html",
    "href": "posts/germeval-sent-wordvec-xgb/germeval-sent-wordvec-xgb.html",
    "title": "germeval03-sent-wordvec-xgb",
    "section": "",
    "text": "Erstellen Sie ein prädiktives Modell für Textdaten. Nutzen Sie Sentiments und TextFeatures im Rahmen von Feature-Engineering. Nutzen Sie außerdem deutsche Word-Vektoren für das Feature-Engineering.\nAls Lernalgorithmus verwenden Sie XGB.\nVerwenden Sie die GermEval-2018-Daten.\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\nDie Daten sind auch über das R-Paket PradaData zu beziehen.\n\nlibrary(tidyverse)\ndata(\"germeval_train\", package = \"pradadata\")\ndata(\"germeval_test\", package = \"pradadata\")\n\nDie AV lautet c1. Die (einzige) UV lautet: text.\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks.\nNutzen Sie Tidymodels.\nNutzen Sie das sentiws Lexikon.\n❗ Achten Sie darauf, die Variable c2 zu entfernen bzw. nicht zu verwenden."
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb/germeval-sent-wordvec-xgb.html#learnermodell",
    "href": "posts/germeval-sent-wordvec-xgb/germeval-sent-wordvec-xgb.html#learnermodell",
    "title": "germeval03-sent-wordvec-xgb",
    "section": "Learner/Modell",
    "text": "Learner/Modell\n\nmod &lt;-\n  boost_tree(mode = \"classification\",\n             learn_rate = .01, \n             tree_depth = 5\n             )"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb/germeval-sent-wordvec-xgb.html#rezept",
    "href": "posts/germeval-sent-wordvec-xgb/germeval-sent-wordvec-xgb.html#rezept",
    "title": "germeval03-sent-wordvec-xgb",
    "section": "Rezept",
    "text": "Rezept\nPfad zu den Wordvecktoren:\n\npath_wordvec &lt;- \"/Users/sebastiansaueruser/datasets/word-embeddings/wikipedia2vec/part-0.arrow\"\n\n\nsource(\"https://raw.githubusercontent.com/sebastiansauer/Datenwerk2/main/funs/def_recipe_wordvec_senti.R\")\n\nrec &lt;- def_recipe_wordvec_senti(data_train = d_train,\n                                path_wordvec = path_wordvec)"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb/germeval-sent-wordvec-xgb.html#workflow",
    "href": "posts/germeval-sent-wordvec-xgb/germeval-sent-wordvec-xgb.html#workflow",
    "title": "germeval03-sent-wordvec-xgb",
    "section": "Workflow",
    "text": "Workflow\n\nsource(\"https://raw.githubusercontent.com/sebastiansauer/Datenwerk2/main/funs/def_df.R\")\nwf &lt;- def_wf()\n\nwf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n13 Recipe Steps\n\n• step_text_normalization()\n• step_mutate()\n• step_mutate()\n• step_mutate()\n• step_mutate()\n• step_textfeature()\n• step_tokenize()\n• step_stopwords()\n• step_word_embeddings()\n• step_zv()\n• ...\n• and 3 more steps.\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  tree_depth = 5\n  learn_rate = 0.01\n\nComputational engine: xgboost"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb/germeval-sent-wordvec-xgb.html#check",
    "href": "posts/germeval-sent-wordvec-xgb/germeval-sent-wordvec-xgb.html#check",
    "title": "germeval03-sent-wordvec-xgb",
    "section": "Check",
    "text": "Check\n\ntic()\nrec_prepped &lt;- prep(rec)\ntoc()\n\n67.325 sec elapsed\n\nrec_prepped\n\n\nobj_size(rec_prepped)\n\n3.17 GB\n\n\nGroß!\n\ntidy(rec_prepped)\n\n# A tibble: 13 × 6\n   number operation type               trained skip  id                      \n    &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;              &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;                   \n 1      1 step      text_normalization TRUE    FALSE text_normalization_QTRCS\n 2      2 step      mutate             TRUE    FALSE mutate_z4zTn            \n 3      3 step      mutate             TRUE    FALSE mutate_bjCuT            \n 4      4 step      mutate             TRUE    FALSE mutate_OVxpj            \n 5      5 step      mutate             TRUE    FALSE mutate_TRK3c            \n 6      6 step      textfeature        TRUE    FALSE textfeature_6BkkC       \n 7      7 step      tokenize           TRUE    FALSE tokenize_csz3N          \n 8      8 step      stopwords          TRUE    FALSE stopwords_HU9cX         \n 9      9 step      word_embeddings    TRUE    FALSE word_embeddings_2ZNxu   \n10     10 step      zv                 TRUE    FALSE zv_FNUiA                \n11     11 step      normalize          TRUE    FALSE normalize_bOlig         \n12     12 step      impute_mean        TRUE    FALSE impute_mean_kRaUZ       \n13     13 step      mutate             TRUE    FALSE mutate_PpudL            \n\n\n\nd_rec_baked &lt;- bake(rec_prepped, new_data = NULL)\n\nhead(d_rec_baked)\n\n# A tibble: 6 × 121\n     id c1      emo_count schimpf_count emoji_count textfeature_text_copy_n_wo…¹\n  &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;                        &lt;dbl&gt;\n1     1 OTHER       0.575        -0.450      -0.353                      -0.495 \n2     2 OTHER      -1.11         -0.450      -0.353                      -0.0874\n3     3 OTHER       0.186        -0.450       0.774                      -0.903 \n4     4 OTHER       0.202        -0.450      -0.353                      -0.0874\n5     5 OFFENSE     0.168        -0.450      -0.353                      -0.393 \n6     6 OTHER      -1.12         -0.450      -0.353                       2.46  \n# ℹ abbreviated name: ¹​textfeature_text_copy_n_words\n# ℹ 115 more variables: textfeature_text_copy_n_uq_words &lt;dbl&gt;,\n#   textfeature_text_copy_n_charS &lt;dbl&gt;,\n#   textfeature_text_copy_n_uq_charS &lt;dbl&gt;,\n#   textfeature_text_copy_n_digits &lt;dbl&gt;,\n#   textfeature_text_copy_n_hashtags &lt;dbl&gt;,\n#   textfeature_text_copy_n_uq_hashtags &lt;dbl&gt;, …\n\n\n\nsum(is.na(d_rec_baked))\n\n[1] 0\n\n\n\nobj_size(d_rec_baked)\n\n4.85 MB"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb/germeval-sent-wordvec-xgb.html#fit",
    "href": "posts/germeval-sent-wordvec-xgb/germeval-sent-wordvec-xgb.html#fit",
    "title": "germeval03-sent-wordvec-xgb",
    "section": "Fit",
    "text": "Fit\n\ntic()\nfit_wordvec_senti_xgb &lt;-\n  fit(wf,\n      data = d_train)\ntoc()\n\n35.314 sec elapsed\n\nbeep()\n\n\nfit_wordvec_senti_xgb\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n13 Recipe Steps\n\n• step_text_normalization()\n• step_mutate()\n• step_mutate()\n• step_mutate()\n• step_mutate()\n• step_textfeature()\n• step_tokenize()\n• step_stopwords()\n• step_word_embeddings()\n• step_zv()\n• ...\n• and 3 more steps.\n\n── Model ───────────────────────────────────────────────────────────────────────\n##### xgb.Booster\nraw: 42.4 Kb \ncall:\n  xgboost::xgb.train(params = list(eta = 0.01, max_depth = 5, gamma = 0, \n    colsample_bytree = 1, colsample_bynode = 1, min_child_weight = 1, \n    subsample = 1), data = x$data, nrounds = 15, watchlist = x$watchlist, \n    verbose = 0, nthread = 1, objective = \"binary:logistic\")\nparams (as set within xgb.train):\n  eta = \"0.01\", max_depth = \"5\", gamma = \"0\", colsample_bytree = \"1\", colsample_bynode = \"1\", min_child_weight = \"1\", subsample = \"1\", nthread = \"1\", objective = \"binary:logistic\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.evaluation.log()\n# of features: 119 \nniter: 15\nnfeatures : 119 \nevaluation_log:\n    iter training_logloss\n       1        0.6904064\n       2        0.6877236\n---                      \n      14        0.6590144\n      15        0.6568817\n\n\nObjekt-Größe:\n\nlobstr::obj_size(fit_wordvec_senti_xgb)\n\n3.17 GB\n\n\nGroß!\nWie wir gesehen haben, ist das Rezept riesig.\n\nlibrary(butcher)\nout &lt;- butcher(fit_wordvec_senti_xgb)\nlobstr::obj_size(out)\n\n3.16 GB"
  },
  {
    "objectID": "posts/germeval-sent-wordvec-xgb/germeval-sent-wordvec-xgb.html#test-set-güte",
    "href": "posts/germeval-sent-wordvec-xgb/germeval-sent-wordvec-xgb.html#test-set-güte",
    "title": "germeval03-sent-wordvec-xgb",
    "section": "Test-Set-Güte",
    "text": "Test-Set-Güte\nVorhersagen im Test-Set:\n\ntic()\npreds &lt;-\n  predict(fit_wordvec_senti_xgb, new_data = germeval_test)\ntoc()\n\n22.669 sec elapsed\n\n\nUnd die Vorhersagen zum Test-Set hinzufügen, damit man TRUTH und ESTIMATE vergleichen kann:\n\nd_test &lt;-\n  germeval_test |&gt; \n  bind_cols(preds) |&gt; \n  mutate(c1 = as.factor(c1))\n\n\nmy_metrics &lt;- metric_set(accuracy, f_meas)\nmy_metrics(d_test,\n           truth = c1,\n           estimate = .pred_class)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.689\n2 f_meas   binary         0.400"
  },
  {
    "objectID": "posts/Klausur-raten/Klausur-raten.html",
    "href": "posts/Klausur-raten/Klausur-raten.html",
    "title": "Klausur-raten",
    "section": "",
    "text": "Aufgabe\nEine Studentin muss (oder will ?) eine Statistik-Klausur schreiben. Die Klausur besteht ausschließlich aus 27 Richtig-Falsch-Aufgaben, Aufgaben also, die mit entweder Ja oder Nein zu beantworten sind (per Ankreuzen). Nach (mehr oder weniger) reiflicher Überlegung entschließt sie sich, die Klausur nur durch Münzwurf zu beantworten. Also nix lernen, nix wissen, einfach nur raten. Bei jeder Aufgabe.\nDie Münze, die die Studentin benutzt, hat eine Wahrscheinlichkeit für einen “Treffer” (richtige Antwort angekreuzt) von \\(p = 0.25\\).\nWie groß ist die Wahrscheinlichkeit für genau \\(k=13\\) Treffer in der Klausur?\nBeachten Sie die Bearbeitungshinweise.\n         \n\n\nLösung\n\nsol &lt;- dbinom(x = k_treffer,  # Anzahl Treffer\n              size = anz_aufgaben,  # Anzahl Aufgaben in der Klausur\n              prob = p_treffer)  # Wahrscheinlichkeit für einen Treffer\n\nAntwort: Der gesuchte Werte beträgt: 0.01.\n\nAufgaben-ID: Klausur-raten, Toleranzbreite: 0.025\n\nCategories:\n\nprobability\ndyn\nbayes\nnum"
  },
  {
    "objectID": "posts/mariokart-mean4/mariokart-mean4.html",
    "href": "posts/mariokart-mean4/mariokart-mean4.html",
    "title": "mariokart-mean4",
    "section": "",
    "text": "Aufgabe\nImportieren Sie den Datensatz mariokart in R. Berechnen Sie den mittleren Verkaufspreis (total_pr) für Spiele, die neu sind oder (auch) über Lenkräder (wheels) verfügen.\nHinweise:\n\nRunden Sie auf 1 Dezimalstelle.\n\n         \n\n\nLösung\nPakete starten:\n\nlibrary(tidyverse)\nlibrary(easystats)\n\nDaten importieren:\n\nd_url &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\"\nd &lt;- data_read(d_url)\n\n\nsolution &lt;-\nd  %&gt;% \n  filter(cond == \"new\" | wheels &gt; 0) %&gt;% \n  summarise(pr_mean = mean(total_pr))\n\nsolution\n\n   pr_mean\n1 52.73218\n\n\nLösung: 52.73.\n\nCategories:\n\ndatawrangling\ndplyr\neda\nnum"
  },
  {
    "objectID": "posts/germeval08-extract-spacy/germeval08-extract-spacy.html",
    "href": "posts/germeval08-extract-spacy/germeval08-extract-spacy.html",
    "title": "germeval08-extract-spacy",
    "section": "",
    "text": "Extrahieren Sie deutsche Worembedding aus SpaCy für den GermEval-Datensatz (Train).\nNutzen Sie die GermEval-2018-Daten.\nDie Daten sind unter CC-BY-4.0 lizensiert. Author: Wiegand, Michael (Spoken Language Systems, Saarland University (2010-2018), Leibniz Institute for the German Language (since 2019)),\nDie Daten sind auch über das R-Paket PradaData zu beziehen.\n\nlibrary(tidyverse)\ndata(\"germeval_train\", package = \"pradadata\")\n\nHinweise:\n\nOrientieren Sie sich im Übrigen an den allgemeinen Hinweisen des Datenwerks."
  },
  {
    "objectID": "posts/germeval08-extract-spacy/germeval08-extract-spacy.html#setup",
    "href": "posts/germeval08-extract-spacy/germeval08-extract-spacy.html#setup",
    "title": "germeval08-extract-spacy",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\n\n\nimport spacy\nimport de_core_news_sm\nimport pandas as pd\nnlp = de_core_news_sm.load()"
  },
  {
    "objectID": "posts/germeval08-extract-spacy/germeval08-extract-spacy.html#daten-in-python-importieren",
    "href": "posts/germeval08-extract-spacy/germeval08-extract-spacy.html#daten-in-python-importieren",
    "title": "germeval08-extract-spacy",
    "section": "Daten in Python importieren",
    "text": "Daten in Python importieren\n\ncsv_file_path = '/home/sebastian/git-repos/pradadata/data-raw/germeval_train.csv'\n\ngermeval_train = pd.read_csv(csv_file_path)"
  },
  {
    "objectID": "posts/germeval08-extract-spacy/germeval08-extract-spacy.html#vorbereiten",
    "href": "posts/germeval08-extract-spacy/germeval08-extract-spacy.html#vorbereiten",
    "title": "germeval08-extract-spacy",
    "section": "Vorbereiten",
    "text": "Vorbereiten\nAls String konvertieren:\n\ntweets = germeval_train['text']\ntweets2 = tweets.astype(str)\ntweets3 = tweets2.to_string()\n\nNLP-Features berechnen:\n\ndoc = nlp(tweets3)"
  },
  {
    "objectID": "posts/germeval08-extract-spacy/germeval08-extract-spacy.html#wortvektoren-berechnen",
    "href": "posts/germeval08-extract-spacy/germeval08-extract-spacy.html#wortvektoren-berechnen",
    "title": "germeval08-extract-spacy",
    "section": "Wortvektoren berechnen",
    "text": "Wortvektoren berechnen\n\nwordvec = [token.vector for token in doc]\nlen(wordvec)"
  },
  {
    "objectID": "posts/germeval08-extract-spacy/germeval08-extract-spacy.html#export",
    "href": "posts/germeval08-extract-spacy/germeval08-extract-spacy.html#export",
    "title": "germeval08-extract-spacy",
    "section": "Export",
    "text": "Export\nals Pandas DF:\n\ndf = pd.DataFrame(wordvec)\n\ndimensions = df.shape\ndimensions\n\nIn CSV schreiben:\n\ndf.to_csv(\"germeval_spacy_embed.csv\")\n\n\nCategories:\n\nwordembedding\ntextmining\npython\nstring"
  },
  {
    "objectID": "posts/vis-mtcars/vis-mtcars.html",
    "href": "posts/vis-mtcars/vis-mtcars.html",
    "title": "vis-mtcars",
    "section": "",
    "text": "In dieser Fallstudie (YACSDA: Yet another Case Study on Data Analysis) untersuchen wir den Datensatz mtcars.\nSie können den Datensatz so beziehen:\n\ndata(\"mtcars\")\nd &lt;- mtcars \n\nEin Codebook finden Sie hier.\nDie Forschungsfrage lautet:\nWas ist der Einfluss der Schaltung und der PS-Zahl auf den Spritverbrauch?\n\nAbhängige Variable (metrisch), y: Spritverbrauch (mpg)\nUnabhängige Variable 1 (nominal), x1: Schaltung (am)\nUnabhängige Variable 2 (metrisch), x2: PS-Zahl (hp)\n\nVisualisieren Sie dazu folgende Aspekte der Forschungsfrage!"
  },
  {
    "objectID": "posts/vis-mtcars/vis-mtcars.html#umbenennen",
    "href": "posts/vis-mtcars/vis-mtcars.html#umbenennen",
    "title": "vis-mtcars",
    "section": "Umbenennen",
    "text": "Umbenennen\nZur einfacheren Verarbeitung nenne ich die Variablen um:\n\nd &lt;-\n  d |&gt; \n  rename(y = mpg, x1 = am, x2 = hp)"
  },
  {
    "objectID": "posts/vis-mtcars/vis-mtcars.html#visualisieren-sie-die-verteilung-von-y-auf-zwei-verschiedene-arten.",
    "href": "posts/vis-mtcars/vis-mtcars.html#visualisieren-sie-die-verteilung-von-y-auf-zwei-verschiedene-arten.",
    "title": "vis-mtcars",
    "section": "Visualisieren Sie die Verteilung von y auf zwei verschiedene Arten.",
    "text": "Visualisieren Sie die Verteilung von y auf zwei verschiedene Arten.\nDas R-Paket ggpubr erstellt schöne Diagramme (basierend auf ggplot) auf einfache Art. Nehmen wir ein Dichtediagramm; die Variable y soll auf der X-Achse stehen:\n\nggdensity(d, x = \"y\")\n\n\n\n\n\n\n\n\nBeachten Sie, dass die Variable in Anführungsstriche gesetzt werden muss: x = \"y\".\nOder ein Histogramm:\n\ngghistogram(d, x = \"y\")\n\nWarning: Using `bins = 30` by default. Pick better value with the argument\n`bins`."
  },
  {
    "objectID": "posts/vis-mtcars/vis-mtcars.html#fügen-sie-relevante-kennzahlen-zur-letzten-visualisierung-hinzu.",
    "href": "posts/vis-mtcars/vis-mtcars.html#fügen-sie-relevante-kennzahlen-zur-letzten-visualisierung-hinzu.",
    "title": "vis-mtcars",
    "section": "Fügen Sie relevante Kennzahlen zur letzten Visualisierung hinzu.",
    "text": "Fügen Sie relevante Kennzahlen zur letzten Visualisierung hinzu.\nUm Diagramme mit Statistiken anzureichen, bietet sich das Paket ggstatsplot an:\n\ngghistostats(d, x = y)\n\n\n\n\n\n\n\n\nBeachten Sie, dass die Variable nicht in Anführungsstriche gesetzt werden darf: x = y."
  },
  {
    "objectID": "posts/vis-mtcars/vis-mtcars.html#visualisieren-sie-die-verteilung-von-x1-und-x2.",
    "href": "posts/vis-mtcars/vis-mtcars.html#visualisieren-sie-die-verteilung-von-x1-und-x2.",
    "title": "vis-mtcars",
    "section": "Visualisieren Sie die Verteilung von x1 und x2.",
    "text": "Visualisieren Sie die Verteilung von x1 und x2.\n\nx1\n\nd_counted &lt;- \n  d |&gt; \n  count(x1) \n\n\nggbarplot(data = d_counted, y = \"n\", x = \"x1\", label = TRUE)\n\n\n\n\n\n\n\n\n\n\nx2\n\ngghistostats(d, x = x2)"
  },
  {
    "objectID": "posts/vis-mtcars/vis-mtcars.html#visualisieren-sie-die-verteilung-von-y-bedingt-auf-x1",
    "href": "posts/vis-mtcars/vis-mtcars.html#visualisieren-sie-die-verteilung-von-y-bedingt-auf-x1",
    "title": "vis-mtcars",
    "section": "Visualisieren Sie die Verteilung von y bedingt auf x1",
    "text": "Visualisieren Sie die Verteilung von y bedingt auf x1\n\ngghistogram(d, x = \"y\", fill = \"x1\")\n\nWarning: Using `bins = 30` by default. Pick better value with the argument\n`bins`.\n\n\nWarning: The following aesthetics were dropped during statistical transformation: fill.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n\n\nOder so:\n\ngghistogram(d, x = \"y\", facet.by = \"x1\")\n\nWarning: Using `bins = 30` by default. Pick better value with the argument\n`bins`."
  },
  {
    "objectID": "posts/vis-mtcars/vis-mtcars.html#fügen-sie-relevante-kennzahlen-zur-letzten-visualisierung-hinzu",
    "href": "posts/vis-mtcars/vis-mtcars.html#fügen-sie-relevante-kennzahlen-zur-letzten-visualisierung-hinzu",
    "title": "vis-mtcars",
    "section": "Fügen Sie relevante Kennzahlen zur letzten Visualisierung hinzu",
    "text": "Fügen Sie relevante Kennzahlen zur letzten Visualisierung hinzu\n\ngrouped_gghistostats(d, x = y, grouping.var = x1)"
  },
  {
    "objectID": "posts/vis-mtcars/vis-mtcars.html#visualisieren-sie-den-zusammenhang-von-y-und-x2",
    "href": "posts/vis-mtcars/vis-mtcars.html#visualisieren-sie-den-zusammenhang-von-y-und-x2",
    "title": "vis-mtcars",
    "section": "Visualisieren Sie den Zusammenhang von y und x2",
    "text": "Visualisieren Sie den Zusammenhang von y und x2\n\nggscatter(d, x = \"x2\", y = \"y\")"
  },
  {
    "objectID": "posts/vis-mtcars/vis-mtcars.html#verbessern-sie-das-letzte-diagramm-so-dass-es-übersichtlicher-wird",
    "href": "posts/vis-mtcars/vis-mtcars.html#verbessern-sie-das-letzte-diagramm-so-dass-es-übersichtlicher-wird",
    "title": "vis-mtcars",
    "section": "Verbessern Sie das letzte Diagramm, so dass es übersichtlicher wird",
    "text": "Verbessern Sie das letzte Diagramm, so dass es übersichtlicher wird\nEs gibt mehrere Wege, das Diagramm übersichtlicher zu machen. Logarithmieren ist ein Weg.\n\nd |&gt; \n  mutate(x2 = log(x2)) |&gt; \n  ggscatter(x = \"x2\", y = \"y\")\n\n\n\n\n\n\n\n\nSynonym könnten wir schreiben:\n\nd_logged &lt;- \n  d |&gt; \n  mutate(x2 = log(x2))\n  \n\nggscatter(d_logged, x = \"x2\", y = \"y\")"
  },
  {
    "objectID": "posts/vis-mtcars/vis-mtcars.html#fügen-sie-dem-letzten-diagramm-relevante-kennzahlen-hinzu",
    "href": "posts/vis-mtcars/vis-mtcars.html#fügen-sie-dem-letzten-diagramm-relevante-kennzahlen-hinzu",
    "title": "vis-mtcars",
    "section": "Fügen Sie dem letzten Diagramm relevante Kennzahlen hinzu",
    "text": "Fügen Sie dem letzten Diagramm relevante Kennzahlen hinzu\n\nggscatterstats(d_logged, x = x2, y = y)"
  },
  {
    "objectID": "posts/vis-mtcars/vis-mtcars.html#fügen-sie-dem-diagramm-zum-zusammenhang-von-y-und-x2-eine-regressionsgerade-hinzu",
    "href": "posts/vis-mtcars/vis-mtcars.html#fügen-sie-dem-diagramm-zum-zusammenhang-von-y-und-x2-eine-regressionsgerade-hinzu",
    "title": "vis-mtcars",
    "section": "Fügen Sie dem Diagramm zum Zusammenhang von y und x2 eine Regressionsgerade hinzu",
    "text": "Fügen Sie dem Diagramm zum Zusammenhang von y und x2 eine Regressionsgerade hinzu\n\nggscatter(d_logged, x = \"x2\", y = \"y\", add = \"reg.line\", \n             add.params = list(color = \"blue\"))"
  },
  {
    "objectID": "posts/vis-mtcars/vis-mtcars.html#ersetzen-sie-die-regressionsgerade-durch-eine-loess-gerade",
    "href": "posts/vis-mtcars/vis-mtcars.html#ersetzen-sie-die-regressionsgerade-durch-eine-loess-gerade",
    "title": "vis-mtcars",
    "section": "Ersetzen Sie die Regressionsgerade durch eine LOESS-Gerade",
    "text": "Ersetzen Sie die Regressionsgerade durch eine LOESS-Gerade\n\nggscatter(d_logged, x = \"x2\", y = \"y\", add = \"loess\", \n             add.params = list(color = \"blue\"))"
  },
  {
    "objectID": "posts/vis-mtcars/vis-mtcars.html#gruppieren-sie-das-letzte-diagramm-nach-x1",
    "href": "posts/vis-mtcars/vis-mtcars.html#gruppieren-sie-das-letzte-diagramm-nach-x1",
    "title": "vis-mtcars",
    "section": "Gruppieren Sie das letzte Diagramm nach x1",
    "text": "Gruppieren Sie das letzte Diagramm nach x1\n\nggscatter(d_logged, x = \"x2\", y = \"y\", add = \"loess\", \n             add.params = list(color = \"blue\"),\n          facet.by = \"x1\")"
  },
  {
    "objectID": "posts/vis-mtcars/vis-mtcars.html#dichotomisieren-sie-y-und-zählen-sie-die-häufigkeiten",
    "href": "posts/vis-mtcars/vis-mtcars.html#dichotomisieren-sie-y-und-zählen-sie-die-häufigkeiten",
    "title": "vis-mtcars",
    "section": "Dichotomisieren Sie y und zählen Sie die Häufigkeiten",
    "text": "Dichotomisieren Sie y und zählen Sie die Häufigkeiten\nNehmen wir einen Mediansplit, um zu dichotomisieren.\n\nd &lt;-\n  d |&gt; \n  mutate(y_dicho = ifelse(y &gt; median(y), \"high\", \"low\"))\n\n\nd |&gt; \n  count(y_dicho) |&gt; \n  ggbarplot(x = \"y_dicho\", y = \"n\")\n\n\n\n\n\n\n\n\nGleich viele! Das sollte nicht verwundern."
  },
  {
    "objectID": "posts/vis-mtcars/vis-mtcars.html#gruppieren-sie-das-letzte-diagramm-nach-den-stufen-von-x1",
    "href": "posts/vis-mtcars/vis-mtcars.html#gruppieren-sie-das-letzte-diagramm-nach-den-stufen-von-x1",
    "title": "vis-mtcars",
    "section": "Gruppieren Sie das letzte Diagramm nach den Stufen von x1",
    "text": "Gruppieren Sie das letzte Diagramm nach den Stufen von x1\n\nd_count &lt;- \nd |&gt; \n  count(y_dicho, x1) \n\nd_count\n\n  y_dicho x1  n\n1    high  0  4\n2    high  1 11\n3     low  0 15\n4     low  1  2\n\n\n\nggbarplot(d_count, x = \"y_dicho\", y = \"n\", facet.by = \"x1\")"
  },
  {
    "objectID": "posts/vis-mtcars/vis-mtcars.html#variieren-sie-das-letzte-diagramm-so-dass-anteile-relative-häufigkeiten-statt-absoluter-häufigkeiten-gezeigt-werden",
    "href": "posts/vis-mtcars/vis-mtcars.html#variieren-sie-das-letzte-diagramm-so-dass-anteile-relative-häufigkeiten-statt-absoluter-häufigkeiten-gezeigt-werden",
    "title": "vis-mtcars",
    "section": "Variieren Sie das letzte Diagramm so, dass Anteile (relative Häufigkeiten) statt absoluter Häufigkeiten gezeigt werden",
    "text": "Variieren Sie das letzte Diagramm so, dass Anteile (relative Häufigkeiten) statt absoluter Häufigkeiten gezeigt werden\n\nd_count &lt;-\n  d_count |&gt; \n  mutate(prop = n / sum(n)) |&gt; \n  mutate(prop = round(prop, 2))\n\nd_count\n\n  y_dicho x1  n prop\n1    high  0  4 0.12\n2    high  1 11 0.34\n3     low  0 15 0.47\n4     low  1  2 0.06\n\n\nCheck:\n\nd_count |&gt; \n  summarise(sum(prop))\n\n  sum(prop)\n1      0.99\n\n\nGut! Die Anteile summieren sich zu ca. 1 (100 Prozent).\n\nggbarplot(d_count, x = \"y_dicho\", y = \"prop\", facet.by = \"x1\", label = TRUE)\n\n\n\n\n\n\n\n\nMan beachten, dass sich die Anteile auf das “Gesamt-N” beziehen.\n\nCategories:\n\nvis\nyacsda\nggquick\nmtcars\nstring"
  },
  {
    "objectID": "posts/mariokart-mean3/mariokart-mean3.html",
    "href": "posts/mariokart-mean3/mariokart-mean3.html",
    "title": "mariokart-mean3",
    "section": "",
    "text": "Aufgabe\nImportieren Sie den Datensatz mariokart in R. Berechnen Sie den mittleren Verkaufspreis (total_pr) für Spiele, die sowohl neu sind als auch über Lenkräder (wheels) verfügen.\nHinweise:\n\nRunden Sie auf 1 Dezimalstelle.\n\n         \n\n\nLösung\nPakete starten:\n\nlibrary(tidyverse)\nlibrary(easystats)\n\nDaten importieren:\n\nd_url &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\"\nd &lt;- data_read(d_url)\n\n\nsolution &lt;-\nd  %&gt;% \n  filter(cond == \"new\" & wheels &gt; 0) %&gt;% \n  summarise(pr_mean = mean(total_pr))\n\nsolution\n\n   pr_mean\n1 54.28418\n\n\nLösung: 54.28.\n\nCategories:\n\ndatawrangling\ndplyr\neda\nnum"
  },
  {
    "objectID": "posts/penguins-regr02/penguins-regr02.html",
    "href": "posts/penguins-regr02/penguins-regr02.html",
    "title": "penguins-regr02",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nAufgabe\nBeantworten Sie folgende Forschungsfrage:\nGibt es einen Zusammenhang von Schnabellänge und Gewicht (AV) bei Pinguinen?\nHinweise:\n\nNutzen Sie den Datensatz aus dem R-Paket palmerpenguins.\nVerwenden Sie das Rope-Verfahren\n\n         \n\n\nLösung\nWir rufen Stan:\n\nlibrary(rstanarm)\n\nLoading required package: Rcpp\n\n\nThis is rstanarm version 2.26.1\n\n\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n\n\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n\n\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n\n\n  options(mc.cores = parallel::detectCores())\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.6.0 (red = needs update)\n✔ bayestestR  0.13.1   ✔ correlation 0.8.4 \n✔ datawizard  0.9.0    ✔ effectsize  0.8.6 \n✔ insight     0.19.6   ✔ modelbased  0.8.6 \n✔ performance 0.10.8   ✔ parameters  0.21.3\n✔ report      0.5.7    ✖ see         0.8.0 \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`.\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\ndata(penguins)\nm1 &lt;- stan_glm(body_mass_g ~ bill_length_mm, \n               seed = 42,\n               refresh = 0,\n               data = penguins)\n\n\nplot(rope(m1))\n\n\n\n\n\n\n\n\nHier ist also keine klare Aussage zur Frage, ob der Effekt vernachlässigbar klein ist oder größer, möglich.\n\nCategories:\n\nlm\nbayes\nrope\nstring"
  },
  {
    "objectID": "posts/tidymodels-vorlage2/tidymodels-vorlage2.html",
    "href": "posts/tidymodels-vorlage2/tidymodels-vorlage2.html",
    "title": "tidymodels-vorlage2",
    "section": "",
    "text": "Aufgabe\nSchreiben Sie eine Vorlage für eine prädiktive Analyse mit Tidymodels!\n\nHinweise:\n\nBerechnen Sie ein Modell\nTunen Sie mind. einen Parameter des Modells\nVerwenden Sie Kreuzvalidierung\nVerwenden Sie Standardwerte, wo nicht anders angegeben.\nFixieren Sie Zufallszahlen auf den Startwert 42.\n\n         \n\n\nLösung\n\n# Setup:\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(tictoc)  # Zeitmessung\nlibrary(&lt;other_package_you_might_need_for_modelling&gt;)  # tidymodels uses existing packages for modelling so you need to make them available\n\n\n# Data:\nd_path &lt;- \"Enter data path here\"\nd &lt;- read_csv(d_path)\n\nset.seed(42)\nd_split &lt;- initial_split(d)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\n\n# model:\nmod1 &lt;-\n  &lt;enter_parsnip_model_name_here&gt;(mode = \"&lt;choose_regression_or_classification&gt;\",\n           cost_complexity = tune())\n\n\n# cv:\nset.seed(42)\nrsmpl &lt;- vfold_cv(d_train)\n\n\n# recipe:\nrec1 &lt;- recipe(&lt;enter_output_variable&gt; ~  ., data = d_train)\n\n\n# workflow:\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(mod1) %&gt;% \n  add_recipe(rec1)\n\n\n# tuning:\ntic()\nwf1_fit &lt;-\n  wf1 %&gt;% \n  tune_grid(\n    resamples = rsmpl)\ntoc()\n\n# best candidate:\nshow_best(wf1_fit)\n\n\n# finalize wf:\nwf1_final &lt;-\n  wf1 %&gt;% \n  finalize_workflow(select_best(wf1_fit))\n\n\nwf1_fit_final &lt;-\n  wf1_final %&gt;% \n  last_fit(d_split)\n\n\n# Modellgüte im Test-Set:\ncollect_metrics(wf1_fit_final)\n\n\nCategories:\n\ntidymodels\nstatlearning\ntemplate\nstring"
  },
  {
    "objectID": "posts/summarise02/summarise02.html",
    "href": "posts/summarise02/summarise02.html",
    "title": "summarise02",
    "section": "",
    "text": "Aufgabe\nIm Datensatz mariokart:\n\nGruppieren Sie danach, ob ein Foto bei der Auktion dabei war (stock_photo).\nFassen Sie die Spalte total_pr zusammen und zwar zum maximalwert - pro Gruppe!\nBerechnen Sie den Mittelwert dieser beiden Zahlen!\n\nGeben Sie diese Zahl als Antwort zurück!\n         \n\n\nLösung\nPakete starten:\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.6.0 (red = needs update)\n✔ bayestestR  0.13.1   ✔ correlation 0.8.4 \n✔ datawizard  0.9.0    ✔ effectsize  0.8.6 \n✔ insight     0.19.6   ✔ modelbased  0.8.6 \n✔ performance 0.10.8   ✔ parameters  0.21.3\n✔ report      0.5.7    ✖ see         0.8.0 \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`.\n\nlibrary(tidyverse)  # startet das Paket tidyverse\n\nDaten importieren:\n\nmariokart &lt;- data_read(\"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\")\n\nZusammenfassen:\n\nmariokart_gruppiert &lt;- group_by(mariokart, stock_photo)  # Gruppieren\nmariokart_klein &lt;- summarise(mariokart_gruppiert, max_preis = max(total_pr))  # zusammenfassen\nmariokart_klein\n\n# A tibble: 2 × 2\n  stock_photo max_preis\n  &lt;chr&gt;           &lt;dbl&gt;\n1 no               327.\n2 yes               75 \n\n\n\nsummarise(mariokart_klein, max_preis_mw = mean(max_preis))\n\n# A tibble: 1 × 1\n  max_preis_mw\n         &lt;dbl&gt;\n1         201.\n\n\nmin analog.\nDie Lösung lautet: 201\n\nCategories:\n\ndatawrangling\neda\ntidyverse\ndplyr\nnum"
  },
  {
    "objectID": "posts/penguins-stan-01/penguins-stan-01.html",
    "href": "posts/penguins-stan-01/penguins-stan-01.html",
    "title": "penguins-stan-01",
    "section": "",
    "text": "Aufgabe\nWir untersuchen Einflussfaktoren bzw. Prädiktoren auf das Körpergewicht von Pinguinen. In dieser Aufgabe untersuchen wir in dem Zusammenhang den Zusammenhang von Schnabellänge (als UV) und Körpergewicht (als AV).\nWie groß ist der statistische Einfluss der UV auf die AV?\n\nBerechnen Sie den Punktschätzer des Effekts!\nWie viele Parameter hat das Modell?\nGeben Sie die Breite eines 90%-HDI an (zum Effekt)!\nWie groß ist die Wahrscheinlichkeit, dass der Effekt vorhanden ist (also größer als Null ist), die “Effektwahrscheinlichkeit”?\nWie groß ist das 95%-HDI, wenn Sie nur die Spezies Adelie untersuchen?\nGeben Sie die Prioris an für m1 für die Regressionskoeffizienten!\n\nHinweise:\n\nNutzen Sie den Datensatz zu den Palmer Penguins.\nVerwenden Sie Methoden der Bayes-Statistik und die Software Stan.\nFixieren Sie die Zufallszahlen auf den Startwert 42!\nSie können den Datensatz z.B. hier beziehen oder über das R-Paket palmerpenguins.\nGeben Sie keine Prozentzahlen, sondern stets Anteile an.\nBeachten Sie die übrigen Hinweise.\n\n         \n\n\nLösung\nZentrieren ist eigentlich immer nützlich, aber hier streng genommen nicht unbedingt nötig. Der Hauptgrund ist, dass Stan für uns den Prior für den Intercept festlegt, und zwar auf Basis der Daten, wir uns also nicht um die komische Frage zu kümmern brauchen, welchen Prior wir für den unzentrierten Achsenabschnitt vergeben wollten: Wie schwer sind Pinguins der Schnabellänge Null? Mit zentrierten Prädiktoren ist die Frage nach dem Prior viel einfacher zu beantworten: Wie schwer ist ein Pinguin mit mittelgroßem Schnabel?\nSetup:\n\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(rstanarm)\n\ndata(\"penguins\", package = \"palmerpenguins\")\n\nEs wird in dieser Aufgabe vorausgesetzt, dass Sie den Datensatz selbständig importieren können. Tipp: Kurzes Googeln hilft ggf., den Datensatz zu finden.\nAlternativ könnten Sie den Datensatz als CSV-Datei importieren:\n\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\"\npenguins &lt;- data_read(d_path)  # oder z.B. mit read_csv \n\nEin Blick in die Daten zur Kontrolle, ob das Importieren richtig funktioniert hat:\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nVertrauen ist gut, aber - was Golems betrifft - ist Kontrolle eindeutig besser ;-)\n\nPunktschätzer\n\n\nm1 &lt;- stan_glm(body_mass_g ~  bill_length_mm,  # Regressionsgleichung\n               data = penguins, #  Daten\n               seed = 42,  # Reproduzierbarkeit\n               refresh = 0)  # nicht so viel Output\n\n\nparameters(m1, ci_method = \"hdi\", ci = .9)\n\nParameter      | Median |            90% CI |     pd |  Rhat |     ESS |                       Prior\n----------------------------------------------------------------------------------------------------\n(Intercept)    | 361.85 | [-108.79, 834.20] | 89.55% | 0.999 | 3920.00 | Normal (4201.75 +- 2004.89)\nbill_length_mm |  87.45 | [  77.17,  98.51] |   100% | 0.999 | 3931.00 |     Normal (0.00 +- 367.22)\n\n\n\nUncertainty intervals (highest-density) and p-values (two-tailed)\n  computed using a MCMC distribution approximation.\n\n\n\nAnzahl Parameter\n\nDas Modell hat 3 Paramter:\n\n\\(\\beta_0\\) (oder \\(\\alpha\\))\n\\(\\beta_01\\)\n\\(\\sigma\\)\n\n\nBreite des Intervalls\n\nDazu liest man die Intervallgrenzen (90% CI) in der richtigen Zeile ab (Tabelle parameters):\n\n 97.70  - 76.24\n\n[1] 21.46\n\n\nEinheit: mm\n\nEffektwahrscheinlichkeit\n\n\nm1_post &lt;-\n  m1 %&gt;% \n  as_tibble()\n\nm1_post %&gt;% \n  count(bill_length_mm &gt; 0)\n\n# A tibble: 1 × 2\n  `bill_length_mm &gt; 0`     n\n  &lt;lgl&gt;                &lt;int&gt;\n1 TRUE                  4000\n\n\nAlso: 100% oder 1 (4000 von 4000 Stichproben finden dieses Ergebnis in unserem Modell).\nMan kann diesen Wert aus der Tabelle oben (Ausgabe von parameters()) einfach in der Spalte pd ablesen. pd steht für probability of direction, s. Details hier.\nOder so, ist auch einfach:\n\npd_m1 &lt;- p_direction(m1) # aus Paket easystats\npd_m1\n\nProbability of Direction\n\nParameter      |     pd\n-----------------------\n(Intercept)    | 89.55%\nbill_length_mm |   100%\n\n\nUnd plotten ist meist hilfreich: plot(pd_m1).\nMan kann sich auch ein “Dashboard” mit allen Ergebnissen des Modells ausgeben lassen:\n\nmodel_dashboard(m1)\n\n\nNur Adelie:\n\nWelche Spezies gibt es im Datensatz?\n\npenguins %&gt;% \n  count(species)\n\n# A tibble: 3 × 2\n  species       n\n  &lt;fct&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\nFiltern:\n\npenguins_adelie &lt;-\n  penguins %&gt;% \n  filter(species == \"Adelie\")\n\nModell berechnen:\n\nm2 &lt;- stan_glm(body_mass_g ~  bill_length_mm,  # Regressionsgleichung\n               data = penguins_adelie, #  Daten\n               seed = 42,  # Repro.\n               refresh = 0)  # nicht so viel Output\n\nDas Modell ist - bis auf die Daten - identisch zu m1.\n\nparameters(m2)\n\nParameter      | Median |            95% CI |     pd |  Rhat |     ESS |                       Prior\n----------------------------------------------------------------------------------------------------\n(Intercept)    |  18.20 | [-881.48, 930.13] | 51.75% | 1.000 | 3988.00 | Normal (3700.66 +- 1146.42)\nbill_length_mm |  94.71 | [  71.21, 118.31] |   100% | 1.000 | 3962.00 |     Normal (0.00 +- 430.43)\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a MCMC distribution approximation.\n\n\n\nhdi(m2, parameters = \"bill_length_mm\")\n\nHighest Density Interval\n\nParameter      |         95% HDI\n--------------------------------\nbill_length_mm | [70.88, 117.70]\n\n\nS. auch Tabelle oben.\n\n118.09 - 71.86\n\n[1] 46.23\n\n\n\nPrioris\n\n\ndescribe_prior(m1, component = \"auxiliary\")\n\n       Parameter Prior_Distribution Prior_Location Prior_Scale\n1    (Intercept)             normal       4201.754   2004.8863\n2 bill_length_mm             normal          0.000    367.2233\n\n\nSteht auch in der Tabelle, die von parameters ausgegeben wird.\n\nCategories:\n\nbayes\nregression\nstring"
  },
  {
    "objectID": "posts/purrr-map04/purrr-map04.html",
    "href": "posts/purrr-map04/purrr-map04.html",
    "title": "purrr-map04",
    "section": "",
    "text": "Exercise\nImportieren Sie das Grundatzprogramm der Partei AfD (in der aktuellsten Version). Tokenisieren Sie nach Seiten. Dann verschachteln Sie die Spalte, in denen der Text der Seite steht, zu einer Listenspalte. Schließlich zählen Sie die Anzahl der Wörter pro Seite und berichten gängige deskriptive Statistiken dazu.\n         \n\n\nSolution\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nText aus PDF-Dateien kann man mit dem Paket pdftools einlesen:\n\nlibrary(pdftools)\n\nUsing poppler version 22.02.0\n\nd_path &lt;- \"~/Literatur/_Div/Politik/afd-grundsatzprogramm-2022.pdf\"\n\nd &lt;- tibble(text = pdf_text(d_path))\n\nZu Seiten tokenisieren brauchen wir nicht; das Datenmaterial ist bereits nach Seiten organisiert.\nJetzt “verschachteln” (to nest) wir die Spalte mit dem Text:\n\nd2 &lt;-\n  d %&gt;% \n  nest(data = text)\n\nhead(d2)\n\n# A tibble: 1 × 1\n  data             \n  &lt;list&gt;           \n1 &lt;tibble [96 × 1]&gt;\n\n\nDann zählen wir die Wörter pro Seite:\n\nd3 &lt;-\n  d2 %&gt;% \n  mutate(word_count_per_page = map(data, ~ str_count(.x$text, \"\\\\w+\")))\n\nhead(d3)\n\n# A tibble: 1 × 2\n  data              word_count_per_page\n  &lt;list&gt;            &lt;list&gt;             \n1 &lt;tibble [96 × 1]&gt; &lt;int [96]&gt;         \n\n\nWie sieht eine Zelle aus data aus?\n\nd3$data[[1]]\n\n# A tibble: 96 × 1\n   text                                                                         \n   &lt;chr&gt;                                                                        \n 1 \"PROGRAMM FÜR\\nDEUTSCHLAND.\\nDas Grundsatzprogramm der Alternative für Deuts…\n 2 \"2   Programm für Deutschland | Inhalt\\n\\n\\n\\n\\n    Präambel\\t\\t\\t\\t\\t\\t    …\n 3 \"3   Programm für Deutschland | Inhalt\\n\\n\\n\\n\\n    7 | Kultur, Sprache und …\n 4 \"4   Programm für Deutschland | Inhalt\\n\\n\\n\\n\\n    11 | Finanzen und Steuer…\n 5 \"Präambel.\\n\"                                                                \n 6 \"6   Programm für Deutschland | Präambel\\n\\n\\n\\n\\n    MUT ZU DEUTSCHLAND.   …\n 7 \"KAPITEL 1\\n\\n\\n\\n\\nDemokratie und\\nGrundwerte\\n\"                            \n 8 \"8   Programm für Deutschland | Demokratie und Grundwerte\\n\\n\\n\\n\\n    DEMOK…\n 9 \"9   Programm für Deutschland | Demokratie und Grundwerte\\n\\n\\n\\n\\n    1.1 V…\n10 \"10   Programm für Deutschland | Demokratie und Grundwerte\\n\\n\\n\\n\\n     ble…\n# ℹ 86 more rows\n\n\nWie sieht eine Zelle aus word_count_per_page aus?\n\nd3$data[[1]]\n\n# A tibble: 96 × 1\n   text                                                                         \n   &lt;chr&gt;                                                                        \n 1 \"PROGRAMM FÜR\\nDEUTSCHLAND.\\nDas Grundsatzprogramm der Alternative für Deuts…\n 2 \"2   Programm für Deutschland | Inhalt\\n\\n\\n\\n\\n    Präambel\\t\\t\\t\\t\\t\\t    …\n 3 \"3   Programm für Deutschland | Inhalt\\n\\n\\n\\n\\n    7 | Kultur, Sprache und …\n 4 \"4   Programm für Deutschland | Inhalt\\n\\n\\n\\n\\n    11 | Finanzen und Steuer…\n 5 \"Präambel.\\n\"                                                                \n 6 \"6   Programm für Deutschland | Präambel\\n\\n\\n\\n\\n    MUT ZU DEUTSCHLAND.   …\n 7 \"KAPITEL 1\\n\\n\\n\\n\\nDemokratie und\\nGrundwerte\\n\"                            \n 8 \"8   Programm für Deutschland | Demokratie und Grundwerte\\n\\n\\n\\n\\n    DEMOK…\n 9 \"9   Programm für Deutschland | Demokratie und Grundwerte\\n\\n\\n\\n\\n    1.1 V…\n10 \"10   Programm für Deutschland | Demokratie und Grundwerte\\n\\n\\n\\n\\n     ble…\n# ℹ 86 more rows\n\n\nAh! Darin steckt nur eine einzelne Zahl!\n\nd3$data[[1]] %&gt;% str()\n\ntibble [96 × 1] (S3: tbl_df/tbl/data.frame)\n $ text: chr [1:96] \"PROGRAMM FÜR\\nDEUTSCHLAND.\\nDas Grundsatzprogramm der Alternative für Deutschland.\\n\" \"2   Programm für Deutschland | Inhalt\\n\\n\\n\\n\\n    Präambel\\t\\t\\t\\t\\t\\t                                        \"| __truncated__ \"3   Programm für Deutschland | Inhalt\\n\\n\\n\\n\\n    7 | Kultur, Sprache und Identität\\t\\t\\t\\t                   \"| __truncated__ \"4   Programm für Deutschland | Inhalt\\n\\n\\n\\n\\n    11 | Finanzen und Steuern\\t\\t\\t\\t\\t                         \"| __truncated__ ...\n\n\nDas heißt, wir können vereinfachen, entschacheln:\n\nd4 &lt;-\n  d3 %&gt;% \n  unnest(word_count_per_page)\n\nhead(d4)\n\n# A tibble: 6 × 2\n  data              word_count_per_page\n  &lt;list&gt;                          &lt;int&gt;\n1 &lt;tibble [96 × 1]&gt;                   9\n2 &lt;tibble [96 × 1]&gt;                 410\n3 &lt;tibble [96 × 1]&gt;                 516\n4 &lt;tibble [96 × 1]&gt;                 297\n5 &lt;tibble [96 × 1]&gt;                   1\n6 &lt;tibble [96 × 1]&gt;                 414\n\n\nVisualisierung:\n\nd4 %&gt;% \n  ggplot(aes(x = word_count_per_page)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.6.0 (red = needs update)\n✔ bayestestR  0.13.1   ✔ correlation 0.8.4 \n✔ datawizard  0.9.0    ✔ effectsize  0.8.6 \n✔ insight     0.19.6   ✔ modelbased  0.8.6 \n✔ performance 0.10.8   ✔ parameters  0.21.3\n✔ report      0.5.7    ✖ see         0.8.0 \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`.\n\ndescribe_distribution(d4$word_count_per_page)\n\n  Mean |     SD |    IQR |          Range | Skewness | Kurtosis |  n | n_Missing\n--------------------------------------------------------------------------------\n285.84 | 172.27 | 322.75 | [1.00, 516.00] |    -0.64 |    -1.24 | 96 |         0\n\n\n\nCategories:\n\nR\nmap\ntidyverse"
  },
  {
    "objectID": "posts/Stichprobenziehen1/Stichprobenziehen1.html",
    "href": "posts/Stichprobenziehen1/Stichprobenziehen1.html",
    "title": "Stichprobenziehen1",
    "section": "",
    "text": "Exercise\nIn dieser Übung untersuchen wir den Effekt der Stichprobengröße auf die Genauigkeit der Schätzung. Und zwar auf praktische Art und Weise.\nAls praktisches Beispiel soll uns dabei die Körpergröße dienen. Wir erfragen die Körpergröße der Studis und betrachten den Mittelwert einer Stichrpobe in Abhängigkeit der Größe der Stichprobe.\n\nGeben Sie anonym Ihre Körpergröße hier ein.\nSie können die Daten hier beziehen.\nBerechnen Sie den Mittelwert der Körpergröße für eine zufällige Stichprobe der Größen \\(n=5\\) und \\(n=50\\)\nDann berechnen Sie die den “echten” Mittelwert der Studis; damit ist der Mittelwert aller Werte der Tabelle gemeint.\nDiskutieren Sie die Ergebnisse!\nWird die Schätzung genauer bei größerer Stichprobe?\nWird die Schätzung “robuster” (weniger schwankend) bei größerer Stichprobe?\n\n         \n\n\nSolution\nIndividuell\n\nCategories:\n\nlm\ninference\nqm2"
  },
  {
    "objectID": "posts/filter-na4/filter-na4.html",
    "href": "posts/filter-na4/filter-na4.html",
    "title": "filter-na4",
    "section": "",
    "text": "Liefern Sie einen visuellen Überblick über fehlende Werte im Datensatz penguins!"
  },
  {
    "objectID": "posts/filter-na4/filter-na4.html#setup",
    "href": "posts/filter-na4/filter-na4.html#setup",
    "title": "filter-na4",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\"\nd &lt;- read_csv(d_path)\n\nRows: 344 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): species, island, sex\ndbl (6): rownames, bill_length_mm, bill_depth_mm, flipper_length_mm, body_ma...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnrow(d)\n\n[1] 344"
  },
  {
    "objectID": "posts/filter-na4/filter-na4.html#weg-1",
    "href": "posts/filter-na4/filter-na4.html#weg-1",
    "title": "filter-na4",
    "section": "Weg 1",
    "text": "Weg 1\n\nlibrary(visdat)\nvis_dat(d)"
  },
  {
    "objectID": "posts/filter-na4/filter-na4.html#weg-2",
    "href": "posts/filter-na4/filter-na4.html#weg-2",
    "title": "filter-na4",
    "section": "Weg 2",
    "text": "Weg 2\n\nd_na_only &lt;- \n  d %&gt;% \n  rowwise() %&gt;% \n  mutate(na_n = sum(is.na(cur_data()))) %&gt;% \n  ungroup()\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `na_n = sum(is.na(cur_data()))`.\nℹ In row 1.\nCaused by warning:\n! `cur_data()` was deprecated in dplyr 1.1.0.\nℹ Please use `pick()` instead.\n\nd_na_only %&gt;% \n  ggplot(aes(x = na_n)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\nCategories:\n\n2023\neda\nna\nstring"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Many shiny pieces",
    "section": "",
    "text": "flights-delay-simplified\n\n\n\n\n\n\nlm\n\n\nregression\n\n\ninteraction\n\n\nyacsda\n\n\n\n\n\n\n\n\n\nJun 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nflights-delay\n\n\n\n\n\n\nlm\n\n\nregression\n\n\ninteraction\n\n\nyacsda\n\n\n\n\n\n\n\n\n\nJun 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nmario-compare-models\n\n\n\n\n\n\nlm\n\n\nregression\n\n\ninteraction\n\n\n\n\n\n\n\n\n\nJun 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nflights-yacsda-eda\n\n\n\n\n\n\neda\n\n\nyacsda\n\n\nvariability\n\n\nassociation\n\n\n\n\n\n\n\n\n\nMay 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\noecd-yacsda\n\n\n\n\n\n\neda\n\n\ndatawrangling\n\n\nvis\n\n\nyacsda\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nsmartphone1\n\n\n\n\n\n\nR\n\n\neda\n\n\ndatawrangling\n\n\nvis\n\n\nyacsda\n\n\n\n\n\n\n\n\n\nMay 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\npenguins-vis-bodymass1\n\n\n\n\n\n\nvis\n\n\n\n\n\n\n\n\n\nApr 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\npenguins-vis-bodymass2\n\n\n\n\n\n\nvis\n\n\n\n\n\n\n\n\n\nApr 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nmutate03\n\n\n\n\n\n\ndatawrangling\n\n\neda\n\n\ntidyverse\n\n\ndplyr\n\n\nnum\n\n\n\n\n\n\n\n\n\nApr 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\ndplyr-mtcars1\n\n\n\n\n\n\ndatawrangling\n\n\neda\n\n\ntidyverse\n\n\ndplyr\n\n\nnum\n\n\n\n\n\n\n\n\n\nApr 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nimport-xls\n\n\n\n\n\n\nR\n\n\ndata\n\n\n\n\n\n\n\n\n\nApr 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nr-quiz\n\n\n\n\n\n\nR\n\n\nen\n\n\nquiz\n\n\n\n\n\n\n\n\n\nMar 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nwskt-df-r\n\n\n\n\n\n\nprobability\n\n\nR\n\n\n\n\n\n\n\n\n\nJan 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nKausal\n\n\n\n\n\n\ncausal\n\n\n\n\n\n\n\n\n\nJan 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nstan_glm_parameterzahl_simple\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\nparameters\n\n\n\n\n\n\n\n\n\nJan 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nchatgpt-sentiment-loop-all\n\n\n\n\n\n\ntextmining\n\n\nnlp\n\n\ntransformer\n\n\nchatgpt\n\n\nsentiment\n\n\n\n\n\n\n\n\n\nDec 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nkollision-eignung\n\n\n\n\n\n\ncausal\n\n\n\n\n\n\n\n\n\nDec 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nhintertuer\n\n\n\n\n\n\ncausal\n\n\n\n\n\n\n\n\n\nDec 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nsaratoga-cor1\n\n\n\n\n\n\nR\n\n\nvis\n\n\ncausal\n\n\neda\n\n\n\n\n\n\n\n\n\nDec 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nsaratoga-cor2\n\n\n\n\n\n\nR\n\n\nvis\n\n\ncausal\n\n\neda\n\n\n\n\n\n\n\n\n\nDec 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWskt-Schluckspecht\n\n\n\n\n\n\npost\n\n\nbayes\n\n\nregression\n\n\n\n\n\n\n\n\n\nDec 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nchatgpt-sentiment-simple\n\n\n\n\n\n\ntextmining\n\n\nnlp\n\n\ntransformer\n\n\nchatgpt\n\n\nsentiment\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ngermeval03-sent-wordvec-glm\n\n\n\n\n\n\ntextmining\n\n\ndatawrangling\n\n\ngermeval\n\n\nprediction\n\n\ntidymodels\n\n\nsentiment\n\n\nstring\n\n\ntune\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ngermeval03-sent-wordvec-elasticnet\n\n\n\n\n\n\ntextmining\n\n\ndatawrangling\n\n\ngermeval\n\n\nprediction\n\n\ntidymodels\n\n\nsentiment\n\n\nstring\n\n\nelasticnet\n\n\ntune\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nScikit-Learn-LLM Zero Shot Learners\n\n\n\n\n\n\ntextmining\n\n\nnlp\n\n\ntransformer\n\n\nchatgpt\n\n\nsentiment\n\n\nscikit\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nchatgpt-sentiment-loop\n\n\n\n\n\n\ntextmining\n\n\nnlp\n\n\ntransformer\n\n\nchatgpt\n\n\nsentiment\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ngermeval03-sent-wordvec-rf-tune\n\n\n\n\n\n\ntextmining\n\n\ndatawrangling\n\n\ngermeval\n\n\nprediction\n\n\ntidymodels\n\n\nsentiment\n\n\nstring\n\n\nrandom-forest\n\n\ntune\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ngermeval03-sent-wordvec-xgb-tune\n\n\n\n\n\n\ntextmining\n\n\ndatawrangling\n\n\ngermeval\n\n\nprediction\n\n\ntidymodels\n\n\nsentiment\n\n\nstring\n\n\nxgb\n\n\ntune\n\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ngermeval03-sent-wordvec-rf-plain\n\n\n\n\n\n\ntextmining\n\n\ndatawrangling\n\n\ndatawrangling\n\n\ngermeval\n\n\nprediction\n\n\ntidymodels\n\n\nsentiment\n\n\nstring\n\n\nrandom-forest\n\n\ntune\n\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ngermeval03-sent-wordvec-xgb-plain\n\n\n\n\n\n\ntextmining\n\n\ndatawrangling\n\n\ngermeval\n\n\nprediction\n\n\ntidymodels\n\n\nsentiment\n\n\nstring\n\n\nxgb\n\n\ntune\n\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ngermeval03-sent-wordvec-xgb\n\n\n\n\n\n\ntextmining\n\n\ndatawrangling\n\n\ngermeval\n\n\nprediction\n\n\ntidymodels\n\n\nsentiment\n\n\nstring\n\n\nxgb\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwfsets1\n\n\n\n\n\n\nR\n\n\nstatlearning\n\n\ntidymodels\n\n\nwfsets\n\n\ntemplate\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ngermeval10-wordvec-rf\n\n\n\n\n\n\ntextmining\n\n\ndatawrangling\n\n\ngermeval\n\n\nprediction\n\n\ntidymodels\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ncount-emojis\n\n\n\n\n\n\ntextmining\n\n\ntidymodels\n\n\ncount\n\n\ngermeval\n\n\nemoji\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ngermeval04\n\n\n\n\n\n\n2023\n\n\ntextmining\n\n\ndatawrangling\n\n\ngermeval\n\n\nprediction\n\n\ntidymodels\n\n\nsentiment\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ngermeval03-sent-textfeatures-rand-for\n\n\n\n\n\n\n2023\n\n\ntextmining\n\n\ndatawrangling\n\n\ngermeval\n\n\nprediction\n\n\ntidymodels\n\n\nsentiment\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ngermeval02\n\n\n\n\n\n\ntextmining\n\n\ntidymodels\n\n\ngermeval\n\n\nsentiment\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ngermeval05\n\n\n\n\n\n\ntextmining\n\n\ndatawrangling\n\n\ngermeval\n\n\nprediction\n\n\ntidymodels\n\n\nwordvec\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nemojis1\n\n\n\n\n\n\nemoji\n\n\ntextmining\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ngermeval08-schimpf\n\n\n\n\n\n\n2023\n\n\ntextmining\n\n\ndatawrangling\n\n\ngermeval\n\n\nprediction\n\n\ntidymodels\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ncount-words01\n\n\n\n\n\n\ntextmining\n\n\ntidymodels\n\n\ncount\n\n\ngermeval\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPupil-size\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\nregression\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ngermeval09-tfidf\n\n\n\n\n\n\ntextmining\n\n\ndatawrangling\n\n\ngermeval\n\n\nprediction\n\n\ntidymodels\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ngermeval01-textfeatures\n\n\n\n\n\n\n2023\n\n\ntextmining\n\n\ndatawrangling\n\n\ngermeval\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ngermeval06\n\n\n\n\n\n\ntextmining\n\n\ndatawrangling\n\n\ngermeval\n\n\nprediction\n\n\ntidymodels\n\n\nwordvec\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmovie-sentiment1\n\n\n\n\n\n\ntextmining\n\n\nimdb\n\n\nschoice\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ngermeval-textfeatures01\n\n\n\n\n\n\ntidymodels\n\n\ntextmining\n\n\nprediction\n\n\nsentiment\n\n\ngermeval\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ncount-emoji-emo\n\n\n\n\n\n\ntextmining\n\n\nnlp\n\n\nemoji\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ngermeval-senti01\n\n\n\n\n\n\ntidymodels\n\n\ntextmining\n\n\nprediction\n\n\nsentiment\n\n\ngermeval\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ngermeval08-extract-spacy\n\n\n\n\n\n\nwordvec\n\n\ntextmining\n\n\npython\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nReThink4e1\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\nschoice\n\n\n\n\n\n\n\n\n\nNov 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTyp-Fehler-R-08-name-clash\n\n\n\n\n\n\nR\n\n\nerror\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTyp-Fehler-R-07\n\n\n\n\n\n\nR\n\n\nerror\n\n\nmchoice\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels-remove-na\n\n\n\n\n\n\ntidymodels\n\n\nstatlearning\n\n\ntemplate\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nna-per-col\n\n\n\n\n\n\nR\n\n\ndatawrangling\n\n\nna\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ngermeval07\n\n\n\n\n\n\n2023\n\n\ntextmining\n\n\ndatawrangling\n\n\ngermeval\n\n\nprediction\n\n\ntidymodels\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels-error1introd\n\n\n\n\n\n\ntidymodels\n\n\nstatlearning\n\n\nerror\n\n\nna\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels-vorlage3\n\n\n\n\n\n\ntidymodels\n\n\nstatlearning\n\n\ntemplate\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels-remove-na2\n\n\n\n\n\n\ntidymodels\n\n\nstatlearning\n\n\ntemplate\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nkekse02\n\n\n\n\n\n\nprobability\n\n\nbayes-grid\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\npostvert-vis-zwielicht\n\n\n\n\n\n\n2023\n\n\nvis\n\n\nbayes\n\n\npost\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRethink2m5\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\nbayes-grid\n\n\nrethink-chap2\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nbath42\n\n\n\n\n\n\nquiz\n\n\nprobability\n\n\nbayes\n\n\nnum\n\n\nqm2\n\n\nqm2-pruefung\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRethink2m2\n\n\n\n\n\n\nprobability\n\n\nbayes-grid\n\n\nbayes\n\n\nrethink-chap2\n\n\nstring\n\n\nqm2\n\n\nqm2-pruefung\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRethink2m3\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\nrethink-chap2\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRethink2m4\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\nbayes-grid\n\n\nrethink-chap2\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntotale-Wskt1\n\n\n\n\n\n\nR\n\n\nprobability\n\n\nbayes\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ngroesse01\n\n\n\n\n\n\n2023\n\n\nbayes\n\n\nbayesbox\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nGem-Wskt2\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\ncloze\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwskt-quiz10\n\n\n\n\n\n\nquiz\n\n\nprobability\n\n\nbayes\n\n\ndistribution\n\n\nquiz1-qm2-ws23\n\n\nschoice\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwskt-quiz17\n\n\n\n\n\n\nquiz\n\n\nprobability\n\n\nbayes\n\n\nquiz1-qm2-ws23\n\n\nschoice\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\noptions-print\n\n\n\n\n\n\n2023\n\n\nR\n\n\ntidyverse\n\n\nmarkdown\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nalphafehler-inflation3\n\n\n\n\n\n\nprobability\n\n\nR\n\n\ninference\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwskt-quiz19\n\n\n\n\n\n\nquiz\n\n\nprobability\n\n\nbayes\n\n\ndistribution\n\n\nquiz1-qm2-ws23\n\n\nschoice\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nalphafehler-inflation4\n\n\n\n\n\n\nprobability\n\n\nR\n\n\ninference\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nKaefer2\n\n\n\n\n\n\nR\n\n\nbayes\n\n\nbayesbox\n\n\nnum\n\n\nqm2\n\n\nqm2-pruefung\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels-tree1\n\n\n\n\n\n\nstatlearning\n\n\ntrees\n\n\ntidymodels\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDAG-Graph\n\n\n\n\n\n\nfopro\n\n\nresearchdesign\n\n\ncausal\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nsentiws2\n\n\n\n\n\n\ntextmining\n\n\ntokenizer\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\niq03\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\niq04\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nabh-ereignisse2\n\n\n\n\n\n\nR\n\n\nprobability\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwuerfel06\n\n\n\n\n\n\nR\n\n\nprobability\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwskt-quiz18\n\n\n\n\n\n\nquiz\n\n\nprobability\n\n\nbayes\n\n\nquiz1-qm2-ws23\n\n\nschoice\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nalphafehler-inflation2\n\n\n\n\n\n\nprobability\n\n\nR\n\n\ninference\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwskt-quiz20\n\n\n\n\n\n\nquiz\n\n\nprobability\n\n\ninference\n\n\nquiz1-qm2-ws23\n\n\nschoice\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwskt-quiz16\n\n\n\n\n\n\nquiz\n\n\nprobability\n\n\nbayes\n\n\nquiz1-qm2-ws23\n\n\nschoice\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nGem-Wskt3\n\n\n\n\n\n\nprobability\n\n\ndyn\n\n\nbayes\n\n\nnum\n\n\nqm2\n\n\nqm2-pruefung\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwskt-quiz11\n\n\n\n\n\n\nquiz\n\n\nprobability\n\n\nbayes\n\n\ndistribution\n\n\nquiz1-qm2-ws23\n\n\nschoice\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\niq05\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\niq02\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nKlausuren-bestehen\n\n\n\n\n\n\nR\n\n\nprobability\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmodellguete-testset\n\n\n\n\n\n\nregression\n\n\nperformance\n\n\nrmse\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nReThink3m1\n\n\n\n\n\n\nbayes\n\n\nppv\n\n\nprobability\n\n\nstring\n\n\nqm2\n\n\nqm2-pruefung\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nregex02\n\n\n\n\n\n\ntextmining\n\n\nregex\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwithin-design-analysis1\n\n\n\n\n\n\nregression\n\n\nwithin-design\n\n\nresearchdesign\n\n\nfopro\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nadjustieren1a\n\n\n\n\n\n\nregression\n\n\n2023\n\n\nstring\n\n\nqm2\n\n\nqm2-pruefung\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBed-Wskt2\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\nnum\n\n\nqm2\n\n\nqm2-pruefung\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ncount-lexicon\n\n\n\n\n\n\ntextmining\n\n\nnlp\n\n\nregex\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwskt-quiz02\n\n\n\n\n\n\nquiz\n\n\nprobability\n\n\nquiz1-qm2-ws23\n\n\nschoice\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwskt-quiz05\n\n\n\n\n\n\nquiz\n\n\nprobability\n\n\ndistribution\n\n\nquiz1-qm2-ws23\n\n\nschoice\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels-ames-05\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nprediction\n\n\nyacsda\n\n\nstatlearning\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nregex03\n\n\n\n\n\n\nregex\n\n\ntextmining\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\niq10\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\nbayes\n\n\nbayes-grid\n\n\nnum\n\n\nqm2\n\n\nqm2-pruefung\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nadjustieren2a\n\n\n\n\n\n\nregression\n\n\n2023\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwskt-quiz04\n\n\n\n\n\n\nquiz\n\n\nprobability\n\n\ndistribution\n\n\nquiz1-qm2-ws23\n\n\nschoice\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwskt-quiz03\n\n\n\n\n\n\nquiz\n\n\nprobability\n\n\nquiz1-qm2-ws23\n\n\nschoice\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nausreisser1\n\n\n\n\n\n\neda\n\n\ndatawrangling\n\n\ntidyverse\n\n\nausreisser\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWeinhaendler\n\n\n\n\n\n\nprobability\n\n\nbayes-grid\n\n\nbayes\n\n\nstring\n\n\nqm2\n\n\nqm2-pruefung\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBed-Wskt3\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ncorona-blutgruppe\n\n\n\n\n\n\nprobability\n\n\ndependent\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nUrne2\n\n\n\n\n\n\nR\n\n\nprobability\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nNerd-gelockert\n\n\n\n\n\n\nR\n\n\nprobability\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRethink2m1\n\n\n\n\n\n\nprobability\n\n\nbayes-grid\n\n\nrethink-chap2\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nsicherheit2\n\n\n\n\n\n\nR\n\n\nprobability\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRethink2m6\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\nbayes-grid\n\n\nrethink-chap2\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmtcars-abhaengig_var3a\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBayes-Theorem1\n\n\n\n\n\n\nbayes\n\n\nprobability\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nbayes2\n\n\n\n\n\n\nR\n\n\nbayes\n\n\nprobability\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRethink2m7\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\nbayes-grid\n\n\nrethink-chap2\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nk-coins-k-hits\n\n\n\n\n\n\nprobability\n\n\ndyn\n\n\nbayes\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nabh-ereignisse\n\n\n\n\n\n\nR\n\n\nprobability\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ngroesse02\n\n\n\n\n\n\n2023\n\n\nbayes\n\n\nbayes-grid\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nAdditionssatz1\n\n\n\n\n\n\nprobability\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels-tree2\n\n\n\n\n\n\nstatlearning\n\n\ntrees\n\n\ntidymodels\n\n\nspeed\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nKaefer1\n\n\n\n\n\n\nR\n\n\nbayes\n\n\nbayesbox\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\niq09\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels-tree5\n\n\n\n\n\n\nstatlearning\n\n\ntrees\n\n\ntidymodels\n\n\nspeed\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\niq07\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwskt-quiz14\n\n\n\n\n\n\nquiz\n\n\nprobability\n\n\nbayes\n\n\nquiz1-qm2-ws23\n\n\nschoice\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwskt-quiz13\n\n\n\n\n\n\nquiz\n\n\nprobability\n\n\nbayes\n\n\nquiz1-qm2-ws23\n\n\nschoice\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nVar-vs-Stufe\n\n\n\n\n\n\nfopro\n\n\nresearchdesign\n\n\nschoice\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\niq01\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\nexam-22\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\niq06\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels-tree4\n\n\n\n\n\n\nstatlearning\n\n\ntrees\n\n\ntidymodels\n\n\nspeed\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels-tree3\n\n\n\n\n\n\nstatlearning\n\n\ntrees\n\n\ntidymodels\n\n\nspeed\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\niq08\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwuerfel05\n\n\n\n\n\n\nR\n\n\nprobability\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwskt-quiz12\n\n\n\n\n\n\nquiz\n\n\nprobability\n\n\nbayes\n\n\nquiz1-qm2-ws23\n\n\nschoice\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwskt-quiz15\n\n\n\n\n\n\nquiz\n\n\nprobability\n\n\nbayes\n\n\nquiz1-qm2-ws23\n\n\nschoice\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nKrebs1\n\n\n\n\n\n\nbayes\n\n\nprobability\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwskt-quiz08\n\n\n\n\n\n\nquiz\n\n\nprobability\n\n\nbayes\n\n\nquiz1-qm2-ws23\n\n\nschoice\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBed-Wskt1\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwskt-quiz06\n\n\n\n\n\n\nquiz\n\n\nprobability\n\n\nbayes\n\n\nquiz1-qm2-ws23\n\n\nschoice\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwskt-quiz01\n\n\n\n\n\n\nquiz\n\n\nprobability\n\n\nquiz1-qm2-ws23\n\n\nschoice\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nregex01\n\n\n\n\n\n\ntextmining\n\n\nregex\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nReThink3m5\n\n\n\n\n\n\nbayes\n\n\nppv\n\n\nprobability\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBsp-Binomial\n\n\n\n\n\n\nprobability\n\n\nbinomial\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwskt-quiz07\n\n\n\n\n\n\nquiz\n\n\nprobability\n\n\nbayes\n\n\nquiz1-qm2-ws23\n\n\nschoice\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwskt-quiz09\n\n\n\n\n\n\nquiz\n\n\nprobability\n\n\nbayes\n\n\ndistribution\n\n\nquiz1-qm2-ws23\n\n\nschoice\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nReThink3m4\n\n\n\n\n\n\nbayes\n\n\nppv\n\n\nprobability\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nUrne1\n\n\n\n\n\n\nR\n\n\nprobability\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nadjustieren2_var1\n\n\n\n\n\n\nlm\n\n\nregression\n\n\nbayes\n\n\nadjust\n\n\nstring\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nKlausur-raten\n\n\n\n\n\n\nprobability\n\n\ndyn\n\n\nbayes\n\n\nnum\n\n\nqm2-pruefung\n\n\nqm2\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nsicherheit\n\n\n\n\n\n\nR\n\n\nprobability\n\n\nnum\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrepro1-sessioninfo\n\n\n\n\n\n\nR\n\n\nrepro\n\n\nstring\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmtcars-abhaengig_var3\n\n\n\n\n\n\ndyn\n\n\nprobability\n\n\nnum\n\n\n\n\n\n\n\n\n\nOct 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nvis-gapminder\n\n\n\n\n\n\nvis\n\n\nyacsda\n\n\nggquick\n\n\ngapminder\n\n\nstring\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nvis-penguins\n\n\n\n\n\n\nvis\n\n\nyacsda\n\n\nggquick\n\n\npenguins\n\n\nstring\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nvis-mtcars\n\n\n\n\n\n\nvis\n\n\nyacsda\n\n\nggquick\n\n\nmtcars\n\n\nstring\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\npenguins-stan-01\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\nstring\n\n\nqm2\n\n\nqm2-pruefung\n\n\n\n\n\n\n\n\n\nJul 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nkausal28\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\nschoice\n\n\n\n\n\n\n\n\n\nJun 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nkausal29\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\nschoice\n\n\n\n\n\n\n\n\n\nJun 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nppv-dyn1\n\n\n\n\n\n\nbayes\n\n\nppv\n\n\nregression\n\n\nnum\n\n\n\n\n\n\n\n\n\nJun 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nppv-mtcars1\n\n\n\n\n\n\nbayes\n\n\nppv\n\n\nregression\n\n\nnum\n\n\n\n\n\n\n\n\n\nJun 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ndurch3-durch5\n\n\n\n\n\n\nR\n\n\nchallenge\n\n\nstring\n\n\n\n\n\n\n\n\n\nJun 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nstep-dummy\n\n\n\n\n\n\ntidymodels\n\n\nstatlearning\n\n\nschoice\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\names-kaggle1\n\n\n\n\n\n\nregression\n\n\ndata\n\n\nkaggle\n\n\nstring\n\n\nkaggle\n\n\n\n\n\n\n\n\n\nJun 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntargets-multiple-data-files\n\n\n\n\n\n\nprojectmgt\n\n\ntargets\n\n\nrepro\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nregex-insert-char\n\n\n\n\n\n\ntextmining\n\n\nregex\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ncount-emoji\n\n\n\n\n\n\ntextmining\n\n\nnlp\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\npca\n\n\n\n\n\n\neda\n\n\nstatlearning\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels-lasso2\n\n\n\n\n\n\ntidymodels\n\n\nstatlearning\n\n\nlasso\n\n\nlm\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBootstrap1\n\n\n\n\n\n\nstatlearning\n\n\nds1\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels1\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nprediction\n\n\nyacsda\n\n\nstatlearning\n\n\ndyn\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nbike04\n\n\n\n\n\n\nstatlearning\n\n\ntidymodels\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nbike03\n\n\n\n\n\n\nstatlearning\n\n\ntidymodels\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTest-MSE2\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nstatlearning\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels-lasso3\n\n\n\n\n\n\ntidymodels\n\n\nstatlearning\n\n\nlasso\n\n\nlm\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrf-finalize\n\n\n\n\n\n\ntidymodels\n\n\nstatlearning\n\n\ntemplate\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nbike02\n\n\n\n\n\n\nstatlearning\n\n\ntidymodels\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels-vorlage\n\n\n\n\n\n\ntidymodels\n\n\nstatlearning\n\n\ntemplate\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntitanic_casestudy\n\n\n\n\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCluster02\n\n\n\n\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nFlex-vs-nichtflex-Methode3\n\n\n\n\n\n\nstatlearning\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nbootstrap\n\n\n\n\n\n\nstatlearning\n\n\ninference\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwfsets_penguins02\n\n\n\n\n\n\nR\n\n\nstatlearning\n\n\ntidymodels\n\n\nnum\n\n\nwfsets\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nnyc_casestudy\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nstatlearning\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nFlex-vs-nichtflex-Methode2\n\n\n\n\n\n\nstatlearning\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nFlex-vs-nichtflex-Methode\n\n\n\n\n\n\nstatlearning\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nsmape\n\n\n\n\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nShrinkage1\n\n\n\n\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nlm-mario2\n\n\n\n\n\n\nR\n\n\nlm\n\n\npredict\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels-ames-03\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nprediction\n\n\nyacsda\n\n\nstatlearning\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels-ames-04\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nprediction\n\n\nyacsda\n\n\nstatlearning\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels-ames-02\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nprediction\n\n\nyacsda\n\n\nstatlearning\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nbreiman\n\n\n\n\n\n\nds1\n\n\nprediction\n\n\nstatlearning\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nlm-mario3\n\n\n\n\n\n\nR\n\n\nlm\n\n\npredict\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nds-quiz\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nstatlearning\n\n\nmchoice\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntmdb06\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nstatlearning\n\n\ntmdb\n\n\nrandom-forest\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels_workflowset01\n\n\n\n\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntmdb01\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nstatlearning\n\n\ntmdb\n\n\nrandom-forest\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntmdb08\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nstatlearning\n\n\ntmdb\n\n\nrandom-forest\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ndiamonds-tidymodels01\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nstatlearning\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels-penguins02\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nprediction\n\n\nyacsda\n\n\nstatlearning\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels-penguins05\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nprediction\n\n\nyacsda\n\n\nstatlearning\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBoosting1\n\n\n\n\n\n\nmchoice\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nsupervisedlearning\n\n\n\n\n\n\nstatlearning\n\n\nds1\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels-poly01\n\n\n\n\n\n\nR\n\n\nstatlearning\n\n\ntidymodels\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntmdb07\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nstatlearning\n\n\ntmdb\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nregr-tree01\n\n\n\n\n\n\nstatlearning\n\n\ntrees\n\n\ntidymodels\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nn-se\n\n\n\n\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels-penguins04\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nprediction\n\n\nyacsda\n\n\nstatlearning\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels-penguins03\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nprediction\n\n\nyacsda\n\n\nstatlearning\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels2\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nprediction\n\n\nyacsda\n\n\nstatlearning\n\n\nerror\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrf-usemodels\n\n\n\n\n\n\ntidymodels\n\n\nstatlearning\n\n\ntemplate\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nds-quiz2\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nstatlearning\n\n\nmchoice\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\npredictioncontest1\n\n\n\n\n\n\nR\n\n\nds1\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPCA1\n\n\n\n\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nOLS-Minimierung\n\n\n\n\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nbike01\n\n\n\n\n\n\nstatlearning\n\n\ntidymodels\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels3\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nprediction\n\n\nyacsda\n\n\nstatlearning\n\n\nlm\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTest-MSE1\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nstatlearning\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwfsets_penguins01\n\n\n\n\n\n\nR\n\n\nstatlearning\n\n\ntidymodels\n\n\nnum\n\n\nwfsets\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmse\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nstatlearning\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCluster01\n\n\n\n\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCV1\n\n\n\n\n\n\nstatlearning\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrf-finalize2\n\n\n\n\n\n\ntidymodels\n\n\nstatlearning\n\n\ntemplate\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels-lasso\n\n\n\n\n\n\ntidymodels\n\n\nstatlearning\n\n\nlasso\n\n\nlm\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nlm-mario1\n\n\n\n\n\n\nR\n\n\nlm\n\n\npredict\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ngini-plot\n\n\n\n\n\n\n2023\n\n\nvis\n\n\nstatlearning\n\n\ntrees\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrf-finalize3\n\n\n\n\n\n\ntidymodels\n\n\nstatlearning\n\n\ntemplate\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels-ames-01\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nprediction\n\n\nyacsda\n\n\nstatlearning\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidytext\n\n\n\n\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels-penguins06\n\n\n\n\n\n\ntidymodels\n\n\nstatlearning\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels-penguins01\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nprediction\n\n\nyacsda\n\n\nstatlearning\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nknn-ames01\n\n\n\n\n\n\nstatlearning\n\n\ntidymodels\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBoosting2\n\n\n\n\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSzenario-charakterisieren1\n\n\n\n\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nregr-tree03\n\n\n\n\n\n\nstatlearning\n\n\ntrees\n\n\ntidymodels\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntmdb02\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nstatlearning\n\n\ntmdb\n\n\ntrees\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntmdb05\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nstatlearning\n\n\ntmdb\n\n\nrandom-forest\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nregr-tree02\n\n\n\n\n\n\nstatlearning\n\n\ntrees\n\n\ntidymodels\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTengku-Hanis01\n\n\n\n\n\n\ntidymodels\n\n\nprediction\n\n\nyacsda\n\n\nstatlearning\n\n\ntrees\n\n\nspeed\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels-penguins07\n\n\n\n\n\n\ntidymodels\n\n\nstatlearning\n\n\ntrees\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels-vorlage2\n\n\n\n\n\n\ntidymodels\n\n\nstatlearning\n\n\ntemplate\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntmdb04\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nstatlearning\n\n\ntmdb\n\n\nrandom-forest\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntmdb03\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nstatlearning\n\n\ntmdb\n\n\nrandom-forest\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidymodels-poly02\n\n\n\n\n\n\nR\n\n\nstatlearning\n\n\ntidymodels\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nfilter-na1\n\n\n\n\n\n\n2023\n\n\neda\n\n\nna\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nfilter-na5\n\n\n\n\n\n\n2023\n\n\neda\n\n\nna\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nfilter-na2\n\n\n\n\n\n\n2023\n\n\neda\n\n\nna\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nfilter-na3\n\n\n\n\n\n\n2023\n\n\neda\n\n\nna\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nfilter-na4\n\n\n\n\n\n\n2023\n\n\neda\n\n\nna\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nnasa06\n\n\n\n\n\n\ndata\n\n\neda\n\n\nlagemaße\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmariokart-korr2\n\n\n\n\n\n\ndatawrangling\n\n\ndplyr\n\n\neda\n\n\nassociation\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmtcars-regr01\n\n\n\n\n\n\nlm\n\n\nmtcars\n\n\nassociation\n\n\nregression\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nkorr-als-regr\n\n\n\n\n\n\nlm\n\n\nregression\n\n\nstring\n\n\nassociation\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nregression1a\n\n\n\n\n\n\nregression\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmariokart-korr4\n\n\n\n\n\n\ndatawrangling\n\n\ndplyr\n\n\neda\n\n\nassociation\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmariokart-korr3\n\n\n\n\n\n\ndatawrangling\n\n\ndplyr\n\n\neda\n\n\nassociation\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nlm1\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nstats-nutshell\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nkorr02\n\n\n\n\n\n\ndyn\n\n\neda\n\n\nassociation\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRegression5\n\n\n\n\n\n\ndyn\n\n\nregression\n\n\nlm\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRegression3\n\n\n\n\n\n\ndyn\n\n\nregression\n\n\nlm\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nLinearitaet1a\n\n\n\n\n\n\nlm\n\n\nregression\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRegression4\n\n\n\n\n\n\ndyn\n\n\nregression\n\n\nlm\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ninterpret-koeff-lm\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nnichtlineare-regr1\n\n\n\n\n\n\nlm\n\n\nvis\n\n\nqm2\n\n\nregression\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nregression1b\n\n\n\n\n\n\nregression\n\n\nR\n\n\nlm\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmariokart-korr1\n\n\n\n\n\n\ndatawrangling\n\n\ndplyr\n\n\neda\n\n\nassociation\n\n\nnum\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nnasa03\n\n\n\n\n\n\ndata\n\n\neda\n\n\nassociation\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nregression1\n\n\n\n\n\n\nregression\n\n\ndyn\n\n\nlm\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRegression6\n\n\n\n\n\n\ndyn\n\n\nregression\n\n\nexam-22\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nAussagen-einfache-Regr\n\n\n\n\n\n\nregression\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nkorr01\n\n\n\n\n\n\ndyn\n\n\neda\n\n\nassociation\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\npenguins-regr02\n\n\n\n\n\n\nlm\n\n\nbayes\n\n\nrope\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nanim01\n\n\n\n\n\n\n2023\n\n\nvis\n\n\nanimation\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nanim03\n\n\n\n\n\n\n2023\n\n\nvis\n\n\nanimation\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nanim02\n\n\n\n\n\n\n2023\n\n\nvis\n\n\nanimation\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nnasa04\n\n\n\n\n\n\ndata\n\n\neda\n\n\nlagemaße\n\n\nvis\n\n\nanimation\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nnasa05\n\n\n\n\n\n\ndata\n\n\neda\n\n\nlagemaße\n\n\nvis\n\n\nanimation\n\n\nstring\n\n\n\n\n\n\n\n\n\nMay 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nKennwert-robust\n\n\n\n\n\n\neda\n\n\nlagemaße\n\n\nvariability\n\n\nschoice\n\n\n\n\n\n\n\n\n\nApr 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nnasa07\n\n\n\n\n\n\ndata\n\n\neda\n\n\nlagemaße\n\n\nvariability\n\n\nstring\n\n\n\n\n\n\n\n\n\nApr 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nvis-mariokart-variab\n\n\n\n\n\n\ndatawrangling\n\n\neda\n\n\ntidyverse\n\n\nvis\n\n\nvariability\n\n\nstring\n\n\n\n\n\n\n\n\n\nApr 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nsd-vergleich\n\n\n\n\n\n\ndatawrangling\n\n\neda\n\n\ntidyverse\n\n\nvis\n\n\nvariability\n\n\nschoice\n\n\n\n\n\n\n\n\n\nApr 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nnasa01\n\n\n\n\n\n\ndata\n\n\neda\n\n\nlagemaße\n\n\nvariability\n\n\nstring\n\n\n\n\n\n\n\n\n\nApr 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nStreuung-Histogramm\n\n\n\n\n\n\neda\n\n\nvariability\n\n\ndyn\n\n\nschoice\n\n\n\n\n\n\n\n\n\nApr 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmariokart-sd1\n\n\n\n\n\n\ndatawrangling\n\n\ndplyr\n\n\neda\n\n\nvariability\n\n\nnum\n\n\n\n\n\n\n\n\n\nApr 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nsummarise06\n\n\n\n\n\n\ndatawrangling\n\n\neda\n\n\ntidyverse\n\n\ndplyr\n\n\nvariability\n\n\nnum\n\n\n\n\n\n\n\n\n\nApr 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nKennwert-robust2\n\n\n\n\n\n\neda\n\n\nvariability\n\n\nschoice\n\n\n\n\n\n\n\n\n\nApr 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmariokart-desk01\n\n\n\n\n\n\ndatawrangling\n\n\neda\n\n\ntidyverse\n\n\nvis\n\n\nvariability\n\n\nnum\n\n\n\n\n\n\n\n\n\nApr 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmariokart-sd2\n\n\n\n\n\n\ndatawrangling\n\n\ndplyr\n\n\neda\n\n\nvariability\n\n\nnum\n\n\n\n\n\n\n\n\n\nApr 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nsummarise04\n\n\n\n\n\n\ndatawrangling\n\n\neda\n\n\ntidyverse\n\n\ndplyr\n\n\nvariability\n\n\nnum\n\n\n\n\n\n\n\n\n\nApr 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmariokart-sd3\n\n\n\n\n\n\ndatawrangling\n\n\ndplyr\n\n\neda\n\n\nvariability\n\n\nnum\n\n\n\n\n\n\n\n\n\nApr 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nsummarise05\n\n\n\n\n\n\ndatawrangling\n\n\neda\n\n\ntidyverse\n\n\ndplyr\n\n\nvariability\n\n\nnum\n\n\n\n\n\n\n\n\n\nApr 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmw-berechnen\n\n\n\n\n\n\neda\n\n\ndatawrangling\n\n\ndyn\n\n\nnum\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmw-berechnen2\n\n\n\n\n\n\neda\n\n\ndatawrangling\n\n\ndyn\n\n\nnum\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmariokart-max2\n\n\n\n\n\n\ndatawrangling\n\n\ndplyr\n\n\neda\n\n\nnum\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmariokart-mean1\n\n\n\n\n\n\ndatawrangling\n\n\ndplyr\n\n\neda\n\n\nnum\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwrangle10\n\n\n\n\n\n\neda\n\n\nlagemaße\n\n\nnum\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nsummarise01\n\n\n\n\n\n\ndatawrangling\n\n\neda\n\n\ntidyverse\n\n\ndplyr\n\n\nnum\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmariokart-max1\n\n\n\n\n\n\ndatawrangling\n\n\ndplyr\n\n\neda\n\n\nnum\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSchiefe1\n\n\n\n\n\n\nschoice\n\n\neda\n\n\ndistributions\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSchiefe-erkennen\n\n\n\n\n\n\neda\n\n\ndistributions\n\n\nschoice\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nnasa02\n\n\n\n\n\n\ndata\n\n\neda\n\n\nlagemaße\n\n\nstring\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nsummarise03\n\n\n\n\n\n\ndatawrangling\n\n\neda\n\n\ntidyverse\n\n\ndplyr\n\n\nnum\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmariokart-mean2\n\n\n\n\n\n\ndatawrangling\n\n\ndplyr\n\n\neda\n\n\nnum\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmariokart-mean4\n\n\n\n\n\n\ndatawrangling\n\n\ndplyr\n\n\neda\n\n\nnum\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmariokart-mean3\n\n\n\n\n\n\ndatawrangling\n\n\ndplyr\n\n\neda\n\n\nnum\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nsummarise02\n\n\n\n\n\n\ndatawrangling\n\n\neda\n\n\ntidyverse\n\n\ndplyr\n\n\nnum\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmutate02\n\n\n\n\n\n\ndatawrangling\n\n\neda\n\n\ntidyverse\n\n\ndplyr\n\n\nnum\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidydata1\n\n\n\n\n\n\ndatawrangling\n\n\ntidy\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwrangle9\n\n\n\n\n\n\neda\n\n\n2023\n\n\nnum\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwrangle7\n\n\n\n\n\n\neda\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nfilter01\n\n\n\n\n\n\ndatawrangling\n\n\neda\n\n\ntidyverse\n\n\ndplyr\n\n\nnum\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwrangle1\n\n\n\n\n\n\neda\n\n\ndatawrangling\n\n\ntidyverse\n\n\ndplyr\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\naffairs-dplyr\n\n\n\n\n\n\ndatawrangling\n\n\neda\n\n\nstring\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nMWberechnen\n\n\n\n\n\n\neda\n\n\ndatawrangling\n\n\nnum\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmutate01\n\n\n\n\n\n\ndatawrangling\n\n\neda\n\n\ntidyverse\n\n\ndplyr\n\n\nnum\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ndplyr-uebersetzen\n\n\n\n\n\n\ndatawrangling\n\n\ntidyverse\n\n\nstring\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwrangle4\n\n\n\n\n\n\neda\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwrangle3\n\n\n\n\n\n\ndatawrangling\n\n\neda\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwrangle5\n\n\n\n\n\n\neda\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nhaeufigkeit01\n\n\n\n\n\n\ndatawrangling\n\n\neda\n\n\ncount\n\n\nstring\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nLogikpruefung2\n\n\n\n\n\n\nR\n\n\n2023\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTyp-Fehler-R-01\n\n\n\n\n\n\nR\n\n\n2023\n\n\nstring\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nthere-is-no-package\n\n\n\n\n\n\nR\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWertberechnen2\n\n\n\n\n\n\nR\n\n\ndyn\n\n\nnum\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWertzuweisen_mc\n\n\n\n\n\n\nR\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTyp-Fehler-R-06a\n\n\n\n\n\n\nR\n\n\n2023\n\n\nstring\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nargumente\n\n\n\n\n\n\nR\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nLogikpruefung1\n\n\n\n\n\n\nR\n\n\n2023\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nimport-mtcars\n\n\n\n\n\n\nR\n\n\ndata\n\n\nnum\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWertzuweisen\n\n\n\n\n\n\nR\n\n\n2023\n\n\nstring\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWertpruefen\n\n\n\n\n\n\nR\n\n\n2023\n\n\nstring\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTyp-Fehler-R-02\n\n\n\n\n\n\nR\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTyp-Fehler-R-04\n\n\n\n\n\n\nR\n\n\n2023\n\n\nstring\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTyp-Fehler-R-03\n\n\n\n\n\n\nstring\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPfad\n\n\n\n\n\n\nR\n\n\ndatawrangling\n\n\nqm1\n\n\nqm2\n\n\nstring\n\n\ndata\n\n\nimport\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWertberechnen\n\n\n\n\n\n\nR\n\n\ndyn\n\n\nnum\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nboxhist\n\n\n\n\n\n\nvis\n\n\neda\n\n\nen\n\n\ncloze\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmax-corr1\n\n\n\n\n\n\nvis\n\n\n2023\n\n\nnum\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDiamonds-Histogramm-Vergleich2\n\n\n\n\n\n\nvis\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nboxplots-de1a\n\n\n\n\n\n\nvis\n\n\neda\n\n\nschoice\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmovies-vis2\n\n\n\n\n\n\nvis\n\n\neda\n\n\nstring\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRidges-vergleichen\n\n\n\n\n\n\nvis\n\n\ndyn\n\n\nschoice\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDiamonds-Histogramm-Vergleich\n\n\n\n\n\n\nvis\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmax-corr2\n\n\n\n\n\n\nvis\n\n\n2023\n\n\nnum\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHistogramm-in-Boxplot\n\n\n\n\n\n\nvis\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmin-corr1\n\n\n\n\n\n\nvis\n\n\n2023\n\n\nnum\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwozu-balkendiagramm\n\n\n\n\n\n\nvis\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmovies-vis1\n\n\n\n\n\n\nvis\n\n\neda\n\n\nstring\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBoxplot-Aussagen\n\n\n\n\n\n\nvis\n\n\neda\n\n\ndyn\n\n\nschoice\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwozu-streudiagramm\n\n\n\n\n\n\nvis\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nStreudiagramm\n\n\n\n\n\n\nvis\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ndiamonds-histogram\n\n\n\n\n\n\nvis\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nn-vars-diagram\n\n\n\n\n\n\nvis\n\n\n2023\n\n\nnum\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nvariability01\n\n\n\n\n\n\nvariability\n\n\nbasics\n\n\nschoice\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDef-Statistik01\n\n\n\n\n\n\nschoice\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nKausale-Verben\n\n\n\n\n\n\ncausal\n\n\nresearch-question\n\n\nmchoice\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ntidy1\n\n\n\n\n\n\ntidy\n\n\ndatawrangling\n\n\nschoice\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSkalenniveau1a\n\n\n\n\n\n\ndyn\n\n\nvariable-levels\n\n\nvariable-levels\n\n\nschoice\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nZiele-Statistik\n\n\n\n\n\n\nbasics\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nvariability02\n\n\n\n\n\n\nvariability\n\n\nbasics\n\n\nschoice\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSkalenniveau1b\n\n\n\n\n\n\ndyn\n\n\nvariable-levels\n\n\nvariable-levels\n\n\nmchoice\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nkausal02\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\nexam-22\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nkausal03\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\nexam-22\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwuerfel01\n\n\n\n\n\n\nprobability\n\n\ndice\n\n\nexam-22\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmtcars-post2\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\npost\n\n\nexam-22\n\n\nqm2\n\n\nqm2-pruefung\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmtcars-abhaengig_var2\n\n\n\n\n\n\ndyn\n\n\nprobability\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmtcars-post3\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\npost\n\n\nexam-22\n\n\nqm2\n\n\nqm2-pruefung\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\npenguins-stan-04\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\nexam-22\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\npenguins-stan-03\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\nexam-22\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\npenguins-stan-02\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\nexam-22\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\npenguins-stan-05\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\nexam-22\n\n\nqm2\n\n\nqm2-pruefung\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nkausal01\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\nexam-22\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\npigs2\n\n\n\n\n\n\nbayes\n\n\nqm2\n\n\nqm2-pruefung\n\n\nregression\n\n\nexam-22\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nrope2\n\n\n\n\n\n\nrope\n\n\nbayes\n\n\nregression\n\n\nexam-22\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPPV1a-mtcars\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\nexam-22\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nmtcars-post\n\n\n\n\n\n\nbayes\n\n\npost\n\n\nestimation\n\n\nexam-22\n\n\nqm2\n\n\nqm2-pruefung\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nwskt-mtcars-1l\n\n\n\n\n\n\npost\n\n\nbayes\n\n\nregression\n\n\n\n\n\n\n\n\n\nJan 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nkausal05\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nkausal04\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nkausal-bedrooms1\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nkausal-einfach\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nkausal21\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nkausal26\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nkausal10\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nkausal27\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nkausal20\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nkausal06\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nkausal08\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nkausal09\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nkausal07\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nrandomdag1\n\n\n\n\n\n\ncausal\n\n\ndag\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nkausal_corona_glatze\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nkausal25\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nkausal22\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nMediterran-Alk\n\n\n\n\n\n\ncausal\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nkausal23\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nkausal24\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nkausal24\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nttest-skalenniveau\n\n\n\n\n\n\nttest\n\n\nregression\n\n\nvariable-levels\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nRegression2\n\n\n\n\n\n\nregression\n\n\ndyn\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nRegr-Bayes-interpret\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nNullhyp-Beispiel\n\n\n\n\n\n\n\n\n\n\n\n\ninference\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nInteraktionseffekt1\n\n\n\n\n\n\ninteraction\n\n\nregression\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nRegr-Bayes-interpret03\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\nqm2\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nrope-regression\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\nrope\n\n\nqm2\n\n\nqm2-pruefung\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nRegr-Bayes-interpret02\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nposterior_interval\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\npost\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nmtcars-rope1\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nstan_glm_prioriwerte\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\nqm2\n\n\nqm2-pruefung\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nrope4\n\n\n\n\n\n\nrope\n\n\nbayes\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nrope3\n\n\n\n\n\n\nrope\n\n\nbayes\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\ndiamonds-nullhyp-mws\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nzwert-berechnen\n\n\n\n\n\n\nz-value\n\n\nR\n\n\nmath\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nanova-skalenniveau\n\n\n\n\n\n\nvariable-levels\n\n\nanova\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nGriech-Buchstaben-Inferenz\n\n\n\n\n\n\nqm2\n\n\ninference\n\n\nparameters\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nrope1\n\n\n\n\n\n\nrope\n\n\nbayes\n\n\n\n\n\n\n\n\n\nDec 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nstan_glm_parameterzahl\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\nparameters\n\n\n\n\n\n\n\n\n\nDec 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nBayes-Ziel1\n\n\n\n\n\n\nregression\n\n\nbayes\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nLikelihood2\n\n\n\n\n\n\nregression\n\n\nbayes\n\n\nlikelihood\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nBayesmod-bestimmen01\n\n\n\n\n\n\nregression\n\n\nbayes\n\n\nprior\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPost-befragen1\n\n\n\n\n\n\nregression\n\n\nbayes\n\n\npost\n\n\nqm2-pruefung\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPostvert-Regr-01\n\n\n\n\n\n\nregression\n\n\nbayes\n\n\npost\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nBed-Post-Wskt1\n\n\n\n\n\n\nregression\n\n\nbayes\n\n\npost\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPriorwahl1\n\n\n\n\n\n\nfat-tails\n\n\ndistributions\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nBayesmod-bestimmen02\n\n\n\n\n\n\nregression\n\n\nbayes\n\n\nprior\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nLikelihood-identifizieren\n\n\n\n\n\n\nregression\n\n\nbayes\n\n\nlikelihood\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPriorwahl2\n\n\n\n\n\n\nregression\n\n\nbayes\n\n\ndistribution\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nfattails02\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nfattails01\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nfat-tails\n\n\nnormal-distribution\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nReThink3e1-7\n\n\n\n\n\n\nbayes\n\n\nprobability\n\n\npost\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nVerteilungen-Quiz-05\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\nqm2-pruefung\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nVerteilungen-Quiz-02\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\nquiz\n\n\nqm2\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nVerteilungen-Quiz-03\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nVerteilungen-Quiz-04\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\nqm2-pruefung\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nVerteilungen-Quiz-17\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nVerteilungen-Quiz-10\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nVerteilungen-Quiz-11\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nVerteilungen-Quiz-16\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nVerteilungen-Quiz-18\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nVerteilungen-Quiz-01\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nVerteilungen-Quiz-06\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\nqm2-pruefung\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nVerteilungen-Quiz-08\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\nqm2-pruefung\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nVerteilungen-Quiz-09\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\nqm2-pruefung\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nVerteilungen-Quiz-07\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\nqm2\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nVerteilungen-Quiz-13\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nVerteilungen-Quiz-14\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\nqm2-pruefung\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nVerteilungen-Quiz-15\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nVerteilungen-Quiz-12\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\ntwitter07\n\n\n\n\n\n\ntextmining\n\n\ntwitter\n\n\nprogramming\n\n\n\n\n\n\n\n\n\nNov 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nAnteil-Apple\n\n\n\n\n\n\nbayes\n\n\nbayes-grid\n\n\nqm2-pruefung\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nIQ-Studentis\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\nqm2-pruefung\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nWarum-Bayes\n\n\n\n\n\n\nqm2\n\n\nbayes\n\n\nprobability\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nSim-Prior\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPriori-Streuung\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\ndistribution\n\n\nbayes\n\n\nqm2\n\n\nqm2-pruefung\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nReThink4e3\n\n\n\n\n\n\nbayes\n\n\nprobability\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nReThink4e2\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nfat-tails-Artikel\n\n\n\n\n\n\nprobability\n\n\ndistribution\n\n\nfat-tails\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nstan_glm01\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nReThink3m2\n\n\n\n\n\n\nbayes\n\n\npost\n\n\nprobability\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nKung-height\n\n\n\n\n\n\nbayes\n\n\nppv\n\n\nprobability\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nReThink3m3\n\n\n\n\n\n\nbayes\n\n\nppv\n\n\nprobability\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nZwielichter-Dozent-Bayes\n\n\n\n\n\n\nbayes\n\n\nprobability\n\n\nppv\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nsubjektiv-Bayes\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\ntwitter01\n\n\n\n\n\n\ntextmining\n\n\ntwitter\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\ntwitter06\n\n\n\n\n\n\ntextmining\n\n\ntwitter\n\n\nprogramming\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nLose-Nieten-Binomial-Grid\n\n\n\n\n\n\nprobability\n\n\nbinomial\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nwuerfel04\n\n\n\n\n\n\nprobability\n\n\ndice\n\n\nsimulation\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nwuerfel03\n\n\n\n\n\n\nprobability\n\n\ndice\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nwuerfel02\n\n\n\n\n\n\nprobability\n\n\ndice\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\ntwitter03\n\n\n\n\n\n\ntextmining\n\n\ntwitter\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\ntwitter04\n\n\n\n\n\n\ntextmining\n\n\ntwitter\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\ntwitter05\n\n\n\n\n\n\ntextmining\n\n\ntwitter\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\ntwitter02\n\n\n\n\n\n\ntextmining\n\n\ntwitter\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\neuro-bayes\n\n\n\n\n\n\nprobability\n\n\nbayes-grid\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nkekse01\n\n\n\n\n\n\nprobability\n\n\nbayes-grid\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\npurrr-map01\n\n\n\n\n\n\nR\n\n\nmap\n\n\ntidyverse\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\npurrr-map06\n\n\n\n\n\n\nprogramming\n\n\nloop\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nmtcars-abhaengig\n\n\n\n\n\n\nprobability\n\n\ndependent\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nvoll-normal\n\n\n\n\n\n\nprobability\n\n\nmeta\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nGem-Wskt1\n\n\n\n\n\n\nprobability\n\n\nqm2\n\n\nqm2-pruefung\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\npurrr-map05\n\n\n\n\n\n\nprogramming\n\n\nloop\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\npurrr-map02\n\n\n\n\n\n\nR\n\n\nmap\n\n\ntidyverse\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\npurrr-map03\n\n\n\n\n\n\nR\n\n\nmap\n\n\ntidyverse\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\npurrr-map04\n\n\n\n\n\n\nR\n\n\nmap\n\n\ntidyverse\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nungewiss-arten-regression\n\n\n\n\n\n\nqm2\n\n\ninference\n\n\nlm\n\n\n\n\n\n\n\n\n\nOct 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nInferenz-fuer-alle\n\n\n\n\n\n\nqm2\n\n\ninference\n\n\nuncertainty\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nmtcars-simple1\n\n\n\n\n\n\nregression\n\n\nen\n\n\nbayes\n\n\nfrequentist\n\n\nqm1\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nlog-y-regression3\n\n\n\n\n\n\nstats-nutshell\n\n\nqm2\n\n\nregression\n\n\nlog\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nlog-y-regression2\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nqm2\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nadjustieren1\n\n\n\n\n\n\nqm2\n\n\nlm\n\n\nbayes\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\ninterpret-koeff\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nbayes\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nvorhersageintervall1\n\n\n\n\n\n\nlm\n\n\ninference\n\n\nqm2\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nlm-Standardfehler\n\n\n\n\n\n\ninference\n\n\nlm\n\n\nqm2\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\npunktschaetzer-reicht-nicht\n\n\n\n\n\n\nregression\n\n\nen\n\n\nbayes\n\n\nfrequentist\n\n\nqm1\n\n\nstats-nutshell\n\n\nqm2\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nmtcars-simple2\n\n\n\n\n\n\nregression\n\n\nen\n\n\nbayes\n\n\nfrequentist\n\n\nqm1\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nmtcars-simple3\n\n\n\n\n\n\nregression\n\n\nen\n\n\nbayes\n\n\nfrequentist\n\n\nqm1\n\n\nstats-nutshell\n\n\nqm2\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nttest-als-regression\n\n\n\n\n\n\nregression\n\n\nttest\n\n\nvariable-levels\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nlog-y-regression1\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nqm2\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nadjustieren2\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nqm2\n\n\nbayes\n\n\nadjust\n\n\nqm2-pruefung\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nStichprobenziehen1\n\n\n\n\n\n\nlm\n\n\ninference\n\n\nqm2\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nGriech-Buchstaben-Inferenz\n\n\n\n\n\n\nqm2\n\n\ninference\n\n\n\n\n\n\n\n\n\nJul 10, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Untitled",
    "section": "",
    "text": "Überschrift\nsjdfk\nsdjklf\n\ndfsjk\nsdfjlk\n\n\n \nThis is a form spacer\n\nA text field (1)\n  \n\n\nAnother text field (2)"
  },
  {
    "objectID": "English.html",
    "href": "English.html",
    "title": "English",
    "section": "",
    "text": "Note\n\n\n\n\nThis book is written in German. However, your browser will easily translate the text to your favorite language. Please check your browser’s documentation for details. This should be achievbed with one or two clicks. \\(\\square\\)"
  },
  {
    "objectID": "English.html#translation-to-english",
    "href": "English.html#translation-to-english",
    "title": "English",
    "section": "",
    "text": "Note\n\n\n\n\nThis book is written in German. However, your browser will easily translate the text to your favorite language. Please check your browser’s documentation for details. This should be achievbed with one or two clicks. \\(\\square\\)"
  },
  {
    "objectID": "posts/chatgpt-curl/chatgpt-curl.html",
    "href": "posts/chatgpt-curl/chatgpt-curl.html",
    "title": "NAME-OF-EXERCISE",
    "section": "",
    "text": "Aufgabe\nFragen Sie ChatGPT via API zum Sentiment des ersten Texts aus dem Germeval-2018-Datensatz (Train).\nHinweise:\n\nBeachten Sie die Standardhinweise des Datenwerks.\nNutzen Sie curl oder einen Wrapper um curl in R.\n\n         \n\n\nLösung"
  }
]