[
  {
    "objectID": "posts/kekse02/kekse02.html",
    "href": "posts/kekse02/kekse02.html",
    "title": "kekse02",
    "section": "",
    "text": "Solution\n\n\n\n\n\n\n\n\n\n\n\np_Gitter\nPriori\nLikelihood\nunstd_Post\nPost\n\n\n\n\n0.00\n1\n0.00\n0.00\n0.00\n\n\n0.01\n1\n0.01\n0.01\n0.00\n\n\n0.02\n1\n0.02\n0.02\n0.00\n\n\n0.03\n1\n0.03\n0.03\n0.00\n\n\n0.04\n1\n0.04\n0.04\n0.00\n\n\n0.05\n1\n0.05\n0.05\n0.00\n\n\n0.06\n1\n0.06\n0.06\n0.00\n\n\n0.07\n1\n0.07\n0.07\n0.00\n\n\n0.08\n1\n0.08\n0.08\n0.00\n\n\n0.09\n1\n0.09\n0.09\n0.00\n\n\n0.10\n1\n0.10\n0.10\n0.00\n\n\n0.11\n1\n0.11\n0.11\n0.00\n\n\n0.12\n1\n0.12\n0.12\n0.00\n\n\n0.13\n1\n0.13\n0.13\n0.00\n\n\n0.14\n1\n0.14\n0.14\n0.00\n\n\n0.15\n1\n0.15\n0.15\n0.00\n\n\n0.16\n1\n0.16\n0.16\n0.00\n\n\n0.17\n1\n0.17\n0.17\n0.00\n\n\n0.18\n1\n0.18\n0.18\n0.00\n\n\n0.19\n1\n0.19\n0.19\n0.00\n\n\n0.20\n1\n0.20\n0.20\n0.00\n\n\n0.21\n1\n0.21\n0.21\n0.00\n\n\n0.22\n1\n0.22\n0.22\n0.00\n\n\n0.23\n1\n0.23\n0.23\n0.00\n\n\n0.24\n1\n0.24\n0.24\n0.00\n\n\n0.25\n1\n0.25\n0.25\n0.00\n\n\n0.26\n1\n0.26\n0.26\n0.01\n\n\n0.27\n1\n0.27\n0.27\n0.01\n\n\n0.28\n1\n0.28\n0.28\n0.01\n\n\n0.29\n1\n0.29\n0.29\n0.01\n\n\n0.30\n1\n0.30\n0.30\n0.01\n\n\n0.31\n1\n0.31\n0.31\n0.01\n\n\n0.32\n1\n0.32\n0.32\n0.01\n\n\n0.33\n1\n0.33\n0.33\n0.01\n\n\n0.34\n1\n0.34\n0.34\n0.01\n\n\n0.35\n1\n0.35\n0.35\n0.01\n\n\n0.36\n1\n0.36\n0.36\n0.01\n\n\n0.37\n1\n0.37\n0.37\n0.01\n\n\n0.38\n1\n0.38\n0.38\n0.01\n\n\n0.39\n1\n0.39\n0.39\n0.01\n\n\n0.40\n1\n0.40\n0.40\n0.01\n\n\n0.41\n1\n0.41\n0.41\n0.01\n\n\n0.42\n1\n0.42\n0.42\n0.01\n\n\n0.43\n1\n0.43\n0.43\n0.01\n\n\n0.44\n1\n0.44\n0.44\n0.01\n\n\n0.45\n1\n0.45\n0.45\n0.01\n\n\n0.46\n1\n0.46\n0.46\n0.01\n\n\n0.47\n1\n0.47\n0.47\n0.01\n\n\n0.48\n1\n0.48\n0.48\n0.01\n\n\n0.49\n1\n0.49\n0.49\n0.01\n\n\n0.50\n1\n0.50\n0.50\n0.01\n\n\n0.50\n1\n0.50\n0.50\n0.01\n\n\n0.51\n1\n0.51\n0.51\n0.01\n\n\n0.52\n1\n0.52\n0.52\n0.01\n\n\n0.53\n1\n0.53\n0.53\n0.01\n\n\n0.54\n1\n0.54\n0.54\n0.01\n\n\n0.55\n1\n0.55\n0.55\n0.01\n\n\n0.56\n1\n0.56\n0.56\n0.01\n\n\n0.57\n1\n0.57\n0.57\n0.01\n\n\n0.58\n1\n0.58\n0.58\n0.01\n\n\n0.59\n1\n0.59\n0.59\n0.01\n\n\n0.60\n1\n0.60\n0.60\n0.01\n\n\n0.61\n1\n0.61\n0.61\n0.01\n\n\n0.62\n1\n0.62\n0.62\n0.01\n\n\n0.63\n1\n0.63\n0.63\n0.01\n\n\n0.64\n1\n0.64\n0.64\n0.01\n\n\n0.65\n1\n0.65\n0.65\n0.01\n\n\n0.66\n1\n0.66\n0.66\n0.01\n\n\n0.67\n1\n0.67\n0.67\n0.01\n\n\n0.68\n1\n0.68\n0.68\n0.01\n\n\n0.69\n1\n0.69\n0.69\n0.01\n\n\n0.70\n1\n0.70\n0.70\n0.01\n\n\n0.71\n1\n0.71\n0.71\n0.01\n\n\n0.72\n1\n0.72\n0.72\n0.01\n\n\n0.73\n1\n0.73\n0.73\n0.01\n\n\n0.74\n1\n0.74\n0.74\n0.01\n\n\n0.75\n1\n0.75\n0.75\n0.02\n\n\n0.76\n1\n0.76\n0.76\n0.02\n\n\n0.77\n1\n0.77\n0.77\n0.02\n\n\n0.78\n1\n0.78\n0.78\n0.02\n\n\n0.79\n1\n0.79\n0.79\n0.02\n\n\n0.80\n1\n0.80\n0.80\n0.02\n\n\n0.81\n1\n0.81\n0.81\n0.02\n\n\n0.82\n1\n0.82\n0.82\n0.02\n\n\n0.83\n1\n0.83\n0.83\n0.02\n\n\n0.84\n1\n0.84\n0.84\n0.02\n\n\n0.85\n1\n0.85\n0.85\n0.02\n\n\n0.86\n1\n0.86\n0.86\n0.02\n\n\n0.87\n1\n0.87\n0.87\n0.02\n\n\n0.88\n1\n0.88\n0.88\n0.02\n\n\n0.89\n1\n0.89\n0.89\n0.02\n\n\n0.90\n1\n0.90\n0.90\n0.02\n\n\n0.91\n1\n0.91\n0.91\n0.02\n\n\n0.92\n1\n0.92\n0.92\n0.02\n\n\n0.93\n1\n0.93\n0.93\n0.02\n\n\n0.94\n1\n0.94\n0.94\n0.02\n\n\n0.95\n1\n0.95\n0.95\n0.02\n\n\n0.96\n1\n0.96\n0.96\n0.02\n\n\n0.97\n1\n0.97\n0.97\n0.02\n\n\n0.98\n1\n0.98\n0.98\n0.02\n\n\n0.99\n1\n0.99\n0.99\n0.02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCategories:\n\nprobability\nbayes-grid"
  },
  {
    "objectID": "posts/korr-als-regr/korr-als-regr.html",
    "href": "posts/korr-als-regr/korr-als-regr.html",
    "title": "korr-als-regr",
    "section": "",
    "text": "Exercise\nDie Korrelation pr√ºft, ob zwei Merkmale linear zusammenh√§ngen.\nWie viele andere Verfahren kann die Korrelation als ein Spezialfall der Regression bzw. des linearen Modells \\(y = \\beta_0 + \\beta_1 + \\ldots \\beta_n + \\epsilon\\) betrachtet werden.\nAls ein spezielles Beispiel betrachten wir die Frage, ob das Gewicht eines Diamanten (carat) mit dem Preis (price) zusammenh√§ngt (Datensatz diamonds).\nDen Datensatz k√∂nnen Sie so laden:\n\nlibrary(tidyverse)\ndata(diamonds)\n\n\nGeben Sie das Skalenniveau beider Variablen an!\nBetrachten Sie die Ausgabe von R:\n\n\nlm1 <- lm(price ~ carat, data = diamonds)\nsummary(lm1)\n\n\nCall:\nlm(formula = price ~ carat, data = diamonds)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-18585   -805    -19    537  12732 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -2256.4       13.1    -173   <2e-16 ***\ncarat         7756.4       14.1     551   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1550 on 53938 degrees of freedom\nMultiple R-squared:  0.849, Adjusted R-squared:  0.849 \nF-statistic: 3.04e+05 on 1 and 53938 DF,  p-value: <2e-16\n\n\nWie (bzw. wo) ist aus dieser Ausgabe die Korrelation herauszulesen?\n\nMacht es einen Unterschied, ob man Preis mit Karat bzw. Karat mit Preis korreliert?\nIn der klassischen Inferenzstatistik ist der \\(p\\)-Wert eine zentrale Gr√∂√üe; ist er klein (\\(p<.05\\)) so nennt man die zugeh√∂rige Statistik signifikant und verwirft die getestete Hypothese.\nIm Folgenden sehen Sie einen Korrelationstest auf statistische Signifikanz, mit R durchgef√ºhrt. Zeigt der Test ein (statistisch) signifikantes Ergebnis? Wie gro√ü ist der ‚ÄúUnsicherheitskorridor‚Äù, um den Korrelationswert (zugleich Punktsch√§tzer f√ºr den Populationswert)?\n\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.5.2\n‚úî insight     0.18.2     ‚úî datawizard  0.5.1   \n‚úî bayestestR  0.12.1.1   ‚úî performance 0.9.2   \n‚úî parameters  0.18.2     ‚úî effectsize  0.7.0.5 \n‚úî modelbased  0.8.5      ‚úî correlation 0.8.2   \n‚úî see         0.7.2      ‚úî report      0.5.5   \n\ndiamonds %>% \n  sample_n(30) %>% \n  select(price, carat) %>% \n  correlation()\n\n# Correlation Matrix (pearson-method)\n\nParameter1 | Parameter2 |    r |       95% CI | t(28) |         p\n-----------------------------------------------------------------\nprice      |      carat | 0.94 | [0.88, 0.97] | 14.87 | < .001***\n\np-value adjustment method: Holm (1979)\nObservations: 30\n\n\n         \n\n\nSolution\n\ncarat ist metrisch (verh√§ltnisskaliert) und price ist metrisch (verh√§ltnisskaliert)\n\\(R^2\\) kann bei einer einfachen (univariaten) Regression als das Quadrat von \\(r\\) berechnet werden. Daher \\(r = \\sqrt{R^2}\\).\n\n\nsqrt(0.8493)\n\n[1] 0.92\n\n\nZum Vergleich\n\ndiamonds %>% \n  summarise(r = cor(price, carat))\n\n# A tibble: 1 √ó 1\n      r\n  <dbl>\n1 0.922\n\n\nMan kann den Wert der Korrelation auch noch anderweitig berechnen (\\(\\beta\\) umrechnen in \\(\\rho\\)).\n\nNein. Die Korrelation ist eine symmetrische Relation.\nJa; die Zahl ‚Äú3.81e-14‚Äù bezeichnet eine positive Zahl kleiner eins mit 13 Nullern vor der ersten Ziffer, die nicht Null ist (3.81 in diesem Fall). Der ‚ÄúUnsicherheitskorridor‚Äù reicht von etwa 0.87 bis 0.97.\n\n\nCategories:\n\ncorrelation\nlm"
  },
  {
    "objectID": "posts/Inferenz-fuer-alle/Inferenz-fuer-alle.html",
    "href": "posts/Inferenz-fuer-alle/Inferenz-fuer-alle.html",
    "title": "Inferenz-fuer-alle",
    "section": "",
    "text": "Solution\n\nF√ºr (grunds√§tzlich) alle: F√ºr jede Statistik kann man prinzipiell von der jeweiligen Stichprobe (auf Basis derer die Statistik berechnet wurde) auf eine zugeh√∂rige Grundgesamtheit schlie√üen.\nF√ºr (grunds√§tzlich) alle: Die Methoden der Inferenzstatistik sind prinzipiell unabh√§ngig von den Spezifika bestimmter Forschungsfragen oder -bereiche. In den meisten Forschungsfragen ist man daran interessiert allgemeing√ºltige Aussagen zu treffen. Da Statistiken sich nur auf eine Stichprobe - also einen zumeist nur kleinen Teil einer Grundgesamtheit beziehen - wird man sich kaum mit einer Statistik zufrieden geben, sondern nach Inferenzstatistik verlangen.\nIn einigen Ausnahmef√§llen wird man auf eine Inferenzstatistik verzichten. Etwa wenn man bereits eine Vollerhebung durchgef√ºhrt hat, z.B. alle Mitarbeitis eines Unternehmens befragt hat, dann kennt man ja bereits den wahren Populationswert. Ein anderer Fall ist, wenn man nicht an Verallgemeinerungen interessiert ist: Kennt man etwa die √úberlebenschance \\(p\\) des Titanic-Ungl√ºcks, so ist es fraglich auf welche Grundgesamtheit man die Statistik \\(p\\) bzw. zu welchem Paramter \\(\\pi\\) (kleines Pi) man generalisieren m√∂chte.\n\n\nCategories:\n\nqm2\nqm2-thema01"
  },
  {
    "objectID": "posts/Rethink_2m3/Rethink_2m3.html",
    "href": "posts/Rethink_2m3/Rethink_2m3.html",
    "title": "Rethink_2m3",
    "section": "",
    "text": "Solution\nZur Erinnerung:\n\\[\\begin{aligned}\nPr(A) &= Pr(A \\cap B) + Pr(A \\cap A^C)  \\qquad \\text{| bei disjunkten Ereignissen}\\\\\nPr(A \\cap B) &= Pr(A|B) \\cdot Pr(B)\\\\\nPr(A \\cap B^C) &= Pr(A|B^C) \\cdot Pr(B^C)\n\\end{aligned}\\]\nWobei \\(A^C\\) das komlement√§re Ereignis zu \\(A\\) meint.\nThe solution is taken from this source.\n\n# probability of land, given Earth:\np_le <- 0.3\n\n# probability of land, given Mars:\np_lm <- 1.0\n\n# probability of Earth:\np_e <- 0.5\n\n# prob. of Mars:\np_m <- 0.5\n\n# probability of land:\n# das ist die Summe zweier gemeinsamer Wahrscheinlichkeiten (Randwahrscheinlichkeit)\n# also die Summe zweier Zellen aus der Kontingenztabelle\np_l <- (p_e * p_le) + (p_m * p_lm)\n\n\n# probability of Earth, given land (using Bayes' Theorem):\np_el <- (p_le * p_e) / p_l\np_el\n\n[1] 0.2307692\n\n#> [1] 0.231\n\n\nCategories:\n\nprobability\nbayes"
  },
  {
    "objectID": "posts/lm1/lm1.html#answerlist",
    "href": "posts/lm1/lm1.html#answerlist",
    "title": "lm1",
    "section": "Answerlist",
    "text": "Answerlist\n\nmpg und hp sind positiv korreliert laut dem Modell.\nDer Achsenabschnitt ist nahe Null.\nDie Analyse beinhaltet einen nominal skalierten Pr√§diktor.\nDas gesch√§tzte Betagewicht f√ºr hp liegt bei 30.099.\nDas gesch√§tzte Betagewicht f√ºr hp liegt bei -0.068."
  },
  {
    "objectID": "posts/lm1/lm1.html#answerlist-1",
    "href": "posts/lm1/lm1.html#answerlist-1",
    "title": "lm1",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nFalsch\nRichtig\n\n\nCategories:\n\nregression\nlm\nqm2\nstats-nutshell"
  },
  {
    "objectID": "posts/Rethink_2m4/Rethink_2m4.html",
    "href": "posts/Rethink_2m4/Rethink_2m4.html",
    "title": "Rethink_2m4",
    "section": "",
    "text": "Solution\nLet‚Äôs label the cards bb (black on both sides), bw (black on one, white on the other), and ww (both sides are white), respectively.\nWanted is the probability that both sides are black (bb), given one side is black (1b): \\(Pr(bb|1b)\\).\nLet‚Äôs count the ways how the data - one black side - can come up in each conjecture (hypothesis), bb, bw, ww. Let‚Äôs denote ‚Äúfirst side white‚Äù as 1b‚Äù and ‚Äúfirst side black‚Äù as 2b.\nbb: 2 valid paths\n\n\n\n\n\n\nbw: 1 valid path\n\n\n\n\n\n\nww: 0 valid path\n\n\n\n\n\n\n\nd <-\n  tibble::tribble(\n  ~Hyp, ~Prior,\n  \"bb\",     1, \n  \"bw\",     1,   \n  \"ww\",     1, \n  ) %>% \n  mutate(Likelihood = c(2,1,0),\n         unstand_post = Prior*Likelihood,\n         std_post = unstand_post / sum(unstand_post))\n\n\n\n\n\n\n\n  \n  \n    \n      Hyp\n      Prior\n      Likelihood\n      unstand_post\n      std_post\n    \n  \n  \n    bb\n1\n2\n2\n0.67\n    bw\n1\n1\n1\n0.33\n    ww\n1\n0\n0\n0.00\n  \n  \n  \n\n\n\n\nThe following solution is taken from this source.\n\ncard_bb_likelihood <- 2\ncard_bw_likelihood <- 1\ncard_ww_likelihood <- 0\n\nlikelihood <- c(card_bb_likelihood, card_bw_likelihood, card_ww_likelihood)\nprior <- c(1, 1, 1)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\nposterior[1]\n\n[1] 0.6666667\n\n\n\nCategories:\n\nprobability\n‚Äò2022‚Äô"
  },
  {
    "objectID": "posts/ttest-skalenniveau/ttest-skalenniveau.html",
    "href": "posts/ttest-skalenniveau/ttest-skalenniveau.html",
    "title": "ttest-skalenniveau",
    "section": "",
    "text": "Der t-Test ist ein inferenzstatistisches Verfahren des Frequentismus. Welches Skalenniveau passt zu diesem Verfahren?\nHinweisse:\n\nDie folgende Abbildung gibt Tipps.\nInformationen, die zur L√∂sung einer Aufgabe nicht n√∂tig sind, sollte man ignorieren.\n\n\n\n\n\n\n\n\n\n\n\n\n\nUV: nominal (mehrstufig), AV: metrisch\nUV: metrisch, AV: nominal (zweistufig)\nUV: nominal (mehrstufig), AV: nominal (mehrstufig)\nUV: metrisch, AV: nominal (zweistufig)\nUV: nominal (zweistufig), AV: metrisch"
  },
  {
    "objectID": "posts/ttest-skalenniveau/ttest-skalenniveau.html#answerlist-1",
    "href": "posts/ttest-skalenniveau/ttest-skalenniveau.html#answerlist-1",
    "title": "ttest-skalenniveau",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nFalsch\nWahr\n\n\nCategories:\n\nttest\nregr\nvariable-levels"
  },
  {
    "objectID": "posts/Bed-Wskt2/Bed-Wskt2.html",
    "href": "posts/Bed-Wskt2/Bed-Wskt2.html",
    "title": "Bed-Wskt2",
    "section": "",
    "text": "Solution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.58\n\nA_cond_B <- AandB / B_marg %>% round(2)\nAneg_cond_B <- AnegandB / B_marg %>% round(2)\nA_cond_Bneg <- AandBneg / Bneg_marg %>% round(2)\nAneg_cond_Bneg <- AnegandBneg / Bneg_marg %>% round(2)\n\n\\(Pr(A) = 0.62\\).\n\\(Pr(B) = 0.7154\\).\n\\(Pr(AB) = 0.4154\\).\n\\(Pr(A|B)= 0.5769444\\).\n\\(Pr(\\neg A|B) = 0.4222222\\).\n\\(Pr(A|\\neg B) = 0.7142857\\).\n\\(Pr(\\neg A|\\neg B) = 0.2857143\\).\n\nCategories:\n\nprobability\nconditional"
  },
  {
    "objectID": "posts/Rethink_2m5/Rethink_2m5.html",
    "href": "posts/Rethink_2m5/Rethink_2m5.html",
    "title": "Rethink_2m5",
    "section": "",
    "text": "Solution\nThe only difference to the question 2M4 is that we now have two bb cards, rendering the prior plausibility twice as high.\nLet‚Äôs label the cards bb (black on both sides), bw (black on one, white on the other), and ww (both sides are white), respectively.\nWanted is the probability that the second side of the card is black (2b), given one side is black (1b): \\(Pr(2b|1b)\\).\n\nd <-\n  tibble::tribble(\n  ~Hyp, ~Prior,\n  \"bb\",     2L, \n  \"bw\",     1L,   \n  \"ww\",     1L, \n  ) %>% \n  mutate(Likelihood = c(2,1,0),\n         unstand_post = Prior*Likelihood,\n         std_post = unstand_post / sum(unstand_post))\n\n\n\n\n\n\n\n  \n  \n    \n      Hyp\n      Prior\n      Likelihood\n      unstand_post\n      std_post\n    \n  \n  \n    bb\n2\n2\n4\n0.80\n    bw\n1\n1\n1\n0.20\n    ww\n1\n0\n0\n0.00\n  \n  \n  \n\n\n\n\nThe following solution is taken from this source.\n\ncard_bb_likelihood <- 2\ncard_bw_likelihood <- 1\ncard_ww_likelihood <- 0\n\nlikelihood <- c(card_bb_likelihood, card_bw_likelihood, card_ww_likelihood,\n                card_bb_likelihood)\nprior <- c(1, 1, 1, 1)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\nposterior[1] + posterior[4]\n\n[1] 0.8\n\n\n\nCategories:\n\nprobability\nbayes"
  },
  {
    "objectID": "posts/Rethink_2m2/Rethink_2m2.html",
    "href": "posts/Rethink_2m2/Rethink_2m2.html",
    "title": "Rethink_2m2",
    "section": "",
    "text": "Solution\nThe solution is taken from this source.\n\nlibrary(tidyverse)\n\ndist <- \n  tibble(\n    # Gridwerte bestimmen:\n    p_grid = seq(from = 0, to = 1, length.out = 20),\n    # Priori-Wskt bestimmen:\n    prior  = case_when(\n      p_grid < 0.5 ~ 0,\n      p_grid >= 0.5 ~ 1)) %>%\n  mutate(\n    # Likelihood berechnen:\n    likelihood_1 = dbinom(3, size = 3, prob = p_grid),\n    likelihood_2 = dbinom(3, size = 4, prob = p_grid),\n    likelihood_3 = dbinom(5, size = 7, prob = p_grid),\n    # unstand. Posterior-Wskt:\n    unstand_post_1 = likelihood_1 * prior,\n    unstand_post_2 = likelihood_2 * prior,\n    unstand_post_3 = likelihood_3 * prior,\n    # stand. Post-Wskt:\n    std_post_1 = unstand_post_1 / sum(unstand_post_1),\n    std_post_2 = unstand_post_2 / sum(unstand_post_1),\n    std_post_3 = unstand_post_3 / sum(unstand_post_1)\n    ) \n\nJetzt k√∂nnen wir das Diagramm zeichnen:\n\nggplot(dist) +\n  aes(x = p_grid, y= std_post_1) +\n  geom_line()+\n  geom_point() +\n  labs(x = \"p(W)\",\n       y = \"Posteriori-Wahrscheinlichkeit\",\n       title = \"Daten: WWW\")\n\n\n\n\n\n\n\n\n\nggplot(dist) +\n  aes(x = p_grid, y= std_post_2) +\n  geom_line()+\n  geom_point() +\n  labs(x = \"p(W)\",\n       y = \"Posteriori-Wahrscheinlichkeit\",\n       title = \"Daten: WWWL\")\n\n\n\n\n\n\n\n\n\nggplot(dist) +\n  aes(x = p_grid, y= std_post_3) +\n  geom_line()+\n  geom_point() +\n  labs(x = \"p(W)\",\n       y = \"Posteriori-Wahrscheinlichkeit\",\n       title = \"Daten: LWWLWWW\")\n\n\n\n\n\n\n\n\nEtwas eleganter (und deutlich komplizierter) kann man es auch so in R schreiben (Quelle):\n\ndist <- tibble(p_grid = seq(from = 0, to = 1, length.out = 20)) %>%\n  mutate(prior = case_when(\n    p_grid < 0.5 ~ 0L,\n    TRUE ~ 1L),\n    likelihood_1 = dbinom(3, size = 3, prob = p_grid),\n    likelihood_2 = dbinom(3, size = 4, prob = p_grid),\n    likelihood_3 = dbinom(5, size = 7, prob = p_grid),\n    across(starts_with(\"likelihood\"), ~ .x * prior),\n    across(starts_with(\"likelihood\"), ~ .x / sum(.x))) %>%\n  pivot_longer(cols = starts_with(\"likelihood\"), names_to = \"pattern\",\n               values_to = \"posterior\") %>%\n  separate(pattern, c(NA, \"pattern\"), sep = \"_\", convert = TRUE) %>%\n  mutate(obs = case_when(pattern == 1L ~ \"W, W, W\",\n                         pattern == 2L ~ \"W, W, W, L\",\n                         pattern == 3L ~ \"L, W, W, L, W, W, W\"))\n\nggplot(dist, aes(x = p_grid, y = posterior)) +\n  facet_wrap(vars(fct_inorder(obs)), nrow = 1) +\n  geom_line() +\n  geom_point() +\n  labs(x = \"Proportion Water (p)\", y = \"Posterior Density\")\n\n\n\n\n\n\n\n\n\nCategories:\n\nprobability\nbayes-grid"
  },
  {
    "objectID": "posts/nasa01/nasa01.html",
    "href": "posts/nasa01/nasa01.html",
    "title": "nasa01",
    "section": "",
    "text": "Solution\nDekade berechnen:\n\nd <-\n  d %>% \n  mutate(decade = round(Year/10))\n\nStatistiken pro Dekade:\n\nd_summarized <- \n  d %>% \n  group_by(decade) %>% \n  summarise(temp_mean = mean(Jan),\n            temp_sd = sd(Jan))\n\nd_summarized\n\n\n\n\n\n\n\n  \n  \n    \n      decade\n      temp_mean\n      temp_sd\n    \n  \n  \n    188\n‚àí0.20\n0.24\n    189\n‚àí0.44\n0.22\n    190\n‚àí0.27\n0.17\n    191\n‚àí0.40\n0.22\n    192\n‚àí0.28\n0.16\n    193\n‚àí0.13\n0.22\n    194\n0.03\n0.21\n    195\n‚àí0.05\n0.18\n    196\n0.03\n0.15\n    197\n‚àí0.07\n0.17\n    198\n0.21\n0.19\n    199\n0.36\n0.13\n    200\n0.52\n0.19\n    201\n0.64\n0.20\n    202\n0.96\n0.14\n  \n  \n  \n\n\n\n\nZur Veranschaulichung visualisieren wir die Ergebnisse:\n\n\n\n\n\n\n\n\n\n\nCategories:\n~"
  },
  {
    "objectID": "posts/purrr-map01/purrr-map01.html",
    "href": "posts/purrr-map01/purrr-map01.html",
    "title": "purrr-map01",
    "section": "",
    "text": "Exercise\nErstellen Sie einen Tibble mit folgenden Spalten:\n\nBuchstaben A-Z, so dass in der 1. Zeile ‚ÄúA‚Äù steht, in der 2. Zeile ‚ÄúB‚Äù etc.\nBuchstaben a-z, so dass in der 1. Zeile ‚Äúa‚Äù steht, in der 2. Zeile ‚Äúb‚Äù etc.\nBuchstabenkombination der ersten beiden Spalten, so dass in der 1. Zeile ‚ÄúA-a‚Äù steht, in der 2. Zeile ‚ÄúB-b‚Äù etc.\n\n         \n\n\nSolution\nGeht es vielleicht so?\n\nd <-\n  tibble(\n    letter1 = LETTERS,\n    letter2 = letters,\n    letters = paste(letter1, letter2, collapse = \"-\")\n  )\n\nhead(d)\n\n# A tibble: 6 √ó 3\n  letter1 letter2 letters                                                       \n  <chr>   <chr>   <chr>                                                         \n1 A       a       A a-B b-C c-D d-E e-F f-G g-H h-I i-J j-K k-L l-M m-N n-O o-P‚Ä¶\n2 B       b       A a-B b-C c-D d-E e-F f-G g-H h-I i-J j-K k-L l-M m-N n-O o-P‚Ä¶\n3 C       c       A a-B b-C c-D d-E e-F f-G g-H h-I i-J j-K k-L l-M m-N n-O o-P‚Ä¶\n4 D       d       A a-B b-C c-D d-E e-F f-G g-H h-I i-J j-K k-L l-M m-N n-O o-P‚Ä¶\n5 E       e       A a-B b-C c-D d-E e-F f-G g-H h-I i-J j-K k-L l-M m-N n-O o-P‚Ä¶\n6 F       f       A a-B b-C c-D d-E e-F f-G g-H h-I i-J j-K k-L l-M m-N n-O o-P‚Ä¶\n\n\nNein, leider nicht.\nOK, neuer Versuch:\n\nd <-\n  tibble(\n    letter1 = LETTERS,\n    letter2 = letters) %>% \n  unite(\"letters\", c(letter1, letter2), remove = FALSE)\n\n\nhead(d)\n\n# A tibble: 6 √ó 3\n  letters letter1 letter2\n  <chr>   <chr>   <chr>  \n1 A_a     A       a      \n2 B_b     B       b      \n3 C_c     C       c      \n4 D_d     D       d      \n5 E_e     E       e      \n6 F_f     F       f      \n\n\nProbieren wir es mit purrr::map():\n\nd <-\n  tibble(\n    letter1 = LETTERS,\n    letter2 = letters,\n    letters = map2_chr(letter1, letter2, ~ paste(c(.x, .y), collapse =\"-\"))\n  )\n\nhead(d)\n\n# A tibble: 6 √ó 3\n  letter1 letter2 letters\n  <chr>   <chr>   <chr>  \n1 A       a       A-a    \n2 B       b       B-b    \n3 C       c       C-c    \n4 D       d       D-d    \n5 E       e       E-e    \n6 F       f       F-f    \n\n\nInfos zur Funktion paste() findet sich z.B. hier.\n\nCategories:\n\nr\nmap\ntidyverse"
  },
  {
    "objectID": "posts/purrr-map06/purrr-map06.html",
    "href": "posts/purrr-map06/purrr-map06.html",
    "title": "purrr-map06",
    "section": "",
    "text": "Exercise\nErstellen Sie eine Tabelle mit mit folgenden Spalten:\n\nID-Spalte: \\(1,2,..., 10\\)\nEine Spalte mit Namem ds (ds wie Plural von Datensatz), die als geschachtelt (nested) pro Element jeweils einen der folgenden Datens√§tze enth√§lt: mtcars, iris, chickweight, ToothGrowth (alle in R enthalten)\n\nBerechnen Sie eine Spalte, die die Anzahl der Spalten von ds z√§hlt!\n         \n\n\nSolution\nHier sind einige Datens√§tze, in einer Liste zusammengefasst:\n\nds <- list(mtcars = mtcars, iris = iris, chickweight =  ChickWeight, toothgrowth = ToothGrowth)\n\nDaraus erstellen wir eine Tabelle mit Listenspalte f√ºr die Daten:\n\nd <- \n  tibble(id = 1:length(ds),\n         ds = ds)\n\n\nd2 <- \n  d %>% \n  mutate(n_col = map(ds, ncol)) \n\nhead(d2)\n\n# A tibble: 4 √ó 3\n     id ds                   n_col       \n  <int> <named list>         <named list>\n1     1 <df [32 √ó 11]>       <int [1]>   \n2     2 <df [150 √ó 5]>       <int [1]>   \n3     3 <nfnGrpdD [578 √ó 4]> <int [1]>   \n4     4 <df [60 √ó 3]>        <int [1]>   \n\n\nEntnesten wir noch n_col:\n\nd2 %>% \n  unnest(n_col)\n\n# A tibble: 4 √ó 3\n     id ds                   n_col\n  <int> <named list>         <int>\n1     1 <df [32 √ó 11]>          11\n2     2 <df [150 √ó 5]>           5\n3     3 <nfnGrpdD [578 √ó 4]>     4\n4     4 <df [60 √ó 3]>            3\n\n\n\nCategories:\n~"
  },
  {
    "objectID": "posts/corona-blutgruppe/corona-blutgruppe.html",
    "href": "posts/corona-blutgruppe/corona-blutgruppe.html",
    "title": "corona-blutgruppe",
    "section": "",
    "text": "Solution\n\n\n\nDie L√∂sung lautet: unabh√§ngig.\n\\(S\\) und \\(A\\) sind unabh√§ngig: Offenbar ist die Wahrscheinlichkeit eines schweren Verlaufs gleich gro√ü unabh√§ngig davon, ob die Blutgruppe A ist oder nicht. In diesem Fall spricht man von stochastischer Unabh√§ngigkeit.\n\\(Pr(S|A) = Pr(S|A^C) = Pr(S)\\)\n\nCategories:\n\nprobabillity\ndependent"
  },
  {
    "objectID": "posts/mtcars-abhaengig/mtcars-abhaengig.html",
    "href": "posts/mtcars-abhaengig/mtcars-abhaengig.html",
    "title": "mtcars-abhaengig",
    "section": "",
    "text": "Solution\n\nSchauen wir mal in den Datensatz:\n\n\nmtcars %>% \n  select(hp, hp_high, mpg, mpg_high) %>% \n  slice_head(n = 5)\n\n                   hp hp_high  mpg mpg_high\nMazda RX4         110   FALSE 21.0     TRUE\nMazda RX4 Wag     110   FALSE 21.0     TRUE\nDatsun 710         93   FALSE 22.8     TRUE\nHornet 4 Drive    110   FALSE 21.4     TRUE\nHornet Sportabout 175    TRUE 18.7    FALSE\n\n\n\n\n\n\nmtcars %>% \n  #select(hp_high, mpg_high) %>% \n  ggplot() +\n  aes(x = hp_high, fill = mpg_high) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\nHey, sowas von abh√§ngig voneinander, die zwei Variablen, mpg_high und hp_high!\nDer rechte Balken zeigt \\(Pr(\\text{mpg_high}|\\text{ hp_high})\\) und \\(Pr(\\neg \\text{mpg_high}|\\text{hp_high})\\).Der linke Balken zeigt \\(Pr(\\text{mpg_high}|\\neg \\text{hp_high})\\) und \\(Pr(\\neg \\text{mpg_high}|\\neg \\text{hp_high})\\).\n\nBerechnen wir die relevanten Anteile:\n\n\nmtcars %>% \n  #select(hp_high, mpg_high) %>% \n  count(hp_high, mpg_high) %>%  # Anzahl pro Zelle der Kontingenztabelle\n  group_by(hp_high) %>%  # die Anteile pro \"Balken\" s. Diagramm\n  mutate(prop = n / sum(n))\n\n# A tibble: 4 √ó 4\n# Groups:   hp_high [2]\n  hp_high mpg_high     n   prop\n  <lgl>   <lgl>    <int>  <dbl>\n1 FALSE   FALSE        3 0.176 \n2 FALSE   TRUE        14 0.824 \n3 TRUE    FALSE       14 0.933 \n4 TRUE    TRUE         1 0.0667\n\n\nAm besten, Sie f√ºhren den letzten Code Schritt f√ºr Schritt aus und schauen sich jeweils das Ergebnis an, das hilft beim Verstehen.\nAlternativ kann man sich die H√§ufigkeiten auch sch√∂n bequem ausgeben lassen:\n\nlibrary(mosaic)\ntally(mpg_high ~ hp_high, \n      data = mtcars, \n      format = \"proportion\")\n\n        hp_high\nmpg_high       TRUE      FALSE\n   TRUE  0.06666667 0.82352941\n   FALSE 0.93333333 0.17647059\n\n\n\nCategories:\n\nprobability\ndependent"
  },
  {
    "objectID": "posts/mtcars-simple1/mtcars-simple1.html",
    "href": "posts/mtcars-simple1/mtcars-simple1.html",
    "title": "mtcars-simple1",
    "section": "",
    "text": "Solution\nCompute Model:\n\nlm1_freq <- lm(mpg ~ hp + cyl + disp, data = mtcars)\n\nlibrary(rstanarm)\nlm1_bayes <- stan_glm(mpg ~ hp + cyl + disp, data = mtcars, refresh = 0)\n\nGet parameters:\n\nlibrary(easystats)\n\n\nparameters(lm1_freq)\n\nParameter   | Coefficient |   SE |         95% CI | t(28) |      p\n------------------------------------------------------------------\n(Intercept) |       34.18 | 2.59 | [28.88, 39.49] | 13.19 | < .001\nhp          |       -0.01 | 0.01 | [-0.04,  0.02] | -1.00 | 0.325 \ncyl         |       -1.23 | 0.80 | [-2.86,  0.41] | -1.54 | 0.135 \ndisp        |       -0.02 | 0.01 | [-0.04,  0.00] | -1.81 | 0.081 \n\n\n\nparameters(lm1_bayes)\n\nParameter   | Median |         95% CI |     pd | % in ROPE |  Rhat |     ESS |                   Prior\n------------------------------------------------------------------------------------------------------\n(Intercept) |  34.28 | [28.87, 39.60] |   100% |        0% | 1.000 | 2600.00 | Normal (20.09 +- 15.07)\nhp          |  -0.01 | [-0.05,  0.02] | 82.93% |      100% | 1.000 | 2448.00 |   Normal (0.00 +- 0.22)\ncyl         |  -1.25 | [-2.84,  0.34] | 93.75% |     2.55% | 1.000 | 2108.00 |   Normal (0.00 +- 8.44)\ndisp        |  -0.02 | [-0.04,  0.00] | 96.05% |      100% | 1.001 | 2405.00 |   Normal (0.00 +- 0.12)\n\n\nThe coefficient is estimated as about -0.01\n\nCategories:\n\nregresssion\nen\nbayes\nfrequentist\nqm1\nstats-nutshell"
  },
  {
    "objectID": "posts/log-y-regr3/log-y-regr3.html",
    "href": "posts/log-y-regr3/log-y-regr3.html",
    "title": "log-y-regr3",
    "section": "",
    "text": "library(tidyverse)\nlibrary(easystats)\n\nIn dieser Aufgabe modellieren wir den (kausalen) Effekt von Schulbildung auf das Einkommen.\nImportieren Sie zun√§chst den Datensatz und verschaffen Sie sich einen √úberblick.\n\nd_path <- \"https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Treatment.csv\"\n\nd <- data_read(d_path)\n\nDokumentation und Quellenangaben zum Datensatz finden sich hier.\n\nglimpse(d)\n\nRows: 2,675\nColumns: 11\n$ V1      <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,‚Ä¶\n$ treat   <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR‚Ä¶\n$ age     <int> 37, 30, 27, 33, 22, 23, 32, 22, 19, 21, 18, 27, 17, 19, 27, 23‚Ä¶\n$ educ    <int> 11, 12, 11, 8, 9, 12, 11, 16, 9, 13, 8, 10, 7, 10, 13, 10, 12,‚Ä¶\n$ ethn    <chr> \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\",‚Ä¶\n$ married <lgl> TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, ‚Ä¶\n$ re74    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ re75    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ re78    <dbl> 9930.05, 24909.50, 7506.15, 289.79, 4056.49, 0.00, 8472.16, 21‚Ä¶\n$ u74     <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR‚Ä¶\n$ u75     <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR‚Ä¶\n\n\nWelcher der Pr√§diktoren hat den st√§rkesten Einfluss auf das Einkommen?\nHinweise:\n\nVerwenden Sie lm zur Modellierung.\nOperationalisieren Sie das Einkommen mit der Variable re74.\nGehen Sie von einem kausalen Effekt der Pr√§diktoren aus.\nGehen Sie von einem multiplikativen Modell aus (log-y).\nLassen Sie die Variablen zur Arbeitslosigkeit au√üen vor.\n\n\n\n\ntreat\nage\neduc\nethn\nmarried"
  },
  {
    "objectID": "posts/log-y-regr3/log-y-regr3.html#answerlist-1",
    "href": "posts/log-y-regr3/log-y-regr3.html#answerlist-1",
    "title": "log-y-regr3",
    "section": "Answerlist",
    "text": "Answerlist\n\nTRUE\nFALSE\nFALSE\nFALSE\nFALSE\n\n\nCategories:\n\nstats-nutshell\nqm2\nregression\nlog"
  },
  {
    "objectID": "posts/twitter01/twitter01.html",
    "href": "posts/twitter01/twitter01.html",
    "title": "twitter01",
    "section": "",
    "text": "Solution\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.2 ‚îÄ‚îÄ\n‚úî ggplot2 3.3.6      ‚úî purrr   0.3.5 \n‚úî tibble  3.1.8      ‚úî dplyr   1.0.10\n‚úî tidyr   1.2.1      ‚úî stringr 1.4.1 \n‚úî readr   2.1.3      ‚úî forcats 0.5.2 \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n\nlibrary(rtweet)\n\n\nAttaching package: 'rtweet'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\n\nEinloggen bei Twitter; zuerst die Credentials bereithalten:\n\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\n\nDann anmelden, z.B. als Bot:\n\nauth <- rtweet_bot(api_key = API_Key,\n                   api_secret = API_Key_Secret,\n                   access_token = Access_Token,\n                   access_secret = Access_Token_Secret)\n\n‚Ä¶ Oder als App, das bringt bessere Raten mit sich:\n\nauth <- rtweet_app(bearer_token = Bearer_Token)\n\nTest:\n\nsesa_test <- get_timeline(user = \"sauer_sebastian\", n = 3) %>% \n  select(full_text)\n\nsesa_test\n\n1 RT @fuecks: By the way: Systematic destruction of life-sustaining infrastructures ‚Ä¶\n2 RT @NoContextBrits: No shortbread for little Nazis. https://t.co/F6FUPvRz94        \n3 RT @ernst_gennat: 2 oder 3 Jahre #Tempolimit von 120 km/h. Abschlie√üend Evaluation‚Ä¶\nTweets an Karl Lauterbach suchen:\n\nkarl1 <- search_tweets(\"@karl_lauterbach\")\n\nIn Ausz√ºgen:\n\"@Karl_Lauterbach Ein Minister der alle paar Stunden Zeit hat einen Mist zu verbreiten....\"   \n\"@Karl_Lauterbach @focusonline Long Covid ist nichts anderes als schwere Nebenwirkungen der Gentherapie!\"  \"@Karl_Lauterbach @focusonline Wer sch√ºtzt uns vor Long Lauterbach?\"\n\"@Karl_Lauterbach Also Karl, prim√§r fordere ich und viele andere eher erstmal dein sofortigen R√ºcktritt.\"  \"@Karl_Lauterbach Behalt deinen Senf f√ºr dich!\"                                                            \"@Karl_Lauterbach Oh Gott üò±\"     \n\"@Karl_Lauterbach Ach nein, der Clown mit Lebensangst ‚Ä¶.\\n\\nhttps://t.co/8cQZeHh6Ew\"                       \"@Karl_Lauterbach Ich kenne nur Leute mit Long Covid, die mehrfach geimpft sind! Das ist kein Witz! Scheinbar liegt‚Äôs wohl doch an den Spritzen???\"                                                            \"@Karl_Lauterbach @focusonline Interessiert keine Sau üòâ\"                      \n\"RT @Karl_Lauterbach @focusonline ‚ÄûLauterbachs Aussagen k√∂nnen fundamental nicht stimmen‚Äú\\nhttps://t.co/rfxnWAWiZX\"                                                                          \"@Karl_Lauterbach @focusonline ü§°üòÇüòÇüòÇüòÇüòÇüòÇ\"                                         \n\"@Karl_Lauterbach Jau und sie sind kein f√§higer Gesundheitsminister, sondern lediglich ein gekaufter Coronaminister\"        \nPuh, viele toxische Tweets, wie es scheint.\nUnd ohne Retweets (RT) und ohne Replies:\n\nkarl2 <- search_tweets(\"@karl_lauterbach\", \n  include_rts = FALSE, `-filter` = \"replies\")\n\nTweets, die an Karl Lauterbach gerichtet sind, per API-Anweisung:\n\nkarl3 <- search_tweets(\"to:karl_lauterbach\", n = 100)\n\n\"@Karl_Lauterbach Vielen Dank, dass LongCovid ein gefundenes fressen f√ºr die jenigen ist, die nicht mehr Arbeiten wollen.\"       \n \"@Karl_Lauterbach verpiss dich einfach! Immer dieser Schwachsinn\"    \n\"@Karl_Lauterbach @focusonline Das sind genau die Impfnebenwirkungen! Will man nun das wenden um die Impfnebenwirkungen zu vertuschen? \\nWof√ºr ist die Impfung gut wenn nicht mal Long-Covid verhindert wird, die Ansteckung konnte sie noch nie verhindern!\\nWarum sind 89% Covid Patienten geimpfte in den Spit√§ler?\"\n\"@Karl_Lauterbach Was spielen Sie eigentlich f√ºr ein schmutziges Spiel?\\n\\nhttps://t.co/8LJIzxyF7G\"   \n \"@Karl_Lauterbach @focusonline Bessen von Covid! St√§ndig wird das Netz durchsucht, nach Artikeln,die instrumentalisiert werden, um f√ºr Impfung zu werben. Was h√§tte nur ein vern√ºnftiger Gesundheitsminister mit so viel Zeit Vern√ºnftiges im Gesundheitswesen auf die Beine stellen k√∂nnen...\"    \n\"@Karl_Lauterbach Mit Dauerschaden wegen der Impfung üíâ bin ich Arbeitslos geworden in der Pflege ü§∑‚Äç‚ôÇÔ∏è Ist das normal Herr @Karl_Lauterbach ?\"          \nOb man mit @karl_lauterbach sucht oder `to:karl_lauterbach‚Äù, scheint keinen gro√üen Unterschied zu machen (?).\n\nCategories:\n\ntextmining\n‚Äò2022‚Äô"
  },
  {
    "objectID": "posts/log-y-regr2/log-y-regr2.html",
    "href": "posts/log-y-regr2/log-y-regr2.html",
    "title": "log-y-regr2",
    "section": "",
    "text": "Solution\n\nd2 <-\n  d %>% \n  filter(re74 > 0) %>% \n  mutate(re74_log = log(re74))\n\n\nm <- lm(re74_log ~ educ, data = d2)\n\n\nggplot(d2) +\n  aes(x = re74) +\n  geom_density() +\n  labs(title = \"Income raw\")\n\n\nggplot(d2) +\n  aes(x = re74_log) +\n  geom_density() +\n  labs(title = \"Income log transformed\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBetrachten wir die deskriptiven Statistiken:\n\nd2 %>% \n  select(re74, re74_log) %>% \n  describe_distribution()\n\nVariable |     Mean |       SD |      IQR |             Range | Skewness | Kurtosis |    n | n_Missing\n------------------------------------------------------------------------------------------------------\nre74     | 20938.28 | 12631.52 | 15086.30 | [17.63, 1.37e+05] |     1.62 |     6.81 | 2329 |         0\nre74_log |     9.73 |     0.76 |     0.80 |     [2.87, 11.83] |    -1.67 |     6.01 | 2329 |         0\n\n\nDie Log-Transformation hat in diesem Fall nicht wirklich zu einer Normalisierung der Variablen beigetragen. Aber das war auch nicht unser Ziel.\n\nCategories:\n\nregression\nlm\nqm2\nstats-nutshell"
  },
  {
    "objectID": "posts/adjustieren1/adjustieren1.html",
    "href": "posts/adjustieren1/adjustieren1.html",
    "title": "adjustieren1",
    "section": "",
    "text": "Solution\n\nlibrary(rstanarm)\nlm2 <- stan_glm(mpg ~ hp_z + am, data = mtcars,\n                refresh = 0)\nsummary(lm2)\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 26.6    1.5 24.7  26.6  28.5 \nhp          -0.1    0.0 -0.1  -0.1   0.0 \nam           5.3    1.1  3.8   5.3   6.6 \nsigma        3.0    0.4  2.5   3.0   3.5 \nDie Spalte mean gibt den mittleren gesch√§tzten Wert f√ºr den jeweiligen Koeffizienten an, also den Sch√§tzwert zum Koeffizienten.\nDie Koeffizienten zeigen, dass der Achsenabschnitt f√ºr Autos mit Automatikgetriebe um etwa 5 Meilen geringer ist als f√ºr Autos mit manueller Schaltung: Ein durchschnittliches Auto mit manueller Schaltung kommt also etwa 5 Meilen weiter als ein Auto mit Automatikschaltung, glaubt unser Modell.\n\nmtcars %>% \n  mutate(am = factor(am)) %>% \n  ggplot() +\n  aes(x = hp_z, y = mpg, color = am) +\n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\n\n\nMan k√∂nnte hier noch einen Interaktionseffekt erg√§nzen.\n\nCategories:\n\nqm2\nqm2-thema01\nws22"
  },
  {
    "objectID": "posts/euro-bayes/euro-bayes.html",
    "href": "posts/euro-bayes/euro-bayes.html",
    "title": "euro-bayes",
    "section": "",
    "text": "Solution\n\n\n\n\n\n\n\n\n\n\n\np_Gitter\nPriori\nLikelihood\nunstd_Post\nPost\n\n\n\n\n0.0\n1\n0.00\n0.00\n0.00\n\n\n0.1\n1\n0.00\n0.00\n0.00\n\n\n0.2\n1\n0.00\n0.00\n0.00\n\n\n0.3\n1\n0.00\n0.00\n0.00\n\n\n0.4\n1\n0.00\n0.00\n0.00\n\n\n0.5\n1\n0.01\n0.01\n0.27\n\n\n0.6\n1\n0.02\n0.02\n0.73\n\n\n0.7\n1\n0.00\n0.00\n0.00\n\n\n0.8\n1\n0.00\n0.00\n0.00\n\n\n0.9\n1\n0.00\n0.00\n0.00\n\n\n1.0\n1\n0.00\n0.00\n0.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDie Wahrscheinlichkeit \\(Pr(\\pi = 1/2 \\, | \\, X=140)\\) wenn \\(X \\sim Bin(250, 1/2)\\) betr√§gt ca. 27% oder .27.\nAllerdings w√ºrden viele Statistiker:innen nicht (nur) fragen, wie wahrscheinlich 140 Treffer sind. Stattdessen k√∂nnte man von folgender √úberlegung ausgehen.\nZuerst: Welcher Wert w√§re am wahrscheinlichsten, wenn die M√ºnze fair w√§re?\n\n\n[1] 126\n\n\nDer 126. Wert in der Liste 0:250 ist der wahrscheinlichste (also 125 Treffer).\nWenn die M√ºnze fair ist, dann w√§ren doch 15 Treffer mehr als 125 genauso so unwahrscheinlich wie 15 Treffer weniger als 125 Treffer. Beide Ereignisse - 110 und 140 Treffer - sind ja gleich weit entfernt von denjenigen Wert, der am wahrscheinlichsten ist, wenn die M√ºnze fair ist.\nEi typischi Statistiki w√ºrde also eher fragen: ‚ÄúWie wahrscheinlich ist es, dass man ein Ergebnis erh√§lt, dass mind. 15 Treffer entfernt ist von der Trefferzahl, die bei einer fairen M√ºnze zu erwarten ist?‚Äù. Aber genug davon f√ºr diese Aufgabe :-)\n\nCategories:\n\nprobability\nbayes-grid"
  },
  {
    "objectID": "posts/kekse01/kekse01.html",
    "href": "posts/kekse01/kekse01.html",
    "title": "kekse01",
    "section": "",
    "text": "Solution\n\n\n\n\n\n\n\n\n\n\n\np_Gitter\nPriori\nLikelihood\nunstd_Post\nPost\n\n\n\n\n1\n1\n0.75\n0.75\n0.6\n\n\n2\n1\n0.50\n0.50\n0.4\n\n\n\n\n\nDie Antwort lautet: .6\n\nCategories:\n\nprobability\nbayes-grid"
  },
  {
    "objectID": "posts/interpret-koeff/interpret-koeff.html",
    "href": "posts/interpret-koeff/interpret-koeff.html",
    "title": "interpret-koeff",
    "section": "",
    "text": "Solution\n\nIntercept (\\(\\beta_0\\)): Der Achsenabschnitt gibt den gesch√§tzten mittleren Y-Wert (Spritverbrauch) an, wenn \\(x=0\\), also f√ºr ein Auto mit 0 PS (was nicht wirklich Sinn macht). hp (\\(\\beta_1\\)) ist der Regressionskoeffizient oder Regressionsgewicht und damit die Steigung der Regressionsgeraden.\nhp (\\(\\beta_1\\)) ist der Regressionskoeffizient oder Regressionsgewicht und gibt den statistischen ‚ÄúEffekt‚Äù der PS-Zahl auf den Spritverbrauch an. Vorsicht: Dieser ‚ÄúEffekt‚Äù darf nicht vorschnell als kausaler Effekt verstanden werden. Daher muss man vorsichtig sein, wenn man von einem ‚ÄúEffekt‚Äù spricht. Vorsichtiger w√§re zu sagen: ‚ÄúEin Auto mit einem PS mehr, kommt im Mittel 0,1 Meilen weniger weit mit einer Gallone Sprit, laut diesem Modell‚Äù.\n\n\nCategories:\n\nregression\nlm\nbayes\nstats-nutshell"
  },
  {
    "objectID": "posts/ungewiss-arten-regr/ungewiss-arten-regr.html",
    "href": "posts/ungewiss-arten-regr/ungewiss-arten-regr.html",
    "title": "ungewiss-arten-regr",
    "section": "",
    "text": "Solution\n\n\n\n\n7.04\n0.04\n\n\nCategories:\n\nqm2\ninference\nlm"
  },
  {
    "objectID": "posts/vorhersageintervall1/vorhersageintervall1.html",
    "href": "posts/vorhersageintervall1/vorhersageintervall1.html",
    "title": "vorhersageintervall1",
    "section": "",
    "text": "Vorhersagen, etwa in einem Regressionsmodell, sind mit mehreren Arten von Unsicherheit konfrontiert.\nBerechnen Sie dazu ein Regressionsmodell, Datensatz mtcars, mit hp als Pr√§diktor (UV) und mpg als AV (Kriterium)!\nDann sagen Sie bitte den Wert der AV f√ºr eine Beobachtungseinheit mit mittlerer Auspr√§gung im Pr√§ktor vorher:\nEinmal nur unter Ber√ºcksichtigung der Unsicherheit innerhalb des Modells (‚ÄúKonfidenzintervall‚Äù); einmal unter Ber√ºcksichtigung der Unsicherheit innerhalb des Modells sowie die Unsicherheit durch die Koffizienten (‚ÄúVohersageintervall‚Äù).\nHinweise:\n\npredict() ist eine Funktion, die Sie zur Vorhersage von Regressionsmodellen verwenden k√∂nnen.\nVerwenden Sie lm() zur Berechnung eines Regressionsmodells.\nDas Argument type von predict() erlaubt Ihnen die Wahl der Art der Vorhersage, betrachten Sie Hilfe der Funktion z.B. hier.\n\nBei welchem Intervall ist die Ungewissheit in der Vorhersage gr√∂√üer?\n\n\n\nKonfidenzintervall\nVohersageintervall\nGleich gro√ü\nKommt auf weitere Faktoren an, keine pauschale Antwort m√∂glich"
  },
  {
    "objectID": "posts/vorhersageintervall1/vorhersageintervall1.html#answerlist-1",
    "href": "posts/vorhersageintervall1/vorhersageintervall1.html#answerlist-1",
    "title": "vorhersageintervall1",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\nFalsch\nFalsch\n\n\nCategories:\n\nlm\ninference\nqm2"
  },
  {
    "objectID": "posts/voll-normal/voll-normal.html",
    "href": "posts/voll-normal/voll-normal.html",
    "title": "voll-normal",
    "section": "",
    "text": "Solution\n\nIntelligenz, Aussehen, Gesundheit, Herkunft, Hautfarbe, sexuelle Identit√§t oder Neigung, ‚Ä¶\nF√ºr unabh√§ngige Ereignisse ist die Wahrscheinlichkeit, dass sie alle eintreten, gleich dem Produkt ihrer Einzelwahrscheinlichkeiten:\n\n\\(VN = Pr(E_i)^{10} = 0.9^{10} \\approx 0.3486784\\)\nDie Wahrscheinlichkeit, dass \\(VN\\) nicht eintritt (Nicht-Voll-Normal, NVN), ist dann die Gegenwahrscheinlichkeit: \\(NVN = 1- VN\\).\n\nMehrere der Annahmen sind diskutabel. So k√∂nnten die Eigenschaften nicht unabh√§ngig sein, dann w√§re der hier gezeigte Rechenweg nicht anwendbar. Die Wahrscheinlichkeit f√ºr ‚Äúnormal‚Äù k√∂nnte h√∂her oder niedriger sein, wobei 90% nicht ganz unplausibel ist. Schlie√ülich unterliegt das Ereignis \\(E_N\\) mit den Ergebnissen \\(n\\) bzw. \\(nn\\) sozialpsychologischen bzw. soziologischen Einfl√ºssen und kann variieren.\n\n\nCategories:\n\nprobability\nmeta"
  },
  {
    "objectID": "posts/lm-Standardfehler/lm-Standardfehler.html",
    "href": "posts/lm-Standardfehler/lm-Standardfehler.html",
    "title": "lm-Standardfehler",
    "section": "",
    "text": "Man kann angeben, wie genau eine Sch√§tzung von Regressionskoeffizienten die Grundgesamtheit widerspiegelt. Zumeist wird dazu der Standardfehler (engl. standard error, SE) verwendet.\nIn dieser √úbung untersuchen wir, wie sich der SE als Funktion der Stichprobengr√∂√üe, \\(n\\), verh√§lt.\nErstellen Sie dazu folgenden Datensatz:\n\nlibrary(tidyverse)\n\nn <- 2^4\n\nd <-\n  tibble(x = rnorm(n = n),  # im Default: mean = 0, sd = 1\n         y = x + rnorm(n, mean = 0, sd = .5))\n\nHier ist das Ergebnis. Uns interessiert v.a. Std. Error f√ºr den Pr√§diktor x:\n\nlm(y ~ x, data = d) %>% \nsummary()\n\n\nCall:\nlm(formula = y ~ x, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.60191 -0.42922  0.09198  0.32313  0.59878 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.1226     0.1061  -1.156    0.267    \nx             1.0385     0.0888  11.694  1.3e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4223 on 14 degrees of freedom\nMultiple R-squared:  0.9071,    Adjusted R-squared:  0.9005 \nF-statistic: 136.8 on 1 and 14 DF,  p-value: 1.302e-08\n\n\nHier haben wir eine Tabelle mit zwei Variablen, x und y, definiert mit n=16.\nVerdoppeln Sie die Stichprobengr√∂√üe 5 Mal und betrachten Sie, wie sich die Sch√§tzgenauigkeit, gemessen √ºber den SE, ver√§ndert. Berechnen Sie dazu f√ºr jedes n eine Regression mit x als Pr√§diktor und y als AV!\nBei welcher Stichprobengr√∂√üe ist SE am kleinsten?\n\n\n\n\\(2^5\\)\n\\(2^6\\)\n\\(2^7\\)\n\\(2^8\\)\n\\(2^9\\)"
  },
  {
    "objectID": "posts/lm-Standardfehler/lm-Standardfehler.html#answerlist-1",
    "href": "posts/lm-Standardfehler/lm-Standardfehler.html#answerlist-1",
    "title": "lm-Standardfehler",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nFalsch\nWahr. Die gr√∂√üte Stichprobe impliziert den kleinsten SE, ceteris paribus.\n\n\nCategories:\n\ninference\nlm\nqm2"
  },
  {
    "objectID": "posts/punktschaetzer-reicht-nicht/punktschaetzer-reicht-nicht.html",
    "href": "posts/punktschaetzer-reicht-nicht/punktschaetzer-reicht-nicht.html",
    "title": "punktschaetzer-reicht-nicht",
    "section": "",
    "text": "Solution\nModell m1 hat eine kleinere Ungewissheit im Hinblick auf die Modellkoeffizienten \\(\\beta_0, \\beta_1\\) und ist daher gegen√ºber m2 zu bevorzugen.\n\nCategories:\n\nqm2\nqm2-thema01\nws22"
  },
  {
    "objectID": "posts/Lose-Nieten-Binomial-Grid/Lose-Nieten-Binomial-Grid.html",
    "href": "posts/Lose-Nieten-Binomial-Grid/Lose-Nieten-Binomial-Grid.html",
    "title": "Lose-Nieten-Binomial-Grid",
    "section": "",
    "text": "Solution\n\nWie gro√ü ist die Wahrscheinlichkeit f√ºr genau \\(k=0,1,...,10\\) Treffer?\n\n\nd_a <- \n  tibble(\n    k = 0:10,\n    wskt = dbinom(k, size = 10, prob = .01))\n\nd_a %>% \n  ggplot() +\n  aes(x = k, y = wskt) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = 1:10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n      k\n      wskt\n    \n  \n  \n    0\n9.04 √ó 10‚àí1\n    1\n9.14 √ó 10‚àí2\n    2\n4.15 √ó 10‚àí3\n    3\n1.12 √ó 10‚àí4\n    4\n1.98 √ó 10‚àí6\n    5\n2.40 √ó 10‚àí8\n    6\n2.02 √ó 10‚àí10\n    7\n1.16 √ó 10‚àí12\n    8\n4.41 √ó 10‚àí15\n    9\n9.90 √ó 10‚àí18\n    10\n1.00 √ó 10‚àí20\n  \n  \n  \n\n\n\n\n\nSagen wir, Sie haben 3 Treffer in den 10 Losen. Yeah! Jetzt sei \\(p\\) unbekannt und Sie sind indifferent zu den einzelnen Werten von \\(p\\). Visualisieren Sie die Posteriori-Wahrscheinlichkeitsverteilung mit ca. 100 Gridwerten. Was beobachten Sie?\n\n\nd2 <-\n  tibble(\n    p_grid = seq(0, 1, by = 0.01),\n    prior = 1,\n    Likelihood = dbinom(x = 3, size = 10, prob = p_grid),\n    unstand_post = prior * Likelihood,\n    std_post = unstand_post / sum(unstand_post)\n  )\n\nd2 %>% \n  ggplot() +\n  aes(x = p_grid, y = std_post) +\n  geom_point() +\n  geom_line()\n\n\n\n\n\n\n\n\nDer Modus liegt bei ca 1/3. Der Bereich plausibler Werte f√ºr \\(p\\) liegt ca. zwischen 0.1 und und 0.7, grob visuell gesch√§tzt. Mehr dazu sp√§ter.\n\nVariieren Sie \\(n\\), aber halten Sie die Trefferquote bei 1/3. Was beobachten Sie?\n\n\n# n = 2\nd3 <-\n  tibble(\n    p_grid = seq(0,1, by = 0.01),\n    prior = 1,\n    Likelihood = dbinom(x = 2, size = 6, prob = p_grid),\n    unstand_post = prior * Likelihood,\n    std_post = unstand_post / sum(unstand_post)\n  )\n\nd3 %>% \n  ggplot() +\n  aes(x = p_grid, y = std_post) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"n=20\")\n\n\n# n = 20\nd4 <-\n  tibble(\n    p_grid = seq(0,1, by = 0.01),\n    prior = 1,\n    Likelihood = dbinom(x = 20, size = 60, prob = p_grid),\n    unstand_post = prior * Likelihood,\n    std_post = unstand_post / sum(unstand_post)\n  )\n\nd4 %>% \n  ggplot() +\n  aes(x = p_grid, y = std_post) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"n = 20\")\n\n# n = 200\nd5 <-\n  tibble(\n    p_grid = seq(0,1, by = 0.01),\n    prior = 1,\n    Likelihood = dbinom(x = 200, size = 600, prob = p_grid),\n    unstand_post = prior * Likelihood,\n    std_post = unstand_post / sum(unstand_post)\n  )\n\nd5 %>% \n  ggplot() +\n  aes(x = p_grid, y = std_post) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"n = 20\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDer Modus und andere Ma√üe der zentralen Tendenz bleiben gleich; die Streuung wird geringer.\n\nCategories:\n\nprobability\nbinomial"
  },
  {
    "objectID": "posts/Gem-Wskt1/Gem-Wskt1.html",
    "href": "posts/Gem-Wskt1/Gem-Wskt1.html",
    "title": "Gem-Wskt1",
    "section": "",
    "text": "Solution\nDie gemeinsame Wahrscheinlichkeit betr√§gt 0.07.\n\n\n\n\n\n\n  \n  \n    \n      Lerntyp\n      Klausurergebnis\n      n\n      n_group\n      prop_conditional_group\n      joint_prob\n    \n  \n  \n    Viellerner\nDurchfallen\n5\n42\n0.1190476\n0.07142857\n  \n  \n  \n\n\n\n\nDie gemeinsame Wahrscheinlichkeit berechnet sich hier als der Quotient der Zellenh√§ufigkeit und der Gesamth√§ufigkeit.\nMan kann auch die Formel f√ºr gemeinsame Wahrscheinlichkeiten anwenden: \\(Pr(A) \\cdot \\Pr(B)\\).\nDazu berechnet man die Anteile f√ºr jedes der beiden Ereignisse und multipliziert diese beiden Anteile:\n`Klausurergebnis_selected * Lerntyp_selected*\n\nCategories:\n\nprobability\n‚Äò2022‚Äô"
  },
  {
    "objectID": "posts/Rethink_2E4/Rethink_2E4.html",
    "href": "posts/Rethink_2E4/Rethink_2E4.html",
    "title": "Rethink_2E4",
    "section": "",
    "text": "Solution\nThe solution is taken from this source.\nThe idea is that probability is only a subjective perception of the likelihood that something will happen. In the globe tossing example, the result will always be either ‚Äúland‚Äù or ‚Äúwater‚Äù (i.e., 0 or 1). When we toss the globe, we don‚Äôt know what the result will be, but we know it will always be ‚Äúland‚Äù or ‚Äúwater.‚Äù To express our uncertainty in the outcome, we use probability. Because we know that water is more likely than land, we may say that the probability of ‚Äúwater‚Äù is 0.7; however, we‚Äôll never actually observe a result of 0.7 waters, or observe any probability. We will only ever observe the two results of ‚Äúland‚Äù and ‚Äúwater.‚Äù\n\nCategories:\n\nprobability\nphilosophy"
  },
  {
    "objectID": "posts/nasa03/nasa03.html",
    "href": "posts/nasa03/nasa03.html",
    "title": "nasa03",
    "section": "",
    "text": "Solution\ntemp_is_above erstellen:\n\nd <-\n  d %>% \n  mutate(temp_is_above = case_when(\n    Jan > 0 ~ \"yes\",\n    Jan <= 0 ~ \"no\"\n  ))\n\nJahrhundert berechnen:\n\nd <-\n  d %>% \n  mutate(century = case_when(\n    Year < 1900 ~ \"19th\",\n    Year >= 1900 ~ \"20th\"\n  ))\n\nErh√∂hte Werte der Januar-Temperatur pro Jahrhundert berechnen:\n\nd_summarized <- \nd %>% \n  group_by(century) %>% \n  count(temp_is_above)\n\nd_summarized\n\n# A tibble: 4 √ó 3\n# Groups:   century [2]\n  century temp_is_above     n\n  <chr>   <chr>         <int>\n1 19th    no               19\n2 19th    yes               1\n3 20th    no               56\n4 20th    yes              67\n\n\nDer Befehl count() z√§hlt aus, wie h√§ufig die Auspr√§gungen der angegebenen Variablen X sind, m.a.W. er gibt die Verteilung von X wieder.\nEs macht vermutlich Sinn, noch die Anteile (relative H√§ufigkeiten) zu den absoluten H√§ufigkeiten zu erg√§nzen:\n\nd_summarized %>% \n  mutate(prop = n / sum(n))\n\n# A tibble: 4 √ó 4\n# Groups:   century [2]\n  century temp_is_above     n  prop\n  <chr>   <chr>         <int> <dbl>\n1 19th    no               19 0.95 \n2 19th    yes               1 0.05 \n3 20th    no               56 0.455\n4 20th    yes              67 0.545\n\n\nOdds Ratio berechnen:\nWir bezeichnen mit c19 (f√ºr ‚ÄúChance 1‚Äù) das Verh√§ltnis von erh√∂hter Temperatur zu nicht erh√∂hter Temperatur im 19. Jahrhundert.\n\nc19 <- 1 / 19\n\nMit c20 bezeichnen wir die analoge Chance f√ºr das 20. Jahrhundert:\n\nc20 <- 56 / 67\n\nDas Verh√§ltnis der beiden Chancen gibt das Chancenverh√§ltnis (Odds Ratio, OR):\n\nc19 / c20\n\n[1] 0.06296992\n\n\nGenauso gut kann man das OR von c20 zu c19 ausrechnen, der Effekt bleibt identisch:\n\nc20 / c19\n\n[1] 15.8806\n\n\nIn beiden F√§llen ist es ein Faktor von knapp 16.\n\nCategories:\n~"
  },
  {
    "objectID": "posts/Rethink_2m7/Rethink_2m7.html",
    "href": "posts/Rethink_2m7/Rethink_2m7.html",
    "title": "Rethink_2m7",
    "section": "",
    "text": "Solution\nLet‚Äôs label the cards bb (black on both sides), bw (black on one, white on the other), and ww (both sides are white), respectively.\nWanted is the probability \\(Pr(c1=bb|1b)\\), the probability of drawing (as card 1) a bb card, given that we observerd b in the first draw, denoted as 1b.\nHere, we have to consider two cards. Let‚Äôs use this notation ww-bb for the sequence ‚Äúfirst card is white on both sides, second card is black on both sides‚Äù.\nThe data observed is: first card has one black side, the second card has one white side, i.,e b-w.\nConsider the following simple ‚Äútree diagram‚Äù showing the paths according to the four hypothesis, first level denotes the first card, the second (more indented) denotes the second card:\nHyp. A: bb-ww:\n\nb\n\nw\nw\n\nb\n\nw\nw\n\n\nThere are 4 paths consistend with the data, b-w.\nHyp. B: bw-ww:\n\nb\n\nw\nw\n\nw\n\nw\nw\n\n\nThere are 2 paths consistent with the data,* b-w*.\nHyp. A: bb-bw:\n\nb\n\nb\nw\n\nb\n\nb\nw\n\n\nThere are 2 paths consistent with the data.+, b-w.\nLooking at the tree, we realize that out of all 8 allowed paths, 6 feature the bb card as first card:\n\\(Pr(c1=bb|b-w) = 6/8 = 3/4 = 0.75\\)\nwhere c1 means ‚Äúcard 1‚Äù, and b-w means ‚Äúfirst draw showed a b side, and second card showed a w face‚Äù.\nAs for an other approach, consider the solution taken from this source.\n\n# 2 choices for first card (either bw or bb), with 3 options for second card: 2 ww + 1 bw\ncard_bb_likelihood <- 2 * 3 \ncard_wb_likelihood <- 1 * 2 \ncard_ww_likelihood <- 0\n\nlikelihood <- c(card_bb_likelihood, card_wb_likelihood, card_ww_likelihood)\nprior <- c(1,1,1)\nposterior <- prior * likelihood\nposterior <- posterior / sum(posterior)\n\nposterior[1]\n\n[1] 0.75\n\n\n\nCategories:\n\nprobability\nbayes"
  },
  {
    "objectID": "posts/Bsp-Binomial/Bsp-Binomial.html",
    "href": "posts/Bsp-Binomial/Bsp-Binomial.html",
    "title": "Bsp-Binomial",
    "section": "",
    "text": "Solution\nZur Erinnerung: Die Inferenzstatistik macht Aussagen bzgl. einer Population, nicht einer Stichprobe. Solche Aussagen sind ungewiss, also mit einer Unsicherheit behaftet, da wir nicht die ganze Population kennen. Aber die Daten der Stichprobe werden als Grundlage der Sch√§tzung herangezogen.\n\nAuswahl geeigneter Kandidatis in einem Assessment-Verfahren. Man hat \\(n=40\\) Bewerbis, und die Wahrscheinlichkeit geeigneter Kandidatis liege bei \\(p=10%\\). Welche Spannweite an geeigneten Bewerbis kann man erwarten?\nSocial Influencing. Sie posten 100 Videoclips; davon werden 9 viral. Welche Spannweite plausibler Werte f√ºr eine Erfolgsquote kann man zugrunde legen?\nApp-Wartung. Sie pr√ºfen eine Anzahl (\\(n=42\\)) alter Apps, aus einer fr√ºheren Kampagne. Sie finden, dass \\(k=19\\) noch funktionieren. Welche Quote an ‚Äútechnisch veraltet‚Äù muss man in der Population erwarten, und in welchem Bereich k√∂nnte sich diese Quote bewegen?\nSchulungsprogramm. Sie entwickeln ein Schulungsprogramm, das im gro√üen Stil in einer Firma eingesetzt werden soll; mehrere Tausend Personen sollen das Programm durchlaufen. In einer Pilotstudie mit \\(n=90\\) Personen erreichen \\(k=42\\) nicht das Lernziel. Welche Parameterwerte f√ºr \\(p\\) (Lernziel erreicht) sind plausibel?\n\n\nCategories:\n\nprobability\nbinomial\nexample"
  },
  {
    "objectID": "posts/randomdag1/randomdag1.html",
    "href": "posts/randomdag1/randomdag1.html",
    "title": "randomdag1",
    "section": "",
    "text": "Gegeben sei der DAG g (s. u.). Der DAG verf√ºgt √ºber \\(n = 6\\) Variablen, die als Knoten im Graph dargestellt sind und mit \\(x_1, x_2, \\ldots x_n\\) bezeichnet sind.\nWelche minimale Variablenmenge muss kontrolliert werden, um den kausalen Effekt von der UV zur AV zu identifizieren?\nUV: x2.\nAV: x6.\nHinweise:\n\nMengen sind mittels geschweifter Klammern gekennzeichnet, z.B. {x1, x2} meint die Menge mit den zwei Elementen x1 und x2.\nDie leere Menge { } bedeutet, dass keine Variable kontrolliert werden muss, um den kausalen Effekt zu identifizieren.\nAlle Variablen werden als gemessen vorausgesetzt.\nEs ist m√∂glich, dass es keine L√∂sung gibt, dass es also keine Adjustierungsmenge gibt, um den kausalen Effekt zu identifizieren. Wenn dies der Fall sein sollte, w√§hlen Sie ‚Äú/‚Äù.\nEs ist m√∂glich, dass einzelne Variablen keine Kanten besitzen, also keine Verbindung zu anderen Variablen (Knoten) haben. Diese Variablen sind dann kausal unabh√§ngig von den √ºbrigen Variablen.\n\n\n\n\n\n\n\n\n\n\n\n\n\n{ x1, x2 }\n{ x3, x6 }\n{ x1, x6 }\n{ x2, x3 }\n{ }"
  },
  {
    "objectID": "posts/randomdag1/randomdag1.html#answerlist-1",
    "href": "posts/randomdag1/randomdag1.html#answerlist-1",
    "title": "randomdag1",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nFalsch\nRichtig\n\n\nCategories:\n\ncausal\ndag"
  },
  {
    "objectID": "posts/nasa02/nasa02.html",
    "href": "posts/nasa02/nasa02.html",
    "title": "nasa02",
    "section": "",
    "text": "Solution\nDekade berechnen:\n\nd <-\n  d %>% \n  mutate(decade = round(Year/10))\n\nKorrelation:\n\nd %>% \n  summarise(temp_cor = cor(Jan, Feb))\n\n# A tibble: 1 √ó 1\n  temp_cor\n     <dbl>\n1    0.940\n\n\nKorrelation pro Dekade:\n\nd_summarized <- \n  d %>% \n  group_by(decade) %>% \n  summarise(temp_cor = cor(Jan, Feb))\n\n\n\n\n\n\n\n  \n  \n    \n      decade\n      temp_cor\n    \n  \n  \n    188\n0.87\n    189\n0.79\n    190\n0.53\n    191\n0.83\n    192\n0.82\n    193\n0.73\n    194\n0.50\n    195\n0.78\n    196\n0.56\n    197\n0.79\n    198\n0.80\n    199\n0.53\n    200\n0.56\n    201\n0.66\n    202\n0.96\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nDie Korrelation der Temperaturen und damit die √Ñhnlichkeit der Muster hat im Laufe der Dekaden immer mal wieder geschwankt.\n\nCategories:\n~"
  },
  {
    "objectID": "posts/Rethink_2m1/Rethink_2m1.html",
    "href": "posts/Rethink_2m1/Rethink_2m1.html",
    "title": "Rethink_2m1",
    "section": "",
    "text": "Solution\nThe solution is taken from this source.\n\nlibrary(tidyverse)\n\ndist <- \n  tibble(\n    # Gridwerte bestimmen:\n    p_grid = seq(from = 0, to = 1, length.out = 20),\n    # Priori-Wskt bestimmen:\n    prior = rep(1, times = 20)) %>%\n  mutate(\n    # Likelihood berechnen:\n    likelihood_1 = dbinom(3, size = 3, prob = p_grid),  # WWW\n    likelihood_2 = dbinom(3, size = 4, prob = p_grid),  # WWWL\n    likelihood_3 = dbinom(5, size = 7, prob = p_grid),  # LWWLWWW\n    # unstand. Posterior-Wskt:\n    unstand_post_1 = likelihood_1 * prior,\n    unstand_post_2 = likelihood_2 * prior,\n    unstand_post_3 = likelihood_3 * prior,\n    # stand. Post-Wskt:\n    std_post_1 = unstand_post_1 / sum(unstand_post_1),\n    std_post_2 = unstand_post_2 / sum(unstand_post_2),\n    std_post_3 = unstand_post_3 / sum(unstand_post_3)\n    ) \n\nJetzt k√∂nnen wir das Diagramm zeichnen:\n\nggplot(dist) +\n  aes(x = p_grid, y= std_post_1) +\n  geom_line()+\n  geom_point() +\n  labs(x = \"p(W)\",\n       y = \"Posteriori-Wahrscheinlichkeit\",\n       title = \"Daten: WWW\")\n\n\n\n\n\n\n\nggplot(dist) +\n  aes(x = p_grid, y= std_post_2) +\n  geom_line()+\n  geom_point() +\n  labs(x = \"p(W)\",\n       y = \"Posteriori-Wahrscheinlichkeit\",\n       title = \"Daten: WWWL\")\n\n\n\n\n\n\n\nggplot(dist) +\n  aes(x = p_grid, y= std_post_3) +\n  geom_line()+\n  geom_point() +\n  labs(x = \"p(W)\",\n       y = \"Posteriori-Wahrscheinlichkeit\",\n       title = \"Daten: LWWLWWW\")\n\n\n\n\n\n\n\n\nEtwas eleganter (und komplizierter) kann man es auch so in R schreiben (Quelle):\n\nlibrary(tidyverse)\n\ndist <- tibble(p_grid = seq(from = 0, to = 1, length.out = 20),\n               prior = rep(1, times = 20)) %>%\n  mutate(likelihood_1 = dbinom(3, size = 3, prob = p_grid),\n         likelihood_2 = dbinom(3, size = 4, prob = p_grid),\n         likelihood_3 = dbinom(5, size = 7, prob = p_grid),\n         across(starts_with(\"likelihood\"), ~ .x * prior),\n         across(starts_with(\"likelihood\"), ~ .x / sum(.x))) %>%\n  pivot_longer(cols = starts_with(\"likelihood\"), names_to = \"pattern\",\n               values_to = \"posterior\") %>%\n  separate(pattern, c(NA, \"pattern\"), sep = \"_\", convert = TRUE) %>%\n  mutate(obs = case_when(pattern == 1L ~ \"W, W, W\",\n                         pattern == 2L ~ \"W, W, W, L\",\n                         pattern == 3L ~ \"L, W, W, L, W, W, W\"))\n\nggplot(dist, aes(x = p_grid, y = posterior)) +\n  facet_wrap(vars(fct_inorder(obs)), nrow = 1) +\n  geom_line() +\n  geom_point() +\n  labs(x = \"Proportion Water (p)\", y = \"Posterior Density\")\n\n\n\n\n\n\n\n\n\nCategories:\n\nprobability\nbayes-grid"
  },
  {
    "objectID": "posts/Rethink_2m6/Rethink_2m6.html",
    "href": "posts/Rethink_2m6/Rethink_2m6.html",
    "title": "Rethink_2m6",
    "section": "",
    "text": "Solution\nLet‚Äôs label the cards bb (black on both sides), bw (black on one, white on the other), and ww (both sides are white), respectively.\nWanted is the probability that the second side of the card is black (2b), given one side is black (1b): \\(Pr(2b|1b)\\).\n\nd <-\n  tibble::tribble(\n  ~Hyp, ~Prior,\n  \"bb\",     1L, \n  \"bw\",     2L,   \n  \"ww\",     3L, \n  ) %>% \n  mutate(Likelihood = c(2,1,0),\n         unstand_post = Prior*Likelihood,\n         std_post = unstand_post / sum(unstand_post))\n\nd %>% \n  gt() %>% \n  fmt_number(columns = 5)\n\n\n\n\n\n  \n  \n    \n      Hyp\n      Prior\n      Likelihood\n      unstand_post\n      std_post\n    \n  \n  \n    bb\n1\n2\n2\n0.50\n    bw\n2\n1\n2\n0.50\n    ww\n3\n0\n0\n0.00\n  \n  \n  \n\n\n\n\n\nCategories:\n\nprobability\nbayes"
  },
  {
    "objectID": "posts/griech-buchstaben/griech-buchstaben.html",
    "href": "posts/griech-buchstaben/griech-buchstaben.html",
    "title": "Griech-Buchstaben-Inferenz",
    "section": "",
    "text": "Solution\n\n\n\n\n\nKennwert\nStatistik\nParameter\n\n\n\n\nMittelwert\n\\[\\bar{X}\\]\n\\[\\mu\\]\n\n\nMittelwertsdifferenz\n\\[d=\\bar{X}_1-\\bar{X}_2\\]\n\\[\\mu_1\\]- \\[\\mu_2\\]\n\n\nStreuung\nsd\n\\[\\sigma\\]\n\n\nAnteil\np\n\\[\\pi\\]\n\n\nKorrelation\nr\n\\[\\rho\\]\n\n\nRegressionsgewicht\nb\n\\[\\beta\\]\n\n\n\n\n\n\nCategories:\n\nqm2\nqm2-thema01"
  },
  {
    "objectID": "posts/mtcars-simple2/mtcars-simple2.html",
    "href": "posts/mtcars-simple2/mtcars-simple2.html",
    "title": "mtcars-simple2",
    "section": "",
    "text": "Solution\nCompute Model:\n\nlm1_freq <- lm(mpg ~ hp + cyl + disp, data = mtcars)\n\nlibrary(rstanarm)\nlm1_bayes <- stan_glm(mpg ~ hp + cyl + disp, data = mtcars, refresh = 0)\n\nGet R2:\n\nlibrary(easystats)\n\n\nr2(lm1_freq)\n\n# R2 for Linear Regression\n       R2: 0.768\n  adj. R2: 0.743\n\n\n\nr2(lm1_bayes)\n\n# Bayesian R2 with Compatibility Interval\n\n  Conditional R2: 0.746 (95% CI [0.601, 0.859])\n\n\nThe coefficient is estimated as about 0.77.\n\nCategories:\n\nregresssion\nen\nbayes\nfrequentist\nqm1\nstats-nutshell"
  },
  {
    "objectID": "posts/purrr-map05/purrr-map05.html",
    "href": "posts/purrr-map05/purrr-map05.html",
    "title": "purrr-map05",
    "section": "",
    "text": "Exercise\nErstellen Sie eine Tabelle mit mit folgenden Spalten:\n\nID-Spalte: \\(1,2,..., 10\\)\nEine Spalte, in der jede Zelle eine Tabelle mit einem Vektor \\(x\\), einer standardnormalverteilten Zufallszahlen (n=1000), enth√§lt\n\nBerechnen Sie den Mittelwert von jedem \\(x\\)! Diese Ergebnisse sollen als weitere Spalte der Tabelle hinzugef√ºgt werden.\n         \n\n\nSolution\n\nd <- tibble(\n  id = 1:10) %>% \n  mutate(x = map(id, ~ rnorm(n = 1e3))\n) \n\nstr(d)\n\ntibble [10 √ó 2] (S3: tbl_df/tbl/data.frame)\n $ id: int [1:10] 1 2 3 4 5 6 7 8 9 10\n $ x :List of 10\n  ..$ : num [1:1000] 0.559 -0.588 -0.452 -1.966 1.329 ...\n  ..$ : num [1:1000] -0.54581 0.89293 -0.66738 1.26568 -0.00726 ...\n  ..$ : num [1:1000] -1.527 0.934 -0.289 0.508 1.108 ...\n  ..$ : num [1:1000] 0.0425 0.7778 -0.7187 -1.3656 -2.5356 ...\n  ..$ : num [1:1000] 1.323 -0.453 0.628 1.592 0.187 ...\n  ..$ : num [1:1000] -0.751 -0.482 0.439 1.804 0.971 ...\n  ..$ : num [1:1000] -0.268 0.182 1.62 0.372 -0.261 ...\n  ..$ : num [1:1000] 0.0568 0.8057 0.1397 0.907 0.4752 ...\n  ..$ : num [1:1000] 0.831 -0.743 -0.402 1.098 -0.905 ...\n  ..$ : num [1:1000] 0.987 -0.225 1.544 -1.503 -0.502 ...\n\n\nSo kann man sich die Mittelwerte ausgeben lassen:\n\nd$x %>% \n  map(mean)\n\n[[1]]\n[1] -0.03513212\n\n[[2]]\n[1] 0.01395107\n\n[[3]]\n[1] 0.01891006\n\n[[4]]\n[1] -0.02203389\n\n[[5]]\n[1] 0.05744261\n\n[[6]]\n[1] 0.004502295\n\n[[7]]\n[1] -0.00387942\n\n[[8]]\n[1] -0.01771766\n\n[[9]]\n[1] -0.06057087\n\n[[10]]\n[1] 0.01811559\n\n\nJetzt f√ºgen wir den letzten Schritt als Spalte hinzu:\n\nd2 <-\n  d %>% \n  mutate(x_mean = map_dbl(x, ~ mean(.x))) \n\nhead(d2)\n\n# A tibble: 6 √ó 3\n     id x               x_mean\n  <int> <list>           <dbl>\n1     1 <dbl [1,000]> -0.0351 \n2     2 <dbl [1,000]>  0.0140 \n3     3 <dbl [1,000]>  0.0189 \n4     4 <dbl [1,000]> -0.0220 \n5     5 <dbl [1,000]>  0.0574 \n6     6 <dbl [1,000]>  0.00450\n\n\nHier h√§tten wir auch schreiben k√∂nnen:\n\nd %>% \n  mutate(x_mean = map(x, mean)) %>% \n  unnest(x_mean) %>% \n  head()\n\n# A tibble: 6 √ó 3\n     id x               x_mean\n  <int> <list>           <dbl>\n1     1 <dbl [1,000]> -0.0351 \n2     2 <dbl [1,000]>  0.0140 \n3     3 <dbl [1,000]>  0.0189 \n4     4 <dbl [1,000]> -0.0220 \n5     5 <dbl [1,000]>  0.0574 \n6     6 <dbl [1,000]>  0.00450\n\n\n\nCategories:\n~"
  },
  {
    "objectID": "posts/purrr-map02/purrr-map02.html",
    "href": "posts/purrr-map02/purrr-map02.html",
    "title": "purrr-map02",
    "section": "",
    "text": "Exercise\nBestimmen Sie die h√§ufigsten Worte im Grundatzprogramm der Partei AfD (in der aktuellsten Version).\n         \n\n\nSolution\nText aus PDF-Dateien kann man mit dem Paket pdftools einlesen:\n\nlibrary(pdftools)\n\nUsing poppler version 22.02.0\n\nd_path <- \"/Users/sebastiansaueruser/Google Drive/Literatur/_Div/Politik/afd-grundsatzprogramm-2022.pdf\"\n\nd <- tibble(text = pdf_text(d_path))\n\nDann erstellen wir eine Tidy-Version und tokenisieren nach W√∂rtern:\n\nlibrary(tidytext)\nd2 <-\n  d %>% \n  unnest_tokens(output = word, input = text)\n\nhead(d2)\n\n# A tibble: 6 √ó 1\n  word             \n  <chr>            \n1 programm         \n2 f√ºr              \n3 deutschland      \n4 das              \n5 grundsatzprogramm\n6 der              \n\n\nDann z√§hlen wir die W√∂rter:\n\nd2 %>% \n  count(word, sort = TRUE) %>% \n  head(20)\n\n# A tibble: 20 √ó 2\n   word            n\n   <chr>       <int>\n 1 die          1151\n 2 und          1147\n 3 der           870\n 4 zu            435\n 5 f√ºr           392\n 6 in            392\n 7 den           271\n 8 von           257\n 9 ist           251\n10 das           225\n11 werden        214\n12 eine          211\n13 nicht         196\n14 ein           191\n15 deutschland   190\n16 sind          187\n17 wir           176\n18 afd           171\n19 des           169\n20 sich          158\n\n\n\nCategories:\n\nr\nmap\ntidyverse"
  },
  {
    "objectID": "posts/twitter03/twitter03.html",
    "href": "posts/twitter03/twitter03.html",
    "title": "twitter03",
    "section": "",
    "text": "Solution\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.2 ‚îÄ‚îÄ\n‚úî ggplot2 3.3.6      ‚úî purrr   0.3.5 \n‚úî tibble  3.1.8      ‚úî dplyr   1.0.10\n‚úî tidyr   1.2.1      ‚úî stringr 1.4.1 \n‚úî readr   2.1.3      ‚úî forcats 0.5.2 \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n\nlibrary(rtweet)\n\n\nAttaching package: 'rtweet'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\n\nEinloggen bei Twitter; zuerst die Credentials bereithalten:\n\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\n\nDann anmelden:\n\nauth <- rtweet_app(bearer_token = Bearer_Token)\n\nTweets an Karl Lauterbach suchen:\n\nkarl1 <- search_tweets(\"@karl_lauterbach min_faves:100 OR min_retweets:100\", n = 10)\n\n\nkarl1 %>% \n  select(retweet_count, favorite_count)\n\n# A tibble: 10 √ó 2\n   retweet_count favorite_count\n           <int>          <int>\n 1            56            210\n 2            56            229\n 3            44           1626\n 4            60            225\n 5            30            494\n 6             5            148\n 7            27            435\n 8            12            178\n 9            13            162\n10            46            375\n\nCategories:\n\ntextmining\ntwitter"
  },
  {
    "objectID": "posts/mtcars-simple3/mtcars-simple3.html",
    "href": "posts/mtcars-simple3/mtcars-simple3.html",
    "title": "mtcars-simple3",
    "section": "",
    "text": "We will use the dataset mtcars in this exercise.\nAssume your causal model of your research dictates that fuel economy is a linear function of horse power, cylinder count and displacement of the engine.\nWhich of the predictors in the above model has the weakest causal impact on the output variable?\nNotes:\n\nUse can either use frequentist or bayesian modeling.\nUse R for all computations.\nThere are multiple ways to find a solution.\n\n\n\n\ncyl\nhp\ndisp\nAll are equally strong\nnone of the above"
  },
  {
    "objectID": "posts/mtcars-simple3/mtcars-simple3.html#answerlist-1",
    "href": "posts/mtcars-simple3/mtcars-simple3.html#answerlist-1",
    "title": "mtcars-simple3",
    "section": "Answerlist",
    "text": "Answerlist\n\nwrong\ncorrect\nwrong\nwrong\nwrong\n\n\nCategories:\n\nregresssion\nen\nbayes\nfrequentist\nqm1\nstats-nutshell"
  },
  {
    "objectID": "posts/ttest-als-regr/ttest-als-regr.html",
    "href": "posts/ttest-als-regr/ttest-als-regr.html",
    "title": "ttest-als-regr",
    "section": "",
    "text": "Der t-Test kann als Spezialfall der Regressionsanalyse gedeutet werden.\nHierbei ist es wichtig, sich das Skalenniveau der Variablen, die ein t-Test verarbeitet, vor Augen zu f√ºhren.\nHinweisse:\n\nDie folgende Abbildung gibt Tipps.\nInformationen, die zur L√∂sung einer Aufgabe nicht n√∂tig sind, sollte man ignorieren.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBenennen Sie die Skalenniveaus der UV eines t-Tests! Geben Sie nur ein Wort ein. Verwenden Sie nur Kleinbuchstaben (z.B. regression).\nBenennen Sie die Skalenniveaus der AV eines t-Tests! Geben Sie nur ein Wort ein. Verwenden Sie nur Kleinbuchstaben (z.B. regression).\nNennen Sie eine beispielhafte Forschungsfrage f√ºr einen t-Test.\nSkizzieren Sie ein Diagramm einer Regression, die analytisch identisch (oder sehr √§hnlich) zu einem t-Test ist!"
  },
  {
    "objectID": "posts/ttest-als-regr/ttest-als-regr.html#answerlist-1",
    "href": "posts/ttest-als-regr/ttest-als-regr.html#answerlist-1",
    "title": "ttest-als-regr",
    "section": "Answerlist",
    "text": "Answerlist\n\nUV: bin√§r\nAV: metrisch\nUnterscheiden sich die mittleren Einparkzeiten von Frauen und M√§nnern?\nAus dem Datensatz mtcars:\n\n\ndata(mtcars)\nmtcars %>% \n  ggplot() +\n  aes(x = am, y = mpg) +\n  geom_point(alpha = .5) +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nCategories:\n\nregr\nttest\nvariable-levels"
  },
  {
    "objectID": "posts/log-y-regr1/log-y-regr1.html",
    "href": "posts/log-y-regr1/log-y-regr1.html",
    "title": "log-y-regr1",
    "section": "",
    "text": "Solution\n\nd2 <-\n  d %>% \n  filter(re74 > 0) %>% \n  mutate(re74_log = log(re74))\n\n\nm <- lm(re74_log ~ educ, data = d2)\n\nHier sind die parameters des Modells.\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(2327)\np\n\n\n\n\n(Intercept)\n8.83\n0.06\n(8.70, 8.95)\n142.91\n< .001\n\n\neduc\n0.07\n4.94e-03\n(0.07, 0.08)\n15.16\n< .001\n\n\n\n\nF√ºr jedes Jahr Bildung steigt das Einkommen also ca. um den Faktor 1.07.\nEtwas genauer:\n\\(\\hat{\\beta_1} = 0.07\\) bedeutet, dass ein Jahr Bildung zu einen erwarteten Unterschied im Einkommen in H√∂he von 0.07 in Log-Einkommen f√ºhrt. Anders gesagt wird das Einkommen um exp(0.07) erh√∂ht. Dabei gilt \\(e^{0.07} \\approx 1.07\\):\n\nexp(0.07)\n\n[1] 1.072508\n\n\nDie L√∂sung lautet also: ‚ÄúPro Jahr Bildung steigt das Einkommen - laut Modell um den Faktor ca. 1.07‚Äù.\nMan darf dabei nicht vergessen, dass wir wir uns hier auf die Schnelle ein Modell ausgedacht haben. Ob es in Wirklichkeit so ist, wie unser Modell meint, ist eine andere Sache!\n\nCategories:\n\nregression\nlm\nqm2\nstats-nutshell"
  },
  {
    "objectID": "posts/adjustieren2/adjustieren2.html",
    "href": "posts/adjustieren2/adjustieren2.html",
    "title": "adjustieren2",
    "section": "",
    "text": "Solution\n\n\n\n\nUnser Modell lm1 sch√§tzt den Preis eines Diamanten mittlerer Gr√∂√üe auf etwa 3932.5 (was immer auch die Einheiten sind, Dollar vermutlich).\nprice ~ carat_z + cut\n\nDieses zweite Modell k√∂nnten wir so berechnen:\n\nlm2 <- stan_glm(price ~ carat_z + cut, data = diamonds,\n                chains = 1,\n                refresh = 0)\nparameters(lm2)\n\nParameter    |  Median |             95% CI |   pd | % in ROPE |  Rhat |     ESS |                       Prior\n--------------------------------------------------------------------------------------------------------------\n(Intercept)  | 2406.07 | [2334.35, 2488.89] | 100% |        0% | 1.000 |  321.00 | Normal (3932.80 +- 9973.60)\ncarat_z      | 7870.55 | [7843.93, 7897.58] | 100% |        0% | 1.005 | 1047.00 |   Normal (0.00 +- 21040.85)\ncutGood      | 1118.79 | [1027.17, 1203.09] | 100% |        0% | 0.999 |  434.00 |   Normal (0.00 +- 34685.38)\ncutIdeal     | 1799.71 | [1714.31, 1869.75] | 100% |        0% | 0.999 |  338.00 |   Normal (0.00 +- 20362.28)\ncutPremium   | 1437.68 | [1353.65, 1512.30] | 100% |        0% | 0.999 |  351.00 |   Normal (0.00 +- 22862.49)\ncutVery Good | 1508.84 | [1422.69, 1582.24] | 100% |        0% | 1.000 |  344.00 |   Normal (0.00 +- 23922.15)\n\n\nEin ‚Äúnormales‚Äù (frequentistisches) lm k√§me zu √§hnlichen Ergebnissen:\n\nlm(price ~ carat_z + cut, data = diamonds)\n\n\nCall:\nlm(formula = price ~ carat_z + cut, data = diamonds)\n\nCoefficients:\n (Intercept)       carat_z       cutGood      cutIdeal    cutPremium  \n        2405          7871          1120          1801          1439  \ncutVery Good  \n        1510  \n\n\nMan k√∂nnte hier noch einen Interaktionseffekt erg√§nzen, wenn man Grund zur Annahme hat, dass es einen gibt.\n\nCategories:\n\nqm2\n‚Äò2022‚Äô"
  },
  {
    "objectID": "posts/Griech-Buchstaben-Inferenz/Griech-Buchstaben-Inferenz.html",
    "href": "posts/Griech-Buchstaben-Inferenz/Griech-Buchstaben-Inferenz.html",
    "title": "Griech-Buchstaben-Inferenz",
    "section": "",
    "text": "Solution\n\n\n\n\n\nKennwert\nStatistik\nParameter\n\n\n\n\nMittelwert\n\\[\\bar{X}\\]\n\\[\\mu\\]\n\n\nMittelwertsdifferenz\n\\[d=\\bar{X}_1-\\bar{X}_2\\]\n\\[\\mu_1\\]- \\[\\mu_2\\]\n\n\nStreuung\nsd\n\\[\\sigma\\]\n\n\nAnteil\np\n\\[\\pi\\]\n\n\nKorrelation\nr\n\\[\\rho\\]\n\n\nRegressionsgewicht\nb\n\\[\\beta\\]\n\n\n\n\n\n\nCategories:\n\nqm2\nqm2-thema01"
  },
  {
    "objectID": "posts/twitter02/twitter02.html",
    "href": "posts/twitter02/twitter02.html",
    "title": "twitter02",
    "section": "",
    "text": "Solution\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.2 ‚îÄ‚îÄ\n‚úî ggplot2 3.3.6      ‚úî purrr   0.3.5 \n‚úî tibble  3.1.8      ‚úî dplyr   1.0.10\n‚úî tidyr   1.2.1      ‚úî stringr 1.4.1 \n‚úî readr   2.1.3      ‚úî forcats 0.5.2 \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n\nlibrary(rtweet)\n\n\nAttaching package: 'rtweet'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\n\nEinloggen bei Twitter; zuerst die Credentials bereithalten:\n\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\n\n\nauth <- rtweet_app(bearer_token = Bearer_Token)\n\nAus der Hilfe zu search_tweets:\nDescription\nReturns Twitter statuses matching a user provided search query. ONLY RETURNS DATA FROM THE PAST 6-9 DAYS.\nTweets an Karl Lauterbach suchen:\n\nkarl_tweets <- search_tweets(q = \"@karl_lauterbach\", n = 150000, retryonratelimit = TRUE)\n\nWir k√∂nnten n auch auf Inf setzen, aber, da wir auf das Refreshen des Rate Limits warten m√ºssen, k√∂nnte sehr lange dauern. Daher nehmen wir hier nur einen k√ºrzeren Wert.\n\ndim(karl_tweets)\n\n[1] 18000    43\n\nhead(karl_tweets)\n\n# A tibble: 6 √ó 43\n  created_at               id id_str      full_‚Ä¶¬π trunc‚Ä¶¬≤ displ‚Ä¶¬≥ entities     metad‚Ä¶‚Å¥\n  <dttm>                <dbl> <chr>       <chr>   <lgl>     <dbl> <list>       <list> \n1 2022-10-23 13:30:18 1.58e18 1584145185‚Ä¶ \"Bei ‚Å¶@‚Ä¶ FALSE       122 <named list> <df>   \n2 2022-10-22 18:34:37 1.58e18 1583859379‚Ä¶ \"Es is‚Ä¶ FALSE       263 <named list> <df>   \n3 2022-10-22 17:56:39 1.58e18 1583849826‚Ä¶ \"Die S‚Ä¶ FALSE       215 <named list> <df>   \n4 2022-10-24 08:10:35 1.58e18 1584427113‚Ä¶ \"Zu we‚Ä¶ FALSE       219 <named list> <df>   \n5 2022-10-24 08:10:35 1.58e18 1584427113‚Ä¶ \"RT @K‚Ä¶ FALSE       140 <named list> <df>   \n6 2022-10-24 08:10:25 1.58e18 1584427072‚Ä¶ \"RT @U‚Ä¶ FALSE       139 <named list> <df>   \n# ‚Ä¶ with 35 more variables: source <chr>, in_reply_to_status_id <dbl>,\n#   in_reply_to_status_id_str <chr>, in_reply_to_user_id <dbl>,\n#   in_reply_to_user_id_str <chr>, in_reply_to_screen_name <chr>, geo <list>,\n#   coordinates <list>, place <list>, contributors <lgl>, is_quote_status <lgl>,\n#   retweet_count <int>, favorite_count <int>, favorited <lgl>, retweeted <lgl>,\n#   possibly_sensitive <lgl>, lang <chr>, quoted_status_id <dbl>,\n#   quoted_status_id_str <chr>, quoted_status <list>, retweeted_status <list>, ‚Ä¶\n# ‚Ñπ Use `colnames()` to see all variable names\n\nCategories:\n\ntextmining\ntwitter"
  },
  {
    "objectID": "posts/purrr-map03/purrr-map03.html",
    "href": "posts/purrr-map03/purrr-map03.html",
    "title": "purrr-map03",
    "section": "",
    "text": "Exercise\nImportieren Sie das Grundatzprogramm der Partei AfD (in der aktuellsten Version). Tokenisieren Sie nach S√§tzen. Dann entfernen Sie alle Zahlen. Dann z√§hlen Sie die Anzahl der W√∂rter pro Satz und berichten g√§ngige deskriptive Statistiken dazu.\n         \n\n\nSolution\nText aus PDF-Dateien kann man mit dem Paket pdftools einlesen:\n\nlibrary(pdftools)\n\nUsing poppler version 22.02.0\n\nd_path <- \"/Users/sebastiansaueruser/Google Drive/Literatur/_Div/Politik/afd-grundsatzprogramm-2022.pdf\"\n\nd <- tibble(text = pdf_text(d_path))\n\nDann erstellen wir eine Tidy-Version und tokenisieren nach S√§tzen:\n\nlibrary(tidytext)\nd2 <-\n  d %>% \n  unnest_sentences(output = word, input = text)\n\nhead(d2)\n\n# A tibble: 6 √ó 1\n  word                                                                          \n  <chr>                                                                         \n1 programm f√ºr deutschland.                                                     \n2 das grundsatzprogramm der alternative f√ºr deutschland.                        \n3 2   programm f√ºr deutschland | inhalt         pr√§ambel                       ‚Ä¶\n4 familien st√§rken        43             und parteiferne rechnungsh√∂fe         ‚Ä¶\n5 3   programm f√ºr deutschland | inhalt         7 | kultur, sprache und identit‚Ä¶\n6 f√∂rder- und                         10.10.3 deutsche literatur im inland digi‚Ä¶\n\n\nDann entfernen wir die Zahlen:\n\nd3 <- \n  d2 %>% \n  mutate(word = str_remove_all(word, pattern = \"[:digit:]+\"))\n\nPr√ºfen wir, ob es geklappt hat:\n\nd2$word[10]\n\n[1] \"weniger subventionen    88      13.7 fischerei, forst und jagd: im einklang mit der natur     88      13.8 fl√§chenkonkurrenz:           nicht zu lasten der land- und forstwirtschaft            88\"\n\nd3$word[10]\n\n[1] \"weniger subventionen          . fischerei, forst und jagd: im einklang mit der natur           . fl√§chenkonkurrenz:           nicht zu lasten der land- und forstwirtschaft            \"\n\n\nOk.\nDann z√§hlen wir die W√∂rter pro Satz:\n\nd4 <- \n  d3 %>% \n  summarise(word_count_per_sentence = str_count(word, \"\\\\w+\"))\n\nhead(d4)\n\n# A tibble: 6 √ó 1\n  word_count_per_sentence\n                    <int>\n1                       3\n2                       6\n3                     196\n4                      40\n5                     254\n6                      15\n\n\nVisualisierung:\n\nd4 %>% \n  ggplot(aes(x = word_count_per_sentence)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.5.2 (red = needs update)\n‚úî insight     0.18.4    ‚úñ datawizard  0.6.1  \n‚úî bayestestR  0.13.0    ‚úñ performance 0.9.2  \n‚úñ parameters  0.18.2    ‚úñ effectsize  0.7.0.5\n‚úî modelbased  0.8.5     ‚úñ correlation 0.8.2  \n‚úî see         0.7.3     ‚úî report      0.5.5  \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`.\n\ndescribe_distribution(d4)\n\nVariable                |  Mean |    SD | IQR |          Range | Skewness | Kurtosis |    n | n_Missing\n-------------------------------------------------------------------------------------------------------\nword_count_per_sentence | 21.86 | 17.24 |  19 | [0.00, 254.00] |     3.84 |    37.52 | 1208 |         0\n\n\n\nCategories:\n\nr\nmap\ntidyverse"
  },
  {
    "objectID": "posts/purrr-map04/purrr-map04.html",
    "href": "posts/purrr-map04/purrr-map04.html",
    "title": "purrr-map04",
    "section": "",
    "text": "Exercise\nImportieren Sie das Grundatzprogramm der Partei AfD (in der aktuellsten Version). Tokenisieren Sie nach Seiten. Dann verschachteln Sie die Spalte, in denen der Text der Seite steht, zu einer Listenspalte. Schlie√ülich z√§hlen Sie die Anzahl der W√∂rter pro Seite und berichten g√§ngige deskriptive Statistiken dazu.\n         \n\n\nSolution\nText aus PDF-Dateien kann man mit dem Paket pdftools einlesen:\n\nlibrary(pdftools)\n\nUsing poppler version 22.02.0\n\nd_path <- \"/Users/sebastiansaueruser/Google Drive/Literatur/_Div/Politik/afd-grundsatzprogramm-2022.pdf\"\n\nd <- tibble(text = pdf_text(d_path),\n            page = 1:length(text))\n\nZu Seiten tokenisieren brauchen wir nicht; das Datenmaterial ist bereits nach Seiten organisiert.\nJetzt ‚Äúverschachteln‚Äù (to nest) wir die Spalte mit dem Text:\n\nd2 <-\n  d %>% \n  nest(data = text)\n\nhead(d2)\n\n# A tibble: 6 √ó 2\n   page data            \n  <int> <list>          \n1     1 <tibble [1 √ó 1]>\n2     2 <tibble [1 √ó 1]>\n3     3 <tibble [1 √ó 1]>\n4     4 <tibble [1 √ó 1]>\n5     5 <tibble [1 √ó 1]>\n6     6 <tibble [1 √ó 1]>\n\n\nDann z√§hlen wir die W√∂rter pro Seite:\n\nd3 <-\n  d2 %>% \n  mutate(word_count_per_page = map(data, ~ str_count(.x$text, \"\\\\w+\")))\n\nhead(d3)\n\n# A tibble: 6 √ó 3\n   page data             word_count_per_page\n  <int> <list>           <list>             \n1     1 <tibble [1 √ó 1]> <int [1]>          \n2     2 <tibble [1 √ó 1]> <int [1]>          \n3     3 <tibble [1 √ó 1]> <int [1]>          \n4     4 <tibble [1 √ó 1]> <int [1]>          \n5     5 <tibble [1 √ó 1]> <int [1]>          \n6     6 <tibble [1 √ó 1]> <int [1]>          \n\n\nWie sieht eine Zelle aus data aus?\n\nd3$data[[1]]\n\n# A tibble: 1 √ó 1\n  text                                                                          \n  <chr>                                                                         \n1 \"PROGRAMM F√úR\\nDEUTSCHLAND.\\nDas Grundsatzprogramm der Alternative f√ºr Deutsc‚Ä¶\n\n\nWie sieht eine Zelle aus word_count_per_page aus?\n\nd3$data[[1]]\n\n# A tibble: 1 √ó 1\n  text                                                                          \n  <chr>                                                                         \n1 \"PROGRAMM F√úR\\nDEUTSCHLAND.\\nDas Grundsatzprogramm der Alternative f√ºr Deutsc‚Ä¶\n\n\nAh! Darin steckt nur eine einzelne Zahl!\n\nd3$data[[1]] %>% str()\n\ntibble [1 √ó 1] (S3: tbl_df/tbl/data.frame)\n $ text: chr \"PROGRAMM F√úR\\nDEUTSCHLAND.\\nDas Grundsatzprogramm der Alternative f√ºr Deutschland.\\n\"\n\n\nDas hei√üt, wir k√∂nnen vereinfachen, entschacheln:\n\nd4 <-\n  d3 %>% \n  unnest(word_count_per_page)\n\nhead(d4)\n\n# A tibble: 6 √ó 3\n   page data             word_count_per_page\n  <int> <list>                         <int>\n1     1 <tibble [1 √ó 1]>                   9\n2     2 <tibble [1 √ó 1]>                 410\n3     3 <tibble [1 √ó 1]>                 516\n4     4 <tibble [1 √ó 1]>                 297\n5     5 <tibble [1 √ó 1]>                   1\n6     6 <tibble [1 √ó 1]>                 414\n\n\nVisualisierung:\n\nd4 %>% \n  ggplot(aes(x = word_count_per_page)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.5.2 (red = needs update)\n‚úî insight     0.18.4    ‚úñ datawizard  0.6.1  \n‚úî bayestestR  0.13.0    ‚úñ performance 0.9.2  \n‚úñ parameters  0.18.2    ‚úñ effectsize  0.7.0.5\n‚úî modelbased  0.8.5     ‚úñ correlation 0.8.2  \n‚úî see         0.7.3     ‚úî report      0.5.5  \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`.\n\ndescribe_distribution(d4$word_count_per_page)\n\n  Mean |     SD |    IQR |          Range | Skewness | Kurtosis |  n | n_Missing\n--------------------------------------------------------------------------------\n285.84 | 172.27 | 322.75 | [1.00, 516.00] |    -0.64 |    -1.24 | 96 |         0\n\n\n\nCategories:\n\nr\nmap\ntidyverse"
  },
  {
    "objectID": "posts/Stichprobenziehen1/Stichprobenziehen1.html",
    "href": "posts/Stichprobenziehen1/Stichprobenziehen1.html",
    "title": "Stichprobenziehen1",
    "section": "",
    "text": "Solution\nIndividuell\n\nCategories:\n\nlm\ninference\nqm2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "datenwerk",
    "section": "",
    "text": "probability\n\n\nbayes-grid\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nconditional\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nbayes-grid\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nmap\n\n\ntidyverse\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\nloop\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobabillity\n\n\ndependent\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\ndependent\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntextmining\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nbayes-grid\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nbayes-grid\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nmeta\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nbinomial\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nphilosophy\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nbinomial\n\n\nexample\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nbayes-grid\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\nloop\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nmap\n\n\ntidyverse\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntextmining\n\n\ntwitter\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntextmining\n\n\ntwitter\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nmap\n\n\ntidyverse\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nmap\n\n\ntidyverse\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nqm2\n\n\ninference\n\n\nlm\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\neda\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\neda\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\neda\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncausal\n\n\ndag\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncorrelation\n\n\nlm\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nqm2\n\n\ninference\n\n\nuncertainty\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nqm2\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nttest\n\n\nregr\n\n\nvariable-levels\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregresssion\n\n\nen\n\n\nbayes\n\n\nfrequentist\n\n\nqm1\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstats-nutshell\n\n\nqm2\n\n\nregression\n\n\nlog\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nqm2\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nqm2\n\n\nlm\n\n\nbayes\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nbayes\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlm\n\n\ninference\n\n\nqm2\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninference\n\n\nlm\n\n\nqm2\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregresssion\n\n\nen\n\n\nbayes\n\n\nfrequentist\n\n\nqm1\n\n\nstats-nutshell\n\n\nqm2\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregresssion\n\n\nen\n\n\nbayes\n\n\nfrequentist\n\n\nqm1\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregresssion\n\n\nen\n\n\nbayes\n\n\nfrequentist\n\n\nqm1\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregr\n\n\nttest\n\n\nvariable-levels\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nqm2\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nqm2\n\n\nbayes\n\n\nadjust\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nqm2\n\n\ninference\n\n\nparameters\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlm\n\n\ninference\n\n\nqm2\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nqm2\n\n\nqm2-thema01\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Hallo, Datenwerk",
    "section": "",
    "text": "Autor: Sebastian Sauer\nDer Quellcode findet sich hier.\nDie Lizenz ist permissiv, s. Hinweise hier."
  }
]