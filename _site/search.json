[
  {
    "objectID": "posts/korr-als-regr/korr-als-regr.html",
    "href": "posts/korr-als-regr/korr-als-regr.html",
    "title": "korr-als-regr",
    "section": "",
    "text": "Exercise\nDie Korrelation prÃ¼ft, ob zwei Merkmale linear zusammenhÃ¤ngen.\nWie viele andere Verfahren kann die Korrelation als ein Spezialfall der Regression bzw. des linearen Modells \\(y = \\beta_0 + \\beta_1 + \\ldots \\beta_n + \\epsilon\\) betrachtet werden.\nAls ein spezielles Beispiel betrachten wir die Frage, ob das Gewicht eines Diamanten (carat) mit dem Preis (price) zusammenhÃ¤ngt (Datensatz diamonds).\nDen Datensatz kÃ¶nnen Sie so laden:\n\nlibrary(tidyverse)\ndata(diamonds)\n\n\nGeben Sie das Skalenniveau beider Variablen an!\nBetrachten Sie die Ausgabe von R:\n\n\nlm1 <- lm(price ~ carat, data = diamonds)\nsummary(lm1)\n\n\nCall:\nlm(formula = price ~ carat, data = diamonds)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-18585   -805    -19    537  12732 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -2256.4       13.1    -173   <2e-16 ***\ncarat         7756.4       14.1     551   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1550 on 53938 degrees of freedom\nMultiple R-squared:  0.849, Adjusted R-squared:  0.849 \nF-statistic: 3.04e+05 on 1 and 53938 DF,  p-value: <2e-16\n\n\nWie (bzw. wo) ist aus dieser Ausgabe die Korrelation herauszulesen?\n\nMacht es einen Unterschied, ob man Preis mit Karat bzw. Karat mit Preis korreliert?\nIn der klassischen Inferenzstatistik ist der \\(p\\)-Wert eine zentrale GrÃ¶ÃŸe; ist er klein (\\(p<.05\\)) so nennt man die zugehÃ¶rige Statistik signifikant und verwirft die getestete Hypothese.\nIm Folgenden sehen Sie einen Korrelationstest auf statistische Signifikanz, mit R durchgefÃ¼hrt. Zeigt der Test ein (statistisch) signifikantes Ergebnis? Wie groÃŸ ist der â€œUnsicherheitskorridorâ€, um den Korrelationswert (zugleich PunktschÃ¤tzer fÃ¼r den Populationswert)?\n\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.5.2\nâœ” insight     0.18.2     âœ” datawizard  0.5.1   \nâœ” bayestestR  0.12.1.1   âœ” performance 0.9.2   \nâœ” parameters  0.18.2     âœ” effectsize  0.7.0.5 \nâœ” modelbased  0.8.5      âœ” correlation 0.8.2   \nâœ” see         0.7.2      âœ” report      0.5.5   \n\ndiamonds %>% \n  sample_n(30) %>% \n  select(price, carat) %>% \n  correlation()\n\n# Correlation Matrix (pearson-method)\n\nParameter1 | Parameter2 |    r |       95% CI | t(28) |         p\n-----------------------------------------------------------------\nprice      |      carat | 0.94 | [0.88, 0.97] | 14.87 | < .001***\n\np-value adjustment method: Holm (1979)\nObservations: 30\n\n\n         \n\n\nSolution\n\ncarat ist metrisch (verhÃ¤ltnisskaliert) und price ist metrisch (verhÃ¤ltnisskaliert)\n\\(R^2\\) kann bei einer einfachen (univariaten) Regression als das Quadrat von \\(r\\) berechnet werden. Daher \\(r = \\sqrt{R^2}\\).\n\n\nsqrt(0.8493)\n\n[1] 0.92\n\n\nZum Vergleich\n\ndiamonds %>% \n  summarise(r = cor(price, carat))\n\n# A tibble: 1 Ã— 1\n      r\n  <dbl>\n1 0.922\n\n\nMan kann den Wert der Korrelation auch noch anderweitig berechnen (\\(\\beta\\) umrechnen in \\(\\rho\\)).\n\nNein. Die Korrelation ist eine symmetrische Relation.\nJa; die Zahl â€œ3.81e-14â€ bezeichnet eine positive Zahl kleiner eins mit 13 Nullern vor der ersten Ziffer, die nicht Null ist (3.81 in diesem Fall). Der â€œUnsicherheitskorridorâ€ reicht von etwa 0.87 bis 0.97.\n\n\nCategories:\n\ncorrelation\nlm"
  },
  {
    "objectID": "posts/Inferenz-fuer-alle/Inferenz-fuer-alle.html",
    "href": "posts/Inferenz-fuer-alle/Inferenz-fuer-alle.html",
    "title": "Inferenz-fuer-alle",
    "section": "",
    "text": "Solution\n\nFÃ¼r (grundsÃ¤tzlich) alle: FÃ¼r jede Statistik kann man prinzipiell von der jeweiligen Stichprobe (auf Basis derer die Statistik berechnet wurde) auf eine zugehÃ¶rige Grundgesamtheit schlieÃŸen.\nFÃ¼r (grundsÃ¤tzlich) alle: Die Methoden der Inferenzstatistik sind prinzipiell unabhÃ¤ngig von den Spezifika bestimmter Forschungsfragen oder -bereiche. In den meisten Forschungsfragen ist man daran interessiert allgemeingÃ¼ltige Aussagen zu treffen. Da Statistiken sich nur auf eine Stichprobe - also einen zumeist nur kleinen Teil einer Grundgesamtheit beziehen - wird man sich kaum mit einer Statistik zufrieden geben, sondern nach Inferenzstatistik verlangen.\nIn einigen AusnahmefÃ¤llen wird man auf eine Inferenzstatistik verzichten. Etwa wenn man bereits eine Vollerhebung durchgefÃ¼hrt hat, z.B. alle Mitarbeitis eines Unternehmens befragt hat, dann kennt man ja bereits den wahren Populationswert. Ein anderer Fall ist, wenn man nicht an Verallgemeinerungen interessiert ist: Kennt man etwa die Ãœberlebenschance \\(p\\) des Titanic-UnglÃ¼cks, so ist es fraglich auf welche Grundgesamtheit man die Statistik \\(p\\) bzw. zu welchem Paramter \\(\\pi\\) (kleines Pi) man generalisieren mÃ¶chte.\n\n\nCategories:\n\nqm2\nqm2-thema01"
  },
  {
    "objectID": "posts/lm1/lm1.html#answerlist",
    "href": "posts/lm1/lm1.html#answerlist",
    "title": "lm1",
    "section": "Answerlist",
    "text": "Answerlist\n\nmpg und hp sind positiv korreliert laut dem Modell.\nDer Achsenabschnitt ist nahe Null.\nDie Analyse beinhaltet einen nominal skalierten PrÃ¤diktor.\nDas geschÃ¤tzte Betagewicht fÃ¼r hp liegt bei 30.099.\nDas geschÃ¤tzte Betagewicht fÃ¼r hp liegt bei -0.068."
  },
  {
    "objectID": "posts/lm1/lm1.html#answerlist-1",
    "href": "posts/lm1/lm1.html#answerlist-1",
    "title": "lm1",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nFalsch\nRichtig\n\n\nCategories:\n\nregression\nlm\nqm2\nstats-nutshell"
  },
  {
    "objectID": "posts/ttest-skalenniveau/ttest-skalenniveau.html",
    "href": "posts/ttest-skalenniveau/ttest-skalenniveau.html",
    "title": "ttest-skalenniveau",
    "section": "",
    "text": "Der t-Test ist ein inferenzstatistisches Verfahren des Frequentismus. Welches Skalenniveau passt zu diesem Verfahren?\nHinweisse:\n\nDie folgende Abbildung gibt Tipps.\nInformationen, die zur LÃ¶sung einer Aufgabe nicht nÃ¶tig sind, sollte man ignorieren.\n\n\n\n\n\n\n\n\n\n\n\n\n\nUV: nominal (mehrstufig), AV: metrisch\nUV: metrisch, AV: nominal (zweistufig)\nUV: nominal (mehrstufig), AV: nominal (mehrstufig)\nUV: metrisch, AV: nominal (zweistufig)\nUV: nominal (zweistufig), AV: metrisch"
  },
  {
    "objectID": "posts/ttest-skalenniveau/ttest-skalenniveau.html#answerlist-1",
    "href": "posts/ttest-skalenniveau/ttest-skalenniveau.html#answerlist-1",
    "title": "ttest-skalenniveau",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nFalsch\nWahr\n\n\nCategories:\n\nttest\nregr\nvariable-levels"
  },
  {
    "objectID": "posts/nasa01/nasa01.html",
    "href": "posts/nasa01/nasa01.html",
    "title": "nasa01",
    "section": "",
    "text": "Solution\nDekade berechnen:\n\nd <-\n  d %>% \n  mutate(decade = round(Year/10))\n\nStatistiken pro Dekade:\n\nd_summarized <- \n  d %>% \n  group_by(decade) %>% \n  summarise(temp_mean = mean(Jan),\n            temp_sd = sd(Jan))\n\nd_summarized\n\n\n\n\n\n\n\n  \n  \n    \n      decade\n      temp_mean\n      temp_sd\n    \n  \n  \n    188\nâˆ’0.20\n0.24\n    189\nâˆ’0.44\n0.22\n    190\nâˆ’0.27\n0.17\n    191\nâˆ’0.40\n0.22\n    192\nâˆ’0.28\n0.16\n    193\nâˆ’0.13\n0.22\n    194\n0.03\n0.21\n    195\nâˆ’0.05\n0.18\n    196\n0.03\n0.15\n    197\nâˆ’0.07\n0.17\n    198\n0.21\n0.19\n    199\n0.36\n0.13\n    200\n0.52\n0.19\n    201\n0.64\n0.20\n    202\n0.96\n0.14\n  \n  \n  \n\n\n\n\nZur Veranschaulichung visualisieren wir die Ergebnisse:\n\n\n\n\n\n\n\n\n\n\nCategories:\n~"
  },
  {
    "objectID": "posts/purrr-map01/purrr-map01.html",
    "href": "posts/purrr-map01/purrr-map01.html",
    "title": "purrr-map01",
    "section": "",
    "text": "Exercise\nErstellen Sie einen Tibble mit folgenden Spalten:\n\nBuchstaben A-Z, so dass in der 1. Zeile â€œAâ€ steht, in der 2. Zeile â€œBâ€ etc.\nBuchstaben a-z, so dass in der 1. Zeile â€œaâ€ steht, in der 2. Zeile â€œbâ€ etc.\nBuchstabenkombination der ersten beiden Spalten, so dass in der 1. Zeile â€œA-aâ€ steht, in der 2. Zeile â€œB-bâ€ etc.\n\n         \n\n\nSolution\nGeht es vielleicht so?\n\nd <-\n  tibble(\n    letter1 = LETTERS,\n    letter2 = letters,\n    letters = paste(letter1, letter2, collapse = \"-\")\n  )\n\nhead(d)\n\n# A tibble: 6 Ã— 3\n  letter1 letter2 letters                                                       \n  <chr>   <chr>   <chr>                                                         \n1 A       a       A a-B b-C c-D d-E e-F f-G g-H h-I i-J j-K k-L l-M m-N n-O o-Pâ€¦\n2 B       b       A a-B b-C c-D d-E e-F f-G g-H h-I i-J j-K k-L l-M m-N n-O o-Pâ€¦\n3 C       c       A a-B b-C c-D d-E e-F f-G g-H h-I i-J j-K k-L l-M m-N n-O o-Pâ€¦\n4 D       d       A a-B b-C c-D d-E e-F f-G g-H h-I i-J j-K k-L l-M m-N n-O o-Pâ€¦\n5 E       e       A a-B b-C c-D d-E e-F f-G g-H h-I i-J j-K k-L l-M m-N n-O o-Pâ€¦\n6 F       f       A a-B b-C c-D d-E e-F f-G g-H h-I i-J j-K k-L l-M m-N n-O o-Pâ€¦\n\n\nNein, leider nicht.\nOK, neuer Versuch:\n\nd <-\n  tibble(\n    letter1 = LETTERS,\n    letter2 = letters) %>% \n  unite(\"letters\", c(letter1, letter2), remove = FALSE)\n\n\nhead(d)\n\n# A tibble: 6 Ã— 3\n  letters letter1 letter2\n  <chr>   <chr>   <chr>  \n1 A_a     A       a      \n2 B_b     B       b      \n3 C_c     C       c      \n4 D_d     D       d      \n5 E_e     E       e      \n6 F_f     F       f      \n\n\nProbieren wir es mit purrr::map():\n\nd <-\n  tibble(\n    letter1 = LETTERS,\n    letter2 = letters,\n    letters = map2_chr(letter1, letter2, ~ paste(c(.x, .y), collapse =\"-\"))\n  )\n\nhead(d)\n\n# A tibble: 6 Ã— 3\n  letter1 letter2 letters\n  <chr>   <chr>   <chr>  \n1 A       a       A-a    \n2 B       b       B-b    \n3 C       c       C-c    \n4 D       d       D-d    \n5 E       e       E-e    \n6 F       f       F-f    \n\n\nInfos zur Funktion paste() findet sich z.B. hier.\n\nCategories:\n\nr\nmap\ntidyverse"
  },
  {
    "objectID": "posts/purrr-map06/purrr-map06.html",
    "href": "posts/purrr-map06/purrr-map06.html",
    "title": "purrr-map06",
    "section": "",
    "text": "Exercise\nErstellen Sie eine Tabelle mit mit folgenden Spalten:\n\nID-Spalte: \\(1,2,..., 10\\)\nEine Spalte mit Namem ds (ds wie Plural von Datensatz), die als geschachtelt (nested) pro Element jeweils einen der folgenden DatensÃ¤tze enthÃ¤lt: mtcars, iris, chickweight, ToothGrowth (alle in R enthalten)\n\nBerechnen Sie eine Spalte, die die Anzahl der Spalten von ds zÃ¤hlt!\n         \n\n\nSolution\nHier sind einige DatensÃ¤tze, in einer Liste zusammengefasst:\n\nds <- list(mtcars = mtcars, iris = iris, chickweight =  ChickWeight, toothgrowth = ToothGrowth)\n\nDaraus erstellen wir eine Tabelle mit Listenspalte fÃ¼r die Daten:\n\nd <- \n  tibble(id = 1:length(ds),\n         ds = ds)\n\n\nd2 <- \n  d %>% \n  mutate(n_col = map(ds, ncol)) \n\nhead(d2)\n\n# A tibble: 4 Ã— 3\n     id ds                   n_col       \n  <int> <named list>         <named list>\n1     1 <df [32 Ã— 11]>       <int [1]>   \n2     2 <df [150 Ã— 5]>       <int [1]>   \n3     3 <nfnGrpdD [578 Ã— 4]> <int [1]>   \n4     4 <df [60 Ã— 3]>        <int [1]>   \n\n\nEntnesten wir noch n_col:\n\nd2 %>% \n  unnest(n_col)\n\n# A tibble: 4 Ã— 3\n     id ds                   n_col\n  <int> <named list>         <int>\n1     1 <df [32 Ã— 11]>          11\n2     2 <df [150 Ã— 5]>           5\n3     3 <nfnGrpdD [578 Ã— 4]>     4\n4     4 <df [60 Ã— 3]>            3\n\n\n\nCategories:\n~"
  },
  {
    "objectID": "posts/mtcars-simple1/mtcars-simple1.html",
    "href": "posts/mtcars-simple1/mtcars-simple1.html",
    "title": "mtcars-simple1",
    "section": "",
    "text": "Solution\nCompute Model:\n\nlm1_freq <- lm(mpg ~ hp + cyl + disp, data = mtcars)\n\nlibrary(rstanarm)\nlm1_bayes <- stan_glm(mpg ~ hp + cyl + disp, data = mtcars, refresh = 0)\n\nGet parameters:\n\nlibrary(easystats)\n\n\nparameters(lm1_freq)\n\nParameter   | Coefficient |   SE |         95% CI | t(28) |      p\n------------------------------------------------------------------\n(Intercept) |       34.18 | 2.59 | [28.88, 39.49] | 13.19 | < .001\nhp          |       -0.01 | 0.01 | [-0.04,  0.02] | -1.00 | 0.325 \ncyl         |       -1.23 | 0.80 | [-2.86,  0.41] | -1.54 | 0.135 \ndisp        |       -0.02 | 0.01 | [-0.04,  0.00] | -1.81 | 0.081 \n\n\n\nparameters(lm1_bayes)\n\nParameter   | Median |         95% CI |     pd | % in ROPE |  Rhat |     ESS |                   Prior\n------------------------------------------------------------------------------------------------------\n(Intercept) |  34.28 | [28.87, 39.60] |   100% |        0% | 1.000 | 2600.00 | Normal (20.09 +- 15.07)\nhp          |  -0.01 | [-0.05,  0.02] | 82.93% |      100% | 1.000 | 2448.00 |   Normal (0.00 +- 0.22)\ncyl         |  -1.25 | [-2.84,  0.34] | 93.75% |     2.55% | 1.000 | 2108.00 |   Normal (0.00 +- 8.44)\ndisp        |  -0.02 | [-0.04,  0.00] | 96.05% |      100% | 1.001 | 2405.00 |   Normal (0.00 +- 0.12)\n\n\nThe coefficient is estimated as about -0.01\n\nCategories:\n\nregresssion\nen\nbayes\nfrequentist\nqm1\nstats-nutshell"
  },
  {
    "objectID": "posts/log-y-regr3/log-y-regr3.html",
    "href": "posts/log-y-regr3/log-y-regr3.html",
    "title": "log-y-regr3",
    "section": "",
    "text": "library(tidyverse)\nlibrary(easystats)\n\nIn dieser Aufgabe modellieren wir den (kausalen) Effekt von Schulbildung auf das Einkommen.\nImportieren Sie zunÃ¤chst den Datensatz und verschaffen Sie sich einen Ãœberblick.\n\nd_path <- \"https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Treatment.csv\"\n\nd <- data_read(d_path)\n\nDokumentation und Quellenangaben zum Datensatz finden sich hier.\n\nglimpse(d)\n\nRows: 2,675\nColumns: 11\n$ V1      <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,â€¦\n$ treat   <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRâ€¦\n$ age     <int> 37, 30, 27, 33, 22, 23, 32, 22, 19, 21, 18, 27, 17, 19, 27, 23â€¦\n$ educ    <int> 11, 12, 11, 8, 9, 12, 11, 16, 9, 13, 8, 10, 7, 10, 13, 10, 12,â€¦\n$ ethn    <chr> \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\",â€¦\n$ married <lgl> TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, â€¦\n$ re74    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,â€¦\n$ re75    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,â€¦\n$ re78    <dbl> 9930.05, 24909.50, 7506.15, 289.79, 4056.49, 0.00, 8472.16, 21â€¦\n$ u74     <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRâ€¦\n$ u75     <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRâ€¦\n\n\nWelcher der PrÃ¤diktoren hat den stÃ¤rkesten Einfluss auf das Einkommen?\nHinweise:\n\nVerwenden Sie lm zur Modellierung.\nOperationalisieren Sie das Einkommen mit der Variable re74.\nGehen Sie von einem kausalen Effekt der PrÃ¤diktoren aus.\nGehen Sie von einem multiplikativen Modell aus (log-y).\nLassen Sie die Variablen zur Arbeitslosigkeit auÃŸen vor.\n\n\n\n\ntreat\nage\neduc\nethn\nmarried"
  },
  {
    "objectID": "posts/log-y-regr3/log-y-regr3.html#answerlist-1",
    "href": "posts/log-y-regr3/log-y-regr3.html#answerlist-1",
    "title": "log-y-regr3",
    "section": "Answerlist",
    "text": "Answerlist\n\nTRUE\nFALSE\nFALSE\nFALSE\nFALSE\n\n\nCategories:\n\nstats-nutshell\nqm2\nregression\nlog"
  },
  {
    "objectID": "posts/twitter01/twitter01.html",
    "href": "posts/twitter01/twitter01.html",
    "title": "twitter01",
    "section": "",
    "text": "Solution\n\nlibrary(tidyverse)\n\nâ”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 1.3.2 â”€â”€\nâœ” ggplot2 3.3.6      âœ” purrr   0.3.5 \nâœ” tibble  3.1.8      âœ” dplyr   1.0.10\nâœ” tidyr   1.2.1      âœ” stringr 1.4.1 \nâœ” readr   2.1.3      âœ” forcats 0.5.2 \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\n\nlibrary(rtweet)\n\n\nAttaching package: 'rtweet'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\n\nEinloggen bei Twitter; zuerst die Credentials bereithalten:\n\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\n\nDann anmelden, z.B. als Bot:\n\nauth <- rtweet_bot(api_key = API_Key,\n                   api_secret = API_Key_Secret,\n                   access_token = Access_Token,\n                   access_secret = Access_Token_Secret)\n\nâ€¦ Oder als App, das bringt bessere Raten mit sich:\n\nauth <- rtweet_app(bearer_token = Bearer_Token)\n\nTest:\n\nsesa_test <- get_timeline(user = \"sauer_sebastian\", n = 3) %>% \n  select(full_text)\n\nsesa_test\n\n1 RT @fuecks: By the way: Systematic destruction of life-sustaining infrastructures â€¦\n2 RT @NoContextBrits: No shortbread for little Nazis. https://t.co/F6FUPvRz94        \n3 RT @ernst_gennat: 2 oder 3 Jahre #Tempolimit von 120 km/h. AbschlieÃŸend Evaluationâ€¦\nTweets an Karl Lauterbach suchen:\n\nkarl1 <- search_tweets(\"@karl_lauterbach\")\n\nIn AuszÃ¼gen:\n\"@Karl_Lauterbach Ein Minister der alle paar Stunden Zeit hat einen Mist zu verbreiten....\"   \n\"@Karl_Lauterbach @focusonline Long Covid ist nichts anderes als schwere Nebenwirkungen der Gentherapie!\"  \"@Karl_Lauterbach @focusonline Wer schÃ¼tzt uns vor Long Lauterbach?\"\n\"@Karl_Lauterbach Also Karl, primÃ¤r fordere ich und viele andere eher erstmal dein sofortigen RÃ¼cktritt.\"  \"@Karl_Lauterbach Behalt deinen Senf fÃ¼r dich!\"                                                            \"@Karl_Lauterbach Oh Gott ğŸ˜±\"     \n\"@Karl_Lauterbach Ach nein, der Clown mit Lebensangst â€¦.\\n\\nhttps://t.co/8cQZeHh6Ew\"                       \"@Karl_Lauterbach Ich kenne nur Leute mit Long Covid, die mehrfach geimpft sind! Das ist kein Witz! Scheinbar liegtâ€™s wohl doch an den Spritzen???\"                                                            \"@Karl_Lauterbach @focusonline Interessiert keine Sau ğŸ˜‰\"                      \n\"RT @Karl_Lauterbach @focusonline â€Lauterbachs Aussagen kÃ¶nnen fundamental nicht stimmenâ€œ\\nhttps://t.co/rfxnWAWiZX\"                                                                          \"@Karl_Lauterbach @focusonline ğŸ¤¡ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚\"                                         \n\"@Karl_Lauterbach Jau und sie sind kein fÃ¤higer Gesundheitsminister, sondern lediglich ein gekaufter Coronaminister\"        \nPuh, viele toxische Tweets, wie es scheint.\nUnd ohne Retweets (RT) und ohne Replies:\n\nkarl2 <- search_tweets(\"@karl_lauterbach\", \n  include_rts = FALSE, `-filter` = \"replies\")\n\nTweets, die an Karl Lauterbach gerichtet sind, per API-Anweisung:\n\nkarl3 <- search_tweets(\"to:karl_lauterbach\", n = 100)\n\n\"@Karl_Lauterbach Vielen Dank, dass LongCovid ein gefundenes fressen fÃ¼r die jenigen ist, die nicht mehr Arbeiten wollen.\"       \n \"@Karl_Lauterbach verpiss dich einfach! Immer dieser Schwachsinn\"    \n\"@Karl_Lauterbach @focusonline Das sind genau die Impfnebenwirkungen! Will man nun das wenden um die Impfnebenwirkungen zu vertuschen? \\nWofÃ¼r ist die Impfung gut wenn nicht mal Long-Covid verhindert wird, die Ansteckung konnte sie noch nie verhindern!\\nWarum sind 89% Covid Patienten geimpfte in den SpitÃ¤ler?\"\n\"@Karl_Lauterbach Was spielen Sie eigentlich fÃ¼r ein schmutziges Spiel?\\n\\nhttps://t.co/8LJIzxyF7G\"   \n \"@Karl_Lauterbach @focusonline Bessen von Covid! StÃ¤ndig wird das Netz durchsucht, nach Artikeln,die instrumentalisiert werden, um fÃ¼r Impfung zu werben. Was hÃ¤tte nur ein vernÃ¼nftiger Gesundheitsminister mit so viel Zeit VernÃ¼nftiges im Gesundheitswesen auf die Beine stellen kÃ¶nnen...\"    \n\"@Karl_Lauterbach Mit Dauerschaden wegen der Impfung ğŸ’‰ bin ich Arbeitslos geworden in der Pflege ğŸ¤·â€â™‚ï¸ Ist das normal Herr @Karl_Lauterbach ?\"          \nOb man mit @karl_lauterbach sucht oder `to:karl_lauterbachâ€, scheint keinen groÃŸen Unterschied zu machen (?).\n\nCategories:\n\ntextmining\nâ€˜2022â€™"
  },
  {
    "objectID": "posts/log-y-regr2/log-y-regr2.html",
    "href": "posts/log-y-regr2/log-y-regr2.html",
    "title": "log-y-regr2",
    "section": "",
    "text": "Solution\n\nd2 <-\n  d %>% \n  filter(re74 > 0) %>% \n  mutate(re74_log = log(re74))\n\n\nm <- lm(re74_log ~ educ, data = d2)\n\n\nggplot(d2) +\n  aes(x = re74) +\n  geom_density() +\n  labs(title = \"Income raw\")\n\n\nggplot(d2) +\n  aes(x = re74_log) +\n  geom_density() +\n  labs(title = \"Income log transformed\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBetrachten wir die deskriptiven Statistiken:\n\nd2 %>% \n  select(re74, re74_log) %>% \n  describe_distribution()\n\nVariable |     Mean |       SD |      IQR |             Range | Skewness | Kurtosis |    n | n_Missing\n------------------------------------------------------------------------------------------------------\nre74     | 20938.28 | 12631.52 | 15086.30 | [17.63, 1.37e+05] |     1.62 |     6.81 | 2329 |         0\nre74_log |     9.73 |     0.76 |     0.80 |     [2.87, 11.83] |    -1.67 |     6.01 | 2329 |         0\n\n\nDie Log-Transformation hat in diesem Fall nicht wirklich zu einer Normalisierung der Variablen beigetragen. Aber das war auch nicht unser Ziel.\n\nCategories:\n\nregression\nlm\nqm2\nstats-nutshell"
  },
  {
    "objectID": "posts/adjustieren1/adjustieren1.html",
    "href": "posts/adjustieren1/adjustieren1.html",
    "title": "adjustieren1",
    "section": "",
    "text": "Solution\n\nlibrary(rstanarm)\nlm2 <- stan_glm(mpg ~ hp_z + am, data = mtcars,\n                refresh = 0)\nsummary(lm2)\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 26.6    1.5 24.7  26.6  28.5 \nhp          -0.1    0.0 -0.1  -0.1   0.0 \nam           5.3    1.1  3.8   5.3   6.6 \nsigma        3.0    0.4  2.5   3.0   3.5 \nDie Spalte mean gibt den mittleren geschÃ¤tzten Wert fÃ¼r den jeweiligen Koeffizienten an, also den SchÃ¤tzwert zum Koeffizienten.\nDie Koeffizienten zeigen, dass der Achsenabschnitt fÃ¼r Autos mit Automatikgetriebe um etwa 5 Meilen geringer ist als fÃ¼r Autos mit manueller Schaltung: Ein durchschnittliches Auto mit manueller Schaltung kommt also etwa 5 Meilen weiter als ein Auto mit Automatikschaltung, glaubt unser Modell.\n\nmtcars %>% \n  mutate(am = factor(am)) %>% \n  ggplot() +\n  aes(x = hp_z, y = mpg, color = am) +\n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\n\n\nMan kÃ¶nnte hier noch einen Interaktionseffekt ergÃ¤nzen.\n\nCategories:\n\nqm2\nqm2-thema01\nws22"
  },
  {
    "objectID": "posts/interpret-koeff/interpret-koeff.html",
    "href": "posts/interpret-koeff/interpret-koeff.html",
    "title": "interpret-koeff",
    "section": "",
    "text": "Solution\n\nIntercept (\\(\\beta_0\\)): Der Achsenabschnitt gibt den geschÃ¤tzten mittleren Y-Wert (Spritverbrauch) an, wenn \\(x=0\\), also fÃ¼r ein Auto mit 0 PS (was nicht wirklich Sinn macht). hp (\\(\\beta_1\\)) ist der Regressionskoeffizient oder Regressionsgewicht und damit die Steigung der Regressionsgeraden.\nhp (\\(\\beta_1\\)) ist der Regressionskoeffizient oder Regressionsgewicht und gibt den statistischen â€œEffektâ€ der PS-Zahl auf den Spritverbrauch an. Vorsicht: Dieser â€œEffektâ€ darf nicht vorschnell als kausaler Effekt verstanden werden. Daher muss man vorsichtig sein, wenn man von einem â€œEffektâ€ spricht. Vorsichtiger wÃ¤re zu sagen: â€œEin Auto mit einem PS mehr, kommt im Mittel 0,1 Meilen weniger weit mit einer Gallone Sprit, laut diesem Modellâ€.\n\n\nCategories:\n\nregression\nlm\nbayes\nstats-nutshell"
  },
  {
    "objectID": "posts/ungewiss-arten-regr/ungewiss-arten-regr.html",
    "href": "posts/ungewiss-arten-regr/ungewiss-arten-regr.html",
    "title": "ungewiss-arten-regr",
    "section": "",
    "text": "Solution\n\n\n\n\n7.04\n0.04\n\n\nCategories:\n\nqm2\ninference\nlm"
  },
  {
    "objectID": "posts/vorhersageintervall1/vorhersageintervall1.html",
    "href": "posts/vorhersageintervall1/vorhersageintervall1.html",
    "title": "vorhersageintervall1",
    "section": "",
    "text": "Vorhersagen, etwa in einem Regressionsmodell, sind mit mehreren Arten von Unsicherheit konfrontiert.\nBerechnen Sie dazu ein Regressionsmodell, Datensatz mtcars, mit hp als PrÃ¤diktor (UV) und mpg als AV (Kriterium)!\nDann sagen Sie bitte den Wert der AV fÃ¼r eine Beobachtungseinheit mit mittlerer AusprÃ¤gung im PrÃ¤ktor vorher:\nEinmal nur unter BerÃ¼cksichtigung der Unsicherheit innerhalb des Modells (â€œKonfidenzintervallâ€); einmal unter BerÃ¼cksichtigung der Unsicherheit innerhalb des Modells sowie die Unsicherheit durch die Koffizienten (â€œVohersageintervallâ€).\nHinweise:\n\npredict() ist eine Funktion, die Sie zur Vorhersage von Regressionsmodellen verwenden kÃ¶nnen.\nVerwenden Sie lm() zur Berechnung eines Regressionsmodells.\nDas Argument type von predict() erlaubt Ihnen die Wahl der Art der Vorhersage, betrachten Sie Hilfe der Funktion z.B. hier.\n\nBei welchem Intervall ist die Ungewissheit in der Vorhersage grÃ¶ÃŸer?\n\n\n\nKonfidenzintervall\nVohersageintervall\nGleich groÃŸ\nKommt auf weitere Faktoren an, keine pauschale Antwort mÃ¶glich"
  },
  {
    "objectID": "posts/vorhersageintervall1/vorhersageintervall1.html#answerlist-1",
    "href": "posts/vorhersageintervall1/vorhersageintervall1.html#answerlist-1",
    "title": "vorhersageintervall1",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\nFalsch\nFalsch\n\n\nCategories:\n\nlm\ninference\nqm2"
  },
  {
    "objectID": "posts/lm-Standardfehler/lm-Standardfehler.html",
    "href": "posts/lm-Standardfehler/lm-Standardfehler.html",
    "title": "lm-Standardfehler",
    "section": "",
    "text": "Man kann angeben, wie genau eine SchÃ¤tzung von Regressionskoeffizienten die Grundgesamtheit widerspiegelt. Zumeist wird dazu der Standardfehler (engl. standard error, SE) verwendet.\nIn dieser Ãœbung untersuchen wir, wie sich der SE als Funktion der StichprobengrÃ¶ÃŸe, \\(n\\), verhÃ¤lt.\nErstellen Sie dazu folgenden Datensatz:\n\nlibrary(tidyverse)\n\nn <- 2^4\n\nd <-\n  tibble(x = rnorm(n = n),  # im Default: mean = 0, sd = 1\n         y = x + rnorm(n, mean = 0, sd = .5))\n\nHier ist das Ergebnis. Uns interessiert v.a. Std. Error fÃ¼r den PrÃ¤diktor x:\n\nlm(y ~ x, data = d) %>% \nsummary()\n\n\nCall:\nlm(formula = y ~ x, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.60191 -0.42922  0.09198  0.32313  0.59878 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.1226     0.1061  -1.156    0.267    \nx             1.0385     0.0888  11.694  1.3e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4223 on 14 degrees of freedom\nMultiple R-squared:  0.9071,    Adjusted R-squared:  0.9005 \nF-statistic: 136.8 on 1 and 14 DF,  p-value: 1.302e-08\n\n\nHier haben wir eine Tabelle mit zwei Variablen, x und y, definiert mit n=16.\nVerdoppeln Sie die StichprobengrÃ¶ÃŸe 5 Mal und betrachten Sie, wie sich die SchÃ¤tzgenauigkeit, gemessen Ã¼ber den SE, verÃ¤ndert. Berechnen Sie dazu fÃ¼r jedes n eine Regression mit x als PrÃ¤diktor und y als AV!\nBei welcher StichprobengrÃ¶ÃŸe ist SE am kleinsten?\n\n\n\n\\(2^5\\)\n\\(2^6\\)\n\\(2^7\\)\n\\(2^8\\)\n\\(2^9\\)"
  },
  {
    "objectID": "posts/lm-Standardfehler/lm-Standardfehler.html#answerlist-1",
    "href": "posts/lm-Standardfehler/lm-Standardfehler.html#answerlist-1",
    "title": "lm-Standardfehler",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nFalsch\nWahr. Die grÃ¶ÃŸte Stichprobe impliziert den kleinsten SE, ceteris paribus.\n\n\nCategories:\n\ninference\nlm\nqm2"
  },
  {
    "objectID": "posts/punktschaetzer-reicht-nicht/punktschaetzer-reicht-nicht.html",
    "href": "posts/punktschaetzer-reicht-nicht/punktschaetzer-reicht-nicht.html",
    "title": "punktschaetzer-reicht-nicht",
    "section": "",
    "text": "Solution\nModell m1 hat eine kleinere Ungewissheit im Hinblick auf die Modellkoeffizienten \\(\\beta_0, \\beta_1\\) und ist daher gegenÃ¼ber m2 zu bevorzugen.\n\nCategories:\n\nqm2\nqm2-thema01\nws22"
  },
  {
    "objectID": "posts/nasa03/nasa03.html",
    "href": "posts/nasa03/nasa03.html",
    "title": "nasa03",
    "section": "",
    "text": "Solution\ntemp_is_above erstellen:\n\nd <-\n  d %>% \n  mutate(temp_is_above = case_when(\n    Jan > 0 ~ \"yes\",\n    Jan <= 0 ~ \"no\"\n  ))\n\nJahrhundert berechnen:\n\nd <-\n  d %>% \n  mutate(century = case_when(\n    Year < 1900 ~ \"19th\",\n    Year >= 1900 ~ \"20th\"\n  ))\n\nErhÃ¶hte Werte der Januar-Temperatur pro Jahrhundert berechnen:\n\nd_summarized <- \nd %>% \n  group_by(century) %>% \n  count(temp_is_above)\n\nd_summarized\n\n# A tibble: 4 Ã— 3\n# Groups:   century [2]\n  century temp_is_above     n\n  <chr>   <chr>         <int>\n1 19th    no               19\n2 19th    yes               1\n3 20th    no               56\n4 20th    yes              67\n\n\nDer Befehl count() zÃ¤hlt aus, wie hÃ¤ufig die AusprÃ¤gungen der angegebenen Variablen X sind, m.a.W. er gibt die Verteilung von X wieder.\nEs macht vermutlich Sinn, noch die Anteile (relative HÃ¤ufigkeiten) zu den absoluten HÃ¤ufigkeiten zu ergÃ¤nzen:\n\nd_summarized %>% \n  mutate(prop = n / sum(n))\n\n# A tibble: 4 Ã— 4\n# Groups:   century [2]\n  century temp_is_above     n  prop\n  <chr>   <chr>         <int> <dbl>\n1 19th    no               19 0.95 \n2 19th    yes               1 0.05 \n3 20th    no               56 0.455\n4 20th    yes              67 0.545\n\n\nOdds Ratio berechnen:\nWir bezeichnen mit c19 (fÃ¼r â€œChance 1â€) das VerhÃ¤ltnis von erhÃ¶hter Temperatur zu nicht erhÃ¶hter Temperatur im 19. Jahrhundert.\n\nc19 <- 1 / 19\n\nMit c20 bezeichnen wir die analoge Chance fÃ¼r das 20. Jahrhundert:\n\nc20 <- 56 / 67\n\nDas VerhÃ¤ltnis der beiden Chancen gibt das ChancenverhÃ¤ltnis (Odds Ratio, OR):\n\nc19 / c20\n\n[1] 0.06296992\n\n\nGenauso gut kann man das OR von c20 zu c19 ausrechnen, der Effekt bleibt identisch:\n\nc20 / c19\n\n[1] 15.8806\n\n\nIn beiden FÃ¤llen ist es ein Faktor von knapp 16.\n\nCategories:\n~"
  },
  {
    "objectID": "posts/randomdag1/randomdag1.html",
    "href": "posts/randomdag1/randomdag1.html",
    "title": "randomdag1",
    "section": "",
    "text": "Gegeben sei der DAG g (s. u.). Der DAG verfÃ¼gt Ã¼ber \\(n = 6\\) Variablen, die als Knoten im Graph dargestellt sind und mit \\(x_1, x_2, \\ldots x_n\\) bezeichnet sind.\nWelche minimale Variablenmenge muss kontrolliert werden, um den kausalen Effekt von der UV zur AV zu identifizieren?\nUV: x2.\nAV: x6.\nHinweise:\n\nMengen sind mittels geschweifter Klammern gekennzeichnet, z.B. {x1, x2} meint die Menge mit den zwei Elementen x1 und x2.\nDie leere Menge { } bedeutet, dass keine Variable kontrolliert werden muss, um den kausalen Effekt zu identifizieren.\nAlle Variablen werden als gemessen vorausgesetzt.\nEs ist mÃ¶glich, dass es keine LÃ¶sung gibt, dass es also keine Adjustierungsmenge gibt, um den kausalen Effekt zu identifizieren. Wenn dies der Fall sein sollte, wÃ¤hlen Sie â€œ/â€.\nEs ist mÃ¶glich, dass einzelne Variablen keine Kanten besitzen, also keine Verbindung zu anderen Variablen (Knoten) haben. Diese Variablen sind dann kausal unabhÃ¤ngig von den Ã¼brigen Variablen.\n\n\n\n\n\n\n\n\n\n\n\n\n\n{ x1, x2 }\n{ x3, x6 }\n{ x1, x6 }\n{ x2, x3 }\n{ }"
  },
  {
    "objectID": "posts/randomdag1/randomdag1.html#answerlist-1",
    "href": "posts/randomdag1/randomdag1.html#answerlist-1",
    "title": "randomdag1",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nFalsch\nRichtig\n\n\nCategories:\n\ncausal\ndag"
  },
  {
    "objectID": "posts/nasa02/nasa02.html",
    "href": "posts/nasa02/nasa02.html",
    "title": "nasa02",
    "section": "",
    "text": "Solution\nDekade berechnen:\n\nd <-\n  d %>% \n  mutate(decade = round(Year/10))\n\nKorrelation:\n\nd %>% \n  summarise(temp_cor = cor(Jan, Feb))\n\n# A tibble: 1 Ã— 1\n  temp_cor\n     <dbl>\n1    0.940\n\n\nKorrelation pro Dekade:\n\nd_summarized <- \n  d %>% \n  group_by(decade) %>% \n  summarise(temp_cor = cor(Jan, Feb))\n\n\n\n\n\n\n\n  \n  \n    \n      decade\n      temp_cor\n    \n  \n  \n    188\n0.87\n    189\n0.79\n    190\n0.53\n    191\n0.83\n    192\n0.82\n    193\n0.73\n    194\n0.50\n    195\n0.78\n    196\n0.56\n    197\n0.79\n    198\n0.80\n    199\n0.53\n    200\n0.56\n    201\n0.66\n    202\n0.96\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nDie Korrelation der Temperaturen und damit die Ã„hnlichkeit der Muster hat im Laufe der Dekaden immer mal wieder geschwankt.\n\nCategories:\n~"
  },
  {
    "objectID": "posts/griech-buchstaben/griech-buchstaben.html",
    "href": "posts/griech-buchstaben/griech-buchstaben.html",
    "title": "Griech-Buchstaben-Inferenz",
    "section": "",
    "text": "Solution\n\n\n\n\n\nKennwert\nStatistik\nParameter\n\n\n\n\nMittelwert\n\\[\\bar{X}\\]\n\\[\\mu\\]\n\n\nMittelwertsdifferenz\n\\[d=\\bar{X}_1-\\bar{X}_2\\]\n\\[\\mu_1\\]- \\[\\mu_2\\]\n\n\nStreuung\nsd\n\\[\\sigma\\]\n\n\nAnteil\np\n\\[\\pi\\]\n\n\nKorrelation\nr\n\\[\\rho\\]\n\n\nRegressionsgewicht\nb\n\\[\\beta\\]\n\n\n\n\n\n\nCategories:\n\nqm2\nqm2-thema01"
  },
  {
    "objectID": "posts/mtcars-simple2/mtcars-simple2.html",
    "href": "posts/mtcars-simple2/mtcars-simple2.html",
    "title": "mtcars-simple2",
    "section": "",
    "text": "Solution\nCompute Model:\n\nlm1_freq <- lm(mpg ~ hp + cyl + disp, data = mtcars)\n\nlibrary(rstanarm)\nlm1_bayes <- stan_glm(mpg ~ hp + cyl + disp, data = mtcars, refresh = 0)\n\nGet R2:\n\nlibrary(easystats)\n\n\nr2(lm1_freq)\n\n# R2 for Linear Regression\n       R2: 0.768\n  adj. R2: 0.743\n\n\n\nr2(lm1_bayes)\n\n# Bayesian R2 with Compatibility Interval\n\n  Conditional R2: 0.746 (95% CI [0.601, 0.859])\n\n\nThe coefficient is estimated as about 0.77.\n\nCategories:\n\nregresssion\nen\nbayes\nfrequentist\nqm1\nstats-nutshell"
  },
  {
    "objectID": "posts/purrr-map05/purrr-map05.html",
    "href": "posts/purrr-map05/purrr-map05.html",
    "title": "purrr-map05",
    "section": "",
    "text": "Exercise\nErstellen Sie eine Tabelle mit mit folgenden Spalten:\n\nID-Spalte: \\(1,2,..., 10\\)\nEine Spalte, in der jede Zelle eine Tabelle mit einem Vektor \\(x\\), einer standardnormalverteilten Zufallszahlen (n=1000), enthÃ¤lt\n\nBerechnen Sie den Mittelwert von jedem \\(x\\)! Diese Ergebnisse sollen als weitere Spalte der Tabelle hinzugefÃ¼gt werden.\n         \n\n\nSolution\n\nd <- tibble(\n  id = 1:10) %>% \n  mutate(x = map(id, ~ rnorm(n = 1e3))\n) \n\nstr(d)\n\ntibble [10 Ã— 2] (S3: tbl_df/tbl/data.frame)\n $ id: int [1:10] 1 2 3 4 5 6 7 8 9 10\n $ x :List of 10\n  ..$ : num [1:1000] 0.559 -0.588 -0.452 -1.966 1.329 ...\n  ..$ : num [1:1000] -0.54581 0.89293 -0.66738 1.26568 -0.00726 ...\n  ..$ : num [1:1000] -1.527 0.934 -0.289 0.508 1.108 ...\n  ..$ : num [1:1000] 0.0425 0.7778 -0.7187 -1.3656 -2.5356 ...\n  ..$ : num [1:1000] 1.323 -0.453 0.628 1.592 0.187 ...\n  ..$ : num [1:1000] -0.751 -0.482 0.439 1.804 0.971 ...\n  ..$ : num [1:1000] -0.268 0.182 1.62 0.372 -0.261 ...\n  ..$ : num [1:1000] 0.0568 0.8057 0.1397 0.907 0.4752 ...\n  ..$ : num [1:1000] 0.831 -0.743 -0.402 1.098 -0.905 ...\n  ..$ : num [1:1000] 0.987 -0.225 1.544 -1.503 -0.502 ...\n\n\nSo kann man sich die Mittelwerte ausgeben lassen:\n\nd$x %>% \n  map(mean)\n\n[[1]]\n[1] -0.03513212\n\n[[2]]\n[1] 0.01395107\n\n[[3]]\n[1] 0.01891006\n\n[[4]]\n[1] -0.02203389\n\n[[5]]\n[1] 0.05744261\n\n[[6]]\n[1] 0.004502295\n\n[[7]]\n[1] -0.00387942\n\n[[8]]\n[1] -0.01771766\n\n[[9]]\n[1] -0.06057087\n\n[[10]]\n[1] 0.01811559\n\n\nJetzt fÃ¼gen wir den letzten Schritt als Spalte hinzu:\n\nd2 <-\n  d %>% \n  mutate(x_mean = map_dbl(x, ~ mean(.x))) \n\nhead(d2)\n\n# A tibble: 6 Ã— 3\n     id x               x_mean\n  <int> <list>           <dbl>\n1     1 <dbl [1,000]> -0.0351 \n2     2 <dbl [1,000]>  0.0140 \n3     3 <dbl [1,000]>  0.0189 \n4     4 <dbl [1,000]> -0.0220 \n5     5 <dbl [1,000]>  0.0574 \n6     6 <dbl [1,000]>  0.00450\n\n\nHier hÃ¤tten wir auch schreiben kÃ¶nnen:\n\nd %>% \n  mutate(x_mean = map(x, mean)) %>% \n  unnest(x_mean) %>% \n  head()\n\n# A tibble: 6 Ã— 3\n     id x               x_mean\n  <int> <list>           <dbl>\n1     1 <dbl [1,000]> -0.0351 \n2     2 <dbl [1,000]>  0.0140 \n3     3 <dbl [1,000]>  0.0189 \n4     4 <dbl [1,000]> -0.0220 \n5     5 <dbl [1,000]>  0.0574 \n6     6 <dbl [1,000]>  0.00450\n\n\n\nCategories:\n~"
  },
  {
    "objectID": "posts/purrr-map02/purrr-map02.html",
    "href": "posts/purrr-map02/purrr-map02.html",
    "title": "purrr-map02",
    "section": "",
    "text": "Exercise\nBestimmen Sie die hÃ¤ufigsten Worte im Grundatzprogramm der Partei AfD (in der aktuellsten Version).\n         \n\n\nSolution\nText aus PDF-Dateien kann man mit dem Paket pdftools einlesen:\n\nlibrary(pdftools)\n\nUsing poppler version 22.02.0\n\nd_path <- \"/Users/sebastiansaueruser/Google Drive/Literatur/_Div/Politik/afd-grundsatzprogramm-2022.pdf\"\n\nd <- tibble(text = pdf_text(d_path))\n\nDann erstellen wir eine Tidy-Version und tokenisieren nach WÃ¶rtern:\n\nlibrary(tidytext)\nd2 <-\n  d %>% \n  unnest_tokens(output = word, input = text)\n\nhead(d2)\n\n# A tibble: 6 Ã— 1\n  word             \n  <chr>            \n1 programm         \n2 fÃ¼r              \n3 deutschland      \n4 das              \n5 grundsatzprogramm\n6 der              \n\n\nDann zÃ¤hlen wir die WÃ¶rter:\n\nd2 %>% \n  count(word, sort = TRUE) %>% \n  head(20)\n\n# A tibble: 20 Ã— 2\n   word            n\n   <chr>       <int>\n 1 die          1151\n 2 und          1147\n 3 der           870\n 4 zu            435\n 5 fÃ¼r           392\n 6 in            392\n 7 den           271\n 8 von           257\n 9 ist           251\n10 das           225\n11 werden        214\n12 eine          211\n13 nicht         196\n14 ein           191\n15 deutschland   190\n16 sind          187\n17 wir           176\n18 afd           171\n19 des           169\n20 sich          158\n\n\n\nCategories:\n\nr\nmap\ntidyverse"
  },
  {
    "objectID": "posts/twitter03/twitter03.html",
    "href": "posts/twitter03/twitter03.html",
    "title": "twitter03",
    "section": "",
    "text": "Solution\n\nlibrary(tidyverse)\n\nâ”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 1.3.2 â”€â”€\nâœ” ggplot2 3.3.6      âœ” purrr   0.3.5 \nâœ” tibble  3.1.8      âœ” dplyr   1.0.10\nâœ” tidyr   1.2.1      âœ” stringr 1.4.1 \nâœ” readr   2.1.3      âœ” forcats 0.5.2 \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\n\nlibrary(rtweet)\n\n\nAttaching package: 'rtweet'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\n\nEinloggen bei Twitter; zuerst die Credentials bereithalten:\n\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\n\nDann anmelden:\n\nauth <- rtweet_app(bearer_token = Bearer_Token)\n\nTweets an Karl Lauterbach suchen:\n\nkarl1 <- search_tweets(\"@karl_lauterbach min_faves:100 OR min_retweets:100\", n = 10)\n\n\nkarl1 %>% \n  select(retweet_count, favorite_count)\n\n# A tibble: 10 Ã— 2\n   retweet_count favorite_count\n           <int>          <int>\n 1            56            210\n 2            56            229\n 3            44           1626\n 4            60            225\n 5            30            494\n 6             5            148\n 7            27            435\n 8            12            178\n 9            13            162\n10            46            375\n\nCategories:\n\ntextmining\ntwitter"
  },
  {
    "objectID": "posts/mtcars-simple3/mtcars-simple3.html",
    "href": "posts/mtcars-simple3/mtcars-simple3.html",
    "title": "mtcars-simple3",
    "section": "",
    "text": "We will use the dataset mtcars in this exercise.\nAssume your causal model of your research dictates that fuel economy is a linear function of horse power, cylinder count and displacement of the engine.\nWhich of the predictors in the above model has the weakest causal impact on the output variable?\nNotes:\n\nUse can either use frequentist or bayesian modeling.\nUse R for all computations.\nThere are multiple ways to find a solution.\n\n\n\n\ncyl\nhp\ndisp\nAll are equally strong\nnone of the above"
  },
  {
    "objectID": "posts/mtcars-simple3/mtcars-simple3.html#answerlist-1",
    "href": "posts/mtcars-simple3/mtcars-simple3.html#answerlist-1",
    "title": "mtcars-simple3",
    "section": "Answerlist",
    "text": "Answerlist\n\nwrong\ncorrect\nwrong\nwrong\nwrong\n\n\nCategories:\n\nregresssion\nen\nbayes\nfrequentist\nqm1\nstats-nutshell"
  },
  {
    "objectID": "posts/ttest-als-regr/ttest-als-regr.html",
    "href": "posts/ttest-als-regr/ttest-als-regr.html",
    "title": "ttest-als-regr",
    "section": "",
    "text": "Der t-Test kann als Spezialfall der Regressionsanalyse gedeutet werden.\nHierbei ist es wichtig, sich das Skalenniveau der Variablen, die ein t-Test verarbeitet, vor Augen zu fÃ¼hren.\nHinweisse:\n\nDie folgende Abbildung gibt Tipps.\nInformationen, die zur LÃ¶sung einer Aufgabe nicht nÃ¶tig sind, sollte man ignorieren.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBenennen Sie die Skalenniveaus der UV eines t-Tests! Geben Sie nur ein Wort ein. Verwenden Sie nur Kleinbuchstaben (z.B. regression).\nBenennen Sie die Skalenniveaus der AV eines t-Tests! Geben Sie nur ein Wort ein. Verwenden Sie nur Kleinbuchstaben (z.B. regression).\nNennen Sie eine beispielhafte Forschungsfrage fÃ¼r einen t-Test.\nSkizzieren Sie ein Diagramm einer Regression, die analytisch identisch (oder sehr Ã¤hnlich) zu einem t-Test ist!"
  },
  {
    "objectID": "posts/ttest-als-regr/ttest-als-regr.html#answerlist-1",
    "href": "posts/ttest-als-regr/ttest-als-regr.html#answerlist-1",
    "title": "ttest-als-regr",
    "section": "Answerlist",
    "text": "Answerlist\n\nUV: binÃ¤r\nAV: metrisch\nUnterscheiden sich die mittleren Einparkzeiten von Frauen und MÃ¤nnern?\nAus dem Datensatz mtcars:\n\n\ndata(mtcars)\nmtcars %>% \n  ggplot() +\n  aes(x = am, y = mpg) +\n  geom_point(alpha = .5) +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nCategories:\n\nregr\nttest\nvariable-levels"
  },
  {
    "objectID": "posts/log-y-regr1/log-y-regr1.html",
    "href": "posts/log-y-regr1/log-y-regr1.html",
    "title": "log-y-regr1",
    "section": "",
    "text": "Solution\n\nd2 <-\n  d %>% \n  filter(re74 > 0) %>% \n  mutate(re74_log = log(re74))\n\n\nm <- lm(re74_log ~ educ, data = d2)\n\nHier sind die parameters des Modells.\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(2327)\np\n\n\n\n\n(Intercept)\n8.83\n0.06\n(8.70, 8.95)\n142.91\n< .001\n\n\neduc\n0.07\n4.94e-03\n(0.07, 0.08)\n15.16\n< .001\n\n\n\n\nFÃ¼r jedes Jahr Bildung steigt das Einkommen also ca. um den Faktor 1.07.\nEtwas genauer:\n\\(\\hat{\\beta_1} = 0.07\\) bedeutet, dass ein Jahr Bildung zu einen erwarteten Unterschied im Einkommen in HÃ¶he von 0.07 in Log-Einkommen fÃ¼hrt. Anders gesagt wird das Einkommen um exp(0.07) erhÃ¶ht. Dabei gilt \\(e^{0.07} \\approx 1.07\\):\n\nexp(0.07)\n\n[1] 1.072508\n\n\nDie LÃ¶sung lautet also: â€œPro Jahr Bildung steigt das Einkommen - laut Modell um den Faktor ca. 1.07â€.\nMan darf dabei nicht vergessen, dass wir wir uns hier auf die Schnelle ein Modell ausgedacht haben. Ob es in Wirklichkeit so ist, wie unser Modell meint, ist eine andere Sache!\n\nCategories:\n\nregression\nlm\nqm2\nstats-nutshell"
  },
  {
    "objectID": "posts/adjustieren2/adjustieren2.html",
    "href": "posts/adjustieren2/adjustieren2.html",
    "title": "adjustieren2",
    "section": "",
    "text": "Solution\n\n\n\n\nUnser Modell lm1 schÃ¤tzt den Preis eines Diamanten mittlerer GrÃ¶ÃŸe auf etwa 3932.5 (was immer auch die Einheiten sind, Dollar vermutlich).\nprice ~ carat_z + cut\n\nDieses zweite Modell kÃ¶nnten wir so berechnen:\n\nlm2 <- stan_glm(price ~ carat_z + cut, data = diamonds,\n                chains = 1,\n                refresh = 0)\nparameters(lm2)\n\nParameter    |  Median |             95% CI |   pd | % in ROPE |  Rhat |     ESS |                       Prior\n--------------------------------------------------------------------------------------------------------------\n(Intercept)  | 2406.07 | [2334.35, 2488.89] | 100% |        0% | 1.000 |  321.00 | Normal (3932.80 +- 9973.60)\ncarat_z      | 7870.55 | [7843.93, 7897.58] | 100% |        0% | 1.005 | 1047.00 |   Normal (0.00 +- 21040.85)\ncutGood      | 1118.79 | [1027.17, 1203.09] | 100% |        0% | 0.999 |  434.00 |   Normal (0.00 +- 34685.38)\ncutIdeal     | 1799.71 | [1714.31, 1869.75] | 100% |        0% | 0.999 |  338.00 |   Normal (0.00 +- 20362.28)\ncutPremium   | 1437.68 | [1353.65, 1512.30] | 100% |        0% | 0.999 |  351.00 |   Normal (0.00 +- 22862.49)\ncutVery Good | 1508.84 | [1422.69, 1582.24] | 100% |        0% | 1.000 |  344.00 |   Normal (0.00 +- 23922.15)\n\n\nEin â€œnormalesâ€ (frequentistisches) lm kÃ¤me zu Ã¤hnlichen Ergebnissen:\n\nlm(price ~ carat_z + cut, data = diamonds)\n\n\nCall:\nlm(formula = price ~ carat_z + cut, data = diamonds)\n\nCoefficients:\n (Intercept)       carat_z       cutGood      cutIdeal    cutPremium  \n        2405          7871          1120          1801          1439  \ncutVery Good  \n        1510  \n\n\nMan kÃ¶nnte hier noch einen Interaktionseffekt ergÃ¤nzen, wenn man Grund zur Annahme hat, dass es einen gibt.\n\nCategories:\n\nqm2\nâ€˜2022â€™"
  },
  {
    "objectID": "posts/Griech-Buchstaben-Inferenz/Griech-Buchstaben-Inferenz.html",
    "href": "posts/Griech-Buchstaben-Inferenz/Griech-Buchstaben-Inferenz.html",
    "title": "Griech-Buchstaben-Inferenz",
    "section": "",
    "text": "Solution\n\n\n\n\n\nKennwert\nStatistik\nParameter\n\n\n\n\nMittelwert\n\\[\\bar{X}\\]\n\\[\\mu\\]\n\n\nMittelwertsdifferenz\n\\[d=\\bar{X}_1-\\bar{X}_2\\]\n\\[\\mu_1\\]- \\[\\mu_2\\]\n\n\nStreuung\nsd\n\\[\\sigma\\]\n\n\nAnteil\np\n\\[\\pi\\]\n\n\nKorrelation\nr\n\\[\\rho\\]\n\n\nRegressionsgewicht\nb\n\\[\\beta\\]\n\n\n\n\n\n\nCategories:\n\nqm2\nqm2-thema01"
  },
  {
    "objectID": "posts/twitter02/twitter02.html",
    "href": "posts/twitter02/twitter02.html",
    "title": "twitter02",
    "section": "",
    "text": "Solution\n\nlibrary(tidyverse)\n\nâ”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 1.3.2 â”€â”€\nâœ” ggplot2 3.3.6      âœ” purrr   0.3.5 \nâœ” tibble  3.1.8      âœ” dplyr   1.0.10\nâœ” tidyr   1.2.1      âœ” stringr 1.4.1 \nâœ” readr   2.1.3      âœ” forcats 0.5.2 \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\n\nlibrary(rtweet)\n\n\nAttaching package: 'rtweet'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\n\nEinloggen bei Twitter; zuerst die Credentials bereithalten:\n\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\n\n\nauth <- rtweet_app(bearer_token = Bearer_Token)\n\nAus der Hilfe zu search_tweets:\nDescription\nReturns Twitter statuses matching a user provided search query. ONLY RETURNS DATA FROM THE PAST 6-9 DAYS.\nTweets an Karl Lauterbach suchen:\n\nkarl_tweets <- search_tweets(q = \"@karl_lauterbach\", n = 150000, retryonratelimit = TRUE)\n\nWir kÃ¶nnten n auch auf Inf setzen, aber, da wir auf das Refreshen des Rate Limits warten mÃ¼ssen, kÃ¶nnte sehr lange dauern. Daher nehmen wir hier nur einen kÃ¼rzeren Wert.\n\ndim(karl_tweets)\n\n[1] 18000    43\n\nhead(karl_tweets)\n\n# A tibble: 6 Ã— 43\n  created_at               id id_str      full_â€¦Â¹ truncâ€¦Â² displâ€¦Â³ entities     metadâ€¦â´\n  <dttm>                <dbl> <chr>       <chr>   <lgl>     <dbl> <list>       <list> \n1 2022-10-23 13:30:18 1.58e18 1584145185â€¦ \"Bei â¦@â€¦ FALSE       122 <named list> <df>   \n2 2022-10-22 18:34:37 1.58e18 1583859379â€¦ \"Es isâ€¦ FALSE       263 <named list> <df>   \n3 2022-10-22 17:56:39 1.58e18 1583849826â€¦ \"Die Sâ€¦ FALSE       215 <named list> <df>   \n4 2022-10-24 08:10:35 1.58e18 1584427113â€¦ \"Zu weâ€¦ FALSE       219 <named list> <df>   \n5 2022-10-24 08:10:35 1.58e18 1584427113â€¦ \"RT @Kâ€¦ FALSE       140 <named list> <df>   \n6 2022-10-24 08:10:25 1.58e18 1584427072â€¦ \"RT @Uâ€¦ FALSE       139 <named list> <df>   \n# â€¦ with 35 more variables: source <chr>, in_reply_to_status_id <dbl>,\n#   in_reply_to_status_id_str <chr>, in_reply_to_user_id <dbl>,\n#   in_reply_to_user_id_str <chr>, in_reply_to_screen_name <chr>, geo <list>,\n#   coordinates <list>, place <list>, contributors <lgl>, is_quote_status <lgl>,\n#   retweet_count <int>, favorite_count <int>, favorited <lgl>, retweeted <lgl>,\n#   possibly_sensitive <lgl>, lang <chr>, quoted_status_id <dbl>,\n#   quoted_status_id_str <chr>, quoted_status <list>, retweeted_status <list>, â€¦\n# â„¹ Use `colnames()` to see all variable names\n\nCategories:\n\ntextmining\ntwitter"
  },
  {
    "objectID": "posts/purrr-map03/purrr-map03.html",
    "href": "posts/purrr-map03/purrr-map03.html",
    "title": "purrr-map03",
    "section": "",
    "text": "Exercise\nImportieren Sie das Grundatzprogramm der Partei AfD (in der aktuellsten Version). Tokenisieren Sie nach SÃ¤tzen. Dann entfernen Sie alle Zahlen. Dann zÃ¤hlen Sie die Anzahl der WÃ¶rter pro Satz und berichten gÃ¤ngige deskriptive Statistiken dazu.\n         \n\n\nSolution\nText aus PDF-Dateien kann man mit dem Paket pdftools einlesen:\n\nlibrary(pdftools)\n\nUsing poppler version 22.02.0\n\nd_path <- \"/Users/sebastiansaueruser/Google Drive/Literatur/_Div/Politik/afd-grundsatzprogramm-2022.pdf\"\n\nd <- tibble(text = pdf_text(d_path))\n\nDann erstellen wir eine Tidy-Version und tokenisieren nach SÃ¤tzen:\n\nlibrary(tidytext)\nd2 <-\n  d %>% \n  unnest_sentences(output = word, input = text)\n\nhead(d2)\n\n# A tibble: 6 Ã— 1\n  word                                                                          \n  <chr>                                                                         \n1 programm fÃ¼r deutschland.                                                     \n2 das grundsatzprogramm der alternative fÃ¼r deutschland.                        \n3 2   programm fÃ¼r deutschland | inhalt         prÃ¤ambel                       â€¦\n4 familien stÃ¤rken        43             und parteiferne rechnungshÃ¶fe         â€¦\n5 3   programm fÃ¼r deutschland | inhalt         7 | kultur, sprache und identitâ€¦\n6 fÃ¶rder- und                         10.10.3 deutsche literatur im inland digiâ€¦\n\n\nDann entfernen wir die Zahlen:\n\nd3 <- \n  d2 %>% \n  mutate(word = str_remove_all(word, pattern = \"[:digit:]+\"))\n\nPrÃ¼fen wir, ob es geklappt hat:\n\nd2$word[10]\n\n[1] \"weniger subventionen    88      13.7 fischerei, forst und jagd: im einklang mit der natur     88      13.8 flÃ¤chenkonkurrenz:           nicht zu lasten der land- und forstwirtschaft            88\"\n\nd3$word[10]\n\n[1] \"weniger subventionen          . fischerei, forst und jagd: im einklang mit der natur           . flÃ¤chenkonkurrenz:           nicht zu lasten der land- und forstwirtschaft            \"\n\n\nOk.\nDann zÃ¤hlen wir die WÃ¶rter pro Satz:\n\nd4 <- \n  d3 %>% \n  summarise(word_count_per_sentence = str_count(word, \"\\\\w+\"))\n\nhead(d4)\n\n# A tibble: 6 Ã— 1\n  word_count_per_sentence\n                    <int>\n1                       3\n2                       6\n3                     196\n4                      40\n5                     254\n6                      15\n\n\nVisualisierung:\n\nd4 %>% \n  ggplot(aes(x = word_count_per_sentence)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.5.2 (red = needs update)\nâœ” insight     0.18.4    âœ– datawizard  0.6.1  \nâœ” bayestestR  0.13.0    âœ– performance 0.9.2  \nâœ– parameters  0.18.2    âœ– effectsize  0.7.0.5\nâœ” modelbased  0.8.5     âœ– correlation 0.8.2  \nâœ” see         0.7.3     âœ” report      0.5.5  \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`.\n\ndescribe_distribution(d4)\n\nVariable                |  Mean |    SD | IQR |          Range | Skewness | Kurtosis |    n | n_Missing\n-------------------------------------------------------------------------------------------------------\nword_count_per_sentence | 21.86 | 17.24 |  19 | [0.00, 254.00] |     3.84 |    37.52 | 1208 |         0\n\n\n\nCategories:\n\nr\nmap\ntidyverse"
  },
  {
    "objectID": "posts/purrr-map04/purrr-map04.html",
    "href": "posts/purrr-map04/purrr-map04.html",
    "title": "purrr-map04",
    "section": "",
    "text": "Exercise\nImportieren Sie das Grundatzprogramm der Partei AfD (in der aktuellsten Version). Tokenisieren Sie nach Seiten. Dann verschachteln Sie die Spalte, in denen der Text der Seite steht, zu einer Listenspalte. SchlieÃŸlich zÃ¤hlen Sie die Anzahl der WÃ¶rter pro Seite und berichten gÃ¤ngige deskriptive Statistiken dazu.\n         \n\n\nSolution\nText aus PDF-Dateien kann man mit dem Paket pdftools einlesen:\n\nlibrary(pdftools)\n\nUsing poppler version 22.02.0\n\nd_path <- \"/Users/sebastiansaueruser/Google Drive/Literatur/_Div/Politik/afd-grundsatzprogramm-2022.pdf\"\n\nd <- tibble(text = pdf_text(d_path),\n            page = 1:length(text))\n\nZu Seiten tokenisieren brauchen wir nicht; das Datenmaterial ist bereits nach Seiten organisiert.\nJetzt â€œverschachtelnâ€ (to nest) wir die Spalte mit dem Text:\n\nd2 <-\n  d %>% \n  nest(data = text)\n\nhead(d2)\n\n# A tibble: 6 Ã— 2\n   page data            \n  <int> <list>          \n1     1 <tibble [1 Ã— 1]>\n2     2 <tibble [1 Ã— 1]>\n3     3 <tibble [1 Ã— 1]>\n4     4 <tibble [1 Ã— 1]>\n5     5 <tibble [1 Ã— 1]>\n6     6 <tibble [1 Ã— 1]>\n\n\nDann zÃ¤hlen wir die WÃ¶rter pro Seite:\n\nd3 <-\n  d2 %>% \n  mutate(word_count_per_page = map(data, ~ str_count(.x$text, \"\\\\w+\")))\n\nhead(d3)\n\n# A tibble: 6 Ã— 3\n   page data             word_count_per_page\n  <int> <list>           <list>             \n1     1 <tibble [1 Ã— 1]> <int [1]>          \n2     2 <tibble [1 Ã— 1]> <int [1]>          \n3     3 <tibble [1 Ã— 1]> <int [1]>          \n4     4 <tibble [1 Ã— 1]> <int [1]>          \n5     5 <tibble [1 Ã— 1]> <int [1]>          \n6     6 <tibble [1 Ã— 1]> <int [1]>          \n\n\nWie sieht eine Zelle aus data aus?\n\nd3$data[[1]]\n\n# A tibble: 1 Ã— 1\n  text                                                                          \n  <chr>                                                                         \n1 \"PROGRAMM FÃœR\\nDEUTSCHLAND.\\nDas Grundsatzprogramm der Alternative fÃ¼r Deutscâ€¦\n\n\nWie sieht eine Zelle aus word_count_per_page aus?\n\nd3$data[[1]]\n\n# A tibble: 1 Ã— 1\n  text                                                                          \n  <chr>                                                                         \n1 \"PROGRAMM FÃœR\\nDEUTSCHLAND.\\nDas Grundsatzprogramm der Alternative fÃ¼r Deutscâ€¦\n\n\nAh! Darin steckt nur eine einzelne Zahl!\n\nd3$data[[1]] %>% str()\n\ntibble [1 Ã— 1] (S3: tbl_df/tbl/data.frame)\n $ text: chr \"PROGRAMM FÃœR\\nDEUTSCHLAND.\\nDas Grundsatzprogramm der Alternative fÃ¼r Deutschland.\\n\"\n\n\nDas heiÃŸt, wir kÃ¶nnen vereinfachen, entschacheln:\n\nd4 <-\n  d3 %>% \n  unnest(word_count_per_page)\n\nhead(d4)\n\n# A tibble: 6 Ã— 3\n   page data             word_count_per_page\n  <int> <list>                         <int>\n1     1 <tibble [1 Ã— 1]>                   9\n2     2 <tibble [1 Ã— 1]>                 410\n3     3 <tibble [1 Ã— 1]>                 516\n4     4 <tibble [1 Ã— 1]>                 297\n5     5 <tibble [1 Ã— 1]>                   1\n6     6 <tibble [1 Ã— 1]>                 414\n\n\nVisualisierung:\n\nd4 %>% \n  ggplot(aes(x = word_count_per_page)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.5.2 (red = needs update)\nâœ” insight     0.18.4    âœ– datawizard  0.6.1  \nâœ” bayestestR  0.13.0    âœ– performance 0.9.2  \nâœ– parameters  0.18.2    âœ– effectsize  0.7.0.5\nâœ” modelbased  0.8.5     âœ– correlation 0.8.2  \nâœ” see         0.7.3     âœ” report      0.5.5  \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`.\n\ndescribe_distribution(d4$word_count_per_page)\n\n  Mean |     SD |    IQR |          Range | Skewness | Kurtosis |  n | n_Missing\n--------------------------------------------------------------------------------\n285.84 | 172.27 | 322.75 | [1.00, 516.00] |    -0.64 |    -1.24 | 96 |         0\n\n\n\nCategories:\n\nr\nmap\ntidyverse"
  },
  {
    "objectID": "posts/Stichprobenziehen1/Stichprobenziehen1.html",
    "href": "posts/Stichprobenziehen1/Stichprobenziehen1.html",
    "title": "Stichprobenziehen1",
    "section": "",
    "text": "Solution\nIndividuell\n\nCategories:\n\nlm\ninference\nqm2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "datenwerk",
    "section": "",
    "text": "r\n\n\nmap\n\n\ntidyverse\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\nloop\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntextmining\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\nloop\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nmap\n\n\ntidyverse\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntextmining\n\n\ntwitter\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntextmining\n\n\ntwitter\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nmap\n\n\ntidyverse\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nmap\n\n\ntidyverse\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nqm2\n\n\ninference\n\n\nlm\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\neda\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\neda\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\neda\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncausal\n\n\ndag\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncorrelation\n\n\nlm\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nqm2\n\n\ninference\n\n\nuncertainty\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nqm2\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nttest\n\n\nregr\n\n\nvariable-levels\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregresssion\n\n\nen\n\n\nbayes\n\n\nfrequentist\n\n\nqm1\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstats-nutshell\n\n\nqm2\n\n\nregression\n\n\nlog\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nqm2\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nqm2\n\n\nlm\n\n\nbayes\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nbayes\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlm\n\n\ninference\n\n\nqm2\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninference\n\n\nlm\n\n\nqm2\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregresssion\n\n\nen\n\n\nbayes\n\n\nfrequentist\n\n\nqm1\n\n\nstats-nutshell\n\n\nqm2\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregresssion\n\n\nen\n\n\nbayes\n\n\nfrequentist\n\n\nqm1\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregresssion\n\n\nen\n\n\nbayes\n\n\nfrequentist\n\n\nqm1\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregr\n\n\nttest\n\n\nvariable-levels\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nqm2\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nqm2\n\n\nbayes\n\n\nadjust\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nqm2\n\n\ninference\n\n\nparameters\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlm\n\n\ninference\n\n\nqm2\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nqm2\n\n\nqm2-thema01\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Hallo, Datenwerk",
    "section": "",
    "text": "Autor: Sebastian Sauer\nDer Quellcode findet sich hier.\nDie Lizenz ist permissiv, s. Hinweise hier."
  }
]