[
  {
    "objectID": "posts/kausal02/kausal02.html",
    "href": "posts/kausal02/kausal02.html",
    "title": "kausal02",
    "section": "",
    "text": "Gegeben sei der DAG g (s.u.). Welche Variable/n sind zu kontrollieren, um den kausalen Effekt von x auf y zu identifizieren?\n\n\n\n\n\n\n\n\n\n\n\n\nz\nkeine, bereits identifiziert\nx\ny\nkeine, nicht identifizierbar"
  },
  {
    "objectID": "posts/kausal02/kausal02.html#answerlist",
    "href": "posts/kausal02/kausal02.html#answerlist",
    "title": "kausal02",
    "section": "",
    "text": "z\nkeine, bereits identifiziert\nx\ny\nkeine, nicht identifizierbar"
  },
  {
    "objectID": "posts/kausal02/kausal02.html#answerlist-1",
    "href": "posts/kausal02/kausal02.html#answerlist-1",
    "title": "kausal02",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal\nexam-22"
  },
  {
    "objectID": "posts/kekse02/kekse02.html",
    "href": "posts/kekse02/kekse02.html",
    "title": "kekse02",
    "section": "",
    "text": "Exercise\nIn Think Bayes stellt Allen Downey folgende Aufgabe:\n“Next let’s solve a cookie problem with 101 bowls:\nBowl 0 contains 0% vanilla cookies,\nBowl 1 contains 1% vanilla cookies,\nBowl 2 contains 2% vanilla cookies,\nand so on, up to\nBowl 99 contains 99% vanilla cookies, and\nBowl 100 contains all vanilla cookies.\nAs in the previous version, there are only two kinds of cookies, vanilla and chocolate. So Bowl 0 is all chocolate cookies, Bowl 1 is 99% chocolate, and so on.\nSuppose we choose a bowl at random, choose a cookie at random, and it turns out to be vanilla. What is the probability that the cookie came from Bowl \\(x\\), for each value of \\(x\\)?”\nHinweise:\n\nUntersuchen Sie die Hypothesen \\(\\pi_0 = 0, \\pi_1 = 0.1, \\pi_2 = 0.2, ..., \\pi_{10} = 1\\) für die Trefferwahrscheinlichkeit\nErstellen Sie ein Bayes-Gitter zur Lösung dieser Aufgabe.\nGehen Sie davon aus, dass Sie (apriori) indifferent gegenüber der Hypothesen zu den Parameterwerten sind.\nGeben Sie Prozentzahlen immer als Anteil an und lassen Sie die führende Null weg (z.B. .42).\n\n         \n\n\nSolution\n\n\n\n\n\np_Gitter\nPriori\nLikelihood\nunstd_Post\nPost\n\n\n\n\n0.00\n1\n0.00\n0.00\n0.00\n\n\n0.01\n1\n0.01\n0.01\n0.00\n\n\n0.02\n1\n0.02\n0.02\n0.00\n\n\n0.03\n1\n0.03\n0.03\n0.00\n\n\n0.04\n1\n0.04\n0.04\n0.00\n\n\n0.05\n1\n0.05\n0.05\n0.00\n\n\n0.06\n1\n0.06\n0.06\n0.00\n\n\n0.07\n1\n0.07\n0.07\n0.00\n\n\n0.08\n1\n0.08\n0.08\n0.00\n\n\n0.09\n1\n0.09\n0.09\n0.00\n\n\n0.10\n1\n0.10\n0.10\n0.00\n\n\n0.11\n1\n0.11\n0.11\n0.00\n\n\n0.12\n1\n0.12\n0.12\n0.00\n\n\n0.13\n1\n0.13\n0.13\n0.00\n\n\n0.14\n1\n0.14\n0.14\n0.00\n\n\n0.15\n1\n0.15\n0.15\n0.00\n\n\n0.16\n1\n0.16\n0.16\n0.00\n\n\n0.17\n1\n0.17\n0.17\n0.00\n\n\n0.18\n1\n0.18\n0.18\n0.00\n\n\n0.19\n1\n0.19\n0.19\n0.00\n\n\n0.20\n1\n0.20\n0.20\n0.00\n\n\n0.21\n1\n0.21\n0.21\n0.00\n\n\n0.22\n1\n0.22\n0.22\n0.00\n\n\n0.23\n1\n0.23\n0.23\n0.00\n\n\n0.24\n1\n0.24\n0.24\n0.00\n\n\n0.25\n1\n0.25\n0.25\n0.00\n\n\n0.26\n1\n0.26\n0.26\n0.01\n\n\n0.27\n1\n0.27\n0.27\n0.01\n\n\n0.28\n1\n0.28\n0.28\n0.01\n\n\n0.29\n1\n0.29\n0.29\n0.01\n\n\n0.30\n1\n0.30\n0.30\n0.01\n\n\n0.31\n1\n0.31\n0.31\n0.01\n\n\n0.32\n1\n0.32\n0.32\n0.01\n\n\n0.33\n1\n0.33\n0.33\n0.01\n\n\n0.34\n1\n0.34\n0.34\n0.01\n\n\n0.35\n1\n0.35\n0.35\n0.01\n\n\n0.36\n1\n0.36\n0.36\n0.01\n\n\n0.37\n1\n0.37\n0.37\n0.01\n\n\n0.38\n1\n0.38\n0.38\n0.01\n\n\n0.39\n1\n0.39\n0.39\n0.01\n\n\n0.40\n1\n0.40\n0.40\n0.01\n\n\n0.41\n1\n0.41\n0.41\n0.01\n\n\n0.42\n1\n0.42\n0.42\n0.01\n\n\n0.43\n1\n0.43\n0.43\n0.01\n\n\n0.44\n1\n0.44\n0.44\n0.01\n\n\n0.45\n1\n0.45\n0.45\n0.01\n\n\n0.46\n1\n0.46\n0.46\n0.01\n\n\n0.47\n1\n0.47\n0.47\n0.01\n\n\n0.48\n1\n0.48\n0.48\n0.01\n\n\n0.49\n1\n0.49\n0.49\n0.01\n\n\n0.50\n1\n0.50\n0.50\n0.01\n\n\n0.50\n1\n0.50\n0.50\n0.01\n\n\n0.51\n1\n0.51\n0.51\n0.01\n\n\n0.52\n1\n0.52\n0.52\n0.01\n\n\n0.53\n1\n0.53\n0.53\n0.01\n\n\n0.54\n1\n0.54\n0.54\n0.01\n\n\n0.55\n1\n0.55\n0.55\n0.01\n\n\n0.56\n1\n0.56\n0.56\n0.01\n\n\n0.57\n1\n0.57\n0.57\n0.01\n\n\n0.58\n1\n0.58\n0.58\n0.01\n\n\n0.59\n1\n0.59\n0.59\n0.01\n\n\n0.60\n1\n0.60\n0.60\n0.01\n\n\n0.61\n1\n0.61\n0.61\n0.01\n\n\n0.62\n1\n0.62\n0.62\n0.01\n\n\n0.63\n1\n0.63\n0.63\n0.01\n\n\n0.64\n1\n0.64\n0.64\n0.01\n\n\n0.65\n1\n0.65\n0.65\n0.01\n\n\n0.66\n1\n0.66\n0.66\n0.01\n\n\n0.67\n1\n0.67\n0.67\n0.01\n\n\n0.68\n1\n0.68\n0.68\n0.01\n\n\n0.69\n1\n0.69\n0.69\n0.01\n\n\n0.70\n1\n0.70\n0.70\n0.01\n\n\n0.71\n1\n0.71\n0.71\n0.01\n\n\n0.72\n1\n0.72\n0.72\n0.01\n\n\n0.73\n1\n0.73\n0.73\n0.01\n\n\n0.74\n1\n0.74\n0.74\n0.01\n\n\n0.75\n1\n0.75\n0.75\n0.02\n\n\n0.76\n1\n0.76\n0.76\n0.02\n\n\n0.77\n1\n0.77\n0.77\n0.02\n\n\n0.78\n1\n0.78\n0.78\n0.02\n\n\n0.79\n1\n0.79\n0.79\n0.02\n\n\n0.80\n1\n0.80\n0.80\n0.02\n\n\n0.81\n1\n0.81\n0.81\n0.02\n\n\n0.82\n1\n0.82\n0.82\n0.02\n\n\n0.83\n1\n0.83\n0.83\n0.02\n\n\n0.84\n1\n0.84\n0.84\n0.02\n\n\n0.85\n1\n0.85\n0.85\n0.02\n\n\n0.86\n1\n0.86\n0.86\n0.02\n\n\n0.87\n1\n0.87\n0.87\n0.02\n\n\n0.88\n1\n0.88\n0.88\n0.02\n\n\n0.89\n1\n0.89\n0.89\n0.02\n\n\n0.90\n1\n0.90\n0.90\n0.02\n\n\n0.91\n1\n0.91\n0.91\n0.02\n\n\n0.92\n1\n0.92\n0.92\n0.02\n\n\n0.93\n1\n0.93\n0.93\n0.02\n\n\n0.94\n1\n0.94\n0.94\n0.02\n\n\n0.95\n1\n0.95\n0.95\n0.02\n\n\n0.96\n1\n0.96\n0.96\n0.02\n\n\n0.97\n1\n0.97\n0.97\n0.02\n\n\n0.98\n1\n0.98\n0.98\n0.02\n\n\n0.99\n1\n0.99\n0.99\n0.02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCategories:\n\nprobability\nbayes-grid"
  },
  {
    "objectID": "posts/Logikpruefung2/Logikpruefung2.html",
    "href": "posts/Logikpruefung2/Logikpruefung2.html",
    "title": "Logikpruefung2",
    "section": "",
    "text": "Aufgabe\nWir definieren x wie folgt:\n\nx &lt;- c(-1, 0, 1)\n\nGeben Sie die Syntax an, für die Prüfung, ob x positiv ist.\n         \n\n\nLösung\n\nx &gt; 0\n\n[1] FALSE FALSE  TRUE\n\n\n\nCategories:\n\nR\n‘2023’\nLogikpruefung2"
  },
  {
    "objectID": "posts/kausal05/kausal05.html",
    "href": "posts/kausal05/kausal05.html",
    "title": "kausal05",
    "section": "",
    "text": "Im Rahmen einer Studie soll untersucht werden, ob eine Influenza-Infektion einen (kausalen) Einfluss auf eine Covid19-Infektion hat.\nIn Wahrheit (aber unbekannt) sei der DAG wie folgt (s.u.).\n\n\n\n\n\n\n\n\n\nIst es sinnvoll, das Auftreten von Fieber (Fever) zu kontrollieren?\n\n\n\nNein, da durch eine Kontrolle von Fever eine Verzerrung erzeugt wird (Kollisionsverzerrung)\nJa, durch eine Kontrolle von Fever ist ein kausaler Effekt identifizierbar\nJa, eine Kontrolle von Fever ist zwar nicht nötig, aber wird zu exakteren Ergebnissen führen\nNein, da eine Kontrolle von Fever eine Verzerrung erzeugt wird (Konfundierung)\nNein, da eine Kontrolle von Fever nicht nötig ist (aber auch nicht schädlich)"
  },
  {
    "objectID": "posts/kausal05/kausal05.html#answerlist",
    "href": "posts/kausal05/kausal05.html#answerlist",
    "title": "kausal05",
    "section": "",
    "text": "Nein, da durch eine Kontrolle von Fever eine Verzerrung erzeugt wird (Kollisionsverzerrung)\nJa, durch eine Kontrolle von Fever ist ein kausaler Effekt identifizierbar\nJa, eine Kontrolle von Fever ist zwar nicht nötig, aber wird zu exakteren Ergebnissen führen\nNein, da eine Kontrolle von Fever eine Verzerrung erzeugt wird (Konfundierung)\nNein, da eine Kontrolle von Fever nicht nötig ist (aber auch nicht schädlich)"
  },
  {
    "objectID": "posts/kausal05/kausal05.html#answerlist-1",
    "href": "posts/kausal05/kausal05.html#answerlist-1",
    "title": "kausal05",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/Bayes-Ziel1/Bayes-Ziel1.html",
    "href": "posts/Bayes-Ziel1/Bayes-Ziel1.html",
    "title": "Bayes-Ziel1",
    "section": "",
    "text": "Was ist nicht Ziel oder Gegenstand einer Bayes-Analyse?\n\n\n\nupdating beliefs\nquantifying uncertainty\nincluding prior knowledge of the domain, possibly of subjective nature\ndrawing inferential conclusions solely based on the likelihood"
  },
  {
    "objectID": "posts/Bayes-Ziel1/Bayes-Ziel1.html#answerlist",
    "href": "posts/Bayes-Ziel1/Bayes-Ziel1.html#answerlist",
    "title": "Bayes-Ziel1",
    "section": "",
    "text": "updating beliefs\nquantifying uncertainty\nincluding prior knowledge of the domain, possibly of subjective nature\ndrawing inferential conclusions solely based on the likelihood"
  },
  {
    "objectID": "posts/Bayes-Ziel1/Bayes-Ziel1.html#answerlist-1",
    "href": "posts/Bayes-Ziel1/Bayes-Ziel1.html#answerlist-1",
    "title": "Bayes-Ziel1",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nWahr\n\n\nCategories:\n\nregression\nbayes"
  },
  {
    "objectID": "posts/Likelihood2/Likelihood2.html",
    "href": "posts/Likelihood2/Likelihood2.html",
    "title": "Likelihood2",
    "section": "",
    "text": "Der Likelihood eines Datensatzes ist definiert als das Produkt der Likelihoods aller Beobachtungen:\n\\[\\mathcal{L} = \\prod_{i=1}^n \\mathcal{L_i}\\]\nwobei die Beobachtungen bzw. ihre Likelihood als unabhängig angenommen werden: \\(\\mathcal{L_i} \\perp \\mathcal{L_j}, \\quad i \\ne j\\).\nJe größer \\(n\\), desto …….. \\(\\mathcal{L}\\)!\nFüllen Sie die Lücke!\n\n\n\ngrößer\nkleiner\nunabhängig voneinander\nkeine Aussage möglich\nkommt auf weitere, hier nicht benannte Bedingungen an"
  },
  {
    "objectID": "posts/Likelihood2/Likelihood2.html#answerlist",
    "href": "posts/Likelihood2/Likelihood2.html#answerlist",
    "title": "Likelihood2",
    "section": "",
    "text": "größer\nkleiner\nunabhängig voneinander\nkeine Aussage möglich\nkommt auf weitere, hier nicht benannte Bedingungen an"
  },
  {
    "objectID": "posts/Likelihood2/Likelihood2.html#answerlist-1",
    "href": "posts/Likelihood2/Likelihood2.html#answerlist-1",
    "title": "Likelihood2",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nRichtig\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nregression\nbayes\nlikelihood"
  },
  {
    "objectID": "posts/Kennwert-robust/Kennwert-robust.html",
    "href": "posts/Kennwert-robust/Kennwert-robust.html",
    "title": "Kennwert-robust",
    "section": "",
    "text": "Question"
  },
  {
    "objectID": "posts/Kennwert-robust/Kennwert-robust.html#answerlist",
    "href": "posts/Kennwert-robust/Kennwert-robust.html#answerlist",
    "title": "Kennwert-robust",
    "section": "Answerlist",
    "text": "Answerlist\n\nMedian\nMittelwert\nKorrelation\nStandardabweichung\nVarianz\nMaximalwert\nMinimalwert"
  },
  {
    "objectID": "posts/Kennwert-robust/Kennwert-robust.html#answerlist-1",
    "href": "posts/Kennwert-robust/Kennwert-robust.html#answerlist-1",
    "title": "Kennwert-robust",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\neda\nlagemaß\nschoice"
  },
  {
    "objectID": "posts/Bayesmod-bestimmen01/Bayesmod-bestimmen01.html",
    "href": "posts/Bayesmod-bestimmen01/Bayesmod-bestimmen01.html",
    "title": "Bayesmod-bestimmen01",
    "section": "",
    "text": "Exercise\nSie möchten, im Rahmen einer Studie, ein einfaches lineare Modell spezifizieren, d.h. den Likelihood und die Priori-Verteilungen benennen.\nFolgende Informationen sind gegeben:\n\nAV: einnahmen\nUV: werbebudget\nAlle empirischen Variablen sind z-standardisiert.\nAlle Variablen sollen als normalverteilt angegeben werden mit Ausnahme der Streuung der AV, diese ist exponenzialverteilt mit Rate 1 zu modellieren.\nStreuungen der Normalverteilung sind mit 2.5 SD anzugeben.\n\nSchreiben Sie in mathematischer Notation folgende Notation auf:\nDie Priori-Verteilung des Regressionsgewichts\nHinweise:\n\nVerzichten Sie auf Leerstellen in Ihrer Antwort. \nBenennen Sie \\(\\beta\\) mit b, \\(\\alpha\\) mit a und \\(\\sigma\\) mit s.\nNutzen Sie die Tilde ~ um stochastische Relationen (Verteilungen) anzuzeigen.\nGeben Sie Normalverteilungen als Normal(x;y) und Exponentialverteilung als Exp(x) an (jeweils mit den korrekten Argumenten in der allgemein üblichen Form).\n\n         \n\n\nSolution\nb~Normal(0, 2.5)\n\nCategories:\n\nregression\nbayes\nprior"
  },
  {
    "objectID": "posts/boxhist/boxhist.html",
    "href": "posts/boxhist/boxhist.html",
    "title": "boxhist",
    "section": "",
    "text": "boxhist.csv draw a histogram, and a boxplot. Based on the graphics, answer the following questions or check the correct statements, respectively. (Comment: The tolerance for numeric answers is \\(\\pm0.1\\), the true/false statements are either about correct or clearly wrong.)\n\n\n\nThe distribution is unimodal.\nThe distribution is not unimodal.\nThe distribution is symmetric.\nThe distribution is right-skewed.\nThe distribution is left-skewed.\nThe boxplot shows outliers.\nThe boxplot shows no outliers.\nA quarter of the observations is smaller than which value?\nA quarter of the observations is greater than which value?\nHalf of the observations are smaller than which value?"
  },
  {
    "objectID": "posts/boxhist/boxhist.html#answerlist",
    "href": "posts/boxhist/boxhist.html#answerlist",
    "title": "boxhist",
    "section": "",
    "text": "The distribution is unimodal.\nThe distribution is not unimodal.\nThe distribution is symmetric.\nThe distribution is right-skewed.\nThe distribution is left-skewed.\nThe boxplot shows outliers.\nThe boxplot shows no outliers.\nA quarter of the observations is smaller than which value?\nA quarter of the observations is greater than which value?\nHalf of the observations are smaller than which value?"
  },
  {
    "objectID": "posts/boxhist/boxhist.html#answerlist-1",
    "href": "posts/boxhist/boxhist.html#answerlist-1",
    "title": "boxhist",
    "section": "Answerlist",
    "text": "Answerlist\n\nTrue.\nFalse.\nTrue.\nFalse.\nFalse.\nFalse.\nTrue.\n2.66.\n3.36.\n3.06.\n\n\nCategories:\n\nvis\neda\nen\ncloze"
  },
  {
    "objectID": "posts/Anteil-Apple/Anteil-Apple.html",
    "href": "posts/Anteil-Apple/Anteil-Apple.html",
    "title": "Anteil-Apple",
    "section": "",
    "text": "Exercise\nZählen Sie, wie viele der Studentis im Raum mindestens ein Apple-Gerät besitzen (iPhone, Macbook,…).\nBerechnen Sie die Posteriori-Verteilung mit der Grid-Methode!\nHinweise:\n\nErstellen Sie eine Bayes-Box (Gittermethode).\nFalls Sie keine Erhebung durchführen können oder wollen, erfinden Sie Zahlen.\nVisualisieren Sie die Post-Verteilung\n\n         \n\n\nSolution\nWir berechnen die Posteriori-Verteilung:\n\nlibrary(tidyverse)\nd &lt;-\n  tibble(\n    p_grid = seq(0,1, by = .01),\n    prior= 1,\n    Likelihood = dbinom(x = 9,\n                        size = 12,\n                        prob = p_grid),\n    post_unstand = prior * Likelihood,\n    post_stand = post_unstand / sum(post_unstand)\n  )\n\nhead(d)\n\n# A tibble: 6 × 5\n  p_grid prior Likelihood post_unstand post_stand\n   &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1   0        1   0            0          0       \n2   0.01     1   2.13e-16     2.13e-16   2.78e-17\n3   0.02     1   1.06e-13     1.06e-13   1.38e-14\n4   0.03     1   3.95e-12     3.95e-12   5.14e-13\n5   0.04     1   5.10e-11     5.10e-11   6.63e-12\n6   0.05     1   3.68e-10     3.68e-10   4.79e-11\n\n\nVisualisieren der Posteriori-Verteilung:\n\nd %&gt;% \n  ggplot(aes(x = p_grid, y = post_stand)) +\n  geom_point() +\n  geom_line()\n\n\n\n\n\n\n\n\n\nCategories:\n\nbayes\nbayes-grid"
  },
  {
    "objectID": "posts/Post-befragen1/Post-befragen1.html",
    "href": "posts/Post-befragen1/Post-befragen1.html",
    "title": "Post-befragen1",
    "section": "",
    "text": "Welcher R-Code passt am besten, um folgende Frage aus der Post-Verteilung herauszulesen:\n\nWie wahrscheinlich ist es, dass die mittlere Größe bei mind. 155 cm liegt?\n\nHinweise:\n\na ist der Achsenabschnitt, b ist das Regressionsgewicht.\npost_tab_df ist eine Tabelle (in Form eines R-Dataframe), die die Stichproben aus der Post-Verteilung enthält.\nEs handelt sich um Regressionsmodell, das mit der Bayes-Methode berechnet wurde.\nDer bzw. die Prädiktoren sind zentriert.\nEs handelt sich um den Datensatz aus McElreath’ Lehrbuch (Statistical Rethinking).\n\nCode A\n\npost_tab_df %&gt;% \n  count(gross = a == 155) %&gt;% \n  mutate(prop = n / sum(n))\n\nCode B\n\npost_tab_df %&gt;% \n\n  count(gross = a &gt; 155) %&gt;% \n  mutate(prop = n / sum(n))\n\nCode C\n\npost_tab_df %&gt;% \n  count(gross = a &lt;= 155) %&gt;% \n  mutate(prop = n / sum(n))\n\nCode D\n\npost_tab_df %&gt;% \n  count(gross = a &gt;= 155) %&gt;% \n  mutate(prop = n / sum(n))\n\nCode E\n\npost_tab_df %&gt;% \n  count(gross = a &lt; 155) %&gt;% \n  mutate(prop = n / sum(n))\n\n\n\n\nCode A\nCode B\nCode C\nCode D\nCode E"
  },
  {
    "objectID": "posts/Post-befragen1/Post-befragen1.html#answerlist",
    "href": "posts/Post-befragen1/Post-befragen1.html#answerlist",
    "title": "Post-befragen1",
    "section": "",
    "text": "Code A\nCode B\nCode C\nCode D\nCode E"
  },
  {
    "objectID": "posts/Post-befragen1/Post-befragen1.html#answerlist-1",
    "href": "posts/Post-befragen1/Post-befragen1.html#answerlist-1",
    "title": "Post-befragen1",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nWahr\nFalsch\n\n\nCategories:\n\nregression\nbayes\nposterior"
  },
  {
    "objectID": "posts/Postvert-Regr-01/Postvert-Regr-01.html",
    "href": "posts/Postvert-Regr-01/Postvert-Regr-01.html",
    "title": "Postvert-Regr-01",
    "section": "",
    "text": "Nach der Berechnung bzw. Schätzung der Modellparameter eines Regressionsmodells (mit Methoden der Bayes-Inferenz) erhält man u.a. auf die Prädiktorwerte \\(x_i\\) (\\(i=1,2,...,n\\)) bedingte Wahrscheinlichkeiten \\(p_i\\) für die AV, \\(y_i\\), oder genauer \\(y_i|x_i,\\theta\\) (mit \\(\\theta\\) für die Modellparameter).\nBetrachten Sie dazu folgende Aussage:\n\\(Pr(y_i|x_i, \\alpha, \\beta, \\sigma) = c\\) für \\(i=1,2,...,n\\)\nWelche der Aussagen ist in diesem Zusammenhang falsch?\n\n\n\nDas Regressionsmodell hat 3 Parameter.\nDas Regressionsmodell hat 1 Prädiktor (im Sinne von 1 Inputvariablen).\n\\(Pr(y_1|x_1, \\alpha, \\beta, \\sigma) &gt; Pr(y_2|x_2, \\alpha, \\beta, \\sigma)\\)\n\\(\\sum_{y_i = -\\infty}^{+\\infty} Pr(y_i|x_i, \\alpha, \\beta, \\sigma) = 1\\)\n\\(Pr(y_i|x_i, \\alpha, \\beta, \\sigma) = p_i, \\qquad p_i \\in [0,1]\\)"
  },
  {
    "objectID": "posts/Postvert-Regr-01/Postvert-Regr-01.html#answerlist",
    "href": "posts/Postvert-Regr-01/Postvert-Regr-01.html#answerlist",
    "title": "Postvert-Regr-01",
    "section": "",
    "text": "Das Regressionsmodell hat 3 Parameter.\nDas Regressionsmodell hat 1 Prädiktor (im Sinne von 1 Inputvariablen).\n\\(Pr(y_1|x_1, \\alpha, \\beta, \\sigma) &gt; Pr(y_2|x_2, \\alpha, \\beta, \\sigma)\\)\n\\(\\sum_{y_i = -\\infty}^{+\\infty} Pr(y_i|x_i, \\alpha, \\beta, \\sigma) = 1\\)\n\\(Pr(y_i|x_i, \\alpha, \\beta, \\sigma) = p_i, \\qquad p_i \\in [0,1]\\)"
  },
  {
    "objectID": "posts/Postvert-Regr-01/Postvert-Regr-01.html#answerlist-1",
    "href": "posts/Postvert-Regr-01/Postvert-Regr-01.html#answerlist-1",
    "title": "Postvert-Regr-01",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch. Das Modell hat tatsächlich der zu schätzende Parameter: \\(\\alpha, \\beta, \\sigma\\).\nFalsch. Das Modell hat tatsächlich einen Prädiktor, \\(x_i\\).\nWahr. Die Aussage ist nicht grundsätzlich richtig.\nFalsch. Die Wahrscheinlichkeiten für alle möglichen \\(y\\) für eine bestimmte Person summiert sich tatsächlich zu 1 auf.\nFalsch. Eine Warhscheinlichkeit kann tatsächlich zwischen 0 und 1 liegen, wobei die Grenzen nur in Extremfällen vorkommen.\n\n\nCategories:\n\nregression\nbayes\nposterior"
  },
  {
    "objectID": "posts/kausal04/kausal04.html",
    "href": "posts/kausal04/kausal04.html",
    "title": "kausal04",
    "section": "",
    "text": "Gegeben sei ein DAG g (s.u.). Was ist die minimale Menge an Variablen (minimal adjustment set), die man kontrollieren muss, um den kausalen Effekt von smoking auf arrest zu identifizieren?\n\n\n\n\n\n\n\n\n\n\n\n\n{ Cholestorol }\n{ Weight }\nkeine, da nicht identifiziferbar\n{ Cholestrol, Unhealty Lifestyle }\n{ Cholestorol, Weight }"
  },
  {
    "objectID": "posts/kausal04/kausal04.html#answerlist",
    "href": "posts/kausal04/kausal04.html#answerlist",
    "title": "kausal04",
    "section": "",
    "text": "{ Cholestorol }\n{ Weight }\nkeine, da nicht identifiziferbar\n{ Cholestrol, Unhealty Lifestyle }\n{ Cholestorol, Weight }"
  },
  {
    "objectID": "posts/kausal04/kausal04.html#answerlist-1",
    "href": "posts/kausal04/kausal04.html#answerlist-1",
    "title": "kausal04",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/kausal03/kausal03.html",
    "href": "posts/kausal03/kausal03.html",
    "title": "kausal03",
    "section": "",
    "text": "Gegeben sei der DAG g (s.u.). Was ist die minimale Menge an Variablen, die man kontrollieren muss, um den kausalen Effekt von x auf y zu identifizieren?\n\n\n\n\n\n\n\n\n\nHinweise:\n\nGebogene Kurven mit doppelter Pfeilspitze zeigen keine Kausaleinflüsse ein (was in DAGs nicht erlaubt wäre).\nStattdessen zeigen Sie eine Assoziation bedingt durch eine (nicht aufgeführte) Konfundierungsvariable an.\n\n\n\n\n{ w1, w2, z2 }\n{ w2, z2 }\n{ w1, w2 }\n{ w1, z2 }\n{ w1 }"
  },
  {
    "objectID": "posts/kausal03/kausal03.html#answerlist",
    "href": "posts/kausal03/kausal03.html#answerlist",
    "title": "kausal03",
    "section": "",
    "text": "{ w1, w2, z2 }\n{ w2, z2 }\n{ w1, w2 }\n{ w1, z2 }\n{ w1 }"
  },
  {
    "objectID": "posts/kausal03/kausal03.html#answerlist-1",
    "href": "posts/kausal03/kausal03.html#answerlist-1",
    "title": "kausal03",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal\nexam-22"
  },
  {
    "objectID": "posts/max-corr1/max-corr1.html",
    "href": "posts/max-corr1/max-corr1.html",
    "title": "max-corr1",
    "section": "",
    "text": "Aufgabe\nWelches Diagramm zeigt den stärksten (absoluten) linearen Zusammenhang (Korrelation)?\nGeben Sie die Nummer ein, die in der Kopfzeile jedes Teildiagramms angezeigt wird.\n         \n\n\nLösung\n\n\n\n\n\n\n\n\n\n\nCategories:\n\nvis\n‘2023’\nnum"
  },
  {
    "objectID": "posts/kausal-bedrooms1/kausal-bedrooms1.html",
    "href": "posts/kausal-bedrooms1/kausal-bedrooms1.html",
    "title": "kausal-bedrooms1",
    "section": "",
    "text": "Exercise\nBetrachten wir den Datensatz SaratogaHouses, den Sie hier herunterladen können. Ein Codebook findet sich hier.\nSie kommen auch so an die Daten ran:\n\nlibrary(mosaicData)\ndata(\"SaratogaHouses\")\n\nGegeben sei in diesem Zusammenhang folgender DAG:\n\ndag1 &lt;- \"\ndag{\na -&gt; p\na -&gt; b -&gt; p\n}\n\"\n\nWobei a für (living) area steht, also der Wohnfläche eines Hauses, b für bedrooms, der Anzahl der Schlafzimmer und p für prize, den Preis, den das Haus beim Verkauf erzielt hat.\nSo sieht das dann aus:\n\nggdag(dag1) + theme_dag()\n\n\n\n\n\n\n\n\nUV sei a; AV sei p.\n\nBerechnen Sie den direkten Effekt der Wohnfläche auf den Preis!\nBerechnen Sie den totalen Effekt der Wohnfläche auf den Preis!\n\nHinweise: - Mit direkter Effekt ist der kausale Effekt von UV auf AV - ohne Zwischenglieder (Mediatoren) - gemeint. - Mit indirekter Effekt ist der kausale Effekt von UV über einen (oder ggf. mehrere) Mediator(en) auf die AV gemeint. - Mit totaler Effekt ist die Summe des direkten plus des oder der indirekten Effekte gemeint. - Geben Sie jeweils den Punktschätzer eines linearen Regressionsmodells an! - Gehen Sie vom oben genannten DAG aus. - Runden Sie ohne Dezimalstellen.\n         \n\n\nSolution\n\nd &lt;-\n  SaratogaHouses %&gt;% \n  select(price, bedrooms, livingArea) %&gt;% \n  drop_na()\n\n\ndirekter Effekt:\n\n\ndirekter_eff_lm &lt;-\n  stan_glm(price ~ bedrooms + livingArea, \n           data = d,\n           refresh = 0)\ncoef(direkter_eff_lm)\n\n(Intercept)    bedrooms  livingArea \n 36852.3683 -14263.9997    125.4254 \n\n\nUm einen direkten Effekt zu berechnen, müssen wir den spezifischen, uniquen Effekt der UV berechnen. Das erreichen wir durch eine multiple Regression, in der also die übrigen Prädiktoren aufgenommen sind. Das Resultat ist ein Koeffizient für die Assoziation der UV mit der AV, bereinigt um die Zusammenhänge der übrigen Prädiktoren.\nZur Erinnerung: Die multiple Regression liefert Koeffizienten pro Prädiktor, die bereinigt sind um den (statistischen) Einfluss der anderen Prädiktoren, mit anderne Worten: die Koeffizienten der multiplen Regression zeigen den Effekt von “nur diesem Prädiktor”.\nDer Punktschätzer für den direkten Effekt (von Wohnfläche) ist:\n\ndirekter_eff &lt;-\n  coef(direkter_eff_lm)[3] %&gt;% \n  round(0)\n\ndirekter_eff\n\nlivingArea \n       125 \n\n\n\ntotaler Effekt:\n\n\n\n(Intercept)  livingArea \n 13388.0637    113.1216 \n\n\nDer totale Effekt lässt sich berechnen, in dem man keine weiteren Prädiktoren neben der UV in die Regression mitaufnimmt. Die einfache (univariate) Regression zeigt den totalen Effekt der UV auf die AV.\nDer Punktschätzer für den totalen Effekt beträgt:\n\n\nlivingArea \n       113 \n\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/variation01/variation01.html",
    "href": "posts/variation01/variation01.html",
    "title": "variation01",
    "section": "",
    "text": "In welchem Datensatz gibt es mehr Variation?\nDatensatz A:\n\n\n\n\n\nx1\n\n\n\n\n0.14\n\n\n-0.06\n\n\n0.04\n\n\n0.06\n\n\n0.04\n\n\n-0.01\n\n\n0.15\n\n\n-9.47e-03\n\n\n0.20\n\n\n-6.27e-03\n\n\n\n\n\nDatensatz B:\n\n\n\n\n\nx2\n\n\n\n\n1.30\n\n\n2.29\n\n\n-1.39\n\n\n-0.28\n\n\n-0.13\n\n\n0.64\n\n\n-0.28\n\n\n-2.66\n\n\n-2.44\n\n\n1.32\n\n\n\n\n\nDatensatz C:\n\n\n\n\n\nx3\n\n\n\n\n-3.07\n\n\n-17.81\n\n\n-1.72\n\n\n12.15\n\n\n18.95\n\n\n-4.30\n\n\n-2.57\n\n\n-17.63\n\n\n4.60\n\n\n-6.40\n\n\n\n\n\nDatensatz D:\n\n\n\n\n\nx4\n\n\n\n\n45.55\n\n\n70.48\n\n\n103.51\n\n\n-60.89\n\n\n50.50\n\n\n-171.70\n\n\n-78.45\n\n\n-85.09\n\n\n-241.42\n\n\n3.61\n\n\n\n\n\n\n\n\nA\nB\nC\nD"
  },
  {
    "objectID": "posts/variation01/variation01.html#answerlist",
    "href": "posts/variation01/variation01.html#answerlist",
    "title": "variation01",
    "section": "",
    "text": "A\nB\nC\nD"
  },
  {
    "objectID": "posts/variation01/variation01.html#answerlist-1",
    "href": "posts/variation01/variation01.html#answerlist-1",
    "title": "variation01",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nWahr\n\n\nCategories:\n\nvariation\nbasics\nschoice"
  },
  {
    "objectID": "posts/mw-berechnen/mw-berechnen.html",
    "href": "posts/mw-berechnen/mw-berechnen.html",
    "title": "mw-berechnen",
    "section": "",
    "text": "Question\n\nAufgabe\nBerechnen Sie den Mittelwert folgender Zahlenreihe; ignorieren sie etwaige fehlende Werte. Runden Sie auf zwei Dezimalstellen.\n\n\n[1]  0.84  0.81  1.02  0.25 -0.18\n\n\n         \n\n\nLösung\nDer Mittelwert liegt bei 0.55.\nDie Antwort lautet 0.55.\nIn R kann man den Mittelwert z.B. so berechnen:\n\nmean(zahlenreihe, na.rm = TRUE)\n\n[1] 0.548\n\n\nDas Argument na.rm = TRUE sorgt dafür, dass R auch bei Vorhandensein fehlender Werte ein Ergebnis ausgibt. Ohne dieses Argument würde R ein sprödes NA zurückgeben, falls fehlende Werte vorliegen. Dieses Verhalten von R ist recht defensiv, getreu dem Motto: Wenn es ein Problem gibt, sollte man so früh wie möglich darüber deutlich informiert werden (und nicht erst, wenn die Marsrakete gestartet ist…).\n\nCategories:\n\neda\ndata-wrangling\ndyn\nnum"
  },
  {
    "objectID": "posts/IQ-Studentis/IQ-Studentis.html",
    "href": "posts/IQ-Studentis/IQ-Studentis.html",
    "title": "IQ-Studentis",
    "section": "",
    "text": "Exercise\nIntelligenz von Studentis\nEine Psychologin möchte die Intelligenz von Studentis bestimmen: Was ist wohl der Mittelwert? Wie schlau sind die schlausten 10%? Von wo bis wo geht das mittlere 90%-Intervall von IQ-Werten? Natürlich ist ihr klar, dass es nicht reicht, einen Mittelwert zu schätzen. Nein, sie will alles, sprich: die Posteriori-Verteilung.\nZuerst überlegt sie sich die Prioris: “Was ist meine Einschätzung zur Intelligenz von Studentis?”. Dazu liest sie alle verfügbare Literatur, beurteilt die methodische Qualität jeder einzelnen Studie und spricht mit den Expertis. Auf dieser Basis kommt sie zu folgenden Prioris:\n\\[\\mu \\sim \\mathcal{N}(115, 5)\\] Ein paar Überlegungen, die unsere Psychologin dazu hatte: Die Studentis sind im Mittel schlauer als die Normalbevölkerung. Um ein Gefühl für die Verteilungsfunktion vom IQ zu bekommen, nutzt sie folgenden R-Befehl:\n\npnorm(q = 115, mean = 100, sd = 15)\n\n[1] 0.8413447\n\n\nDieser Befehl gibt ihr an, welcher Prozentsatz der allgemeinen Bevölkerung (die Wahrscheinlichkeitsmasse) nicht schlauer ist als 115.\nDann versucht sie ein Gefühl für die Streuung (\\(\\sigma\\)) zu bekommen, folgender R-Befehl hilft ihr:\n\nq_iq &lt;- 50\nrate_lambda &lt;- 0.1\npexp(q = q_iq, rate = rate_lambda)\n\n[1] 0.9932621\n\n\nAh! Nimmt man an, dass Sigma exponentialverteilt ist mit einer Rate von 0.1, dass sind etwa 99 Prozent der Leute nicht mehr als q_iq IQ-Punkte vom Mittelwert \\(\\mu\\) entfernt. Das deckt sich mit ihren Informationen aus der Literatur.\nDamit sind die Priors spezifiziert.\n\nGeben Sie die Priors an.\nSimulieren Sie die Prior-Prädiktiv-Verteilung dazu.\nBefragen Sie die Prior-Prädiktiv-Verteilung mit geeigneten Fragen Ihrer Wahl.\n\n         \n\n\nSolution\n\nGeben Sie die Priors an.\n\n\\[\\mu \\sim \\mathcal{N}(115, 5)\\]\n\\[\\sigma \\sim \\mathcal{E}(0.1)\\]\n\nSimulieren Sie die Prior-Prädiktiv-Verteilung dazu.\n\nZiehen wir Zufallszahlen entsprechend der Priori-Werte:\n\nlibrary(tidyverse)\nn &lt;- 1e4\n\nsim &lt;-\n  tibble(\n    sample_mu = rnorm(n,\n      mean = 115,\n      sd   = 10\n    ),\n    sample_sigma = rexp(n,\n      rate = 0.1\n    ),\n    iq = rnorm(n,\n      mean = sample_mu,\n      sd   = sample_sigma\n    )\n  )\n\nWas ist wohl der Mittelwert und die SD dieser Priori-Prädiktiv-Verteilung?\n\nheight_sim_sd &lt;-\n  sd(sim$iq) %&gt;% round()\nheight_sim_sd\n\n[1] 17\n\n\n\nheight_sim_mean &lt;-\n  mean(sim$iq) %&gt;% round()\nheight_sim_mean\n\n[1] 115\n\n\nUnd jetzt plotten wir diese Verteilung:\n\nsim %&gt;%\n  ggplot() +\n  aes(x = iq) +\n  geom_histogram() +\n  geom_point(\n    y = 0, x = height_sim_mean, size = 5,\n    color = \"blue\", alpha = .5\n  ) +\n  geom_vline(\n    xintercept = c(\n      height_sim_mean + height_sim_sd,\n      height_sim_mean - height_sim_sd\n    ),\n    linetype = \"dotted\"\n  ) +\n  labs(caption = \"Der blaue Punkt zeigt den Mittelwert; die gepunkteten Linien MD±SD\") +\n  scale_x_continuous(\n    limits = c(70, 145),\n    breaks = seq(70, 145, by = 5)\n  )\n\n\n\n\n\n\n\n\nOder vielleicht besser als Dichte-Diagramm, das zeigt das “Big Picture” vielleicht besser:\n\nsim %&gt;%\n  ggplot() +\n  aes(x = iq) +\n  geom_density()\n\n\n\n\n\n\n\n\nHm, etwas randlastig die Verteilung.\nZoomen wir etwas mehr rein:\n\nsim %&gt;%\n  ggplot() +\n  aes(x = iq) +\n  geom_density() +\n  scale_x_continuous(limits = c(65, 165))\n\n\n\n\n\n\n\n\n\nBefragen Sie die Prior-Prädiktiv-Verteilung mit geeigneten Fragen Ihrer Wahl.\n\nWas ist der Mittelwert und die SD und die üblichen deskriptiven Kennwerte?\n\nlibrary(easystats)\n\n\nsim %&gt;%\n  select(iq) %&gt;%\n  describe_distribution()\n\nVariable |   Mean |    SD |   IQR |            Range | Skewness | Kurtosis |     n | n_Missing\n----------------------------------------------------------------------------------------------\niq       | 115.49 | 17.26 | 18.51 | [-22.36, 275.94] |     0.21 |     5.63 | 10000 |         0\n\n\nIn welchem Bereich liegen die mittleren 95% der IQ-Werte?\n\nsim %&gt;%\n  eti()\n\nEqual-Tailed Interval\n\nParameter    |         95% ETI\n------------------------------\nsample_mu    | [95.77, 134.90]\nsample_sigma | [ 0.23,  37.46]\niq           | [81.22, 150.08]\n\n\nAlternativ könnten wir in z-transformierten Daten denken:\n\nsim2 &lt;-\n  tibble(\n    sample_mu =\n      rnorm(n,\n        mean = 0,\n        sd   = 1\n      ),\n    sample_sigma =\n      rexp(n,\n        rate = 1\n      )\n  ) %&gt;%\n  mutate(\n    iq =\n      rnorm(n,\n        mean = sample_mu,\n        sd   = sample_sigma\n      )\n  )\n\n\nsim2 %&gt;%\n  ggplot() +\n  aes(x = iq) +\n  geom_density()\n\n\n\n\n\n\n\n\n\nCategories:\n\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/fattails02/fattails02.html",
    "href": "posts/fattails02/fattails02.html",
    "title": "fattails02",
    "section": "",
    "text": "Exercise\nIn seinem Buch “Statistical Consequences of Fat Tails” schreibt der Autor, Nassim Taleb (S. 53):\n\nIn the summer of 1998, the hedge fund called “Long Term Capital Management” (LTCM) proved to have a very short life; it went bust from some deviations in the markets –those “of an unexpected nature”. The loss was a yuuuge deal because two of the partners received the Swedish Riksbank Prize, marketed as the “Nobel” in economics. (…) At least two of the partners made the statement that it was a “10 sigma” event (10 standard deviations), hence they should be absolved of all accusations of incompetence (I was ﬁrst hand witness of two such statements).\n\nWir testen in diesem Zusammenhang zwei Hypothesen: \\(H_N\\), dass der Finanzmarkt normalverteilt ist und \\(H_F\\), dass die Variable fat tailed ist, also nicht normalverteilt, sondernn einer Verteilung entspringt, in der “Extremereignisse” üblicher sind als in einer Normalverteilung.\nUm die Fat-Tails-Verteilung mit \\(n=10\\) zu simulieren, nutzen wir hier folgende Funktion:\n\nfat_tail_data &lt;- rt(n = 100, df = 2)\n\nDabei bedeutet df = 1, dass die Verteilung sehr randlastig (fat tailed) sein soll (genauer gesagt eine t-Verteilung mit zwei Freiheitsgraden). Details dazu sollen uns hier nicht interessieren.\nBerechnen wir die Wahrscheinlichkeit, dass die Daten einer Normalverteilung entspringen (und nicht der Fat-Tail-Verteilung).\nDie Wahrscheinlichkeit eines 10-Sigma-Events ist übrigens … klein. Taleb berichtet sie mit \\(1.31 \\cdot 10^{-23}\\):\n\nL_norm &lt;- 1.31e-23\n\nFür die t-Verteilung ist der entsprechende Wert:\n\nL_fat &lt;- 1 - pt(q = 10, df = 2)\n\nWie hoch ist die Post-Wahrscheinlichkeit, dass die Variable normalverteilt ist?\nHinweise:\n\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\nApriori soll und die Hypothese der Normalverteilung 1000 Mal plausibler sein als die der t-Verteilung.\n\n\n\nAnswerlist\n\nkleiner als 50%\nkleiner als 5%\nkleiner als 0.5%\nkleiner als 0.05%\nkleiner als 0.005%\n\n         \n\n\nSolution\n\nlibrary(tidyverse)\n\nErstellen wir erstmal den ersten Teil einer Bayes-Box:\n\nd &lt;-\n  tibble(H = c(\"Normalverteilt\", \"Randlastig verteilt\"),\n         Prior = c(1e3,1))\n\nd\n\n# A tibble: 2 × 2\n  H                   Prior\n  &lt;chr&gt;               &lt;dbl&gt;\n1 Normalverteilt       1000\n2 Randlastig verteilt     1\n\n\nDann fügen wir den Likelihood jeder Hypothese dazu:\n\nd &lt;-\n  d %&gt;% \n  mutate(L = c(L_norm, L_fat))\n\nd\n\n# A tibble: 2 × 3\n  H                   Prior        L\n  &lt;chr&gt;               &lt;dbl&gt;    &lt;dbl&gt;\n1 Normalverteilt       1000 1.31e-23\n2 Randlastig verteilt     1 4.93e- 3\n\n\nDann berechnen wir die Post-Wahrscheinlichkeit:\n\nd &lt;-\n  d %&gt;% \n  mutate(Post_unstand = Prior * L,\n         Post = Post_unstand / sum(Post_unstand))\nd\n\n# A tibble: 2 × 5\n  H                   Prior        L Post_unstand     Post\n  &lt;chr&gt;               &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 Normalverteilt       1000 1.31e-23     1.31e-20 2.66e-18\n2 Randlastig verteilt     1 4.93e- 3     4.93e- 3 1   e+ 0\n\n\nDie Wahrscheinlichkeit, dass die Variable normalverteilt ist, ist seeeeehr klein, ca. \\(10^{-18}\\).\n\n\nAnswerlist\n\nFALSE\nFALSE\nFALSE\nFALSE\nTRUE\n\n\nCategories:\n\nprobability\nsimulation\nnormal-distribution"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-05/Verteilungen-Quiz-05.html",
    "href": "posts/Verteilungen-Quiz-05/Verteilungen-Quiz-05.html",
    "title": "Verteilungen-Quiz-05",
    "section": "",
    "text": "Ist folgende Aussage wahr?\nIst eine stetige Verteilung symmetrisch, gilt dann\n\\(Pr(X \\ge \\bar{x} + 1) = Pr(X \\le \\bar{x} - 1)\\).\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-05/Verteilungen-Quiz-05.html#answerlist",
    "href": "posts/Verteilungen-Quiz-05/Verteilungen-Quiz-05.html#answerlist",
    "title": "Verteilungen-Quiz-05",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-05/Verteilungen-Quiz-05.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-05/Verteilungen-Quiz-05.html#answerlist-1",
    "title": "Verteilungen-Quiz-05",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-02/Verteilungen-Quiz-02.html",
    "href": "posts/Verteilungen-Quiz-02/Verteilungen-Quiz-02.html",
    "title": "Verteilungen-Quiz-02",
    "section": "",
    "text": "Beziehen Sie sich auf den Standard-Globusversuch mit \\(N=9\\) Würfen und \\(W=6\\) Wassertreffern (binomialverteilt), vgl. hier.\nDie Stichproben-Postverteilung sieht so aus:\n\n\n\n\n\n\n\n\n\nIst es (auf dieser Basis) plausibler von einem 50%-PI [.6,.8] auszugehen als von einem 50%-PI [.05,.95]?\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-02/Verteilungen-Quiz-02.html#answerlist",
    "href": "posts/Verteilungen-Quiz-02/Verteilungen-Quiz-02.html#answerlist",
    "title": "Verteilungen-Quiz-02",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-02/Verteilungen-Quiz-02.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-02/Verteilungen-Quiz-02.html#answerlist-1",
    "title": "Verteilungen-Quiz-02",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/korr-als-regr/korr-als-regr.html",
    "href": "posts/korr-als-regr/korr-als-regr.html",
    "title": "korr-als-regr",
    "section": "",
    "text": "options(digits=2)\noptions(width = 80)\n\n\nExercise\nDie Korrelation prüft, ob zwei Merkmale linear zusammenhängen.\nWie viele andere Verfahren kann die Korrelation als ein Spezialfall der Regression bzw. des linearen Modells \\(y = \\beta_0 + \\beta_1 + \\ldots \\beta_n + \\epsilon\\) betrachtet werden.\nAls ein spezielles Beispiel betrachten wir die Frage, ob das Gewicht eines Diamanten (carat) mit dem Preis (price) zusammenhängt (Datensatz diamonds).\nDen Datensatz können Sie so laden:\n\nlibrary(tidyverse)\ndata(diamonds)\n\n\nGeben Sie das Skalenniveau beider Variablen an!\nBetrachten Sie die Ausgabe von R:\n\n\nlm1 &lt;- lm(price ~ carat, data = diamonds)\nsummary(lm1)\n\n\nCall:\nlm(formula = price ~ carat, data = diamonds)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-18585   -805    -19    537  12732 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -2256.4       13.1    -173   &lt;2e-16 ***\ncarat         7756.4       14.1     551   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1550 on 53938 degrees of freedom\nMultiple R-squared:  0.849, Adjusted R-squared:  0.849 \nF-statistic: 3.04e+05 on 1 and 53938 DF,  p-value: &lt;2e-16\n\n\nWie (bzw. wo) ist aus dieser Ausgabe die Korrelation herauszulesen?\n\nMacht es einen Unterschied, ob man Preis mit Karat bzw. Karat mit Preis korreliert?\nIn der klassischen Inferenzstatistik ist der \\(p\\)-Wert eine zentrale Größe; ist er klein (\\(p&lt;.05\\)) so nennt man die zugehörige Statistik signifikant und verwirft die getestete Hypothese.\nIm Folgenden sehen Sie einen Korrelationstest auf statistische Signifikanz, mit R durchgeführt. Zeigt der Test ein (statistisch) signifikantes Ergebnis? Wie groß ist der “Unsicherheitskorridor”, um den Korrelationswert (zugleich Punktschätzer für den Populationswert)?\n\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.5.2\n✔ insight     0.18.2     ✔ datawizard  0.5.1   \n✔ bayestestR  0.12.1.1   ✔ performance 0.9.2   \n✔ parameters  0.18.2     ✔ effectsize  0.7.0.5 \n✔ modelbased  0.8.5      ✔ correlation 0.8.2   \n✔ see         0.7.2      ✔ report      0.5.5   \n\ndiamonds %&gt;% \n  sample_n(30) %&gt;% \n  select(price, carat) %&gt;% \n  correlation()\n\n# Correlation Matrix (pearson-method)\n\nParameter1 | Parameter2 |    r |       95% CI | t(28) |         p\n-----------------------------------------------------------------\nprice      |      carat | 0.94 | [0.88, 0.97] | 14.87 | &lt; .001***\n\np-value adjustment method: Holm (1979)\nObservations: 30\n\n\n         \n\n\nSolution\n\ncarat ist metrisch (verhältnisskaliert) und price ist metrisch (verhältnisskaliert)\n\\(R^2\\) kann bei einer einfachen (univariaten) Regression als das Quadrat von \\(r\\) berechnet werden. Daher \\(r = \\sqrt{R^2}\\).\n\n\nsqrt(0.8493)\n\n[1] 0.92\n\n\nZum Vergleich\n\ndiamonds %&gt;% \n  summarise(r = cor(price, carat))\n\n# A tibble: 1 × 1\n      r\n  &lt;dbl&gt;\n1 0.922\n\n\nMan kann den Wert der Korrelation auch noch anderweitig berechnen (\\(\\beta\\) umrechnen in \\(\\rho\\)).\n\nNein. Die Korrelation ist eine symmetrische Relation.\nJa; die Zahl “3.81e-14” bezeichnet eine positive Zahl kleiner eins mit 13 Nullern vor der ersten Ziffer, die nicht Null ist (3.81 in diesem Fall). Der “Unsicherheitskorridor” reicht von etwa 0.87 bis 0.97.\n\n\nCategories:\n\ncorrelation\nlm"
  },
  {
    "objectID": "posts/Typ-Fehler-R-01/Typ-Fehler-R-01.html",
    "href": "posts/Typ-Fehler-R-01/Typ-Fehler-R-01.html",
    "title": "Typ-Fehler-R-01",
    "section": "",
    "text": "Aufgabe\nKorrigieren Sie den Fehler in der Syntax:\n\nmean(x = c(1, 5, 10, 52)\n\nÄndern Sie nur diejenigen Teile der Syntax, die zwingend geändert werden müssen, damit der Fehler korrigiert wird.\nGeben Sie in der Lösung keine Leerzeichen ein.\n         \n\n\nLösung\n\nmean(x=c(1,5,10,52))\n\n[1] 17\n\n\nDie Antwort lautet: mean(x=c(1,5,10,52)).\n\nCategories:\n\nR\n‘2023’\nstring"
  },
  {
    "objectID": "posts/tidydata1/tidydata1.html",
    "href": "posts/tidydata1/tidydata1.html",
    "title": "tidydata1",
    "section": "",
    "text": "Laden Sie die folgende Tabellen mit folgendem Befehl aus dem Paket tidyverse:\n\ntable1_path &lt;- \"https://raw.githubusercontent.com/sebastiansauer/Lehre/main/data/tidy-table1.csv\"\ntable1 &lt;- read_csv(table1_path)\n\nInsgesamt sollten Sie als folgende Tabellen in Ihrem environment verfügbar haben:\n\ntable1\ntable2\ntable3\ntable4\ntable5\n\nWelche der Tabellen ist in der Normalform?\n\n\n\ntable1\ntable2\ntable3\ntable4\ntable5"
  },
  {
    "objectID": "posts/tidydata1/tidydata1.html#answerlist",
    "href": "posts/tidydata1/tidydata1.html#answerlist",
    "title": "tidydata1",
    "section": "",
    "text": "table1\ntable2\ntable3\ntable4\ntable5"
  },
  {
    "objectID": "posts/tidydata1/tidydata1.html#answerlist-1",
    "href": "posts/tidydata1/tidydata1.html#answerlist-1",
    "title": "tidydata1",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndatawrangling\ntidy\nschoice"
  },
  {
    "objectID": "posts/Inferenz-fuer-alle/Inferenz-fuer-alle.html",
    "href": "posts/Inferenz-fuer-alle/Inferenz-fuer-alle.html",
    "title": "Inferenz-fuer-alle",
    "section": "",
    "text": "Exercise\nDie Inferenzstatistik ist eine Sammlung an Verfahren zur Bemessung von Unsicherheit in statistischen Schlüssen.\n\nFür welche Statistiken - also Kennzahlen der Deskriptivstatistik wie etwa \\(\\bar{X}, sd, r\\) - kann man die Inferenzstatistik verwenden?\nFür welche Forschungsfragen oder -bereiche kann man die Inferenzstatistik verwenden?\nGibt es besondere Fälle, in denen man nicht die Inferenzstatistik verwenden möchte? Wenn ja, welche?\n\n         \n\n\nSolution\n\nFür (grundsätzlich) alle: Für jede Statistik kann man prinzipiell von der jeweiligen Stichprobe (auf Basis derer die Statistik berechnet wurde) auf eine zugehörige Grundgesamtheit schließen.\nFür (grundsätzlich) alle: Die Methoden der Inferenzstatistik sind prinzipiell unabhängig von den Spezifika bestimmter Forschungsfragen oder -bereiche. In den meisten Forschungsfragen ist man daran interessiert allgemeingültige Aussagen zu treffen. Da Statistiken sich nur auf eine Stichprobe - also einen zumeist nur kleinen Teil einer Grundgesamtheit beziehen - wird man sich kaum mit einer Statistik zufrieden geben, sondern nach Inferenzstatistik verlangen.\nIn einigen Ausnahmefällen wird man auf eine Inferenzstatistik verzichten. Etwa wenn man bereits eine Vollerhebung durchgeführt hat, z.B. alle Mitarbeitis eines Unternehmens befragt hat, dann kennt man ja bereits den wahren Populationswert. Ein anderer Fall ist, wenn man nicht an Verallgemeinerungen interessiert ist: Kennt man etwa die Überlebenschance \\(p\\) des Titanic-Unglücks, so ist es fraglich auf welche Grundgesamtheit man die Statistik \\(p\\) bzw. zu welchem Parameter \\(\\pi\\) (kleines Pi) man generalisieren möchte.\n\n\nCategories:\n\nqm2\ninference"
  },
  {
    "objectID": "posts/there-is-no-package/there-is-no-package.html",
    "href": "posts/there-is-no-package/there-is-no-package.html",
    "title": "there-is-no-package",
    "section": "",
    "text": "Sie führen folgende R-Syntax aus:\nlibrary(tidyverse)\nUnd bekommen als Antwort eine Fehlermeldung quittiert:\nthere is no package called 'tidyverse'.\nWas ist die Ursache bzw. zu tun?\n\n\n\nEs existiert kein Paket namens tidyverse.\nEs existiert kein Paket namens tidyverse auf Ihrem Rechner.\nDas Paket tidyverse ist nicht gestartet.\nDas Paket tidyverse ist kaputt.\nR ist in Sie verliebt und versucht auf ungelenke Weise Kontakt aufzunehmen."
  },
  {
    "objectID": "posts/there-is-no-package/there-is-no-package.html#answerlist",
    "href": "posts/there-is-no-package/there-is-no-package.html#answerlist",
    "title": "there-is-no-package",
    "section": "",
    "text": "Es existiert kein Paket namens tidyverse.\nEs existiert kein Paket namens tidyverse auf Ihrem Rechner.\nDas Paket tidyverse ist nicht gestartet.\nDas Paket tidyverse ist kaputt.\nR ist in Sie verliebt und versucht auf ungelenke Weise Kontakt aufzunehmen."
  },
  {
    "objectID": "posts/there-is-no-package/there-is-no-package.html#answerlist-1",
    "href": "posts/there-is-no-package/there-is-no-package.html#answerlist-1",
    "title": "there-is-no-package",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nR\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/Wertberechnen2/Wertberechnen2.html",
    "href": "posts/Wertberechnen2/Wertberechnen2.html",
    "title": "Wertberechnen2",
    "section": "",
    "text": "Aufgabe\nWelchen Wert bzw. welches Ergebnis liefert folgende R-Syntax für ergebnis zurück?\nx hat zu Beginn den Wert 24.\nHinweise:\n\nsqrt(x) liefert die (positive) Quadratwurzel von x zurück.\nx^2 liefert die zweite Potenz von x zurück.\n\n         \n\n\nLösung\nEs wird 25 zurückgeliefert.\n\nCategories:\n\nR\ndyn\nnum"
  },
  {
    "objectID": "posts/regression1a/regression1a.html",
    "href": "posts/regression1a/regression1a.html",
    "title": "regression1a",
    "section": "",
    "text": "Die folgende Frage bezieht sich auf dieses Ergebnis einer Regressionsanalyse:\n\n\n\nCall:\nlm(formula = y ~ x, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6601 -0.6050  0.0212  0.6730  2.8576 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.0323     0.1066    0.30     0.76    \nx             0.8052     0.0997    8.08    4e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.99 on 85 degrees of freedom\nMultiple R-squared:  0.434, Adjusted R-squared:  0.427 \nF-statistic: 65.2 on 1 and 85 DF,  p-value: 4e-12\n\n\nWelche der folgenden Aussagen passt am besten?\n\n\n\nWenn x=0, dann ist ein Mittelwert von y in Höhe von etwa 0.03 zu erwarten.\nDer Mittelwert der abhängigen Variaben y sinkt mit zunehmenden x.\nWenn x=1, dann ist ein Mittelwert von y in Höhe von ca. 0.03 zu erwarten.\nWenn x=2, dann ist ein Mittelwert von y in Höhe von ca. 0.84 zu erwarten.\nDas (nicht-adjustierte) \\(R^2\\) liegt im Modell bei 0.81."
  },
  {
    "objectID": "posts/regression1a/regression1a.html#answerlist",
    "href": "posts/regression1a/regression1a.html#answerlist",
    "title": "regression1a",
    "section": "",
    "text": "Wenn x=0, dann ist ein Mittelwert von y in Höhe von etwa 0.03 zu erwarten.\nDer Mittelwert der abhängigen Variaben y sinkt mit zunehmenden x.\nWenn x=1, dann ist ein Mittelwert von y in Höhe von ca. 0.03 zu erwarten.\nWenn x=2, dann ist ein Mittelwert von y in Höhe von ca. 0.84 zu erwarten.\nDas (nicht-adjustierte) \\(R^2\\) liegt im Modell bei 0.81."
  },
  {
    "objectID": "posts/regression1a/regression1a.html#answerlist-1",
    "href": "posts/regression1a/regression1a.html#answerlist-1",
    "title": "regression1a",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nregression\n‘2022’"
  },
  {
    "objectID": "posts/iq03/iq03.html",
    "href": "posts/iq03/iq03.html",
    "title": "iq03",
    "section": "",
    "text": "Exercise\nIntelligenz wird häufig mittels einem IQ-Test ermittelt.\nPersonen mit einem Testwert von höchstens 100 Punkten kann man als “nicht überdurchschnittlich intelligent” bezeichnen.\nWie groß ist die Wahrscheinlichkeit, dass die nächste Person, die Sie treffen, nicht überdurchschnittlich intelligent ist?\nHinweise:\n\nNutzen Sie Simulationsmethoden.\nGehen Sie von folgender IQ-Verteilung aus: \\(IQ \\sim N(100,15)\\)\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\nSimulieren Sie \\(n=10^3\\) Stichproben.\nNutzen Sie die Zahl 42 als Startwert für Ihre Zufallszahlen (um die Reproduzierbarkeit zu gewährleisten)\n\n         \n\n\nSolution\n\nlibrary(tidyverse)\n\nWir simulieren die Daten:\n\nset.seed(42)\nd &lt;- tibble(\n  id = 1:10^3,\n  iq = rnorm(n = 10^3, mean = 100, sd= 15))\n\nDa \\(\\sigma=15\\), filtern wir bis höchstens 100:\n\nsolution_d &lt;- \n  d %&gt;% \n  count(iq &lt;= 100) %&gt;% \n  mutate(prop = n / sum(n))\n\nsolution_d\n\n# A tibble: 2 × 3\n  `iq &lt;= 100`     n  prop\n  &lt;lgl&gt;       &lt;int&gt; &lt;dbl&gt;\n1 FALSE         485 0.485\n2 TRUE          515 0.515\n\n\nDie Wahrscheinlichkeit für “nicht überdurchschnittlich intelligent” beträgt ca. 0.52%.\nDas Ereignis “nicht überdurchschnittlich intelligent” kann man vielleicht einfacher - und auf jeden Fall präziser benennen mit \\(iq \\le 100\\).\n\nCategories:\n\nprobability\nsimulation\nnormal-distribution"
  },
  {
    "objectID": "posts/mariokart-max2/mariokart-max2.html",
    "href": "posts/mariokart-max2/mariokart-max2.html",
    "title": "mariokart-max2",
    "section": "",
    "text": "Aufgabe\nImportieren Sie den Datensatz mariokart in R. Berechnen Sie die maximale Verkaufspreise (total_pr) für Spiele, die mit 0, 1, 2, … Lenkräder (wheels) gekauft werden. Dieser Kennwert heiße pr_max. Berücksichtigen Sie aber nur neue Spiele. Bilden Sie von pr_max den Mittelwert und geben Sie diesen an.\nHinweise:\n\nRunden Sie auf 1 Dezimalstelle.\n\n         \n\n\nLösung\nPakete starten:\n\nlibrary(tidyverse)\nlibrary(easystats)\n\nDaten importieren:\n\nd_url &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\"\nd &lt;- data_read(d_url)\n\n\nsolution &lt;-\nd  %&gt;% \n  filter(cond == \"new\") %&gt;% \n  group_by(wheels) %&gt;% \n  summarise(pr_max = max(total_pr)) %&gt;% \n  summarise(pr_max_mean = mean(pr_max))\n\nsolution\n\n# A tibble: 1 × 1\n  pr_max_mean\n        &lt;dbl&gt;\n1        63.2\n\n\nLösung: 63.17.\n\nCategories:\n\ndatawrangling\ndplyr\neda\nnum"
  },
  {
    "objectID": "posts/Warum-Bayes/Warum-Bayes.html",
    "href": "posts/Warum-Bayes/Warum-Bayes.html",
    "title": "Warum-Bayes",
    "section": "",
    "text": "Exercise\nNennen Sie einen (fachlichen) Grund, warum Sie eine Bayes-Analyse machen würden (und nicht etwa ein Analyse auf Basis der frequentistischen Statistik).\n         \n\n\nSolution\nEs existieren mehrere Gründe, einige wichtige sind:\n\nBayes-Analysen erlauben es, Vorwissen in die Analyse einfließen zu lassen.\nBayes-Analysen geben die Wahrscheinlichkeit einer Hypothese bzw. eines Parameterwerts zurück.\nBayes-Analysen erlauben es, Modelle exakt und flexibel zu spezifizieren.\nBayes-Analysen sind bei kleineren Stichproben genauer.\n\n“Quantifizierung” ist keine ausreichende Begründung für die Verwendung der Bayes-Statistik, da auch z.B. eine Frequentistische Analyse Quantifizierung bietet. Hingegen ist “Quantifizierung der Wahrscheinlichkeit der Forschungshypothese” ein valider Grund, denn der Frequentismus erlaubt nicht die Wahrscheinlichkeit einer Hypothese zu quantifizieren.\n“Wahrscheinlichkeitsaussagen” ist ebenfalls keine ausreichende Begründung für Bayes, denn auch im Frequentismus gibt es Wahrscheinlichkeitsaussagen, auch wenn diese weniger stark in die Wahrscheinlichkeitstheorie geknüpft sind als die Bayes-Inferenz (vgl. Jaynes, 2003).\nEs ist als Begründung nicht ausreichend, z.B. von “Erwartungen ans die Auswertung” zu sprechen, wenn man auf die Priori-Verteilung als (valider) Vorteil der Bayes-Inferenz abzielen möchte.\nEbenso ist es nicht ausreichend, allgemein auf eine “höhere Zuverlässigkeit” o.Ä. der Bayes-Inferenz hinzuweisen.\nDas ROPE ist eine praktische, sinnvolle Methode, allerdings gibt es mittlerweile vergleichbare Verfahren im Frequentismus, sog. Äquivalenztests.\nDer Grund, warum Bayes-Analysen bei kleineren Stichproben zu genaueren Ergebnissen kommen, liegt im Priori-Wissen. Spezifiziert man z.B. eine Normalverteilung mit Sigma=1 und findet in den Daten einen Wert von zB. Sigma=6, also einen extremen Ausreißer, so wird die Priori-Verteilung dafür sorgen, den Extremwert “zurechtzustutzen” auf einen Wert näher der Mittelwert der Verteilung. Sofern dies sinnvoll/korrekt ist, wird man mit diesem Vorgehen zu genaueren Ergebnissen kommen. Die Hoffnung ist, dass einzelne Extremwerte eher Messfehler sind.\n\nCategories:\n\nqm2\nbayes\nprobability"
  },
  {
    "objectID": "posts/iq04/iq04.html",
    "href": "posts/iq04/iq04.html",
    "title": "iq04",
    "section": "",
    "text": "Exercise\nIntelligenz wird häufig mittels einem IQ-Test ermittelt.\nWie intelligent muss man sein, um zu den schlauesten 2% Personen in der Allgemeinbevölkerung zu gehören?\nHinweise:\n\nNutzen Sie Simulationsmethoden.\nGehen Sie von folgender IQ-Verteilung aus: \\(IQ \\sim N(100,15)\\)\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\nSimulieren Sie \\(n=10^3\\) Stichproben.\nNutzen Sie die Zahl 42 als Startwert für Ihre Zufallszahlen (um die Reproduzierbarkeit zu gewährleisten)\n\n         \n\n\nSolution\n\nlibrary(tidyverse)\n\nWir simulieren die Daten:\n\nset.seed(42)\nd &lt;- tibble(\n  id = 1:10^3,\n  iq = rnorm(n = 10^3, mean = 100, sd= 15))\n\nWir filtern die schlauesten 2 Prozent:\n\nsolution_d &lt;- \n  d %&gt;% \n  arrange(iq) %&gt;% \n  slice_tail(prop = 0.02) %&gt;% \n  summarise(min(iq))\n\nsolution_d\n\n# A tibble: 1 × 1\n  `min(iq)`\n      &lt;dbl&gt;\n1      130.\n\n\nDie Syntax auf Deutsch übersetzt:\nDefiniere solution_d wie folgt:\nnimm die Tabelle d und dann ...\nsortiere (aufsteigend) die Spalte iq und dann ...\nschneide hinten (\"am Schwanz\") einen Anteil von 2% ab und dann ...\nfasse diese Liste an Werten zusammen zu ihrem Minimum (also dem kleinsten Wert).\nAlternativ könnte man schreiben:\n\nsolution &lt;- \n  d %&gt;% \n  summarise(iq_top_2komma3_prozent = quantile(iq, prob = .98))\n\nsolution\n\n# A tibble: 1 × 1\n  iq_top_2komma3_prozent\n                   &lt;dbl&gt;\n1                   130.\n\n\n\nCategories:\n\nprobability\nsimulation\nnormal-distribution"
  },
  {
    "objectID": "posts/Typ-Fehler-R-07/Typ-Fehler-R-07.html",
    "href": "posts/Typ-Fehler-R-07/Typ-Fehler-R-07.html",
    "title": "Typ-Fehler-R-07",
    "section": "",
    "text": "Question"
  },
  {
    "objectID": "posts/Typ-Fehler-R-07/Typ-Fehler-R-07.html#answerlist",
    "href": "posts/Typ-Fehler-R-07/Typ-Fehler-R-07.html#answerlist",
    "title": "Typ-Fehler-R-07",
    "section": "Answerlist",
    "text": "Answerlist\n\nR ist abgestürzt; am besten neu starten.\nR verträgt im Standard nur Grüße in englischer Sprache. Sprachpakete updaten.\nR wartet auf das Ende der Text-Auszeichnung, also auf das schließende Anführungszeichen. Das muss noch eingegeben werden. Alternativ kann man “Escape” drücken.\nEs gibt kein Problem; man kann einfach den nächsten Befehl eingeben.\nR hat gewartet auf das Ende der Text-Auszeichnung, also auf das schließende Anführungszeichen. Jetzt ist R abgestürzt und muss neu gestartet werden."
  },
  {
    "objectID": "posts/Typ-Fehler-R-07/Typ-Fehler-R-07.html#answerlist-1",
    "href": "posts/Typ-Fehler-R-07/Typ-Fehler-R-07.html#answerlist-1",
    "title": "Typ-Fehler-R-07",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nRichtig\nFalsch\nFalsch\n\n\nCategories:\n\nR\n‘2023’\nmchoice"
  },
  {
    "objectID": "posts/wuerfel01/wuerfel01.html",
    "href": "posts/wuerfel01/wuerfel01.html",
    "title": "wuerfel01",
    "section": "",
    "text": "Wie hoch ist die Wahrscheinlichkeit, mit zwei fairen Würfeln genau 10 Augen zu werfen?\nHinweise:\n\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\nRunden Sie auf zwei Dezimalstellen.\nFixieren Sie die Zufallszahlen auf den Startwert 42.\nMit expand_grid können Sie komfortabel alle 36 Ereignisse dieses Zufallsexperiments in einen Dataframe bringen.\n\nWählen Sie die am besten passende Option:\n\n\n\n.04\n.08\n.12\n.16\n.20"
  },
  {
    "objectID": "posts/wuerfel01/wuerfel01.html#answerlist",
    "href": "posts/wuerfel01/wuerfel01.html#answerlist",
    "title": "wuerfel01",
    "section": "",
    "text": ".04\n.08\n.12\n.16\n.20"
  },
  {
    "objectID": "posts/wuerfel01/wuerfel01.html#answerlist-1",
    "href": "posts/wuerfel01/wuerfel01.html#answerlist-1",
    "title": "wuerfel01",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch.\nWahr.\nFalsch.\nFalsch.\nFalsch.\n\n\nCategories:\n\nprobability\ndice\nexam-22"
  },
  {
    "objectID": "posts/Def-Statistik01/Def-Statistik01.html",
    "href": "posts/Def-Statistik01/Def-Statistik01.html",
    "title": "Def-Statistik01",
    "section": "",
    "text": "Welche Definition von Statistik passt am besten?\n\n\n\nStatistik fasst Daten zusammen.\nStatistik vervielfacht Daten.\nStatistik fasst Daten zusammen, um wesentliche Informationen den Daten zu entnehmen .\nStatistik fasst Daten zusammen, um wesentliche Informationen den Daten zu entnehmen und beschreibt die Ungewissheit unserer Schlüsse."
  },
  {
    "objectID": "posts/Def-Statistik01/Def-Statistik01.html#answerlist",
    "href": "posts/Def-Statistik01/Def-Statistik01.html#answerlist",
    "title": "Def-Statistik01",
    "section": "",
    "text": "Statistik fasst Daten zusammen.\nStatistik vervielfacht Daten.\nStatistik fasst Daten zusammen, um wesentliche Informationen den Daten zu entnehmen .\nStatistik fasst Daten zusammen, um wesentliche Informationen den Daten zu entnehmen und beschreibt die Ungewissheit unserer Schlüsse."
  },
  {
    "objectID": "posts/Def-Statistik01/Def-Statistik01.html#answerlist-1",
    "href": "posts/Def-Statistik01/Def-Statistik01.html#answerlist-1",
    "title": "Def-Statistik01",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nWahr\n\n\nCategories:\nschoice"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-03/Verteilungen-Quiz-03.html",
    "href": "posts/Verteilungen-Quiz-03/Verteilungen-Quiz-03.html",
    "title": "Verteilungen-Quiz-03",
    "section": "",
    "text": "Ist folgende Aussage wahr?\nWenn eine Verteilung einer stetigen Zufallsvariablen \\(X\\) (z.B. die Posteriori-Verteilung einer Bayes-Analyse) normalverteilt ist, gilt dann \\(Pr(X \\ge\\bar{x}) = 1/2\\).\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-03/Verteilungen-Quiz-03.html#answerlist",
    "href": "posts/Verteilungen-Quiz-03/Verteilungen-Quiz-03.html#answerlist",
    "title": "Verteilungen-Quiz-03",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-03/Verteilungen-Quiz-03.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-03/Verteilungen-Quiz-03.html#answerlist-1",
    "title": "Verteilungen-Quiz-03",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-04/Verteilungen-Quiz-04.html",
    "href": "posts/Verteilungen-Quiz-04/Verteilungen-Quiz-04.html",
    "title": "Verteilungen-Quiz-04",
    "section": "",
    "text": "Ist folgende Aussage wahr?\nIst eine stetige Verteilung symmetrisch, dann ist sie normalverteilt.\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-04/Verteilungen-Quiz-04.html#answerlist",
    "href": "posts/Verteilungen-Quiz-04/Verteilungen-Quiz-04.html#answerlist",
    "title": "Verteilungen-Quiz-04",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-04/Verteilungen-Quiz-04.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-04/Verteilungen-Quiz-04.html#answerlist-1",
    "title": "Verteilungen-Quiz-04",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/Diamonds-Histogramm-Vergleich2/Diamonds-Histogramm-Vergleich2.html",
    "href": "posts/Diamonds-Histogramm-Vergleich2/Diamonds-Histogramm-Vergleich2.html",
    "title": "Diamonds-Histogramm-Vergleich2",
    "section": "",
    "text": "Die Daten beziehen sich auf den Datensatz diamonds und sind hier einzusehen bzw. können bei Interesse dort heruntergeladen werden.\n\n\n\n\n\n\n\n\n\n\n\n\nIm Diagramm A wird ein Maß der zentralen Tendenz gruppenbezogen gezeigt, also den jeweiligen Kennwert der Gruppe (Schliffart) wiedergegeben.\nIm Diagramm B wird die Gesamtverteilung über die drei Gruppen hinweg (in hellgrau) dargestellt; in den kräftigeren Farbtönen wird die Verteilung pro Gruppe (Schliffart) dargestellt.\nInsgesamt sind die Verteilung linksschief.\nInsgesamt sind die Verteilung rechtssteil."
  },
  {
    "objectID": "posts/Diamonds-Histogramm-Vergleich2/Diamonds-Histogramm-Vergleich2.html#answerlist",
    "href": "posts/Diamonds-Histogramm-Vergleich2/Diamonds-Histogramm-Vergleich2.html#answerlist",
    "title": "Diamonds-Histogramm-Vergleich2",
    "section": "",
    "text": "Im Diagramm A wird ein Maß der zentralen Tendenz gruppenbezogen gezeigt, also den jeweiligen Kennwert der Gruppe (Schliffart) wiedergegeben.\nIm Diagramm B wird die Gesamtverteilung über die drei Gruppen hinweg (in hellgrau) dargestellt; in den kräftigeren Farbtönen wird die Verteilung pro Gruppe (Schliffart) dargestellt.\nInsgesamt sind die Verteilung linksschief.\nInsgesamt sind die Verteilung rechtssteil."
  },
  {
    "objectID": "posts/Diamonds-Histogramm-Vergleich2/Diamonds-Histogramm-Vergleich2.html#answerlist-1",
    "href": "posts/Diamonds-Histogramm-Vergleich2/Diamonds-Histogramm-Vergleich2.html#answerlist-1",
    "title": "Diamonds-Histogramm-Vergleich2",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\nFalsch\nFalsch\n\n\nCategories:\n\nvis\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/iq05/iq05.html",
    "href": "posts/iq05/iq05.html",
    "title": "iq05",
    "section": "",
    "text": "Exercise\nIntelligenz wird häufig mittels einem IQ-Test ermittelt.\nWie intelligent muss man sein, um zu den schlauesten Promill der Bevölkerung zu gehören?\nHinweise:\n\nNutzen Sie Simulationsmethoden.\nGehen Sie von folgender IQ-Verteilung aus: \\(IQ \\sim N(100,15)\\)\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\nSimulieren Sie \\(n=10^5\\) Stichproben.\nNutzen Sie die Zahl 42 als Startwert für Ihre Zufallszahlen (um die Reproduzierbarkeit zu gewährleisten)\n\n         \n\n\nSolution\n\nlibrary(tidyverse)\n\nWir simulieren die Daten:\n\nset.seed(42)\nd &lt;- tibble(\n  id = 1:10^5,\n  iq = rnorm(n = 10^5, mean = 100, sd= 15))\n\nWir filtern die schlauesten 0,1 Prozent:\n\nd %&gt;% \n  summarise(iq_top_0komma1_prozent = quantile(iq, prob = .999))\n\n# A tibble: 1 × 1\n  iq_top_0komma1_prozent\n                   &lt;dbl&gt;\n1                   146.\n\n\nMan muss mindestens über einen IQ von ca. 145 verfügen.\nAchtung: Das sind immer Zahlen als der “kleinen Welt” des Modells. Sollten unsere Annahmen nicht stimmen (normalverteilt mit MW 100 und SD 15), dann stimmt natürlich unser Ergebnis auch nicht.\nOb unsere Annahmen stimmen, kann der Computer nicht sagen. Das ist weiterhin Menschenjob.\n\nCategories:\n\nprobability\nsimulation\nnormal-distribution"
  },
  {
    "objectID": "posts/iq02/iq02.html",
    "href": "posts/iq02/iq02.html",
    "title": "iq02",
    "section": "",
    "text": "Exercise\nIntelligenz wird häufig mittels einem IQ-Test ermittelt.\nWie groß ist die Wahrscheinlichkeit, dass die nächste Person, die Sie treffen, mindestens zwei Streuungseinheiten über dem Mittelwert liegt?\nHinweise:\n\nNutzen Sie Simulationsmethoden.\nGehen Sie von folgender IQ-Verteilung aus: \\(IQ \\sim N(100,15)\\)\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\nSimulieren Sie \\(n=10^3\\) Stichproben.\nNutzen Sie die Zahl 42 als Startwert für Ihre Zufallszahlen (um die Reproduzierbarkeit zu gewährleisten)\nWir wollen hier keine Post-Verteilung berechnen, sondern lediglich Werte simulieren.\n\n         \n\n\nSolution\n\nlibrary(tidyverse)\n\nWir simulieren die Daten:\n\nset.seed(42)\nd &lt;- tibble(\n  id = 1:10^3,\n  iq = rnorm(n = 10^3, mean = 100, sd= 15))\n\nDa \\(\\sigma=15\\), filtern wir ab 130:\n\nd %&gt;% \n  count(iq &gt;= 130)\n\n# A tibble: 2 × 2\n  `iq &gt;= 130`     n\n  &lt;lgl&gt;       &lt;int&gt;\n1 FALSE         979\n2 TRUE           21\n\n\nDie Wahrscheinlichkeit beträgt ca. 2%.\nJa, diese Aufgaben ist faktisch identische zur Aufgabe iq01. Darum ging es: Sie sollen erkennen, dass ein IQ-Wert von 130 das gleiche ist wie MW+2sd.\nÜbrigens: “Wie viele SD-Einheiten liegt der Wert von Beobachtung \\(i\\) über dem Mittelwert, \\(\\bar{X}\\) ?” ist die Frage, die der z-Wert beantwortet:\n\\(z_i = \\frac{x_i - \\bar{X}}{sd(x)}\\)\n\nCategories:\n\nprobability\nsimulation\nnormal-distribution"
  },
  {
    "objectID": "posts/Sim-Prior/Sim-Prior.html",
    "href": "posts/Sim-Prior/Sim-Prior.html",
    "title": "Sim-Prior",
    "section": "",
    "text": "Exercise\nGegeben dem folgenden Modell, simulieren Sie Daten aus der Prior-Verteilung (Priori-Prädiktiv-Verteilung).\nLikelihood: \\(h_i \\sim \\mathcal{N}(\\mu, \\sigma)\\)\nPrior für \\(\\mu\\): \\(\\mu \\sim \\mathcal{N}(0, 1)\\)\nPrior für \\(\\sigma\\): \\(\\sigma \\sim \\mathcal{U}(0, 10)\\)\n         \n\n\nSolution\n\nlibrary(tidyverse)\n\nn &lt;- 1e4\n\n\nsim &lt;- tibble(\n  mu = rnorm(n = n),  # Default-Werte sind mean=0, sd = 1\n  sigma = runif(n = n, 0, 10)) %&gt;%\n  mutate(\n    y = rnorm(n = n, mean = mu, sd = sigma))\n\nggplot(sim, aes(x = y)) +\n  geom_density() +\n  labs(x = \"y\", y = \"Dichte\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCategories:\n~"
  },
  {
    "objectID": "posts/Kausale-Verben/Kausale-Verben.html",
    "href": "posts/Kausale-Verben/Kausale-Verben.html",
    "title": "Kausale-Verben",
    "section": "",
    "text": "Question"
  },
  {
    "objectID": "posts/Kausale-Verben/Kausale-Verben.html#answerlist",
    "href": "posts/Kausale-Verben/Kausale-Verben.html#answerlist",
    "title": "Kausale-Verben",
    "section": "Answerlist",
    "text": "Answerlist\n\nX hat einen Effekt auf Y\nX steht mit Y in Zusammenhang\nHohe Werte in X geht mit hohen Werten in Y einher (und umgekehrt)\nEs wird ein statistischer Effekt von X auf Y erwartet\nX reallokiert die Ressourcen in Y"
  },
  {
    "objectID": "posts/Kausale-Verben/Kausale-Verben.html#answerlist-1",
    "href": "posts/Kausale-Verben/Kausale-Verben.html#answerlist-1",
    "title": "Kausale-Verben",
    "section": "Answerlist",
    "text": "Answerlist\n\nRichtig\nFalsch\nFalsch\nFalsch\nRichtig\n\n\nCategories:\n\ncausal\nresearch-question\nmchoice"
  },
  {
    "objectID": "posts/Rethink_2m3/Rethink_2m3.html",
    "href": "posts/Rethink_2m3/Rethink_2m3.html",
    "title": "Rethink_2m3",
    "section": "",
    "text": "Exercise\nThis question is taken from McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Ed.). Taylor and Francis, CRC Press.\n2M3. Suppose there are two globes, one for Earth and one for Mars. The Earth globe is 70% covered in water. The Mars globe is 100% land. Further suppose that one of these globes—you don’t know which—was tossed in the air and produced a “land” observatiion. Assume that each globe was equally likely to be tossed. Show that the posterior probability that the globe was the Earth, conditional on seeing “land” (Pr(Earth|land)), is 0.23.\n         \n\n\nSolution\nZur Erinnerung:\n\\[\\begin{aligned}\nPr(A) &= Pr(A \\cap B) + Pr(A \\cap A^C)  \\qquad \\text{| totale Wskt, bei disjunkten Ereignissen}\\\\\nPr(A \\cap B) &= Pr(A|B) \\cdot Pr(B)\\\\\nPr(A \\cap B^C) &= Pr(A|B^C) \\cdot Pr(B^C)\n\\end{aligned}\\]\nWobei \\(A^C\\) das komlementäre Ereignis zu \\(A\\) meint.\nThe solution is taken from this source.\n\n# probability of land, given Earth:\np_le &lt;- 0.3\n\n# probability of land, given Mars:\np_lm &lt;- 1.0\n\n# probability of Earth:\np_e &lt;- 0.5\n\n# prob. of Mars:\np_m &lt;- 0.5\n\n# probability of land:\n# totale Wahrscheinlichkeit für Land\np_l &lt;- (p_e * p_le) + (p_m * p_lm)\np_l\n\n[1] 0.65\n\n\nDann gilt also:\n\n# probability of Earth, given land (using Bayes' Theorem):\np_el &lt;- (p_le * p_e) / p_l\np_el\n\n[1] 0.2307692\n\n\nEinfacher als die Rechnung ist vielleicht ein Baumdiagramm:\n\n\n\n\n\n\n\nCategories:\n\nprobability\nbayes"
  },
  {
    "objectID": "posts/lm1/lm1.html",
    "href": "posts/lm1/lm1.html",
    "title": "lm1",
    "section": "",
    "text": "Laden Sie den Datensatz mtcars aus dieser Quelle.\nBerechnen Sie eine Regression mit mpg als Ausgabevariable und hp als Eingabevariable!\nWelche Aussage ist für diese Analyse richtig?\n\n\n\nmpg und hp sind positiv korreliert laut dem Modell.\nDer Achsenabschnitt ist nahe Null.\nDie Analyse beinhaltet einen nominal skalierten Prädiktor.\nDas geschätzte Betagewicht für hp liegt bei 30.099.\nDas geschätzte Betagewicht für hp liegt bei -0.068."
  },
  {
    "objectID": "posts/lm1/lm1.html#answerlist",
    "href": "posts/lm1/lm1.html#answerlist",
    "title": "lm1",
    "section": "",
    "text": "mpg und hp sind positiv korreliert laut dem Modell.\nDer Achsenabschnitt ist nahe Null.\nDie Analyse beinhaltet einen nominal skalierten Prädiktor.\nDas geschätzte Betagewicht für hp liegt bei 30.099.\nDas geschätzte Betagewicht für hp liegt bei -0.068."
  },
  {
    "objectID": "posts/lm1/lm1.html#answerlist-1",
    "href": "posts/lm1/lm1.html#answerlist-1",
    "title": "lm1",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nFalsch\nRichtig\n\n\nCategories:\n\nregression\nlm\nqm2\nstats-nutshell"
  },
  {
    "objectID": "posts/Rethink_2m4/Rethink_2m4.html",
    "href": "posts/Rethink_2m4/Rethink_2m4.html",
    "title": "Rethink_2m4",
    "section": "",
    "text": "Exercise\nThis question is taken from McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Ed.). Taylor and Francis, CRC Press.\n2M4. Suppose you have a deck with only three cards. Each card has only two sides, and each side is either black or white. One card has two black sides. The second card has one black and one white side. The third card has two white sides. Now suppose all three cards are placed in a bag and shuffled. Someone reaches into the bag and pulls out a card and places it flat on a table. A black side is shown facing up, but you don’t know the color of the side facing down. Show that the probability that the other side is also black is 2/3. Use the counting method (Section 2 of the chapter) to approach this problem. This means counting up the ways that each card could produce the observed data (a black side faceing up on the table).\n         \n\n\nSolution\nLet’s label the cards bb (black on both sides), bw (black on one, white on the other), and ww (both sides are white), respectively.\nWanted is the probability that both sides are black (bb), given one side is black (1b): \\(Pr(bb|1b)\\).\nLet’s count the ways how the data - one black side - can come up in each conjecture (hypothesis), bb, bw, ww. Let’s denote “first side white” as 1b” and “first side black” as 2b.\nbb: 2 valid paths\n\n\n\n\n\n\nbw: 1 valid path\n\n\n\n\n\n\nww: 0 valid path\n\n\n\n\n\n\n\nd &lt;-\n  tibble::tribble(\n  ~Hyp, ~Prior,\n  \"bb\",     1, \n  \"bw\",     1,   \n  \"ww\",     1, \n  ) %&gt;% \n  mutate(Likelihood = c(2,1,0),\n         unstand_post = Prior*Likelihood,\n         std_post = unstand_post / sum(unstand_post))\n\n\n\n\n\n\n\n\n\n\nHyp\nPrior\nLikelihood\nunstand_post\nstd_post\n\n\n\n\nbb\n1\n2\n2\n0.67\n\n\nbw\n1\n1\n1\n0.33\n\n\nww\n1\n0\n0\n0.00\n\n\n\n\n\n\n\nThe following solution is taken from this source.\n\ncard_bb_likelihood &lt;- 2\ncard_bw_likelihood &lt;- 1\ncard_ww_likelihood &lt;- 0\n\nlikelihood &lt;- c(card_bb_likelihood, card_bw_likelihood, card_ww_likelihood)\nprior &lt;- c(1, 1, 1)\nposterior &lt;- likelihood * prior\nposterior &lt;- posterior / sum(posterior)\n\nposterior[1]\n\n[1] 0.6666667\n\n\n\nCategories:\n\nprobability\n‘2022’"
  },
  {
    "objectID": "posts/ReThink3m1/ReThink3m1.html",
    "href": "posts/ReThink3m1/ReThink3m1.html",
    "title": "ReThink3m1",
    "section": "",
    "text": "Exercise\nNehmen wir an, wir haben 8 (Wasser-)“Treffer” (\\(W=8\\)) bei 15 Würfen (\\(N=15\\)) erhalten (wieder im Globusversuch). Gehen Sie wieder von einer “flachen”, also gleichverteilten, Priori-Verteilung aus.\nBerechnen Sie die Posteriori-Verteilung und visualisieren Sie sie. Nutzen Sie die Gittermethode.\nQuelle: McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Aufl.). Taylor and Francis, CRC Press.\n         \n\n\nSolution\n\np_grid &lt;- seq(from = 0, to = 1, length.out = 1000)\nprior &lt;- rep(1, 1000)\nlikelihood &lt;- dbinom(8, size = 15, prob = p_grid)\nposterior &lt;- likelihood * prior\nposterior &lt;- posterior / sum(posterior)\n\ntibble(p = p_grid, posterior = posterior) %&gt;%\n  ggplot(aes(x = p, y = posterior)) +\n # geom_point() +\n  geom_line() +\n  labs(x = \"Anteil Wasser (p)\", y = \"Posterior Density\")\n\n\n\n\n\n\n\n\nQuelle\n\nCategories:\n\nbayes\nppv\nprobability"
  },
  {
    "objectID": "posts/Wertzuweisen_mc/Wertzuweisen_mc.html",
    "href": "posts/Wertzuweisen_mc/Wertzuweisen_mc.html",
    "title": "Wertzuweisen_mc",
    "section": "",
    "text": "Question"
  },
  {
    "objectID": "posts/Wertzuweisen_mc/Wertzuweisen_mc.html#answerlist",
    "href": "posts/Wertzuweisen_mc/Wertzuweisen_mc.html#answerlist",
    "title": "Wertzuweisen_mc",
    "section": "Answerlist",
    "text": "Answerlist\n\nloesung &lt;-42\nloesung &lt; - 42\nloesung-&gt;42\nloesung==42\nloesung&lt;-\"42\""
  },
  {
    "objectID": "posts/Wertzuweisen_mc/Wertzuweisen_mc.html#answerlist-1",
    "href": "posts/Wertzuweisen_mc/Wertzuweisen_mc.html#answerlist-1",
    "title": "Wertzuweisen_mc",
    "section": "Answerlist",
    "text": "Answerlist\n\nRichtig\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nR\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/wrangle9/wrangle9.html",
    "href": "posts/wrangle9/wrangle9.html",
    "title": "wrangle9",
    "section": "",
    "text": "Aufgabe\nUm Variablen in einem Datensatz anzulegen oder zu verändern, nutzt man mutate() im tidyverse. mutate() ist nützlich für vektorielles Rechnen. In diesem Zusammenhang: Was ist das Ergebnis folgenden Ausdrucks?\n\nsum(1:3 + 1:3)\n\n         \n\n\nLösung\n12\n\nsum(1:3 + 1:3)\n\n[1] 12\n\n\n\nCategories:\n\neda\n‘2023’\nnum"
  },
  {
    "objectID": "posts/tidymodels-ames-03/tidymodels-ames-03.html",
    "href": "posts/tidymodels-ames-03/tidymodels-ames-03.html",
    "title": "tidymodels-ames-03",
    "section": "",
    "text": "Aufgabe\nBerechnen Sie ein lineares Modell mit tidymodels und zwar anhand des ames Datensatzes.\nModellgleichung: Sale_Price ~ Gr_Liv_Area, data = ames.\nBerechnen Sie ein multiplikatives (exponenzielles) Modell.\nRücktransformieren Sie die Log-Werte in “Roh-Dollar”.\nGeben Sie den mittleren Vorhersagewert an als Lösung.\nHinweise:\n\nFixieren Sie die Zufallszahlen auf den Startwert 42.\n\n         \n\n\nLösung\n\nlibrary(tidymodels)\ndata(ames)\n\nMultiplikatives Modell:\n\names &lt;- \n  ames %&gt;% \n  mutate(Sale_Price = log10(Sale_Price)) %&gt;% \n  select(Sale_Price, Gr_Liv_Area)\n\nNicht vergessen: AV-Transformation in beiden Samples!\nDatensatz aufteilen:\n\nset.seed(42)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\nModell definieren:\n\nm1 &lt;-\n  linear_reg() # engine ist \"lm\" im Default\n\nModell fitten:\n\nfit1 &lt;-\n  m1 %&gt;% \n  fit(Sale_Price ~ Gr_Liv_Area, data = ames)\n\n\nfit1 %&gt;% pluck(\"fit\") \n\n\nCall:\nstats::lm(formula = Sale_Price ~ Gr_Liv_Area, data = data)\n\nCoefficients:\n(Intercept)  Gr_Liv_Area  \n  4.8552133    0.0002437  \n\n\nModellgüte im Train-Sample:\n\nfit1_performance &lt;-\n  fit1 %&gt;% \n  extract_fit_engine()  # identisch zu pluck(\"fit\")\n\nModellgüte im Train-Sample:\n\nfit1_performance %&gt;% summary()\n\n\nCall:\nstats::lm(formula = Sale_Price ~ Gr_Liv_Area, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.02587 -0.06577  0.01342  0.07202  0.39231 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 4.855e+00  7.355e-03  660.12   &lt;2e-16 ***\nGr_Liv_Area 2.437e-04  4.648e-06   52.43   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1271 on 2928 degrees of freedom\nMultiple R-squared:  0.4842,    Adjusted R-squared:  0.484 \nF-statistic:  2749 on 1 and 2928 DF,  p-value: &lt; 2.2e-16\n\n\nR-Quadrat via easystats:\n\nlibrary(easystats)\nfit1_performance %&gt;% r2()  # rmse()\n\n# R2 for Linear Regression\n       R2: 0.484\n  adj. R2: 0.484\n\n\n\ntidy(fit1_performance)  # ähnlich zu parameters()\n\n# A tibble: 2 × 5\n  term        estimate  std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) 4.86     0.00736        660.        0\n2 Gr_Liv_Area 0.000244 0.00000465      52.4       0\n\n\nVorhersagen im Test-Sample:\n\npreds &lt;- predict(fit1, new_data = ames_test)  # liefert TABELLE (tibble) zurück\nhead(preds)\n\n# A tibble: 6 × 1\n  .pred\n  &lt;dbl&gt;\n1  5.07\n2  5.18\n3  5.31\n4  5.11\n5  5.18\n6  5.10\n\n\npreds ist ein Tibble, also müssen wir noch die Spalte .pred. herausziehen, z.B. mit pluck(preds, \".pred\"):\n\npreds_vec &lt;- preds$.pred\n\n\names_test2 &lt;-\n  ames_test %&gt;% \n  mutate(preds = pluck(preds, \".pred\"),  # pluck aus der Tabelle rausziehen\n         .pred = preds_vec)  # oder  mit dem Dollar-Operator\n\nhead(ames_test2)\n\n# A tibble: 6 × 4\n  Sale_Price Gr_Liv_Area preds .pred\n       &lt;dbl&gt;       &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1       5.02         896  5.07  5.07\n2       5.24        1329  5.18  5.18\n3       5.60        1856  5.31  5.31\n4       5.15        1056  5.11  5.11\n5       5.26        1337  5.18  5.18\n6       4.98         987  5.10  5.10\n\n\nOder mit unnest:\n\names_test2 &lt;-\n  ames_test %&gt;% \n  mutate(preds = preds) %&gt;% \n  unnest(preds) # Listenspalte \"entschachteln\"\n\nhead(ames_test2)\n\n# A tibble: 6 × 3\n  Sale_Price Gr_Liv_Area .pred\n       &lt;dbl&gt;       &lt;int&gt; &lt;dbl&gt;\n1       5.02         896  5.07\n2       5.24        1329  5.18\n3       5.60        1856  5.31\n4       5.15        1056  5.11\n5       5.26        1337  5.18\n6       4.98         987  5.10\n\n\nOder wir binden einfach die Spalte an den Tibble:\n\names_test2 &lt;-\n  ames_test %&gt;% \n  bind_cols(preds = preds)  # nimmt Tabelle und bindet die Spalten dieser Tabelle an eine Tabelle\n\nhead(ames_test2)\n\n# A tibble: 6 × 3\n  Sale_Price Gr_Liv_Area .pred\n       &lt;dbl&gt;       &lt;int&gt; &lt;dbl&gt;\n1       5.02         896  5.07\n2       5.24        1329  5.18\n3       5.60        1856  5.31\n4       5.15        1056  5.11\n5       5.26        1337  5.18\n6       4.98         987  5.10\n\n\nModellgüte im Test-Sample:\n\nrsq(ames_test2,\n    truth = Sale_Price,\n    estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.517\n\n\n\nsol &lt;- 0.51679\n\nZur Interpretation von Log10-Werten\n\n5e5\n\n[1] 5e+05\n\n5*10^5 - 500000\n\n[1] 0\n\n\nRücktransformation (ohne Bias-Korrektur):\n\names_test2 &lt;-\n  ames_test2 %&gt;% \n  mutate(pred_raw = 10^(.pred))\n\nMittelwert der Vorhersagen:\n\nsol &lt;- mean(ames_test2$pred_raw)\nsol\n\n[1] 175973.8\n\n\n\nCategories:\n\nds1\ntidymodels\nprediction\nyacsda\nstat-learning\nnum"
  },
  {
    "objectID": "posts/tidy1/tidy1.html",
    "href": "posts/tidy1/tidy1.html",
    "title": "tidy1",
    "section": "",
    "text": "Das Konzept von “tidy” Daten (“Tidyformat”) spielt in der Datenanalyse eine wichtige Rolle.\nBetrachten Sie die Tabellen im Folgenden. Welche ist “tidy”?\nHinweise:\n\nAlle Variablen sollen nicht konstant sein, also mehr als einen uniquen Wert aufweisen.\nAlle Variablen sollen keine fehlenden Werte aufweisen, also komplett sein.\nAlle Variablen sollen numerisch sein.\n\nTabelle A:\n\n\n\n\n\n\nTabelle A\ngroup\ny\nid1\nid2\n1\n10\n1\n1\nNA\n2\n20\n2\n1\nNA\n1\n30\n3\n2\nNA\n2\n40\n4\n2\nNA\n\n\n\n\nTabelle B:\n\n\n\n\n\n\nTabelle B\ngroup\ny\nid1\nid2\n1\n10\n1\n1\n2\n20\n2\n1\n1\n30\n3\n2\nNA\nNA\nNA\nNA\n2\n40\n4\n2\n\n\n\n\nTabelle C:\n\n\n\n\n\n\nTabelle C\ngroup\ny\nid1\nid2\ngroup\n10\n1\n1\ngroup\n20\n2\n1\n1,2\n30\n3\n2\ngroup\n40\n4\n2\n\n\n\n\nTabelle D:\n\n\n\n\n\n\nTabelle D\ngroup\ny\nid1\nid2\nNA\n10\n1\n1\nNA\n20\n2\n1\nNA\n30\n3\n2\nNA\n40\n4\n2\n\n\n\n\nTabelle E:\n\n\n\n\n\n\nTabelle E\ngroup\ny\nid1\nid2\n1\n10\n1\n1\n2\n20\n2\n1\n1\n30\n3\n2\n2\n40\n4\n2\n\n\n\n\n\n\n\nTabelle A\nTabelle B\nTabelle C\nTabelle D\nTabelle E"
  },
  {
    "objectID": "posts/tidy1/tidy1.html#answerlist",
    "href": "posts/tidy1/tidy1.html#answerlist",
    "title": "tidy1",
    "section": "",
    "text": "Tabelle A\nTabelle B\nTabelle C\nTabelle D\nTabelle E"
  },
  {
    "objectID": "posts/tidy1/tidy1.html#answerlist-1",
    "href": "posts/tidy1/tidy1.html#answerlist-1",
    "title": "tidy1",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch. In einem Tidy-Tibble darf keine leere Spalte vorkommen.\nFalsch. In einem Tidy-Tibble darf keine leere Zeile vorkommen.\nFalsch. Die Spalte groupweißt einen nicht erlaubten Wert auf.\nFalsch. In einem Tidy-Tibble darf keine Spalte nur aus NA bestehen.\nRichtig. Das ist ein ‘tidy Tibble’.\n\n\nCategories:\n\ntidy\ndata-wrangling\nschoice"
  },
  {
    "objectID": "posts/wrangle7/wrangle7.html",
    "href": "posts/wrangle7/wrangle7.html",
    "title": "wrangle7",
    "section": "",
    "text": "Welche Aussage zur Funktion filter() aus dem R-Paket dplyr ist richtig?\n\n\n\nfilter() filtert (behält) Zeilen, für die eine Prüfung TRUE ergibt\nfilter() filtert (behält)Zeilen, für die eine Prüfung FALSE ergibt\nfilter() filtert (behält) Zeilen, für die eine Prüfung TRUE oder NA ergibt\nMöchte man nur nicht-fehlende Zeilen aus der Variable x aus dem Dataframe df filtern (behalten), so formuliert man filter(df, x == NA).\nMöchte man nur nicht-fehlende Zeilen aus der Variable x aus dem Dataframe df filtern (behalten), so formuliert man filter(df, is.na(x))."
  },
  {
    "objectID": "posts/wrangle7/wrangle7.html#answerlist",
    "href": "posts/wrangle7/wrangle7.html#answerlist",
    "title": "wrangle7",
    "section": "",
    "text": "filter() filtert (behält) Zeilen, für die eine Prüfung TRUE ergibt\nfilter() filtert (behält)Zeilen, für die eine Prüfung FALSE ergibt\nfilter() filtert (behält) Zeilen, für die eine Prüfung TRUE oder NA ergibt\nMöchte man nur nicht-fehlende Zeilen aus der Variable x aus dem Dataframe df filtern (behalten), so formuliert man filter(df, x == NA).\nMöchte man nur nicht-fehlende Zeilen aus der Variable x aus dem Dataframe df filtern (behalten), so formuliert man filter(df, is.na(x))."
  },
  {
    "objectID": "posts/wrangle7/wrangle7.html#answerlist-1",
    "href": "posts/wrangle7/wrangle7.html#answerlist-1",
    "title": "wrangle7",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\neda\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/tidymodels-ames-04/tidymodels-ames-04.html",
    "href": "posts/tidymodels-ames-04/tidymodels-ames-04.html",
    "title": "tidymodels-ames-04",
    "section": "",
    "text": "Aufgabe\nBerechnen Sie ein lineares Modell mit tidymodels und zwar anhand des ames Datensatzes.\nModellgleichung: Sale_Price ~ Gr_Liv_Area, data = ames.\nBerechnen Sie ein multiplikatives (exponenzielles) Modell.\nGesucht ist R-Quadrat als Maß für die Modellgüte im TEST-Sample.\nHinweise:\n\nFixieren Sie die Zufallszahlen auf den Startwert 42.\nVerwenden Sie die Funktion last_fit.\n\n         \n\n\nLösung\n\nlibrary(tidymodels)\ndata(ames)\n\nMultiplikatives Modell:\n\names &lt;- \n  ames %&gt;% \n  mutate(Sale_Price = log10(Sale_Price)) %&gt;% \n  select(Sale_Price, Gr_Liv_Area)\n\nNicht vergessen: AV-Transformation in beiden Samples!\nDatensatz aufteilen:\n\nset.seed(42)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\nModell definieren:\n\nm1 &lt;-\n  linear_reg() # engine ist \"lm\" im Default\n\nRezept definieren:\n\nrec1 &lt;- \n  recipe(Sale_Price ~ Gr_Liv_Area, data = ames) \n\nVorhersagen mit last_fit:\n\nfit1_last &lt;- last_fit(object = m1, preprocessor = rec1, split = ames_split)  \nfit1_last\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits             id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;             &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [2342/588]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n\nWir bekommen ein Objekt, in dem Fit, Modellgüte, Vorhersagen und Hinweise enthalten sind.\nOhne Rezept lässt sich last_fit nicht anwenden.\nVorhersagen:\n\nfit1_last %&gt;% collect_predictions() %&gt;% \n  head()\n\n# A tibble: 6 × 5\n  id               .pred  .row Sale_Price .config             \n  &lt;chr&gt;            &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;               \n1 train/test split  5.07     2       5.02 Preprocessor1_Model1\n2 train/test split  5.18     3       5.24 Preprocessor1_Model1\n3 train/test split  5.31    18       5.60 Preprocessor1_Model1\n4 train/test split  5.11    26       5.15 Preprocessor1_Model1\n5 train/test split  5.18    29       5.26 Preprocessor1_Model1\n6 train/test split  5.10    30       4.98 Preprocessor1_Model1\n\n\nModellgüte im Test-Sample:\n\nfit1_last %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.118 Preprocessor1_Model1\n2 rsq     standard       0.517 Preprocessor1_Model1\n\n\nR-Quadrat:\n\nsol &lt;- 0.517\nsol\n\n[1] 0.517\n\n\n\nCategories:\n\nds1\ntidymodels\nprediction\nyacsda\nstat-learning\nnum"
  },
  {
    "objectID": "posts/ttest-skalenniveau/ttest-skalenniveau.html",
    "href": "posts/ttest-skalenniveau/ttest-skalenniveau.html",
    "title": "ttest-skalenniveau",
    "section": "",
    "text": "Der t-Test ist ein inferenzstatistisches Verfahren des Frequentismus. Welches Skalenniveau passt zu diesem Verfahren?\nHinweisse:\n\nDie folgende Abbildung gibt Tipps.\nInformationen, die zur Lösung einer Aufgabe nicht nötig sind, sollte man ignorieren.\n\n\n\n\n\n\n\n\n\n\n\n\n\nUV: nominal (mehrstufig), AV: metrisch\nUV: metrisch, AV: nominal (zweistufig)\nUV: nominal (mehrstufig), AV: nominal (mehrstufig)\nUV: metrisch, AV: nominal (zweistufig)\nUV: nominal (zweistufig), AV: metrisch"
  },
  {
    "objectID": "posts/ttest-skalenniveau/ttest-skalenniveau.html#answerlist",
    "href": "posts/ttest-skalenniveau/ttest-skalenniveau.html#answerlist",
    "title": "ttest-skalenniveau",
    "section": "",
    "text": "UV: nominal (mehrstufig), AV: metrisch\nUV: metrisch, AV: nominal (zweistufig)\nUV: nominal (mehrstufig), AV: nominal (mehrstufig)\nUV: metrisch, AV: nominal (zweistufig)\nUV: nominal (zweistufig), AV: metrisch"
  },
  {
    "objectID": "posts/ttest-skalenniveau/ttest-skalenniveau.html#answerlist-1",
    "href": "posts/ttest-skalenniveau/ttest-skalenniveau.html#answerlist-1",
    "title": "ttest-skalenniveau",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nFalsch\nWahr\n\n\nCategories:\n\nttest\nregression\nvariable-levels"
  },
  {
    "objectID": "posts/kausal-einfach/kausal-einfach.html",
    "href": "posts/kausal-einfach/kausal-einfach.html",
    "title": "kausal-einfach",
    "section": "",
    "text": "Eine Forscher:in aus Kalifornien entdeckt, dass Haiangriffe mit Eisverkauf korreliert sind: Haiangriffe treten gehäuft dann auf, wenn am Strand viel Eis verkauft wird. Dieser Zusammenhang ist zwar nicht perfekt, aber die Forscher:in findet in ihren Daten einen starken, sogar “signifikanten” Zusammenhang.\nWelche Schlüsse sind aus diesen Daten zu ziehen? Wählen Sie die Antwort, die am besten passt!\n\n\n\nDa Eisverkauf die UV und Haiangriff die AV ist, sind die Daten im Sinne eines Kausalschlusses “Eisverkauf führt (tendenziell) zu Haiangriffen” zu interpretieren. Natürlich gilt dies nur für linearen Zusammenhänge, da Korrelationen nur linearen Zusammenhänge identifizieren können.\nEs ist kein Kausalschluss möglich; eine Drittvariable könnte den Zusammenhang der beobachteten Variablen konfundieren.\nDie Daten (soweit bekannt bzw. oben aufgeführt sind) machen deutlich, dass es einen Zusammenhang zwischen den beiden Variablen gibt; folglich ist die eine Variable Ursache und die andere Wirkung. Die Daten lassen aber keine Aussage zu, welche der beiden Variablen Ursache und welche Wirkung ist.\nEs ist davon auszugehen, dass Haiangriff die Ursache ist und Eisverkauf die Wirkung.\nDa es sich nur um Beobachtungsdaten, nicht um Experimentaldaten handelt, ist keine Aussage möglich."
  },
  {
    "objectID": "posts/kausal-einfach/kausal-einfach.html#answerlist",
    "href": "posts/kausal-einfach/kausal-einfach.html#answerlist",
    "title": "kausal-einfach",
    "section": "",
    "text": "Da Eisverkauf die UV und Haiangriff die AV ist, sind die Daten im Sinne eines Kausalschlusses “Eisverkauf führt (tendenziell) zu Haiangriffen” zu interpretieren. Natürlich gilt dies nur für linearen Zusammenhänge, da Korrelationen nur linearen Zusammenhänge identifizieren können.\nEs ist kein Kausalschluss möglich; eine Drittvariable könnte den Zusammenhang der beobachteten Variablen konfundieren.\nDie Daten (soweit bekannt bzw. oben aufgeführt sind) machen deutlich, dass es einen Zusammenhang zwischen den beiden Variablen gibt; folglich ist die eine Variable Ursache und die andere Wirkung. Die Daten lassen aber keine Aussage zu, welche der beiden Variablen Ursache und welche Wirkung ist.\nEs ist davon auszugehen, dass Haiangriff die Ursache ist und Eisverkauf die Wirkung.\nDa es sich nur um Beobachtungsdaten, nicht um Experimentaldaten handelt, ist keine Aussage möglich."
  },
  {
    "objectID": "posts/kausal-einfach/kausal-einfach.html#answerlist-1",
    "href": "posts/kausal-einfach/kausal-einfach.html#answerlist-1",
    "title": "kausal-einfach",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/Bed-Wskt2/Bed-Wskt2.html",
    "href": "posts/Bed-Wskt2/Bed-Wskt2.html",
    "title": "Bed-Wskt2",
    "section": "",
    "text": "Als Bildungsforscher(in) untersuchen Sie den Lernerfolg in einem Statistikkurs.\nEine Gruppe von Studierenden absolviert einen Statistikkurs. Ein Teil lernt gut mit (Ereignis \\(A\\)), ein Teil nicht (Ereignis \\(A^C\\)). Ein Teil besteht die Prüfung (Ereignis \\(B\\)); ein Teil nicht (\\(B^C\\)).\nHinweis: Das Gegenereignis zum Ereignis \\(A\\) wird oft das Komplementärereignis oder kurz Komplement von \\(A\\) genannt und mit \\(A^C\\) bezeichnet.\nWir ziehen zufällig eine/n Studierende/n: Siehe da – Die Person hat bestanden. Yeah!\nAufgabe: Gesucht ist die Wahrscheinlichkeit, dass diese Person gut mitgelernt hat, gegeben der Tatsache, dass dieser Person bestanden hat.\nDie Anteile der Gruppen (bzw. Wahrscheinlichkeit des Ereignisses) lassen sich unten stehender Tabelle entnehmen.\n\n\n\n\n\n\n\n\nrow_ids\nB\nBneg\n\n\n\n\nA\n0.42\n0.20\n\n\nAneg\n0.30\n0.08\n\n\n\n\n\n\nHinweise:\n\nRunden Sie auf 2 Dezimalstellen.\nGeben Sie Anteile stets in der Form 0.42 an (mit führender Null und Dezimalzeichen).\n“Aneg” bezieht sich auf das Komplementärereignis zu A.\n\n\n\n\nZeichnen Sie (per Hand) ein Baumdiagramm, um die gemeinsamen Wahrscheinlichkeiten darzustellen. Weiterhin sollen die Randwahrscheinlichkeiten für \\(A\\) dargestellt sein.\nZeichnen Sie (per Hand) ein Baumdiagramm, um diesen Sachverhalt darzustellen.\nGeben Sie die Wahrscheinlichkeit des gesuchten Ereignisses an."
  },
  {
    "objectID": "posts/Bed-Wskt2/Bed-Wskt2.html#answerlist",
    "href": "posts/Bed-Wskt2/Bed-Wskt2.html#answerlist",
    "title": "Bed-Wskt2",
    "section": "",
    "text": "Zeichnen Sie (per Hand) ein Baumdiagramm, um die gemeinsamen Wahrscheinlichkeiten darzustellen. Weiterhin sollen die Randwahrscheinlichkeiten für \\(A\\) dargestellt sein.\nZeichnen Sie (per Hand) ein Baumdiagramm, um diesen Sachverhalt darzustellen.\nGeben Sie die Wahrscheinlichkeit des gesuchten Ereignisses an."
  },
  {
    "objectID": "posts/Regression2/Regression2.html",
    "href": "posts/Regression2/Regression2.html",
    "title": "Regression2",
    "section": "",
    "text": "Ein Streudiagramm von \\(x\\) und \\(y\\) ergibt folgende Abbildung:\n\n\n\n\n\n\n\n\n\nWählen Sie das am besten passende Modell aus der Liste aus!\n\n\n\n\\(y = 40 + 10 \\cdot x + \\epsilon\\)\n\\(y = -40 + 10 \\cdot x + \\epsilon\\)\n\\(y = 40 + -10 \\cdot x + \\epsilon\\)\n\\(y = -40 + -10 \\cdot x + \\epsilon\\)\n\\(y = 0 + -40 \\cdot x + \\epsilon\\)"
  },
  {
    "objectID": "posts/Regression2/Regression2.html#answerlist",
    "href": "posts/Regression2/Regression2.html#answerlist",
    "title": "Regression2",
    "section": "",
    "text": "\\(y = 40 + 10 \\cdot x + \\epsilon\\)\n\\(y = -40 + 10 \\cdot x + \\epsilon\\)\n\\(y = 40 + -10 \\cdot x + \\epsilon\\)\n\\(y = -40 + -10 \\cdot x + \\epsilon\\)\n\\(y = 0 + -40 \\cdot x + \\epsilon\\)"
  },
  {
    "objectID": "posts/Regression2/Regression2.html#answerlist-1",
    "href": "posts/Regression2/Regression2.html#answerlist-1",
    "title": "Regression2",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nRichtig\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nregression\ndyn"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-17/Verteilungen-Quiz-17.html",
    "href": "posts/Verteilungen-Quiz-17/Verteilungen-Quiz-17.html",
    "title": "Verteilungen-Quiz-17",
    "section": "",
    "text": "Ei Forschi untersucht die mittlere Körpergröße eines bis dato unbekannten Urwaldvolks. Dabei findet sich aposteriori (also als Ergebnis der Untersuchung) \\(\\bar{x} \\sim N(160,5)\\) (in Zentimetern).\nDis Forschi resümiert: “Mit sehr hoher Wahrscheinlichkeit, also 95%, sind diese Menschen im Schnitt größer als 1 Meter 60 Zentimeter groß”.\nIst diese Aussage korrekt (gegeben der Angaben)?\nHinweise:\n\nNutzen Sie Simulationsmethoden zur Lösung\nFixieren Sie die Zufallszahlen auf die Startzahl 42.\nZiehen Sie \\(10^5\\) Zufallszahlen aus der gegebenen Verteilung.\n\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-17/Verteilungen-Quiz-17.html#answerlist",
    "href": "posts/Verteilungen-Quiz-17/Verteilungen-Quiz-17.html#answerlist",
    "title": "Verteilungen-Quiz-17",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-17/Verteilungen-Quiz-17.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-17/Verteilungen-Quiz-17.html#answerlist-1",
    "title": "Verteilungen-Quiz-17",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/boxplots-de1a/boxplots-de1a.html",
    "href": "posts/boxplots-de1a/boxplots-de1a.html",
    "title": "boxplots-de1a",
    "section": "",
    "text": "laut zwei Stichproben (A und B) mithilfe zweier Boxplots dargestellt. Welche der folgenden Aussagen ist korrekt?\nHinweis: Die Aussagen sind entweder eindeutig richtig oder eindeutig falsch.\n\n\n\n\n\n\n\n\nDie zentrale Tendenz der Verteilungen ist (etwa) identisch.\nBeide Verteilungen haben keine Ausreißer.\nDie Streuung in Stichprobe A ist deutlich größer als in Stichprobe B.\nDie Schiefe der beiden Stichproben ist ähnlich.\nVerteilung B ist rechtsschief."
  },
  {
    "objectID": "posts/boxplots-de1a/boxplots-de1a.html#answerlist",
    "href": "posts/boxplots-de1a/boxplots-de1a.html#answerlist",
    "title": "boxplots-de1a",
    "section": "",
    "text": "Die zentrale Tendenz der Verteilungen ist (etwa) identisch.\nBeide Verteilungen haben keine Ausreißer.\nDie Streuung in Stichprobe A ist deutlich größer als in Stichprobe B.\nDie Schiefe der beiden Stichproben ist ähnlich.\nVerteilung B ist rechtsschief."
  },
  {
    "objectID": "posts/boxplots-de1a/boxplots-de1a.html#answerlist-1",
    "href": "posts/boxplots-de1a/boxplots-de1a.html#answerlist-1",
    "title": "boxplots-de1a",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr. Die Verteilungen haben eine etwa identische zentrale Tendenz.\nFalsch. Es gibt Beobachtungen, die mehr als das 1.5-fache des Interquartilsabstand von der Box entfernt sind.\nFalsch. Der Interquartilsabstand in Stichprobe A ist nicht deutlich größer als in B.\nFalsch. Die Schiefe der beiden Verteilungen ist unterschiedlich. Stichprobe A ist etwa symmtrisch. Stichprobe B ist linksschief.\nFalsch. Verteilung B ist linksschief.\n\n\nCategories:\n\nvis\neda\nschoice"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-10/Verteilungen-Quiz-10.html",
    "href": "posts/Verteilungen-Quiz-10/Verteilungen-Quiz-10.html",
    "title": "Verteilungen-Quiz-10",
    "section": "",
    "text": "Ist folgende Aussage wahr?\nSei \\(X \\sim N(100,15)\\), dann ist \\(Pr(X = \\bar{x}) = 1/2\\).\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-10/Verteilungen-Quiz-10.html#answerlist",
    "href": "posts/Verteilungen-Quiz-10/Verteilungen-Quiz-10.html#answerlist",
    "title": "Verteilungen-Quiz-10",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-10/Verteilungen-Quiz-10.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-10/Verteilungen-Quiz-10.html#answerlist-1",
    "title": "Verteilungen-Quiz-10",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/mtcars-post2/mtcars-post2.html",
    "href": "posts/mtcars-post2/mtcars-post2.html",
    "title": "mtcars-post2",
    "section": "",
    "text": "Im Datensatz mtcars: Wie groß ist der Effekt der UV vs auf die AV mpg? Geben Sie die Breite des 95% PI an (im Bezug zur gesuchten Größe). Berechnen Sie das dazu passende Modell mit Methoden der Bayes-Statistik.\nHinweise\nWählen Sie die am besten passende Option:\n\n\n\n0.7\n2.7\n4.7\n6.7\n8.7"
  },
  {
    "objectID": "posts/mtcars-post2/mtcars-post2.html#answerlist",
    "href": "posts/mtcars-post2/mtcars-post2.html#answerlist",
    "title": "mtcars-post2",
    "section": "",
    "text": "0.7\n2.7\n4.7\n6.7\n8.7"
  },
  {
    "objectID": "posts/mtcars-post2/mtcars-post2.html#answerlist-1",
    "href": "posts/mtcars-post2/mtcars-post2.html#answerlist-1",
    "title": "mtcars-post2",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nWahr\nFalsch\n\n\nCategories:\n\nbayes\nregression\npost\nexam-22"
  },
  {
    "objectID": "posts/filter01/filter01.html",
    "href": "posts/filter01/filter01.html",
    "title": "filter01",
    "section": "",
    "text": "Aufgabe\nIm Datensatz mariokart:\nFiltern Sie alle Spiele, die mehr als 50 Euro kosten (total_pr) erzielt haben und die Versandkosten erheben (ship_pr)!\nGeben Sie die Antwort der Zeilen zurück, die nach dem Filtern im Datensatz verbleiben!\n         \n\n\nLösung\nPakete starten:\n\nlibrary(easystats)\nlibrary(tidyverse)  # startet das Paket tidyverse\n\nDaten importieren:\n\nmariokart &lt;- data_read(\"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\")\n\nFiltern:\n\nmariokart2 &lt;- filter(mariokart, total_pr &gt; 50.00 & ship_pr &gt; 0)  # R bentzt Dezimalpunkt\n\nDie Lösung lautet: 32 Zeilen verbleiben im Datensatz nach dem Filtern.\n\nCategories:\n\ndatawrangling\neda\ntidyverse\ndplyr\nnum"
  },
  {
    "objectID": "posts/tidymodels-ames-02/tidymodels-ames-02.html",
    "href": "posts/tidymodels-ames-02/tidymodels-ames-02.html",
    "title": "tidymodels-ames-02",
    "section": "",
    "text": "Aufgabe\nBerechnen Sie ein lineares Modell mit tidymodels und zwar anhand des ames Datensatzes.\nModellgleichung: Sale_Price ~ Gr_Liv_Area, data = ames.\nBerechnen Sie ein multiplikatives (exponenzielles) Modell.\nGesucht ist R-Quadrat als Maß für die Modellgüte im TEST-Sample.\nHinweise:\n\nFixieren Sie die Zufallszahlen auf den Startwert 42.\n\n         \n\n\nLösung\n\nlibrary(tidymodels)\ndata(ames)\n\nMultiplikatives Modell:\n\names &lt;- \n  ames %&gt;% \n  mutate(Sale_Price = log10(Sale_Price)) %&gt;% \n  select(Sale_Price, Gr_Liv_Area)\n\nNicht vergessen: AV-Transformation in beiden Samples!\nDatensatz aufteilen:\n\nset.seed(42)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\nModell definieren:\n\nm1 &lt;-\n  linear_reg() # engine ist \"lm\" im Default\n\nModell fitten:\n\nfit1 &lt;-\n  m1 %&gt;% \n  fit(Sale_Price ~ Gr_Liv_Area, data = ames)\n\n\nfit1 %&gt;% pluck(\"fit\") \n\n\nCall:\nstats::lm(formula = Sale_Price ~ Gr_Liv_Area, data = data)\n\nCoefficients:\n(Intercept)  Gr_Liv_Area  \n  4.8552133    0.0002437  \n\n\nModellgüte im Train-Sample:\n\nfit1_performance &lt;-\n  fit1 %&gt;% \n  extract_fit_engine()  # identisch zu pluck(\"fit\")\n\nModellgüte im Train-Sample:\n\nfit1_performance %&gt;% summary()\n\n\nCall:\nstats::lm(formula = Sale_Price ~ Gr_Liv_Area, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.02587 -0.06577  0.01342  0.07202  0.39231 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 4.855e+00  7.355e-03  660.12   &lt;2e-16 ***\nGr_Liv_Area 2.437e-04  4.648e-06   52.43   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1271 on 2928 degrees of freedom\nMultiple R-squared:  0.4842,    Adjusted R-squared:  0.484 \nF-statistic:  2749 on 1 and 2928 DF,  p-value: &lt; 2.2e-16\n\n\nR-Quadrat via easystats:\n\nlibrary(easystats)\nfit1_performance %&gt;% r2()  # rmse()\n\n# R2 for Linear Regression\n       R2: 0.484\n  adj. R2: 0.484\n\n\n\ntidy(fit1_performance)  # ähnlich zu parameters()\n\n# A tibble: 2 × 5\n  term        estimate  std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) 4.86     0.00736        660.        0\n2 Gr_Liv_Area 0.000244 0.00000465      52.4       0\n\n\nVorhersagen im Test-Sample:\n\npreds &lt;- predict(fit1, new_data = ames_test)  # liefert TABELLE (tibble) zurück\nhead(preds)\n\n# A tibble: 6 × 1\n  .pred\n  &lt;dbl&gt;\n1  5.07\n2  5.18\n3  5.31\n4  5.11\n5  5.18\n6  5.10\n\n\npreds ist ein Tibble, also müssen wir noch die Spalte .pred. herausziehen, z.B. mit pluck(preds, \".pred\"):\n\npreds_vec &lt;- preds$.pred\n\n\names_test2 &lt;-\n  ames_test %&gt;% \n  mutate(preds = pluck(preds, \".pred\"),  # pluck aus der Tabelle rausziehen\n         .pred = preds_vec)  # oder  mit dem Dollar-Operator\n\nhead(ames_test2)\n\n# A tibble: 6 × 4\n  Sale_Price Gr_Liv_Area preds .pred\n       &lt;dbl&gt;       &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1       5.02         896  5.07  5.07\n2       5.24        1329  5.18  5.18\n3       5.60        1856  5.31  5.31\n4       5.15        1056  5.11  5.11\n5       5.26        1337  5.18  5.18\n6       4.98         987  5.10  5.10\n\n\nOder mit unnest:\n\names_test2 &lt;-\n  ames_test %&gt;% \n  mutate(preds = preds) %&gt;% \n  unnest(preds) # Listenspalte \"entschachteln\"\n\nhead(ames_test2)\n\n# A tibble: 6 × 3\n  Sale_Price Gr_Liv_Area .pred\n       &lt;dbl&gt;       &lt;int&gt; &lt;dbl&gt;\n1       5.02         896  5.07\n2       5.24        1329  5.18\n3       5.60        1856  5.31\n4       5.15        1056  5.11\n5       5.26        1337  5.18\n6       4.98         987  5.10\n\n\nOder wir binden einfach die Spalte an den Tibble:\n\names_test2 &lt;-\n  ames_test %&gt;% \n  bind_cols(preds = preds)  # nimmt Tabelle und bindet die Spalten dieser Tabelle an eine Tabelle\n\nhead(ames_test2)\n\n# A tibble: 6 × 3\n  Sale_Price Gr_Liv_Area .pred\n       &lt;dbl&gt;       &lt;int&gt; &lt;dbl&gt;\n1       5.02         896  5.07\n2       5.24        1329  5.18\n3       5.60        1856  5.31\n4       5.15        1056  5.11\n5       5.26        1337  5.18\n6       4.98         987  5.10\n\n\nModellgüte im Test-Sample:\n\nrsq(ames_test2,\n    truth = Sale_Price,\n    estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.517\n\n\n\nsol &lt;- 0.51679\n\nZur Interpretation von Log10-Werten\n\n5e5\n\n[1] 5e+05\n\n5*10^5 - 500000\n\n[1] 0\n\n\n\nCategories:\n\nds1\ntidymodels\nprediction\nyacsda\nstat-learning\nnum"
  },
  {
    "objectID": "posts/wrangle1/wrangle1.html",
    "href": "posts/wrangle1/wrangle1.html",
    "title": "wrangle1",
    "section": "",
    "text": "Welche der folgenden Spalte ist nicht Teil des Datensatzes flights aus dem R-Paket nycflights13?\nAlternativ können Sie den Datensatz hier beziehen. Hilfe zum Datensatz (Codebook) finden Sie hier.\n\n\n\nyear\nmonth\n\nday\ndep_time\nsched_dep_time\nestimated_dep_time\narr_time\nsched_arr_time"
  },
  {
    "objectID": "posts/wrangle1/wrangle1.html#answerlist",
    "href": "posts/wrangle1/wrangle1.html#answerlist",
    "title": "wrangle1",
    "section": "",
    "text": "year\nmonth\n\nday\ndep_time\nsched_dep_time\nestimated_dep_time\narr_time\nsched_arr_time"
  },
  {
    "objectID": "posts/wrangle1/wrangle1.html#answerlist-1",
    "href": "posts/wrangle1/wrangle1.html#answerlist-1",
    "title": "wrangle1",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\n\nFalsch\nFalsch\nFalsch\nWahr\nFalsch\nFalsch\n\n\nCategories:\n\neda\ndatawrangling\ntidyverse\ndplyr\nschoice"
  },
  {
    "objectID": "posts/Rethink_2m5/Rethink_2m5.html",
    "href": "posts/Rethink_2m5/Rethink_2m5.html",
    "title": "Rethink_2m5",
    "section": "",
    "text": "Exercise\nThis question is taken from McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Ed.). Taylor and Francis, CRC Press.\n2M5. Now suppose there are four cards: B/B, B/W, W/W, and another B/B. Again suppose a card is drawn from the bag and a black side appears face up. Again calculate the probability that the other side is black.\n         \n\n\nSolution\nThe only difference to the question 2M4 is that we now have two bb cards, rendering the prior plausibility twice as high.\nLet’s label the cards bb (black on both sides), bw (black on one, white on the other), and ww (both sides are white), respectively.\nWanted is the probability that the second side of the card is black (2b), given one side is black (1b): \\(Pr(2b|1b)\\).\n\nd &lt;-\n  tibble::tribble(\n  ~Hyp, ~Prior,\n  \"bb\",     2L, \n  \"bw\",     1L,   \n  \"ww\",     1L, \n  ) %&gt;% \n  mutate(Likelihood = c(2,1,0),\n         unstand_post = Prior*Likelihood,\n         std_post = unstand_post / sum(unstand_post))\n\n\n\n\n\n\n\n\n\n\nHyp\nPrior\nLikelihood\nunstand_post\nstd_post\n\n\n\n\nbb\n2\n2\n4\n0.80\n\n\nbw\n1\n1\n1\n0.20\n\n\nww\n1\n0\n0\n0.00\n\n\n\n\n\n\n\nThe following solution is taken from this source.\n\ncard_bb_likelihood &lt;- 2\ncard_bw_likelihood &lt;- 1\ncard_ww_likelihood &lt;- 0\n\nlikelihood &lt;- c(card_bb_likelihood, card_bw_likelihood, card_ww_likelihood,\n                card_bb_likelihood)\nprior &lt;- c(1, 1, 1, 1)\nposterior &lt;- likelihood * prior\nposterior &lt;- posterior / sum(posterior)\n\nposterior[1] + posterior[4]\n\n[1] 0.8\n\n\n\nCategories:\n\nprobability\nbayes"
  },
  {
    "objectID": "posts/Typ-Fehler-R-06a/Typ-Fehler-R-06a.html",
    "href": "posts/Typ-Fehler-R-06a/Typ-Fehler-R-06a.html",
    "title": "Typ-Fehler-R-06a",
    "section": "",
    "text": "Aufgabe\nBetrachten Sie folgende R-Syntax, für die R eine Fehlermeldung ausgibt:\n\nx &lt;- c(1, 2, 3)\nsum(abs(mean(x) - x)))\n\nError: &lt;text&gt;:2:22: unexpected ')'\n1: x &lt;- c(1, 2, 3)\n2: sum(abs(mean(x) - x)))\n                        ^\n\n\nGeben Sie die korrekte Syntax an! Ändern Sie nur die notwendigen Zeichen an der Syntax oben. Gehen Sie davon aus, dass die aufgerufenen Funktionen existieren.\nGeben Sie keine Leerzeichen ein.\n         \n\n\nLösung\nHinten ist eine (schließende) Klammer zu viel, die muss weg:\n\nsum(abs(mean(x)-x))  # so geht's\n\nError in mean(x): object 'x' not found\n\n\nDie Antwort lautet: sum(abs(mean(x)-x)).\n\nCategories:\n\nR\n‘2023’\nstring"
  },
  {
    "objectID": "posts/mtcars-abhaengig_var2/mtcars-abhaengig_var2.html",
    "href": "posts/mtcars-abhaengig_var2/mtcars-abhaengig_var2.html",
    "title": "mtcars-abhaengig_var2",
    "section": "",
    "text": "Aufgabe\nIm Folgenden ist der Datensatz mtcars zu analysieren.\nDer Datensatz ist z.B. als CSV-Datei von dieser Webseite abrufbar.\nHilfe zum Datensatz ist via help(\"name_des_datensatzes\") oder auf dieser Webseite abrufbar.\nOb die Variable hp (UV; Ereignis \\(A\\)) und Spritverbrauch (mpg; AV; Ereignis \\(B\\)) wohl voneinander abhängig sind? Was meinen Sie? Was ist Ihre Einschätzung dazu? Vermutlich haben Sie ein (wenn vielleicht auch implizites) Vorab-Wissen zu dieser Frage. Lassen wir dieses Vorab-Wissen aber einmal außen vor und schauen uns rein Daten dazu an. Vereinfachen wir die Frage etwas, indem wir beide Variablen am Mittelwert aufteilen: Wenn eine Beobachtung (d.h. ein Auto) einen Wert in der jeweiligen Variablen höchstens so groß wie der Mittelwert der Variable aufweist, geben wir der Beobachtung der Wert 0, ansonsten den Wert 1. Wir nehmen die Anteile der gesuchten Größen als Schätzwert für deren Wahrscheinlichkeit.\nDie resultierenden binären Variablen nennen wir av_high bzw. uv_high (im schönsten Denglisch).\nBerechnen Sie: \\(Pr(\\neg \\text{uvhigh} \\, | \\, \\text{avhigh})\\)\nHinweise:\n\nDas “Ellbogen-Zeichen” \\(\\neg\\) kennzeichnet eine logische Negierung (das Gegenteil).\nDie angegebene Wahrscheinlichkeit ist eine bedingte Wahrscheinlichkeit.\nWeitere Hinweise\n\n         \n\n\nLösung\nSchauen wir zuerst mal in den Datensatz:\n\nmtcars %&gt;% \n  select(mpg, hp) %&gt;% \n  slice_head(n = 5)\n\n                   mpg  hp\nMazda RX4         21.0 110\nMazda RX4 Wag     21.0 110\nDatsun 710        22.8  93\nHornet 4 Drive    21.4 110\nHornet Sportabout 18.7 175\n\n\nDann berechnen wir die binären Variablen.\nZuerst av_high:\n\n# split by mean:\nd2 &lt;-\n  mtcars %&gt;% \n  select(mpg, hp) %&gt;% \n  mutate(av_high = case_when(\n    mpg &lt;= mean(mpg) ~ 0,\n    mpg &gt; mean(mpg) ~ 1\n  )) %&gt;% \n  select(-mpg) \n\nglimpse(d2)\n\nRows: 32\nColumns: 2\n$ hp      &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, …\n$ av_high &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,…\n\n\nav_high = 1 zeigt hohe Werte in mpg an, und av_high = 0 zeigt geringe Werte (im Verhältnis zum Mittelwert).\nMan beachte, dass hohe Werte in MPG einen geringen Spritverbrauch bedeuten (also eine hohe Sparsamkeit im Verbrauch).\nDann berechnen wir uv_high:\n\nd3 &lt;-\n  d2 %&gt;% \n  select(av_high, hp) %&gt;% \n  mutate(uv_high = case_when(\n    hp &lt;= mean(hp) ~ 0,\n    hp &gt; mean(hp) ~ 1\n  )) %&gt;% \n  select(-hp) \n\nglimpse(d3)\n\nRows: 32\nColumns: 2\n$ av_high &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,…\n$ uv_high &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,…\n\n\nDann zählen wir die gesuchten Wahrscheinlichkeiten bzw. Anteile der AV:\n\nd3 %&gt;% \n  count(av_high)\n\n  av_high  n\n1       0 18\n2       1 14\n\n\nEs gibt also 14 Autos, die den oben gesuchten “hinteren Teil” der Bedingung erfüllen (av_high = 1).\nFiltern wir als nächstes nur in diesen 14 Autos nach dem “vorderen Teil” der gesuchten Wahrscheinlichkeit, also uv_high = 0.\n\nd3 %&gt;% \n  filter(av_high == 1) %&gt;% \n  count(uv_high) %&gt;% \n  mutate(prop = n/sum(n))\n\n  uv_high  n prop\n1       0 14    1\n\n\nEs gibt also 14 von 14 Autos, die diese Bedingung, uv_high = 0 erfüllen. Das sind 100%.\nIn Worten: Von den Autos mit hoher Sparsamkeit haben alle eine geringe PS-Zahl. Das macht intuitiv Sinn.\nDer gesuchte Wert beträgt also 1.\n\nCategories:\n\ndyn\nprobability"
  },
  {
    "objectID": "posts/Rethink_2m2/Rethink_2m2.html",
    "href": "posts/Rethink_2m2/Rethink_2m2.html",
    "title": "Rethink_2m2",
    "section": "",
    "text": "Exercise\nThis question is taken from McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Ed.). Taylor and Francis, CRC Press.\nRecall the globe tossing model from the chapter. Compute and plot the grid approximate posterior distribution for each of the following sets of observations. In each case, assume a uniform prior for p.\nData:\n\nWWW\nWWWL\nLWWLWWW\n\nNow assume a prior for p that is equal to zero when p &lt; 0.5 and is a positive constant when p ≥ 0.5. Again compute and plot the grid approximate posterior distribution for each of the sets of observations in the problem just above.\n         \n\n\nSolution\nThe solution is taken from this source.\n\nlibrary(tidyverse)\n\ndist &lt;- \n  tibble(\n    # Gridwerte bestimmen:\n    p_grid = seq(from = 0, to = 1, length.out = 20),\n    # Priori-Wskt bestimmen:\n    prior  = case_when(\n      p_grid &lt; 0.5 ~ 0,\n      p_grid &gt;= 0.5 ~ 1)) %&gt;%\n  mutate(\n    # Likelihood berechnen:\n    likelihood_1 = dbinom(3, size = 3, prob = p_grid),\n    likelihood_2 = dbinom(3, size = 4, prob = p_grid),\n    likelihood_3 = dbinom(5, size = 7, prob = p_grid),\n    # unstand. Posterior-Wskt:\n    unstand_post_1 = likelihood_1 * prior,\n    unstand_post_2 = likelihood_2 * prior,\n    unstand_post_3 = likelihood_3 * prior,\n    # stand. Post-Wskt:\n    std_post_1 = unstand_post_1 / sum(unstand_post_1),\n    std_post_2 = unstand_post_2 / sum(unstand_post_1),\n    std_post_3 = unstand_post_3 / sum(unstand_post_1)\n    ) \n\nJetzt können wir das Diagramm zeichnen:\n\nggplot(dist) +\n  aes(x = p_grid, y= std_post_1) +\n  geom_line()+\n  geom_point() +\n  labs(x = \"p(W)\",\n       y = \"Posteriori-Wahrscheinlichkeit\",\n       title = \"Daten: WWW\")\n\n\n\n\n\n\n\n\n\nggplot(dist) +\n  aes(x = p_grid, y= std_post_2) +\n  geom_line()+\n  geom_point() +\n  labs(x = \"p(W)\",\n       y = \"Posteriori-Wahrscheinlichkeit\",\n       title = \"Daten: WWWL\")\n\n\n\n\n\n\n\n\n\nggplot(dist) +\n  aes(x = p_grid, y= std_post_3) +\n  geom_line()+\n  geom_point() +\n  labs(x = \"p(W)\",\n       y = \"Posteriori-Wahrscheinlichkeit\",\n       title = \"Daten: LWWLWWW\")\n\n\n\n\n\n\n\n\nEtwas eleganter (und deutlich komplizierter) kann man es auch so in R schreiben (Quelle):\n\ndist &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = 20)) %&gt;%\n  mutate(prior = case_when(\n    p_grid &lt; 0.5 ~ 0L,\n    TRUE ~ 1L),\n    likelihood_1 = dbinom(3, size = 3, prob = p_grid),\n    likelihood_2 = dbinom(3, size = 4, prob = p_grid),\n    likelihood_3 = dbinom(5, size = 7, prob = p_grid),\n    across(starts_with(\"likelihood\"), ~ .x * prior),\n    across(starts_with(\"likelihood\"), ~ .x / sum(.x))) %&gt;%\n  pivot_longer(cols = starts_with(\"likelihood\"), names_to = \"pattern\",\n               values_to = \"posterior\") %&gt;%\n  separate(pattern, c(NA, \"pattern\"), sep = \"_\", convert = TRUE) %&gt;%\n  mutate(obs = case_when(pattern == 1L ~ \"W, W, W\",\n                         pattern == 2L ~ \"W, W, W, L\",\n                         pattern == 3L ~ \"L, W, W, L, W, W, W\"))\n\nggplot(dist, aes(x = p_grid, y = posterior)) +\n  facet_wrap(vars(fct_inorder(obs)), nrow = 1) +\n  geom_line() +\n  geom_point() +\n  labs(x = \"Proportion Water (p)\", y = \"Posterior Density\")\n\n\n\n\n\n\n\n\n\nCategories:\n\nprobability\nbayes-grid"
  },
  {
    "objectID": "posts/affairs-dplyr/affairs-dplyr.html",
    "href": "posts/affairs-dplyr/affairs-dplyr.html",
    "title": "affairs-dplyr",
    "section": "",
    "text": "Aufgabe\nLaden Sie den Datensatz affairs:\n\naffairs_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/AER/Affairs.csv\"\n\n\naffairs &lt;- read.csv(affairs_path)\n\nLesen Sie das Data Dictionnary hier.\nWir definieren als “Halodrie” eine Person mit mindestens einer Affäre (laut Datensatz).\nBearbeiten Sie folgende Aufgaben:\n\nFiltern Sie mal nach Halodries!\nSortieren Sie (absteigend) nach Anzahl der Affären!\nWählen Sie die Spalten zu Anzahl der Affären, ob es Kinder in der Ehe gibt und die Zufriedenheit mit der Ehe. Dann sortieren Sie dann nach Anzahl der Kinder und danach nach der Anzahl der Affären.\nBerechnen Sie die mittlere Anzahl der Affären!\nBerechnen Sie die mittlere Anzahl der Affären pro Geschlecht und aufgeteilt auf Partnerschaften mit bzw. ohne Kinder.\nGeben Sie für jede Person die höhere der zwei Zahlen von Religiösität und Ehezufriedenheit aus!\nBerechnen Sie jeweils das Heiratsalter!\n\n         \n\n\nLösung\n\nlibrary(tidyverse)\n\nAd 1.\n\naffairs %&gt;% \n  filter(affairs &gt; 0) %&gt;% \n  head(10)\n\n   X affairs gender age yearsmarried children religiousness education\n1  6       3   male  27          1.5       no             3        18\n2 12       3 female  27          4.0      yes             3        17\n  occupation rating\n1          4      4\n....\n\n\nHinweis: head(10) begrenzt die Ausgabe auf 10 Zeilen, einfach um den Bildschirm nicht vollzumüllen.\nAd 2.\n\naffairs %&gt;% \n  arrange(-affairs) %&gt;% \n  head(10)\n\n    X affairs gender age yearsmarried children religiousness education\n1  53      12 female  32           10      yes             3        17\n2 122      12   male  37           15      yes             4        14\n  occupation rating\n1          5      2\n2          5      2\n [ reached 'max' / getOption(\"max.print\") -- omitted 8 rows ]\n\n\nAd 3.\n\naffairs %&gt;% \n  select(affairs, rating, children) %&gt;% \n  arrange(children, affairs) %&gt;% \n  head(10)\n\n  affairs rating children\n1       0      4       no\n2       0      4       no\n3       0      3       no\n4       0      5       no\n5       0      3       no\n6       0      5       no\n [ reached 'max' / getOption(\"max.print\") -- omitted 4 rows ]\n\n\nAd 4.\n\naffairs %&gt;% \n  summarise(affairs_mean = mean(affairs)) %&gt;% \n  head(10)\n\n  affairs_mean\n1     1.455907\n\n\nAd 5.\n\naffairs %&gt;% \n  group_by(gender, children) %&gt;% \n  summarise(affairs_mean = mean(affairs)) %&gt;% \n  head(10)\n\n# A tibble: 4 × 3\n# Groups:   gender [2]\n  gender children affairs_mean\n  &lt;chr&gt;  &lt;chr&gt;           &lt;dbl&gt;\n1 female no              0.838\n2 female yes             1.69 \n3 male   no              1.01 \n4 male   yes             1.66 \n\n\nAd 6.\n\naffairs %&gt;% \n  group_by(X) %&gt;% \n  summarise(max(c(religiousness, rating))) %&gt;% \n  head(10)\n\n# A tibble: 10 × 2\n       X `max(c(religiousness, rating))`\n   &lt;int&gt;                           &lt;int&gt;\n 1     4                               4\n 2     5                               4\n 3     6                               4\n 4    11                               4\n 5    12                               5\n 6    16                               5\n 7    23                               3\n....\n\n\nAd 7.\n\naffairs %&gt;% \n  mutate(heiratsalter = age - yearsmarried) %&gt;%\n  head(10)\n\n  X affairs gender age yearsmarried children religiousness education occupation\n1 4       0   male  37           10       no             3        18          7\n  rating heiratsalter\n1      4           27\n [ reached 'max' / getOption(\"max.print\") -- omitted 9 rows ]\n\n\n\nCategories:\n\ndatawrangling\neda\nstring"
  },
  {
    "objectID": "posts/movies-vis2/movies-vis2.html",
    "href": "posts/movies-vis2/movies-vis2.html",
    "title": "movies-vis2",
    "section": "",
    "text": "Aufgabe\nImportieren Sie bitte für diese Aufgabe den Datensatz movies (aus dem R-Paket ggplot2movies). Ein Data-Dictionary findet sich hier.\nErstellen Sie folgende Visualisierung:\n\nGruppenvergleich des Budgets pro Jahr\nBerücksichtigen Sie nur Actionfilme ab 2000\nVerzichten Sie auf Filme mit einer unterdurchschnittlichen Zahl an Bewertungen (votes; gemessen an allen Filmen, gerundet zur nächsten ganzen Zahl)\n\n         \n\n\nLösung\nPakete starten:\n\nlibrary(tidyverse)\nlibrary(DataExplorer)\n\nDaten importieren:\n\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/ggplot2movies/movies.csv\"\nd &lt;- read.csv(d_path)\n\nDurchschnittliche Zahl an Bewertungen:\n\nd %&gt;% \n  summarise(votes_mean = mean(votes))\n\n  votes_mean\n1   632.1304\n\n\nDie durchschnittliche Zahl an Bewertungen beträgt also 632.\n\nd %&gt;% \n  select(budget, rating, year, votes, Action) %&gt;% \n  filter(year &gt;= 2000) %&gt;% \n  filter(Action == 1) %&gt;% \n  filter(votes &gt;= 632) %&gt;% \n  select(-Action) %&gt;% \n  mutate(year = factor(year)) %&gt;% \n  select(budget, year) %&gt;% \n  plot_boxplot(by = \"year\")\n\nWarning: Removed 66 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nHinweis: Die Zahl “5.0e+07” ist eine Zahl in der Exponenzial-Schreibweis, nämlich \\(5\\cdot10^7\\), also \\(5 \\cdot 1000000\\).\n\nCategories:\n\nvis\neda\nstring"
  },
  {
    "objectID": "posts/iq10/iq10.html",
    "href": "posts/iq10/iq10.html",
    "title": "iq10",
    "section": "",
    "text": "Exercise\nEine Forscherin, Prof. Weiss-Ois, untersucht den Effekt von Cannabis auf die Intelligenz.\nDazu untersucht Sie die Intelligenz langjähriger Konsumentis.\nProf. Weiss-Ois geht apriori von gleichverteilter Intelligenz aus. Ihre zentrale Hypothese ist \\(\\mu = 90\\pm5\\).\nMit großer Spannung wurden die Messdaten zur Intelligenz erwartet (die erst nach langem Streit über die zu verwendenden Intelligenztests erhoben werden konnten). Insgesamt wurden \\(N=541\\) Personen untersucht.\nTatsächlich sei die wahre IQ-Verteilung jener Cannabis-Konsumentis wie folgt: \\(IQ \\sim N(92.5, 7.5)\\). Natürlich kennt die Forscherin diese Verteilung nicht.\nWie wahrscheinlich ist die Hypothese der Forscherin im Lichte der Daten?\nHinweise:\n\nNutzen Sie Simulationsmethoden.\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\nSimulieren Sie \\(n=10^4\\) Stichproben.\nNutzen Sie die Zahl 42 als Startwert für Ihre Zufallszahlen (um die Reproduzierbarkeit zu gewährleisten).\nGitterwerte für die Intelligenz könnten z.B. 75 bis 130 sein.\n\n         \n\n\nSolution\n\nlibrary(tidyverse)\n\nZunächst basteln wir die Bayes-Box:\n\nn &lt;- 100\n\nset.seed(42)\npostvert &lt;-\n  tibble(p_grid = seq(from = 75, to = 130, length.out = n),\n         prior  = 1) %&gt;% \n  mutate(likelihood = dnorm(x = p_grid, mean = 92.5, sd = 7.5)) %&gt;% \n  mutate(unstand_post = likelihood * prior,\n         post = unstand_post / sum(unstand_post))\n\nWarum 75 bis 130? Das ist ein beliebiger Wert, in dem Sinne, dass Sie sich inhaltlich überlegen müssen, welchen Wertebereich Sie für plausibel halten. Untersucht man IQ-Mittelwerte so erscheint (mir) ein Wertebereich von 75 bis 130 mehr als ausreichend. Untersucht man Mittelwerte der Körpergröße (heutiger Menschen in bekannten Zivilisationen) so erscheint (mir) ein Wertebereich von 140 cm bis 200cm als ausreichend.\nAus der Post-Verteilung ziehen wir Stichproben:\n\nset.seed(42)\npostvert_stipros &lt;-\n  postvert %&gt;% \n  slice_sample(\n    n = 1e4,\n    weight_by = post,\n    replace = T) %&gt;% \n  select(p_grid)\n\nDamit haben wir unsere Stichproben-Post-Verteilung.\nDa slice_sample auch zufällig Stichproben zieht, müssen wir auch hier die Zufallszahlen fixieren, wenn wir die exakt gleichen Ergebnisse reproduzieren wollen.\nJetzt schauen wir (mit Spannung), wie hoch die Wahrscheinlichkeit ist für Parameterwete (p_grid) innerhalb des Intervalls wie von der Forscherin vorgegeben.\n\npostvert_stipros %&gt;% \n  count(between(p_grid, left = 85, right = 95))\n\n# A tibble: 2 × 2\n  `between(p_grid, left = 85, right = 95)`     n\n  &lt;lgl&gt;                                    &lt;int&gt;\n1 FALSE                                     5002\n2 TRUE                                      4998\n\n\nbetween ist eine Komfortfuntion; ins Deutsche übersetzt sagt die Syntax:\nNimm die Tabelle postvert_stipros und dann ...\n  zähle den Anteil der Werte von p_grid zwischen 85 und 95.\nDie Wahrscheinlichkeit der Hypothese der Forscherin beträgt also ca. 50%.\nOb das viel oder weniger ist, ist eine subjektive Frage. Das beste Vorgehen wäre jetzt, die Hypothesen anderer Forschis dagegen zu legen. Dann würde man sehen, welche Hypothese am besten zu den Daten passt.\nBasteln wir zum Vergleich eine Bayes-Box mit Gitterwerte von von 60 bis 145:\n\nn &lt;- 100\n\nset.seed(42)\npostvert2 &lt;-\n  tibble(p_grid = seq(from = 60, to = 145, length.out = n),\n         prior  = 1) %&gt;% \n  mutate(likelihood = dnorm(x = p_grid, mean = 92.5, sd = 7.5)) %&gt;% \n  mutate(unstand_post = likelihood * prior,\n         post = unstand_post / sum(unstand_post))\n\nset.seed(42)\npostvert_stipros2 &lt;-\n  postvert2 %&gt;% \n  slice_sample(\n    n = 1e4,\n    weight_by = post,\n    replace = T) %&gt;% \n  select(p_grid)\n\nOb sich das Ergebnis ändert, jetzt, da wir einen breiteren Bereich an Gitterwerten untersuchzen?\nJetzt schauen wir wieder (mit Spannung), wie hoch die Wahrscheinlichkeit ist für Parameterwete (p_grid) innerhalb des Intervalls wie von der Forscherin vorgegeben.\n\npostvert_stipros2 %&gt;% \n  count(between(p_grid, left = 85, right = 95)) %&gt;% \n  mutate(prop = n/sum(n))\n\n# A tibble: 2 × 3\n  `between(p_grid, left = 85, right = 95)`     n  prop\n  &lt;lgl&gt;                                    &lt;int&gt; &lt;dbl&gt;\n1 FALSE                                     5455 0.546\n2 TRUE                                      4545 0.454\n\n\nDie Ergebnisse sind leicht anders als oben, wo wir den engeren Wertebereich als mögliche Werte angegeben haben.\nAber bedenken Sie: Wir behaupten in postvert2 ernsthaft, dass ein Mittelwert der Intelligenz in dieser Population mit gleicher Wascheinlichkeit 60 oder 145 sein könnte! Laut Wikipedia beginnt mit 130 die Hochbegabung und unter 85 fängt die Lernbehinderung an. Dass diese Menschen super schlau oder mental behindert sind, erscheint uns gleich plausibel. Das ist eine sehr starke Apriori-Verteilung!\nViel konservativer wäre zu sagen: “Okay, vermutlich sind diese Menschen so schlau wie alle anderen auch, vielleicht etwas mehr oder etwas weniger. Aber mit sehr hoher Wahrscheinlichkeit sind sie im Durchschnitt keine Einsteins oder nicht extrem mental eingeschränkt.”\nWir brauchen also besser nicht gleichverteilte Priori-Verteilungen. Dazu an anderer Stelle mehr.\n\nCategories:\n\nprobability\nsimulation\nnormal-distribution\nbayes\nbayes-box"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-11/Verteilungen-Quiz-11.html",
    "href": "posts/Verteilungen-Quiz-11/Verteilungen-Quiz-11.html",
    "title": "Verteilungen-Quiz-11",
    "section": "",
    "text": "Ist folgende Aussage wahr?\nBei einer Verteilung gilt: \\(\\bar{x} = Md = \\text{Modus}\\).\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-11/Verteilungen-Quiz-11.html#answerlist",
    "href": "posts/Verteilungen-Quiz-11/Verteilungen-Quiz-11.html#answerlist",
    "title": "Verteilungen-Quiz-11",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-11/Verteilungen-Quiz-11.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-11/Verteilungen-Quiz-11.html#answerlist-1",
    "title": "Verteilungen-Quiz-11",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/mtcars-post3/mtcars-post3.html",
    "href": "posts/mtcars-post3/mtcars-post3.html",
    "title": "mtcars-post3",
    "section": "",
    "text": "Im Datensatz mtcars: Wie groß ist die Wahrscheinlichkeit, dass der Effekt der UV vs auf die AV mpg positiv ist? Berechnen Sie das dazu passende Modell mit Methoden der Bayes-Statistik.\nHinweise\nWählen Sie die am besten passende Option:\n\n\n\n.42\n.73\n.23\n1\n0"
  },
  {
    "objectID": "posts/mtcars-post3/mtcars-post3.html#answerlist",
    "href": "posts/mtcars-post3/mtcars-post3.html#answerlist",
    "title": "mtcars-post3",
    "section": "",
    "text": ".42\n.73\n.23\n1\n0"
  },
  {
    "objectID": "posts/mtcars-post3/mtcars-post3.html#answerlist-1",
    "href": "posts/mtcars-post3/mtcars-post3.html#answerlist-1",
    "title": "mtcars-post3",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nWahr\nFalsch\n\n\nCategories:\n\nbayes\nregression\npost\nexam-22"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-16/Verteilungen-Quiz-16.html",
    "href": "posts/Verteilungen-Quiz-16/Verteilungen-Quiz-16.html",
    "title": "Verteilungen-Quiz-16",
    "section": "",
    "text": "Für die Körpergröße des deutschen Mannes \\(X\\) gelte \\(X \\sim N(180,06)\\) (in Zentimetern).\nQuelle Mittelwert Quelle SD geschätzt\nÄhnliche Daten finden sich bei Our World in Data.\nIst folgende Aussage wahr?\nDas 50%-Quantil von \\(X\\) beträgt 180.\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-16/Verteilungen-Quiz-16.html#answerlist",
    "href": "posts/Verteilungen-Quiz-16/Verteilungen-Quiz-16.html#answerlist",
    "title": "Verteilungen-Quiz-16",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-16/Verteilungen-Quiz-16.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-16/Verteilungen-Quiz-16.html#answerlist-1",
    "title": "Verteilungen-Quiz-16",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/nasa01/nasa01.html",
    "href": "posts/nasa01/nasa01.html",
    "title": "nasa01",
    "section": "",
    "text": "Aufgabe\nViele Quellen berichten Klimadaten unserer Erde, z.B. auch National Aeronautics and Space Administration - Goddard Institute for Space Studies.\nVon dieser Quelle beziehen wir diesen Datensatz.\nDie Datensatz sind auf der Webseite wie folgt beschrieben:\nTables of Global and Hemispheric Monthly Means and Zonal Annual Means\nCombined Land-Surface Air and Sea-Surface Water Temperature Anomalies (Land-Ocean Temperature Index, L-OTI)\nThe following are plain-text files in tabular format of temperature anomalies, i.e. deviations from the corresponding 1951-1980 means.\n\nGlobal-mean monthly, seasonal, and annual means, 1880-present, updated through most recent month: TXT, CSV\n\nStarten Sie zunächst das R-Paket tidyverse falls noch nicht geschehen.\n\nlibrary(tidyverse)\n\nImportieren Sie dann die Daten:\n\ndata_path &lt;- \"https://data.giss.nasa.gov/gistemp/tabledata_v4/GLB.Ts+dSST.csv\"\nd &lt;- read_csv(data_path, skip = 1)\n\nWir lassen die 1. Zeile des Datensatzes aus (Argument skip), da dort Metadaten stehen, also keine Daten, sondern Informationen (Daten) zu den eigentlichen Daten.\nAufgabe\nBerechnen Sie die folgende Statistiken pro Dekade:\n\nMittelwert der Temperatur im Januar\nSD der Temperatur im Januar\n\nHinweise:\n\nSie müssen zuerst die Dekade als neue Spalte berechnen.\n\n         \n\n\nLösung\nDekade berechnen:\n\nd &lt;-\n  d %&gt;% \n  mutate(decade = round(Year/10))\n\nStatistiken pro Dekade:\n\nd_summarized &lt;- \n  d %&gt;% \n  group_by(decade) %&gt;% \n  summarise(temp_mean = mean(Jan),\n            temp_sd = sd(Jan))\n\nd_summarized\n\n\n\n\n\n\n\n  \n    \n    \n      decade\n      temp_mean\n      temp_sd\n    \n  \n  \n    188\n−0.20\n0.24\n    189\n−0.44\n0.22\n    190\n−0.27\n0.16\n    191\n−0.39\n0.22\n    192\n−0.28\n0.15\n    193\n−0.13\n0.22\n    194\n0.03\n0.21\n    195\n−0.05\n0.19\n    196\n0.03\n0.15\n    197\n−0.07\n0.17\n    198\n0.21\n0.19\n    199\n0.36\n0.13\n    200\n0.52\n0.19\n    201\n0.64\n0.20\n    202\n0.95\n0.14\n  \n  \n  \n\n\n\n\nZur Veranschaulichung visualisieren wir die Ergebnisse:\n\n\n\n\n\n\n\n\n\nAlternativ können Sie zum Visualisieren der Daten z.B. das Paket ggpubr nutzen:\n\nlibrary(ggpubr)\nggscatter(d_summarized, x = \"decade\", y = \"temp_mean\", add = \"reg.line\")\nggscatter(d_summarized, x = \"decade\", y = \"temp_sd\", add = \"reg.line\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFalls Sie Teile der R-Syntax nicht kennen: Machen Sie sich nichts daraus. 😄\n\nCategories:\n\ndata\neda\nlagemaße\nstring"
  },
  {
    "objectID": "posts/Regression3/Regression3.html",
    "href": "posts/Regression3/Regression3.html",
    "title": "Regression3",
    "section": "",
    "text": "dabei wird noch die Gruppierungsvariable \\(g\\) (mit den Stufen 0 und 1) berücksichtigt (vgl. Farbe und Form der Punkte). Zur besseren Orientierung ist die Regressionsgerade pro Gruppe eingezeichnet.\n\n\nWarning: Using the `size` aesthietic with geom_line was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\n\n\n\n\n\n\n\n\n\nWählen Sie das (für die Population) am besten passende Modell aus der Liste aus!\nHinweis: Ein Interaktionseffekt der Variablen \\(x\\) und \\(g\\) ist mit \\(xg\\) gekennzeichnet.\n\n\n\n\\(y = 40 + 10\\cdot x + 0 \\cdot g + 0 \\cdot xg + \\epsilon\\)\n\\(y = 40 + 10\\cdot x + 40 \\cdot g + -10 \\cdot xg + \\epsilon\\)\n\\(y = -40 + 10\\cdot x + -40 \\cdot g + 10 \\cdot xg + \\epsilon\\)\n\\(y = -40 + -10\\cdot x + -40 \\cdot g + -10 \\cdot xg + \\epsilon\\)"
  },
  {
    "objectID": "posts/Regression3/Regression3.html#answerlist",
    "href": "posts/Regression3/Regression3.html#answerlist",
    "title": "Regression3",
    "section": "",
    "text": "\\(y = 40 + 10\\cdot x + 0 \\cdot g + 0 \\cdot xg + \\epsilon\\)\n\\(y = 40 + 10\\cdot x + 40 \\cdot g + -10 \\cdot xg + \\epsilon\\)\n\\(y = -40 + 10\\cdot x + -40 \\cdot g + 10 \\cdot xg + \\epsilon\\)\n\\(y = -40 + -10\\cdot x + -40 \\cdot g + -10 \\cdot xg + \\epsilon\\)"
  },
  {
    "objectID": "posts/Regression3/Regression3.html#answerlist-1",
    "href": "posts/Regression3/Regression3.html#answerlist-1",
    "title": "Regression3",
    "section": "Answerlist",
    "text": "Answerlist\n\nRichtig\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndyn\nregression"
  },
  {
    "objectID": "posts/Weinhaendler/Weinhaendler.html",
    "href": "posts/Weinhaendler/Weinhaendler.html",
    "title": "Weinhaendler",
    "section": "",
    "text": "Exercise\nSie sind kürzlich in ein Startup-Unternehmen eingestiegen. Das Unternehmen versucht, einen Online-Weinhandel aufzubauen. Kern des Unternehmens ist eine künstliche Intelligenz, die versucht, den Kundis den best möglich passenden Wein anzudreh… zu verkaufen.\nSie haben sich bei Ihrem Bewerbungsgespräch persönlich von der Qualität der Produkte eingehend überzeugt und sind daher hoch motiviert, sich zum Wohle des Unternehmens einzusetzen.\nKürzlich hat eine Beratungsfirma, die Ihre Kunden im Rahmen einer qualitativen Studie untersucht hat, herausgefunden, dass doch ein beachtlicher Teil von einem Menschen, nicht von einem Roboter (bzw. der KI) beim Wein aussuchen beraten werden möchte. Diesen Anteil von Kunden (die nicht von der KI beraten werden möchten) möchten Sie jetzt genauer bestimmen.\nDazu haben Sie \\(N=42\\) Kundis befragt. Gut die Hälfte (\\(n=23\\)) hat sich zugunsten der KI ausgesprochen; der Rest der Kundis möchte lieber von einem Menschen beraten werden.\nGehen Sie im Folgenden davon aus, dass die Studie bzw. die erhaltenen Daten von guter Qualität ist (man also keine Probleme wie mangelnde Repräsentativität erwarten muss).\nVerwenden Sie die Gittermethode und gleichverteilte Priori-Werte.\n\nWie groß ist die Wahrscheinlichkeit, dass die KI-freundlichen Kundis bei Ihnen überwiegen?\nWie groß ist die Wahrscheinlichkeit (laut Modell), dass künftig eine Mehrheit an KI-freundlichen Kundis zu beobachten sein wird?\nWenn Sie nur eine Zahl angeben dürften: Was ist Ihr Schätzwert zum Anteil der KI-Freunde (in dieser Studie)?\n\n         \n\n\nSolution\n\nWie groß ist die Wahrscheinlichkeit (laut Modell), dass die KI-freundlichen Kundis bei Ihnen überwiegen?\n\nDas ist eine Frage nach der kumulative Verteilungsfuntion (cumulative distribution function, cdf).\n\np_grid &lt;- seq(from=0, \n              to=1, \n              length.out=1000)  # Gitterwerte\n\nprior &lt;- rep(1, 1000)  # Priori-Gewichte\n\nset.seed(42)  # Zufallszahlen festlegen\nlikelihood &lt;- dbinom(23, size = 42, prob=p_grid ) \n\nunstandardisierte_posterior &lt;- likelihood * prior \n\nposterior &lt;- unstandardisierte_posterior / sum(unstandardisierte_posterior)\n\nZiehen wir daraus Stichproben:\n\nset.seed(42)  # Zufallszahlen festlegen\nsamples &lt;- \n  tibble(\n    p = sample(p_grid , \n               prob = posterior, \n               size=1e4, \n               replace=TRUE))  \nsamples &lt;-\n  samples %&gt;% \n  mutate(id = 1:nrow(samples))\n\n\nsamples %&gt;% \n  filter(p &gt; 0.5) %&gt;% \n  summarise(wskt_mehrheit_will_ki = n()/nrow(samples))\n\n# A tibble: 1 × 1\n  wskt_mehrheit_will_ki\n                  &lt;dbl&gt;\n1                 0.731\n\n\n\nsamples %&gt;% \n  ggplot() +\n  aes(x = p) +\n  geom_histogram() +\n  geom_vline(xintercept = 0.5) +\n  labs(title = \"Post-Verteilung\")\n\n\n\n\n\n\n\n\n\nWie groß ist die Wahrscheinlichkeit (laut Modell), dass künftig eine Mehrheit an KI-freundlichen Kunfis zu beobachten sein wird?\n\n\nPPV &lt;-\n  samples %&gt;% \n  mutate(Anzahl_will_KI = rbinom(n = 1e4, size = 42, prob = p))\n\n\nPPV %&gt;% \n  ggplot() +\n  aes(x = Anzahl_will_KI) +\n  geom_histogram() +\n  labs(title = \"PPV\")\n\n\n\n\n\n\n\n\nEine Mehrheit entspricht mind. 22 von 42 Personen.\n\nPPV %&gt;% \n  filter(Anzahl_will_KI &gt;= 22) %&gt;% \n  summarise(prob_mehrheit_will_ki = n()/nrow(PPV))\n\n# A tibble: 1 × 1\n  prob_mehrheit_will_ki\n                  &lt;dbl&gt;\n1                 0.623\n\n\n\nWenn Sie nur eine Zahl angeben dürften: Was ist Ihr Schätzwert zum Anteil der KI-Freunde (in dieser Studie)?\n\nMan könnte den Mittelwert oder den Median angeben:\n\nlibrary(rstatix)\nget_summary_stats(samples)\n\n# A tibble: 2 × 13\n  variable     n   min       max   median      q1      q3    iqr     mad    mean\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 id       10000 1     10000     5000.    2.50e+3 7.50e+3 5000.  3.71e+3 5.00e+3\n2 p        10000 0.274     0.789    0.546 4.95e-1 5.96e-1    0.1 7.4 e-2 5.45e-1\n# … with 3 more variables: sd &lt;dbl&gt;, se &lt;dbl&gt;, ci &lt;dbl&gt;\n\n\n\nCategories:\n\nprobability\nbayes-grid"
  },
  {
    "objectID": "posts/rope1/rope1.html",
    "href": "posts/rope1/rope1.html",
    "title": "rope1",
    "section": "",
    "text": "Question\nDas Testen von Nullhypothesen wird u.a. deswegen kritisiert, weil die Nullhypothese zumeist apriori als falsch bekannt ist, weswegen es keinen Sinne mache, so die Kritiker, sie zu testen.\nNennen Sie ein Verfahren von John Kruschke, das einen Äquivalenzbereich testet und insofern eine Alternative zum Testen von Nullhypothesen anbietet.\nHinweise:\n\nGeben Sie nur Kleinbuchstaben ein.\nGeben Sie nur ein einziges Wort ein.\n\n\n\nSolution\nrope"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-18/Verteilungen-Quiz-18.html",
    "href": "posts/Verteilungen-Quiz-18/Verteilungen-Quiz-18.html",
    "title": "Verteilungen-Quiz-18",
    "section": "",
    "text": "Ei Forschi untersucht den Effekt eines Intelligenztraining auf den IQ.\nDabei findet sich aposteriori (also als Ergebnis der Untersuchung) \\(\\bar{x} \\sim N(3,5)\\) (in Standard-IQ-Punkten). Wir messen dabei die Erhöhung des Intelligenzwerts.\nDis Forschi resümiert: “Mit einer Wahrscheinlichkeit von 95% profitiert man von diesem Training”.\nIst diese Aussage korrekt (gegeben der Angaben)?\nHinweise:\n\nNutzen Sie Simulationsmethoden zur Lösung\nFixieren Sie die Zufallszahlen auf die Startzahl 42.\nZiehen Sie \\(10^5\\) Zufallszahlen aus der gegebenen Verteilung.\n\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-18/Verteilungen-Quiz-18.html#answerlist",
    "href": "posts/Verteilungen-Quiz-18/Verteilungen-Quiz-18.html#answerlist",
    "title": "Verteilungen-Quiz-18",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-18/Verteilungen-Quiz-18.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-18/Verteilungen-Quiz-18.html#answerlist-1",
    "title": "Verteilungen-Quiz-18",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/penguins-stan-04/penguins-stan-04.html",
    "href": "posts/penguins-stan-04/penguins-stan-04.html",
    "title": "penguins-stan-04",
    "section": "",
    "text": "Aufgabe\nWir untersuchen Einflussfaktoren bzw. Prädiktoren auf das Körpergewicht von Pinguinen. In dieser Aufgabe untersuchen wir den Zusammenhang von Schnabellänge (als UV) und Körpergewicht (als AV).\nWie groß ist die Wahrscheinlichkeit, dass der Effekt vorhanden ist (also größer als Null ist), die “Effektwahrscheinlichkeit”? Geben Sie die Wahrscheinlichkeit an.\nHinweise:\n\nNutzen Sie den Datensatz zu den Palmer Penguins.\nVerwenden Sie Methoden der Bayes-Statistik und die Software Stan.\nSie können den Datensatz z.B. hier beziehen oder über das R-Paket palmerpenguins.\nWeitere Hinweise\n\n         \n\n\nLösung\nZentrieren ist eigentlich immer nützlich, aber hier streng genommen nicht unbedingt nötig. Der Hauptgrund dafür ist, dass Stan für uns den Prior für den Intercept festlegt, und zwar auf Basis der Daten, wir uns also nicht um die komische Frage zu kümmern brauchen, welchen Prior wir für den unzentrierten Achsenabschnitt vergeben wollten: Wie schwer sind Pinguins der Schnabellänge Null? Mit zentrierten Prädiktoren ist die Frage nach dem Prior viel einfacher zu beantworten: Wie schwer ist ein Pinguin mit mittelgroßem Schnabel?\nSetup:\n\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(rstanarm)\n\ndata(\"penguins\", package = \"palmerpenguins\")\n\nEs wird in dieser Aufgabe vorausgesetzt, dass Sie den Datensatz selbständig importieren können. Tipp: Kurzes Googeln hilft ggf., den Datensatz zu finden.\nAlternativ könnten Sie den Datensatz als CSV-Datei importieren:\n\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\"\npenguins &lt;- data_read(d_path)\n\nEin Blick in die Daten zur Kontrolle, ob das Importieren richtig funktioniert hat:\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nVertrauen ist gut, aber - was Golems betrifft - ist Kontrolle eindeutig besser ;-)\n\nm1 &lt;- stan_glm(body_mass_g ~  bill_length_mm,  # Regressionsgleichung\n               data = penguins, #  Daten\n               seed = 42,  # Repro.\n               refresh = 0)  # nicht so viel Output\n\nMit pd() kann man sich die Effektwahrscheinlichkeit (“probability of direction”) ausgeben lassen:\n\npd(m1)\n\nProbability of Direction\n\nParameter      |     pd\n-----------------------\n(Intercept)    | 89.92%\nbill_length_mm |   100%\n\n\nMehr Informationen zu dieser Statistik findet sich hier oder hier.\nAlternativ bekommt man die Statistik auch mit parameters().\nDie Lösung lautet also 1.\n\nCategories:\n\nbayes\nregression\nexam-22"
  },
  {
    "objectID": "posts/purrr-map01/purrr-map01.html",
    "href": "posts/purrr-map01/purrr-map01.html",
    "title": "purrr-map01",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\nExercise\nErstellen Sie einen Tibble mit folgenden Spalten:\n\nBuchstaben A-Z, so dass in der 1. Zeile “A” steht, in der 2. Zeile “B” etc.\nBuchstaben a-z, so dass in der 1. Zeile “a” steht, in der 2. Zeile “b” etc.\nBuchstabenkombination der ersten beiden Spalten, so dass in der 1. Zeile “A-a” steht, in der 2. Zeile “B-b” etc.\n\n         \n\n\nSolution\nGeht es vielleicht so?\n\nd &lt;-\n  tibble(\n    letter1 = LETTERS,\n    letter2 = letters,\n    letters = paste(letter1, letter2, collapse = \"-\")\n  )\n\nhead(d)\n\n# A tibble: 6 × 3\n  letter1 letter2 letters                                                       \n  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;                                                         \n1 A       a       A a-B b-C c-D d-E e-F f-G g-H h-I i-J j-K k-L l-M m-N n-O o-P…\n2 B       b       A a-B b-C c-D d-E e-F f-G g-H h-I i-J j-K k-L l-M m-N n-O o-P…\n3 C       c       A a-B b-C c-D d-E e-F f-G g-H h-I i-J j-K k-L l-M m-N n-O o-P…\n4 D       d       A a-B b-C c-D d-E e-F f-G g-H h-I i-J j-K k-L l-M m-N n-O o-P…\n5 E       e       A a-B b-C c-D d-E e-F f-G g-H h-I i-J j-K k-L l-M m-N n-O o-P…\n6 F       f       A a-B b-C c-D d-E e-F f-G g-H h-I i-J j-K k-L l-M m-N n-O o-P…\n\n\nNein, leider nicht.\nOK, neuer Versuch:\n\nd &lt;-\n  tibble(\n    letter1 = LETTERS,\n    letter2 = letters) %&gt;% \n  unite(\"letters\", c(letter1, letter2), remove = FALSE)\n\n\nhead(d)\n\n# A tibble: 6 × 3\n  letters letter1 letter2\n  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  \n1 A_a     A       a      \n2 B_b     B       b      \n3 C_c     C       c      \n4 D_d     D       d      \n5 E_e     E       e      \n6 F_f     F       f      \n\n\nProbieren wir es mit purrr::map():\n\nd &lt;-\n  tibble(\n    letter1 = LETTERS,\n    letter2 = letters,\n    letters = map2_chr(letter1, letter2, ~ paste(c(.x, .y), collapse =\"-\"))\n  )\n\nhead(d)\n\n# A tibble: 6 × 3\n  letter1 letter2 letters\n  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  \n1 A       a       A-a    \n2 B       b       B-b    \n3 C       c       C-c    \n4 D       d       D-d    \n5 E       e       E-e    \n6 F       f       F-f    \n\n\nInfos zur Funktion paste() findet sich z.B. hier.\n\nCategories:\n\nr\nmap\ntidyverse"
  },
  {
    "objectID": "posts/purrr-map06/purrr-map06.html",
    "href": "posts/purrr-map06/purrr-map06.html",
    "title": "purrr-map06",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\nExercise\nErstellen Sie eine Tabelle mit mit folgenden Spalten:\n\nID-Spalte: \\(1,2,..., 10\\)\nEine Spalte mit Namem ds (ds wie Plural von Datensatz), die als geschachtelt (nested) pro Element jeweils einen der folgenden Datensätze enthält: mtcars, iris, chickweight, ToothGrowth (alle in R enthalten)\n\nBerechnen Sie eine Spalte, die die Anzahl der Spalten von ds zählt!\n         \n\n\nSolution\nHier sind einige Datensätze, in einer Liste zusammengefasst:\n\nds &lt;- list(mtcars = mtcars, iris = iris, chickweight =  ChickWeight, toothgrowth = ToothGrowth)\n\nDaraus erstellen wir eine Tabelle mit Listenspalte für die Daten:\n\nd &lt;- \n  tibble(id = 1:length(ds),\n         ds = ds)\n\nJetzt führen wir die Funktion ncol aus, und zwar für jedes Element von ds. Wir brauchen also eine Art Schleife, das besorgt map für uns. Viele Funktionen in R sind “auomatisch verschleift” - das nennt man vektorisiert. Vektorisierte Funktionen werden für jedes Element eines Vektors ausgeführt.\nEin Beispiel für eine vektorisierte Funktion ist die Funktion +:\n\nx &lt;- c(1,2,3)\ny &lt;- c(10, 20, 30)\nx + y\n\n[1] 11 22 33\n\n\nMan könnte übrigens auch schreiben:\n\n`+`(x, y)\n\n[1] 11 22 33\n\n\nWas zeigt, dass + eine normale Funktion ist.\nZurück zur eigentlichen Aufgabe. Aber ncol ist eben nicht vektorisiert, darum müssen wir da noch eine Schleife dazu bauen, das macht map.\n\nd2 &lt;- \n  d %&gt;% \n  mutate(n_col = map(ds, ncol)) \n\nhead(d2)\n\n# A tibble: 4 × 3\n     id ds                   n_col       \n  &lt;int&gt; &lt;named list&gt;         &lt;named list&gt;\n1     1 &lt;df [32 × 11]&gt;       &lt;int [1]&gt;   \n2     2 &lt;df [150 × 5]&gt;       &lt;int [1]&gt;   \n3     3 &lt;nfnGrpdD [578 × 4]&gt; &lt;int [1]&gt;   \n4     4 &lt;df [60 × 3]&gt;        &lt;int [1]&gt;   \n\n\nEntnesten wir noch n_col:\n\nd2 %&gt;% \n  unnest(n_col)\n\n# A tibble: 4 × 3\n     id ds                   n_col\n  &lt;int&gt; &lt;named list&gt;         &lt;int&gt;\n1     1 &lt;df [32 × 11]&gt;          11\n2     2 &lt;df [150 × 5]&gt;           5\n3     3 &lt;nfnGrpdD [578 × 4]&gt;     4\n4     4 &lt;df [60 × 3]&gt;            3\n\n\nWir können auch gleich map anweisen, keine Liste, sondern eine Zahl (double, reelle ) Zahl zurückzuliefern, dann sparen wir uns das entschachteln:\n\nd %&gt;% \n  mutate(n_col = map_dbl(ds, ncol)) \n\n# A tibble: 4 × 3\n     id ds                   n_col\n  &lt;int&gt; &lt;named list&gt;         &lt;dbl&gt;\n1     1 &lt;df [32 × 11]&gt;          11\n2     2 &lt;df [150 × 5]&gt;           5\n3     3 &lt;nfnGrpdD [578 × 4]&gt;     4\n4     4 &lt;df [60 × 3]&gt;            3\n\n\n\nCategories:\n\nprogramming\nloop"
  },
  {
    "objectID": "posts/penguins-stan-03/penguins-stan-03.html",
    "href": "posts/penguins-stan-03/penguins-stan-03.html",
    "title": "penguins-stan-03",
    "section": "",
    "text": "Aufgabe\nWir untersuchen Einflussfaktoren bzw. Prädiktoren auf das Körpergewicht von Pinguinen. In dieser Aufgabe untersuchen wir den Zusammenhang von Schnabellänge (als UV) und Körpergewicht (als AV).\nWie groß ist der statistische Einfluss der UV auf die AV?\nGeben Sie den Punktschätzer des Effekts an!\nHinweise:\n\nNutzen Sie den Datensatz zu den Palmer Penguins.\nSie können den Datensatz z.B. hier beziehen oder über das R-Paket palmerpenguins.\nBeziehen Sie sich auf den Median-Schätzwert.\nWeitere Hinweise\n\n         \n\n\nLösung\nZentrieren ist eigentlich immer nützlich, aber hier streng genommen nicht unbedingt nötig. Der Hauptgrund dafür ist, dass Stan für uns den Prior für den Intercept festlegt, und zwar auf Basis der Daten, wir uns also nicht um die komische Frage zu kümmern brauchen, welchen Prior wir für den unzentrierten Achsenabschnitt vergeben wollten: Wie schwer sind Pinguins der Schnabellänge Null? Mit zentrierten Prädiktoren ist die Frage nach dem Prior viel einfacher zu beantworten: Wie schwer ist ein Pinguin mit mittelgroßem Schnabel?\nSetup:\n\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(rstanarm)\n\ndata(\"penguins\", package = \"palmerpenguins\")\n\nEs wird in dieser Aufgabe vorausgesetzt, dass Sie den Datensatz selbständig importieren können. Tipp: Kurzes Googeln hilft ggf., den Datensatz zu finden.\nAlternativ könnten Sie den Datensatz als CSV-Datei importieren:\n\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\"\npenguins &lt;- data_read(d_path)\n\nEin Blick in die Daten zur Kontrolle, ob das Importieren richtig funktioniert hat:\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nVertrauen ist gut, aber - was Golems betrifft - ist Kontrolle eindeutig besser ;-)\n\nm1 &lt;- stan_glm(body_mass_g ~  bill_length_mm,  # Regressionsgleichung\n               data = penguins, #  Daten\n               seed = 42,  # Repro.\n               refresh = 0)  # nicht so viel Output\n\n\nparameters(m1, ci_method = \"hdi\", ci = .9, keep = \"bill_length_mm\")  # nur für \"bill_length_mm\"\n\nParameter      | Median |         90% CI |   pd |  Rhat |     ESS |                Prior\n----------------------------------------------------------------------------------------\nbill_length_mm |  87.45 | [76.24, 97.70] | 100% | 1.000 | 3216.00 | Normal (0 +- 367.22)\n\n\n\nUncertainty intervals (highest-density) and p-values (two-tailed)\n  computed using a MCMC distribution approximation.\n\n\nDie Lösung lautet also, wie aus der Ausgabe zu den Parametern ersichtlich, 87.45.\n\nCategories:\n\nbayes\nregression\nexam-22"
  },
  {
    "objectID": "posts/Regr-Bayes-interpret/Regr-Bayes-interpret.html",
    "href": "posts/Regr-Bayes-interpret/Regr-Bayes-interpret.html",
    "title": "Regr-Bayes-interpret",
    "section": "",
    "text": "Exercise\nBerechnen Sie das Modell und interpretieren Sie die Ausgabe des folgenden Regressionsmodells. Geben Sie für jeden Regressionskoeffizienten an, wie sein Wert zu verstehen ist!\nmpg ~ hp + am + hp:am\nHinweise:\n\nFixieren Sie die Zufallszahlen.\nVerwenden Sie Stan zur Berechnung.\nRunden Sie auf 2 Dezimalstellen.\n\n         \n\n\nSolution\n\nlibrary(tidyverse)  # Datenjudo\nlibrary(rstanarm)  # Stan, komm her\nlibrary(easystats)  # Komfort\n\n\nm1 &lt;- \n  stan_glm(mpg ~ hp + am + hp:am, \n           seed = 42,\n           refresh = 0,\n           data = mtcars)\n\ncoef(m1)\n\n  (Intercept)            hp            am         hp:am \n26.6170440514 -0.0588639914  5.2553599666  0.0003348108 \n\n\n\nIntercept: Ein Auto mit 0 PS und Automatikantrieb (am=0, s. Hilfe zum Datensatz: help(mtcars)) kann laut Modell mit einer Gallone Sprit ca. 26.62 Meilen fahren.\nhp: Pro zusätzlichem PS kann ein Auto mit Automatikantrieb pro Gallone Sprit ca. 0.06 Meilen weniger weit fahren.\nam: Ein Auto mit 0 PS und Schaltgetriebe (am=1) kommt pro Gallone Sprit ca. 5.26 Meilen weiter als ein Auto mit Automatikantrieb.\nhp:am: Der Interaktionseffekt ist praktisch Null: Der Zusammenhang von PS-Zahl und Spritverbrauch unterscheidet sich nicht (wesentlich) zwischen Autos mit bzw. ohne Automatikantrieb.\n\n\nCategories:\n\nbayes\nregression"
  },
  {
    "objectID": "posts/twitter07/twitter07.html",
    "href": "posts/twitter07/twitter07.html",
    "title": "twitter07",
    "section": "",
    "text": "Exercise\nLaden Sie \\(n=10^k\\) Tweets von Twitter herunter (mit \\(k=2\\)) und zwar pro Nutzerkonto wie unten angegeben . die Tweets sollen jeweils an eine prominente Person gerichtet sein.\nBeziehen Sie sich auf diese Politikis.\n         \n         \n\n\nSolution\nWir starten die benötigten R-Pakete:\n\nlibrary(academictwitteR)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(askpass)\nlibrary(rio)\n\nHier ist der Datensatz mit den Twitterkonten, für die wir die Daten herunterladen sollen:\n\npoliticians_path &lt;- \"https://raw.githubusercontent.com/sebastiansauer/datascience-text/main/data/twitter-german-politicians.csv\"\npoliticians &lt;- import(politicians_path)\npoliticians\n\n                                               name  party      screenname\n1                                   Karl Lauterbach    SPD Karl_Lauterbach\n2                                       Olaf Scholz    SPD      OlafScholz\n3                                 Annalena Baerback Gruene       ABaerbock\n4  Bundesministerium für Wirtschaft und Klimaschutz Gruene            BMWK\n5                                    Friedrich Merz    CDU  _FriedrichMerz\n6                                      Markus Söder    CSU   Markus_Soeder\n7                                       Cem Özdemir Gruene    cem_oezdemir\n8                                    Janine Wissler  Linke  Janine_Wissler\n9                                 Martin Schirdewan  Linke      schirdewan\n10                                Christian Lindner    FDP       c_lindner\n11                    Marie-Agnes Strack-Zimmermann    FDP      MAStrackZi\n12                                   Tino Chrupalla    AFD  Tino_Chrupalla\n13                                     Alice Weidel    AFD    Alice_Weidel\n                                  comment\n1                                    &lt;NA&gt;\n2                                    &lt;NA&gt;\n3                                    &lt;NA&gt;\n4  Robert Habeck ist der Minister im BMWK\n5                                CDU-Chef\n6                                CSU-Chef\n7                                    BMEL\n8                            Linke-Chefin\n9                              Linke-Chef\n10                               FDP-Chef\n11     Vorsitzende Verteidigungsausschuss\n12                     AFD-Bundessprecher\n13                   AFD-Bundessprecherin\n\n\nWir müssen noch das Passwort bereitstellen:\n\nbearer_token &lt;- askpass::askpass(\"bearer token\")\n\nUnd dann definieren wir eine Funktion, die das Gewichtheben für uns erledigt:\n\nget_all_tweets_politicians &lt;- function(screenname, n = 1e1) {\n  get_all_tweets(query = paste0(\"to:\", screenname, \" -is:retweet\"),\n                 start_tweets = \"2021-01-01T00:00:00Z\",\n                 end_tweets = \"2021-12-31T23:59:59Z\",\n                 bearer_token = bearer_token,\n                 file = glue::glue(\"~/datasets/Twitter/hate-speech/tweets_to_{screenname}_2021.rds\"),\n                 data_path = glue::glue(\"~/datasets/Twitter/hate-speech/{screenname}\"),\n                 n = n)\n}\n\nJetzt wenden wir die Funktion auf jedes Twitterkonto unserer Liste (alle Politikis) an:\n\nd &lt;- politicians$screenname %&gt;% \n  map(get_all_tweets_politicians)\n\n\nCategories:\n\ntextmining\ntwitter\nprogramming"
  },
  {
    "objectID": "posts/Bed-Post-Wskt1/Bed-Post-Wskt1.html",
    "href": "posts/Bed-Post-Wskt1/Bed-Post-Wskt1.html",
    "title": "Bed-Post-Wskt1",
    "section": "",
    "text": "Beziehen Sie sich auf das Regressionsmodell, für das die Ausgabe mit stan_glm() hier dargestellt ist:\n## stan_glm\n##  family:       gaussian [identity]\n##  formula:      height ~ weight_c\n##  observations: 346\n##  predictors:   2\n## ------\n##             Median MAD_SD\n## (Intercept) 154.6    0.3 \n## weight_c      0.9    0.0 \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 5.1    0.2   \nBetrachten Sie wieder folgende Beziehung (Gleichung bzw. Ungleichung):\n\\[Pr(\\text{height}_i = 155|\\text{weightcentered}_i=0, \\alpha, \\beta, \\sigma) \\quad \\Box \\quad Pr(\\text{height}_i = 156|\\text{weightcentered}_i=0, \\alpha, \\beta, \\sigma)\\] Die in der obigen Beziehung angegebenen Parameter beziehen sich auf das oben dargestellt Modell.\nErgänzen Sie das korrekte Zeichen in das Rechteck \\(\\Box\\)!\n\n\n\n\\(\\lt\\)\n\\(\\le\\)\n\\(\\gt\\)\n\\(\\ge\\)\n\\(=\\)"
  },
  {
    "objectID": "posts/Bed-Post-Wskt1/Bed-Post-Wskt1.html#answerlist",
    "href": "posts/Bed-Post-Wskt1/Bed-Post-Wskt1.html#answerlist",
    "title": "Bed-Post-Wskt1",
    "section": "",
    "text": "\\(\\lt\\)\n\\(\\le\\)\n\\(\\gt\\)\n\\(\\ge\\)\n\\(=\\)"
  },
  {
    "objectID": "posts/Bed-Post-Wskt1/Bed-Post-Wskt1.html#answerlist-1",
    "href": "posts/Bed-Post-Wskt1/Bed-Post-Wskt1.html#answerlist-1",
    "title": "Bed-Post-Wskt1",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nWahr\nFalsch\nFalsch\n\n\nCategories:\n\nregression\nbayes\nposterior"
  },
  {
    "objectID": "posts/Priori-Streuung/Priori-Streuung.html",
    "href": "posts/Priori-Streuung/Priori-Streuung.html",
    "title": "Priori-Streuung",
    "section": "",
    "text": "Welche Verteilung ist (am besten) geeignet, um Streuung (\\(\\sigma\\)) zu modellieren?\n\n\n\nN(0,1)\nN(1,1)\nExp(1)\nExp(0)\nExp(-1)"
  },
  {
    "objectID": "posts/Priori-Streuung/Priori-Streuung.html#answerlist",
    "href": "posts/Priori-Streuung/Priori-Streuung.html#answerlist",
    "title": "Priori-Streuung",
    "section": "",
    "text": "N(0,1)\nN(1,1)\nExp(1)\nExp(0)\nExp(-1)"
  },
  {
    "objectID": "posts/Priori-Streuung/Priori-Streuung.html#answerlist-1",
    "href": "posts/Priori-Streuung/Priori-Streuung.html#answerlist-1",
    "title": "Priori-Streuung",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nWahr\nFalsch\nFalsch\n\nDa Streuung \\(\\sigma\\) per Definition positiv ist, kommt eine Verteilung, die negative Werte erlaubt, nicht in Frage. Die Normalverteilung scheidet also aus.\nDie Rate der Exponentialverteilung regelt gleichzeitig Streuung und Mittelwert. Allerdings hat \\(Exp(0)\\) eine unendliche Streuung, was nicht wünschenswert ist. Eine negative Rate ist für die Exponentialverteilung nicht definiert.\nNormalverteilungen:\n\n\n\n\n\\(N(0,1)\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(N(1,1)\\):\n\n\n\n\n\n\n\n\n\nExponentialverteilungen:\n\n\n\n\\(Exp(1)\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\\(Exp(0)\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\\(Exp(-1)\\):\n\n\nWarning in fun(x_trans, rate = -1): NaNs produced\n\n\nWarning: Removed 101 rows containing missing values (`geom_function()`).\n\n\n\n\n\n\n\n\n\n\nCategories:\n\nprobability\nsimulation\ndistributions\nbayes"
  },
  {
    "objectID": "posts/ReThink4e3/ReThink4e3.html",
    "href": "posts/ReThink4e3/ReThink4e3.html",
    "title": "ReThink4e3",
    "section": "",
    "text": "Exercise\nGegeben dem folgenden Modell, schreiben Sie die passende Form des Bayes-Theorem auf.\nLikelihood: \\(h_i \\sim \\mathcal{N}(\\mu, \\sigma)\\)\nPrior für \\(\\mu\\): \\(\\mu \\sim \\mathcal{N}(178, 20)\\)\nPrior für \\(\\sigma\\): \\(\\sigma \\sim \\mathcal{U}(0, 50)\\)\nQuelle: McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Aufl.). Taylor and Francis, CRC Press.\n         \n\n\nSolution\nDie allgemeine Form des Bayes-Theorem hatten wir so kennen gelernt:\n\\[Pr(H|D) = \\frac{Pr(D|H)\\cdot Pr(H)}{Pr(D)}\\]\n\\(Pr(\\mu, \\sigma|h)\\) gibt die Posteriori-Wahrscheinlichkeit für ein bestimmte Hypothese \\(H\\) an, z.B. für die Hypothese \\(\\mu=0\\).\n\\(Pr(D|H)\\) ist der Likelihood unserer Daten \\(D\\) gegeben der gerade untersuchten Hypothese \\(H\\).\n\\(Pr(H)\\) ist die Apriori-Wahrscheinlichkeit (das “Apriori-Gewicht”) der gerade untersuchten Hypothese.\nDer Zähler gibt die unstandardisierte Posteriori-Wahrscheinlichkeit der gerade untersuchten Hypothese an.\nDer Nenner ist nur ein Normalisierungsfaktor, der dafür sorgt, dass der ganze Bruch die standardisierte Posteriori-Wahrscheinlichkeit angibt.\nIn diesem konkreten Fall untersuchen wir Hypothesen zu einem “Parameter-Pärchen”, \\(\\mu\\sigma\\). Wir fragen also, wie wahrscheinlich es ist, einen gewissen Mittelwert \\(\\mu\\) und (gleichzeitig) eine gewisse Streuung \\(\\sigma\\) aufzufinden.\nZum Beispiel könnten wir fragen: “Wie wahrscheinlich ist es, dass \\(\\mu=194\\) und \\(\\sigma=12\\)?”. Bayes’ Theorem gibt uns die Wahrscheinlichkeit für diese Hypothese.\nZur Erinnerung, Bayes’ Theorem:\n\\[Pr(\\mu \\cap \\sigma|D) = \\frac{Pr(D|\\mu \\cap \\sigma)\\cdot Pr(\\mu) \\cdot Pr(\\sigma)}{Pr(H)}\\]\nHier ist zu beachten, dass die Apriori-Wahrscheinlichkeit auf zwei Termen besteht, \\(Pr(\\mu)\\) und \\(Pr(\\sigma)\\). Sind diese unabhängig, so kann man ihre Wahrscheinlichkeiten multiplizieren, um die gemeinsame Wahrscheinlichkeit zu erhalten, also die Wahrscheinlichkeit für ein bestimmten “Mu-Sigma-Pärchen”, etwa \\(\\mu=194,\\sigma=12\\).\n\nCategories:\n\nbayes\nprobability"
  },
  {
    "objectID": "posts/Nullhyp-Beispiel/Nullhyp-Beispiel.html",
    "href": "posts/Nullhyp-Beispiel/Nullhyp-Beispiel.html",
    "title": "Nullhyp-Beispiel",
    "section": "",
    "text": "Welches der folgenden Beispiele ist kein Beispiel für eine Nullhypothese?\n\n\n\n\\(\\beta_1 &lt;= 0\\)\n\\(\\mu_1 = \\mu_2\\)\n\\(\\mu_1 = \\mu_2 = ... = \\mu_k\\)\n\\(\\rho = 0\\)\n\\(\\pi_1 = \\pi_2\\)"
  },
  {
    "objectID": "posts/Nullhyp-Beispiel/Nullhyp-Beispiel.html#answerlist",
    "href": "posts/Nullhyp-Beispiel/Nullhyp-Beispiel.html#answerlist",
    "title": "Nullhyp-Beispiel",
    "section": "",
    "text": "\\(\\beta_1 &lt;= 0\\)\n\\(\\mu_1 = \\mu_2\\)\n\\(\\mu_1 = \\mu_2 = ... = \\mu_k\\)\n\\(\\rho = 0\\)\n\\(\\pi_1 = \\pi_2\\)"
  },
  {
    "objectID": "posts/Nullhyp-Beispiel/Nullhyp-Beispiel.html#answerlist-1",
    "href": "posts/Nullhyp-Beispiel/Nullhyp-Beispiel.html#answerlist-1",
    "title": "Nullhyp-Beispiel",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nnullhypothesis\nsignificance\ninference"
  },
  {
    "objectID": "posts/mariokart-mean1/mariokart-mean1.html",
    "href": "posts/mariokart-mean1/mariokart-mean1.html",
    "title": "mariokart-mean1",
    "section": "",
    "text": "Aufgabe\nImportieren Sie den Datensatz mariokart in R. Berechnen Sie den mittleren Verkaufspreis (total_pr)!\nHinweise:\n\nRunden Sie auf 1 Dezimalstelle.\n\n         \n\n\nLösung\nPakete starten:\n\nlibrary(tidyverse)\nlibrary(easystats)\n\nDaten importieren:\n\nd_url &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\"\nd &lt;- data_read(d_url)\n\n\nd  %&gt;% \n  summarise(pr_mean = mean(total_pr))\n\n   pr_mean\n1 49.88049\n\n\nLösung: 49.88.\n\nCategories:\n\ndatawrangling\ndplyr\neda\nnum"
  },
  {
    "objectID": "posts/corona-blutgruppe/corona-blutgruppe.html",
    "href": "posts/corona-blutgruppe/corona-blutgruppe.html",
    "title": "corona-blutgruppe",
    "section": "",
    "text": "Exercise\nBetrachten wir das Ereignis “Schwerer Coronaverlauf” (\\(S\\)); ferner betrachten wir das Ereignis “Blutgruppe ist A” (\\(A\\)) und das Gegenereignis von \\(A\\): “Blutgruppe ist nicht A”. Ein Gegenereignis wird auch als Komplementärereignis oder Komplement (complement) mit dem Operator \\(\\bar{A}\\) oder \\(A^C\\) bezeichnet.\nSei \\(Pr(S|A) = 0.01\\) und sei \\(Pr(S|A^C) = 0.01\\).\nWas kann man auf dieser Basis zur Abhängigkeit der Ereignisse \\(S\\) und \\(A\\) sagen?\nGeben Sie ein Adjektiv an, dass diesen Sachverhalt kennzeichnet!\n         \n\n\nSolution\nDie Lösung lautet: unabhängig.\n\\(S\\) und \\(A\\) sind unabhängig: Offenbar ist die Wahrscheinlichkeit eines schweren Verlaufs gleich groß unabhängig davon, ob die Blutgruppe A ist oder nicht. In diesem Fall spricht man von stochastischer Unabhängigkeit.\n\\(Pr(S|A) = Pr(S|A^C) = Pr(S)\\)\n\nCategories:\n\nprobability\ndependent"
  },
  {
    "objectID": "posts/Skalenniveau1a/Skalenniveau1a.html",
    "href": "posts/Skalenniveau1a/Skalenniveau1a.html",
    "title": "Skalenniveau1a",
    "section": "",
    "text": "Verfügt die Variable Rangfolge der Lieblingsspeisen einer Person über ein metrisches Skalenniveau?\n\n\n\nnein\nja"
  },
  {
    "objectID": "posts/Skalenniveau1a/Skalenniveau1a.html#answerlist",
    "href": "posts/Skalenniveau1a/Skalenniveau1a.html#answerlist",
    "title": "Skalenniveau1a",
    "section": "",
    "text": "nein\nja"
  },
  {
    "objectID": "posts/tidymodels-penguins02/tidymodels-penguins02.html",
    "href": "posts/tidymodels-penguins02/tidymodels-penguins02.html",
    "title": "tidymodels-penguins02",
    "section": "",
    "text": "Aufgabe\nBerechnen Sie ein kNN-Modell mit tidymodels und zwar anhand des penguins Datensatzes.\nModellgleichung: body_mass_g ~ bill_length_mm, data = d_train.\nGesucht ist R-Quadrat als Maß für die Modellgüte im TEST-Sample.\nHinweise:\n\nFixieren Sie die Zufallszahlen auf den Startwert 42.\nNutzen Sie eine v=5,r=1 CV.\nTunen Sie nicht.\nEntfernen Sie fehlende Werte in den Variablen.\nVerzichten Sie auf weitere Schritte der Vorverarbeitung.\n\n         \n\n\nLösung\nSetup:\n\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(tictoc)  # Rechenzeit messen, optional\n# data(penguins, package = \"palmerpenguins\")\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/modeldata/penguins.csv\"\nd &lt;- read_csv(d_path)\n\nDatensatz auf NAs prüfen:\n\nd2 &lt;-\n  d %&gt;% \n  drop_na() \n\nDatensatz aufteilen:\n\nset.seed(42)\nd_split &lt;- initial_split(d2)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\nWorkflow:\n\nrec1 &lt;-\n  recipe(body_mass_g ~ bill_length_mm, data = d_train) %&gt;% \n  step_naomit(all_numeric())\n\nknn_model &lt;-\n  nearest_neighbor(\n    mode = \"regression\"\n  ) \n\nwflow &lt;-\n  workflow() %&gt;%\n  add_recipe(rec1) %&gt;%\n  add_model(knn_model)\n\nwflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_naomit()\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-Nearest Neighbor Model Specification (regression)\n\nComputational engine: kknn \n\n\nBacken:\n\nd_baked &lt;- prep(rec1) %&gt;% bake(new_data = NULL)\nd_baked %&gt;% head()\n\n# A tibble: 6 × 2\n  bill_length_mm body_mass_g\n           &lt;dbl&gt;       &lt;dbl&gt;\n1           34.5        2900\n2           52.2        3450\n3           45.4        4800\n4           42.1        4000\n5           50          5350\n6           41.5        4000\n\n\nAuf NA prüfen:\n\nsum(is.na(d_baked))\n\n[1] 0\n\n\nCV:\n\nset.seed(42)\nfolds &lt;- vfold_cv(d_train, v = 5)\nfolds\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits           id   \n  &lt;list&gt;           &lt;chr&gt;\n1 &lt;split [199/50]&gt; Fold1\n2 &lt;split [199/50]&gt; Fold2\n3 &lt;split [199/50]&gt; Fold3\n4 &lt;split [199/50]&gt; Fold4\n5 &lt;split [200/49]&gt; Fold5\n\n\nResampling:\n\nd_resamples &lt;-\n  fit_resamples(\n    wflow,\n    resamples = folds\n  )\n\nd_resamples\n\n# Resampling results\n# 5-fold cross-validation \n# A tibble: 5 × 4\n  splits           id    .metrics         .notes          \n  &lt;list&gt;           &lt;chr&gt; &lt;list&gt;           &lt;list&gt;          \n1 &lt;split [199/50]&gt; Fold1 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n2 &lt;split [199/50]&gt; Fold2 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n3 &lt;split [199/50]&gt; Fold3 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n4 &lt;split [199/50]&gt; Fold4 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n5 &lt;split [200/49]&gt; Fold5 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n\n\nLast Fit:\n\nfit_last &lt;- last_fit(wflow, d_split)\n\nModellgüte im Test-Sample:\n\nfit_last %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard     654.    Preprocessor1_Model1\n2 rsq     standard       0.294 Preprocessor1_Model1\n\n\nR-Quadrat:\n\nsol &lt;- collect_metrics(fit_last)[[\".estimate\"]][2]\nsol\n\n[1] 0.2935091\n\n\n\nCategories:\n\nds1\ntidymodels\nprediction\nyacsda\nstat-learning\nnum"
  },
  {
    "objectID": "posts/Priorwahl1/Priorwahl1.html",
    "href": "posts/Priorwahl1/Priorwahl1.html",
    "title": "Priorwahl1",
    "section": "",
    "text": "Exercise\nEi Forschi wählt für ein Regressionsmodell \\(\\beta \\sim \\mathcal{N}(0,500)\\) (Priori), wobei die empirischen Variablen z-standardisiert sind. Beziehen Sie Stellung zu diesem Prior.\n         \n\n\nSolution\nDie Priori-Verteilung ist nicht sinnvoll spezifiziert. Die Streuung der Normalverteilung ist so groß, dass sie fast schon uniform verteilt ist. Dieser Priori-Verteilung nimmt z.B. an, \\(Pr(|\\beta| &lt; 250) &lt; Pr(|\\beta| &gt; 250)\\), was eine sehr wilde Vorstellung ist. Man könnte sagen: Die Verteilung nimmt an, dass es wahrscheinlicher ist, dass ihr bester Freund 100 Millionen Lichtjahre entfernt lebt, als dass er näher als diese Distanz bei Ihnen lebt.\nWeitere Hinweise hier\nZur Verdeutlichung: Wie wahrscheinlich ist \\(q=1,2,...,10\\) bei einer Normalverteilung zu betrachten?\nFür \\(q=1\\) beträgt die Wahrscheinlichkeit für einen Wert nicht höher als \\(q=1\\) etwa 84%:\n\npnorm(q = 1)\n\n[1] 0.8413447\n\n\nAllgemeiner:\n\noptions(digits = 20)  # Mehr Nachkommastellen\npnorm(q = 1:10)\n\n [1] 0.84134474606854292578 0.97724986805182079141 0.99865010196836989653\n [4] 0.99996832875816688002 0.99999971334842807646 0.99999999901341229958\n [7] 0.99999999999872013490 0.99999999999999933387 1.00000000000000000000\n[10] 1.00000000000000000000\n\n\nDie Wahrscheinlichkeiten für Sigma-Ereignisse bis zu ±7 finden sich z.B. hier.\n\noptions(digits = 2)\n\nVertiefung:\nNassim Taleb hat dieses Argument in seinem Buch “Statistical Consequences of Fat Tails” aufgegriffen (ein anspruchsvolles Buch). Hier finden Sie eine interessante Darstellung eines Arguments daraus.\n\nCategories:\n\nfat-tails\ndistributions"
  },
  {
    "objectID": "posts/kausal21/kausal21.html",
    "href": "posts/kausal21/kausal21.html",
    "title": "kausal21",
    "section": "",
    "text": "Gegeben sei der DAG g (s. u.). Der DAG verfügt über mehrere Variablen, die als Knoten im Graph dargestellt sind und mit \\(x_1, x_2, \\ldots x_n\\) bezeichnet sind.\nWelche minimale Variablenmenge muss kontrolliert werden, um den kausalen Effekt von der UV zur AV zu identifizieren?\nUV: x3.\nAV: x5.\nHinweise:\n\nMengen sind mittels geschweifter Klammern gekennzeichnet, z.B. {x8, x9} meint die Menge mit den zwei Elementen x8 und x9.\nDie leere Menge { } bedeutet, dass keine Variable kontrolliert werden muss, um den kausalen Effekt zu identifizieren.\nAlle Variablen werden als gemessen vorausgesetzt.\nEs ist möglich, dass es keine Lösung gibt, dass es also keine Adjustierungsmenge gibt, um den kausalen Effekt zu identifizieren. Wenn dies der Fall sein sollte, wählen Sie “keine Lösung”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n{ x2, x5 }\n{ x3 }\n{ }\n{ x1, x4 }\n{ x1, x7 }"
  },
  {
    "objectID": "posts/kausal21/kausal21.html#answerlist",
    "href": "posts/kausal21/kausal21.html#answerlist",
    "title": "kausal21",
    "section": "",
    "text": "{ x2, x5 }\n{ x3 }\n{ }\n{ x1, x4 }\n{ x1, x7 }"
  },
  {
    "objectID": "posts/kausal21/kausal21.html#answerlist-1",
    "href": "posts/kausal21/kausal21.html#answerlist-1",
    "title": "kausal21",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nRichtig\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/mtcars-abhaengig/mtcars-abhaengig.html",
    "href": "posts/mtcars-abhaengig/mtcars-abhaengig.html",
    "title": "mtcars-abhaengig",
    "section": "",
    "text": "Exercise\nOb wohl die PS-Zahl (Ereignis \\(A\\)) und der Spritverbrauch (Ereignis \\(B\\)) voneinander abhängig sind? Was meinen Sie? Was ist Ihre Einschätzung dazu? Vermutlich haben Sie ein (wenn vielleicht auch implizites) Vorab-Wissen zu dieser Frage. Lassen wir dieses Vorab-Wissen aber einmal außen vor und schauen uns rein Daten dazu an. Vereinfachen wir die Frage etwas, indem wir fragen, ob die Ereignisse “hoher Spritverbrauch” (A) und “hohe PS-Zahl” voneinander abhängig sind.\nUm es konkret zu machen, nutzen wir den Datensatz mtcars:\n\nlibrary(tidyverse)\ndata(mtcars)\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n\nWeitere Infos zum Datensatz bekommen Sie mit help(mtcars) in R.\nDefinieren wir uns das Ereignis “hohe PS-Zahl” (und nennen wir es hp_high, klingt cooler). Sagen wir, wenn die PS-Zahl größer ist als der Median, dann trifft hp_high zu, ansonsten nicht:\n\nmtcars %&gt;% \n  summarise(median(hp))\n\n  median(hp)\n1        123\n\n\nMit dieser “Wenn-Dann-Abfrage” können wir die Variable hp_high mit den Stufen TRUE und FALSE definieren:\n\nmtcars &lt;-\n  mtcars %&gt;% \n  mutate(hp_high = case_when(\n    hp &gt; 123 ~ TRUE,\n    hp &lt;= 123 ~ FALSE\n  ))\n\nGenauso gehen wir mit dem Spritverbrauch vor (mpg_high):\n\nmtcars &lt;- \n  mtcars %&gt;% \n  mutate(mpg_high = case_when(\n    mpg &gt; median(mpg) ~ TRUE,\n    mpg &lt;= median(mpg) ~ FALSE\n  ))\n\n\nSchauen Sie im Datensatz nach, ob unser Vorgehen (Erstellung von hp_high und mpg_high) überhaupt funktioniert hat. Probieren geht über Studieren.\nVisualisieren Sie in geeigneter Form den Zusammenhang.\nBerechnen Sie \\(Pr(\\text{mpg_high}|\\text{hp_high})\\) und \\(Pr(\\text{mpg_high}|\\neg \\text{hp_high})\\) !\n\n         \n\n\nSolution\n\nSchauen wir mal in den Datensatz:\n\n\nmtcars %&gt;% \n  select(hp, hp_high, mpg, mpg_high) %&gt;% \n  slice_head(n = 5)\n\n                   hp hp_high  mpg mpg_high\nMazda RX4         110   FALSE 21.0     TRUE\nMazda RX4 Wag     110   FALSE 21.0     TRUE\nDatsun 710         93   FALSE 22.8     TRUE\nHornet 4 Drive    110   FALSE 21.4     TRUE\nHornet Sportabout 175    TRUE 18.7    FALSE\n\n\n\n\n\n\nmtcars %&gt;% \n  #select(hp_high, mpg_high) %&gt;% \n  ggplot() +\n  aes(x = hp_high, fill = mpg_high) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\nHey, sowas von abhängig voneinander, die zwei Variablen, mpg_high und hp_high!\nDer rechte Balken zeigt \\(Pr(\\text{mpg_high}|\\text{ hp_high})\\) und \\(Pr(\\neg \\text{mpg_high}|\\text{hp_high})\\).Der linke Balken zeigt \\(Pr(\\text{mpg_high}|\\neg \\text{hp_high})\\) und \\(Pr(\\neg \\text{mpg_high}|\\neg \\text{hp_high})\\).\n\nBerechnen wir die relevanten Anteile:\n\n\nmtcars %&gt;% \n  #select(hp_high, mpg_high) %&gt;% \n  count(hp_high, mpg_high) %&gt;%  # Anzahl pro Zelle der Kontingenztabelle\n  group_by(hp_high) %&gt;%  # die Anteile pro \"Balken\" s. Diagramm\n  mutate(prop = n / sum(n))\n\n# A tibble: 4 × 4\n# Groups:   hp_high [2]\n  hp_high mpg_high     n   prop\n  &lt;lgl&gt;   &lt;lgl&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 FALSE   FALSE        3 0.176 \n2 FALSE   TRUE        14 0.824 \n3 TRUE    FALSE       14 0.933 \n4 TRUE    TRUE         1 0.0667\n\n\nAm besten, Sie führen den letzten Code Schritt für Schritt aus und schauen sich jeweils das Ergebnis an, das hilft beim Verstehen.\nAlternativ kann man sich die Häufigkeiten auch schön bequem ausgeben lassen:\n\nlibrary(mosaic)\ntally(mpg_high ~ hp_high, \n      data = mtcars, \n      format = \"proportion\")\n\n        hp_high\nmpg_high       TRUE      FALSE\n   TRUE  0.06666667 0.82352941\n   FALSE 0.93333333 0.17647059\n\n\n\nCategories:\n\nprobability\ndependent"
  },
  {
    "objectID": "posts/kausal26/kausal26.html",
    "href": "posts/kausal26/kausal26.html",
    "title": "kausal26",
    "section": "",
    "text": "Gegeben sei der DAG g (s. u.). Der DAG verfügt über 7 Variablen, die als Knoten im Graph dargestellt sind (mit \\(x_1, x_2, \\ldots x_n\\) bezeichnet) und über Kanten verbunden sind.\nWelche minimale Variablenmenge muss kontrolliert werden, um den kausalen Effekt von der UV zur AV zu identifizieren?\nUV: x4.\nAV: x5.\nHinweise:\n\nMengen sind mittels geschweifter Klammern gekennzeichnet, z.B. {x8, x9} meint die Menge mit den zwei Elementen x8 und x9.\nDie leere Menge { } bedeutet, dass keine Variable kontrolliert werden muss, um den kausalen Effekt zu identifizieren.\nAlle Variablen werden als gemessen vorausgesetzt.\nEs ist möglich, dass es keine Lösung gibt, dass es also keine Adjustierungsmenge gibt, um den kausalen Effekt zu identifizieren. Wenn dies der Fall sein sollte, wählen Sie “keine Lösung”.\nEs ist möglich, dass einzelne Variablen keine Kanten besitzen, also keine Verbindung zu anderen Variablen (Knoten) haben.\n\n\n\n\n{ x2 }\n{ x3, x4 }\n{ x2, x5 }\n{ x3 }\n{ x3, x6 }"
  },
  {
    "objectID": "posts/kausal26/kausal26.html#answerlist",
    "href": "posts/kausal26/kausal26.html#answerlist",
    "title": "kausal26",
    "section": "",
    "text": "{ x2 }\n{ x3, x4 }\n{ x2, x5 }\n{ x3 }\n{ x3, x6 }"
  },
  {
    "objectID": "posts/kausal26/kausal26.html#answerlist-1",
    "href": "posts/kausal26/kausal26.html#answerlist-1",
    "title": "kausal26",
    "section": "Answerlist",
    "text": "Answerlist\n\nRichtig\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/tidymodels-penguins05/tidymodels-penguins05.html",
    "href": "posts/tidymodels-penguins05/tidymodels-penguins05.html",
    "title": "tidymodels-penguins05",
    "section": "",
    "text": "Aufgabe\nBerechnen Sie ein kNN-Modell mit tidymodels und zwar anhand des penguins Datensatzes.\nModellgleichung: body_mass_g ~ bill_length_mm, data = d_train.\nGesucht ist R-Quadrat als Maß für die Modellgüte im TEST-Sample.\nHinweise:\n\nFixieren Sie die Zufallszahlen auf den Startwert 42.\nNutzen Sie eine v=5,r=2 CV.\nTunen Sie \\(K\\), setzen Sie den Tuning-Wertebereich auf 1 bis 5.\nEntfernen Sie fehlende Werte in den Variablen.\nVerzichten Sie auf weitere Schritte der Vorverarbeitung.\n\n         \n\n\nLösung\nSetup:\n\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(tictoc)  # Rechenzeit messen, optional\n# data(penguins, package = \"palmerpenguins\")\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/modeldata/penguins.csv\"\nd &lt;- read_csv(d_path)\n\nDatensatz auf NAs prüfen:\n\nd2 &lt;-\n  d %&gt;% \n  drop_na() \n\nDatensatz aufteilen:\n\nset.seed(42)\nd_split &lt;- initial_split(d2)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\nWorkflow:\n\nrec1 &lt;-\n  recipe(body_mass_g ~ bill_length_mm, data = d_train) %&gt;% \n  step_naomit(all_numeric())\n\nknn_model &lt;-\n  nearest_neighbor(\n    mode = \"regression\",\n    neighbors = tune()\n  ) \n\nwflow &lt;-\n  workflow() %&gt;%\n  add_recipe(rec1) %&gt;%\n  add_model(knn_model)\n\nwflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_naomit()\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-Nearest Neighbor Model Specification (regression)\n\nMain Arguments:\n  neighbors = tune()\n\nComputational engine: kknn \n\n\nBacken:\n\nd_baked &lt;- prep(rec1) %&gt;% bake(new_data = NULL)\nd_baked %&gt;% head()\n\n# A tibble: 6 × 2\n  bill_length_mm body_mass_g\n           &lt;dbl&gt;       &lt;dbl&gt;\n1           34.5        2900\n2           52.2        3450\n3           45.4        4800\n4           42.1        4000\n5           50          5350\n6           41.5        4000\n\n\nAuf NA prüfen:\n\nsum(is.na(d_baked))\n\n[1] 0\n\n\nCV:\n\nset.seed(42)\nfolds &lt;- vfold_cv(d_train, v = 5, repeats = 2)\nfolds\n\n#  5-fold cross-validation repeated 2 times \n# A tibble: 10 × 3\n   splits           id      id2  \n   &lt;list&gt;           &lt;chr&gt;   &lt;chr&gt;\n 1 &lt;split [199/50]&gt; Repeat1 Fold1\n 2 &lt;split [199/50]&gt; Repeat1 Fold2\n 3 &lt;split [199/50]&gt; Repeat1 Fold3\n 4 &lt;split [199/50]&gt; Repeat1 Fold4\n 5 &lt;split [200/49]&gt; Repeat1 Fold5\n 6 &lt;split [199/50]&gt; Repeat2 Fold1\n 7 &lt;split [199/50]&gt; Repeat2 Fold2\n 8 &lt;split [199/50]&gt; Repeat2 Fold3\n 9 &lt;split [199/50]&gt; Repeat2 Fold4\n10 &lt;split [200/49]&gt; Repeat2 Fold5\n\n\nTunen:\n\nd_resamples &lt;-\n  tune_grid(\n    wflow,\n    resamples = folds,\n    control = control_grid(save_workflow = TRUE),\n    grid = grid_regular(\n      neighbors(range = c(1, 5))\n    )\n  )\n\nd_resamples\n\n# Tuning results\n# 5-fold cross-validation repeated 2 times \n# A tibble: 10 × 5\n   splits           id      id2   .metrics         .notes          \n   &lt;list&gt;           &lt;chr&gt;   &lt;chr&gt; &lt;list&gt;           &lt;list&gt;          \n 1 &lt;split [199/50]&gt; Repeat1 Fold1 &lt;tibble [6 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [199/50]&gt; Repeat1 Fold2 &lt;tibble [6 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [199/50]&gt; Repeat1 Fold3 &lt;tibble [6 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [199/50]&gt; Repeat1 Fold4 &lt;tibble [6 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [200/49]&gt; Repeat1 Fold5 &lt;tibble [6 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [199/50]&gt; Repeat2 Fold1 &lt;tibble [6 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [199/50]&gt; Repeat2 Fold2 &lt;tibble [6 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [199/50]&gt; Repeat2 Fold3 &lt;tibble [6 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [199/50]&gt; Repeat2 Fold4 &lt;tibble [6 × 5]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [200/49]&gt; Repeat2 Fold5 &lt;tibble [6 × 5]&gt; &lt;tibble [0 × 3]&gt;\n\n\nBester Kandidat:\n\nshow_best(d_resamples)\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\n\n# A tibble: 3 × 7\n  neighbors .metric .estimator  mean     n std_err .config             \n      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1         5 rmse    standard    733.    10    19.3 Preprocessor1_Model3\n2         3 rmse    standard    777.    10    23.8 Preprocessor1_Model2\n3         1 rmse    standard    945.    10    28.0 Preprocessor1_Model1\n\n\n\nfitbest &lt;- fit_best(d_resamples)\nfitbest\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_naomit()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nkknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(5L,     data, 5))\n\nType of response variable: continuous\nminimal mean absolute error: 497.0257\nMinimal mean squared error: 407926.4\nBest kernel: optimal\nBest k: 5\n\n\nLast Fit:\n\nfit_last &lt;- last_fit(fitbest, d_split)\nfit_last\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits           id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;           &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [249/84]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n\nModellgüte im Test-Sample:\n\nfit_last %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard     654.    Preprocessor1_Model1\n2 rsq     standard       0.294 Preprocessor1_Model1\n\n\nR-Quadrat:\n\nsol &lt;- collect_metrics(fit_last)[[\".estimate\"]][2]\nsol\n\n[1] 0.2935091\n\n\n\nCategories:\n\nds1\ntidymodels\nprediction\nyacsda\nstat-learning\nnum"
  },
  {
    "objectID": "posts/mtcars-simple1/mtcars-simple1.html",
    "href": "posts/mtcars-simple1/mtcars-simple1.html",
    "title": "mtcars-simple1",
    "section": "",
    "text": "Exercise\nWe will use the dataset mtcars in this exercise.\nAssume your causal model of your research dictates that fuel economy is a linear function of horse power, cylinder count and displacement of the engine.\nCompute the causal effect of horse power given the above model! Report the point estimate.\nNotes:\n\nUse can either use frequentist or bayesian modeling.\nUse R for all computations.\nThere are multiple ways to find a solution.\n\n         \n\n\nSolution\nCompute Model:\n\nlm1_freq &lt;- lm(mpg ~ hp + cyl + disp, data = mtcars)\n\nlibrary(rstanarm)\nlm1_bayes &lt;- stan_glm(mpg ~ hp + cyl + disp, data = mtcars, refresh = 0)\n\nGet parameters:\n\nlibrary(easystats)\n\n\nparameters(lm1_freq)\n\nParameter   | Coefficient |   SE |         95% CI | t(28) |      p\n------------------------------------------------------------------\n(Intercept) |       34.18 | 2.59 | [28.88, 39.49] | 13.19 | &lt; .001\nhp          |       -0.01 | 0.01 | [-0.04,  0.02] | -1.00 | 0.325 \ncyl         |       -1.23 | 0.80 | [-2.86,  0.41] | -1.54 | 0.135 \ndisp        |       -0.02 | 0.01 | [-0.04,  0.00] | -1.81 | 0.081 \n\n\n\nparameters(lm1_bayes)\n\nParameter   | Median |         95% CI |     pd | % in ROPE |  Rhat |     ESS |                   Prior\n------------------------------------------------------------------------------------------------------\n(Intercept) |  34.28 | [28.87, 39.60] |   100% |        0% | 1.000 | 2600.00 | Normal (20.09 +- 15.07)\nhp          |  -0.01 | [-0.05,  0.02] | 82.93% |      100% | 1.000 | 2448.00 |   Normal (0.00 +- 0.22)\ncyl         |  -1.25 | [-2.84,  0.34] | 93.75% |     2.55% | 1.000 | 2108.00 |   Normal (0.00 +- 8.44)\ndisp        |  -0.02 | [-0.04,  0.00] | 96.05% |      100% | 1.001 | 2405.00 |   Normal (0.00 +- 0.12)\n\n\nThe coefficient is estimated as about -0.01\n\nCategories:\n\nregresssion\nen\nbayes\nfrequentist\nqm1\nstats-nutshell"
  },
  {
    "objectID": "posts/kausal10/kausal10.html",
    "href": "posts/kausal10/kausal10.html",
    "title": "kausal10",
    "section": "",
    "text": "Ein Forschungsteam aus Psychologen und Medizinern untersucht die Frage, ob (höhere) Bereitschaft für eine OP und zu Veränderung in ihrer Lebensführung, nach einem Jahr über einen (höheren) Schmerzrückgang führt. Das hießt, Patienten geringerer Bereitschaft sollten es entsprechend zu weniger Schmerzrückgang kommen. Die Bereitschaft der Patienten (ein theoretisches Konstrukt, was nicht direkt beobachtbar ist) wurde mittels eines psychometrisch validierten Fragebogen erhoben. Die Studie umfasst ausschließlich Patienten, die eine OP wegen Rückenschmerzen durchlaufen sind (s. DAG).\nDas Studiendesign impliziert, dass nur Patienten, die eine OP durchlaufen haben, in die Studie aufgenommen wurde. Damit wird per Design diese Variable stratifiziert (kontrolliert).\n\n\n\n\n\n\n\n\n\nDurch die Stratifizierung wird ein Hintertürpfad geöffnet; dieser muss geschlossen werden. Wie sollte dies geschehen (in diesem Modell)?\nIm folgenden Diagramm ist der Kollisionsbias kenntlich gemacht, der durch die Stratifizierung von Surgical Status entsteht:\n\n\n\n\n\n\n\n\n\nHinweis:\n\nWenn von “kausaler Effekt” gesprochen wird, ist stets der (totale) kausale Effekt wie oben definiert gemeint.\nGehen Sie davon aus, dass die Daten zur Studie wie oben dargestellt erhoben und zugänglich sind; die Datenerhebung aber abgeschlossen ist.\n\n\n\n\nEs sollte vom Forschungsteam auf Baseline Pane kontrolliert werden, um den kausalen Effekt zu identifizieren.\nEs sollte vom Forschungsteam auf Underlying Readiness kontrolliert werden, um den kausalen Effekt zu identifizieren.\nEs sollte vom Forschungsteam auf Surgical Status kontrolliert werden, um den kausalen Effekt zu identifizieren.\nEs sollte vom Forschungsteam auf Change in Pain kontrolliert werden, um den kausalen Effekt zu identifizieren.\nEs sollte vom Forschungsteam auf Measured Readiness kontrolliert werden, um den kausalen Effekt zu identifizieren."
  },
  {
    "objectID": "posts/kausal10/kausal10.html#answerlist",
    "href": "posts/kausal10/kausal10.html#answerlist",
    "title": "kausal10",
    "section": "",
    "text": "Es sollte vom Forschungsteam auf Baseline Pane kontrolliert werden, um den kausalen Effekt zu identifizieren.\nEs sollte vom Forschungsteam auf Underlying Readiness kontrolliert werden, um den kausalen Effekt zu identifizieren.\nEs sollte vom Forschungsteam auf Surgical Status kontrolliert werden, um den kausalen Effekt zu identifizieren.\nEs sollte vom Forschungsteam auf Change in Pain kontrolliert werden, um den kausalen Effekt zu identifizieren.\nEs sollte vom Forschungsteam auf Measured Readiness kontrolliert werden, um den kausalen Effekt zu identifizieren."
  },
  {
    "objectID": "posts/kausal10/kausal10.html#answerlist-1",
    "href": "posts/kausal10/kausal10.html#answerlist-1",
    "title": "kausal10",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/log-y-regr3/log-y-regr3.html",
    "href": "posts/log-y-regr3/log-y-regr3.html",
    "title": "log-y-regr3",
    "section": "",
    "text": "library(tidyverse)\nlibrary(easystats)\n\nIn dieser Aufgabe modellieren wir den (kausalen) Effekt von Schulbildung auf das Einkommen.\nImportieren Sie zunächst den Datensatz und verschaffen Sie sich einen Überblick.\n\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Treatment.csv\"\n\nd &lt;- data_read(d_path)\n\nDokumentation und Quellenangaben zum Datensatz finden sich hier.\n\nglimpse(d)\n\nRows: 2,675\nColumns: 11\n$ V1      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…\n$ treat   &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n$ age     &lt;int&gt; 37, 30, 27, 33, 22, 23, 32, 22, 19, 21, 18, 27, 17, 19, 27, 23…\n$ educ    &lt;int&gt; 11, 12, 11, 8, 9, 12, 11, 16, 9, 13, 8, 10, 7, 10, 13, 10, 12,…\n$ ethn    &lt;chr&gt; \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\",…\n$ married &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ re74    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ re75    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ re78    &lt;dbl&gt; 9930.05, 24909.50, 7506.15, 289.79, 4056.49, 0.00, 8472.16, 21…\n$ u74     &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n$ u75     &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n\n\nWelcher der Prädiktoren hat den stärkesten Einfluss auf das Einkommen?\nHinweise:\n\nVerwenden Sie lm zur Modellierung.\nOperationalisieren Sie das Einkommen mit der Variable re74.\nGehen Sie von einem kausalen Effekt der Prädiktoren aus.\nGehen Sie von einem multiplikativen Modell aus (log-y).\nLassen Sie die Variablen zur Arbeitslosigkeit außen vor.\n\n\n\n\ntreat\nage\neduc\nethn\nmarried"
  },
  {
    "objectID": "posts/log-y-regr3/log-y-regr3.html#answerlist",
    "href": "posts/log-y-regr3/log-y-regr3.html#answerlist",
    "title": "log-y-regr3",
    "section": "",
    "text": "treat\nage\neduc\nethn\nmarried"
  },
  {
    "objectID": "posts/log-y-regr3/log-y-regr3.html#answerlist-1",
    "href": "posts/log-y-regr3/log-y-regr3.html#answerlist-1",
    "title": "log-y-regr3",
    "section": "Answerlist",
    "text": "Answerlist\n\nTRUE\nFALSE\nFALSE\nFALSE\nFALSE\n\n\nCategories:\n\nstats-nutshell\nqm2\nregression\nlog"
  },
  {
    "objectID": "posts/twitter01/twitter01.html",
    "href": "posts/twitter01/twitter01.html",
    "title": "twitter01",
    "section": "",
    "text": "Exercise\nLaden Sie die neuesten Tweets an karl_lauterbach herunter!\n         \n\n\nSolution\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(rtweet)\n\n\nAttaching package: 'rtweet'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\n\nEinloggen bei Twitter; zuerst die Credentials bereithalten:\n\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\n\nDann anmelden, z.B. als Bot:\n\nauth &lt;- rtweet_bot(api_key = API_Key,\n                   api_secret = API_Key_Secret,\n                   access_token = Access_Token,\n                   access_secret = Access_Token_Secret)\n\n… Oder als App, das bringt bessere Raten mit sich:\n\nauth &lt;- rtweet_app(bearer_token = Bearer_Token)\n\nTest:\n\nsesa_test &lt;- get_timeline(user = \"sauer_sebastian\", n = 3) %&gt;% \n  select(full_text)\n\nsesa_test\n\n1 RT @fuecks: By the way: Systematic destruction of life-sustaining infrastructures …\n2 RT @NoContextBrits: No shortbread for little Nazis. https://t.co/F6FUPvRz94        \n3 RT @ernst_gennat: 2 oder 3 Jahre #Tempolimit von 120 km/h. Abschließend Evaluation…\nTweets an Karl Lauterbach suchen:\n\nkarl1 &lt;- search_tweets(\"@karl_lauterbach\")\n\nIn Auszügen:\n\"@Karl_Lauterbach Ein Minister der alle paar Stunden Zeit hat einen Mist zu verbreiten....\"   \n\"@Karl_Lauterbach @focusonline Long Covid ist nichts anderes als schwere Nebenwirkungen der Gentherapie!\"  \"@Karl_Lauterbach @focusonline Wer schützt uns vor Long Lauterbach?\"\n\"@Karl_Lauterbach Also Karl, primär fordere ich und viele andere eher erstmal dein sofortigen Rücktritt.\"  \"@Karl_Lauterbach Behalt deinen Senf für dich!\"                                                            \"@Karl_Lauterbach Oh Gott 😱\"     \n\"@Karl_Lauterbach Ach nein, der Clown mit Lebensangst ….\\n\\nhttps://t.co/8cQZeHh6Ew\"                       \"@Karl_Lauterbach Ich kenne nur Leute mit Long Covid, die mehrfach geimpft sind! Das ist kein Witz! Scheinbar liegt’s wohl doch an den Spritzen???\"                                                            \"@Karl_Lauterbach @focusonline Interessiert keine Sau 😉\"                      \n\"RT @Karl_Lauterbach @focusonline „Lauterbachs Aussagen können fundamental nicht stimmen“\\nhttps://t.co/rfxnWAWiZX\"                                                                          \"@Karl_Lauterbach @focusonline 🤡😂😂😂😂😂😂\"                                         \n\"@Karl_Lauterbach Jau und sie sind kein fähiger Gesundheitsminister, sondern lediglich ein gekaufter Coronaminister\"        \nPuh, viele toxische Tweets, wie es scheint.\nUnd ohne Retweets (RT) und ohne Replies:\n\nkarl2 &lt;- search_tweets(\"@karl_lauterbach\", \n  include_rts = FALSE, `-filter` = \"replies\")\n\nTweets, die an Karl Lauterbach gerichtet sind, per API-Anweisung:\n\nkarl3 &lt;- search_tweets(\"to:karl_lauterbach\", n = 100)\n\n\"@Karl_Lauterbach Vielen Dank, dass LongCovid ein gefundenes fressen für die jenigen ist, die nicht mehr Arbeiten wollen.\"       \n \"@Karl_Lauterbach verpiss dich einfach! Immer dieser Schwachsinn\"    \n\"@Karl_Lauterbach @focusonline Das sind genau die Impfnebenwirkungen! Will man nun das wenden um die Impfnebenwirkungen zu vertuschen? \\nWofür ist die Impfung gut wenn nicht mal Long-Covid verhindert wird, die Ansteckung konnte sie noch nie verhindern!\\nWarum sind 89% Covid Patienten geimpfte in den Spitäler?\"\n\"@Karl_Lauterbach Was spielen Sie eigentlich für ein schmutziges Spiel?\\n\\nhttps://t.co/8LJIzxyF7G\"   \n \"@Karl_Lauterbach @focusonline Bessen von Covid! Ständig wird das Netz durchsucht, nach Artikeln,die instrumentalisiert werden, um für Impfung zu werben. Was hätte nur ein vernünftiger Gesundheitsminister mit so viel Zeit Vernünftiges im Gesundheitswesen auf die Beine stellen können...\"    \n\"@Karl_Lauterbach Mit Dauerschaden wegen der Impfung 💉 bin ich Arbeitslos geworden in der Pflege 🤷‍♂️ Ist das normal Herr @Karl_Lauterbach ?\"          \nOb man mit @karl_lauterbach sucht oder `to:karl_lauterbach”, scheint keinen großen Unterschied zu machen (?).\n\nCategories:\n\ntextmining\ntwitter"
  },
  {
    "objectID": "posts/ReThink4e2/ReThink4e2.html",
    "href": "posts/ReThink4e2/ReThink4e2.html",
    "title": "ReThink4e2",
    "section": "",
    "text": "Wie viele Parameter sind im folgenden Modell zu schätzen?\nLikelihood: \\(h_i \\sim \\mathcal{N}(\\mu, \\sigma)\\)\nPrior für \\(\\mu\\): \\(\\mu \\sim \\mathcal{N}(178, 20)\\)\nPrior für \\(\\sigma\\): \\(\\sigma \\sim \\mathcal{U}(0, 50)\\)\nQuelle: McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Aufl.). Taylor and Francis, CRC Press.\n\n\n\n0\n1\n2\n3\nmehr"
  },
  {
    "objectID": "posts/ReThink4e2/ReThink4e2.html#answerlist",
    "href": "posts/ReThink4e2/ReThink4e2.html#answerlist",
    "title": "ReThink4e2",
    "section": "",
    "text": "0\n1\n2\n3\nmehr"
  },
  {
    "objectID": "posts/ReThink4e2/ReThink4e2.html#answerlist-1",
    "href": "posts/ReThink4e2/ReThink4e2.html#answerlist-1",
    "title": "ReThink4e2",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nWahr\nFalsch\nFalsch\n\n\nCategories:\n\nprobability\nbayes"
  },
  {
    "objectID": "posts/twitter06/twitter06.html",
    "href": "posts/twitter06/twitter06.html",
    "title": "twitter06",
    "section": "",
    "text": "Exercise\nLaden Sie \\(n=10^k\\) Tweets von Twitter herunter (mit \\(k=4\\)) via der Twitter API; die Tweets sollen jeweils an eine prominente Person gerichtet sein.\nBeziehen Sie sich auf folgende Personen bzw. Twitter-Accounts:\n\nMarkus_Soeder\nkarl_lauterbach.\n\nBereiten Sie die Textdaten mit grundlegenden Methoden des Textminings auf (Tokenisieren, Stopwörter entfernen, Zahlen entfernen, …).\nNutzen Sie die Daten dann, um eine Sentimentanalyse zu erstellen.\nVergleichen Sie die Ergebnisse für alle untersuchten Personen.\n         \n         \n\n\nSolution\n\nlibrary(rtweet)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()  masks stats::filter()\n✖ purrr::flatten() masks rtweet::flatten()\n✖ dplyr::lag()     masks stats::lag()\n\nlibrary(tidytext)\nlibrary(lsa)  # Stopwörter\n\nLoading required package: SnowballC\n\nlibrary(SnowballC)  # Stemming\n\n\ndata(sentiws, package = \"pradadata\")\n\nZuerst muss man sich anmelden und die Tweets herunterladen:\n\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\n\nauth &lt;- rtweet_app(bearer_token = Bearer_Token)\n\n\ntweets_to_kl &lt;- search_tweets(\"@karl_lauterbach\", n = 1e2, include_rts = FALSE)\n#write_rds(tweets_to_kl, file = \"tweets_to_kl.rds\", compress = \"gz\")\ntweets_to_ms &lt;- search_tweets(\"@Markus_Soeder\", n = 1e4, include_rts = FALSE)\n#write_rds(tweets_to_ms, file = \"tweets_to_ms.rds\", compress = \"gz\")\n\nDie Vorverarbeitung pro Screenname packen wir in eine Funktion, das macht es hinten raus einfacher:\n\nprepare_tweets &lt;- function(tweets){\n  \n  tweets %&gt;% \n    select(full_text) %&gt;% \n    unnest_tokens(output = word, input = full_text) %&gt;% \n    anti_join(tibble(word = lsa::stopwords_de)) %&gt;% \n    mutate(word = str_replace_na(word, \"^[:digit:]+$\")) %&gt;% \n    mutate(word = str_replace_na(word, \"hptts?://\\\\w+\")) %&gt;% \n    mutate(word = str_replace_na(word, \" +\")) %&gt;% \n    drop_na()\n}\n\nTest:\n\nkl_prepped &lt;- \n  prepare_tweets(tweets_to_kl_raw)\n\nJoining, by = \"word\"\n\nhead(kl_prepped)\n\n# A tibble: 6 × 1\n  word                     \n  &lt;chr&gt;                    \n1 tonline⁩                  \n2 spreche                  \n3 neuen                    \n4 pläne                    \n5 bundesgesundheitsminister\n6 karl_lauterbach⁩          \n\n\n\nms_prepped &lt;-\n  prepare_tweets(tweets_to_ms_raw)\n\nJoining, by = \"word\"\n\nhead(ms_prepped)\n\n# A tibble: 6 × 1\n  word         \n  &lt;chr&gt;        \n1 markus_soeder\n2 climate      \n3 activists    \n4 are          \n5 sometimes    \n6 depicted     \n\n\nScheint zu passen.\nDie Sentimentanalyse packen wir auch in eine Funktion:\n\nget_tweets_sentiments &lt;- function(tweets){\n  \n  tweets %&gt;% \n    inner_join(sentiws) %&gt;% \n    group_by(neg_pos) %&gt;% \n    summarise(senti_avg = mean(value, na.rm = TRUE),\n              senti_sd = sd(value, na.rm = TRUE),\n              senti_n = n()) \n}\n\nTest:\n\nkl_prepped %&gt;% \n  get_tweets_sentiments()\n\nJoining, by = \"word\"\n\n\n# A tibble: 2 × 4\n  neg_pos senti_avg senti_sd senti_n\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;int&gt;\n1 neg        -0.313    0.237    3576\n2 pos         0.112    0.145    5800\n\n\nTest:\n\ntweets_to_kl_raw %&gt;% \n  prepare_tweets() %&gt;% \n  get_tweets_sentiments()\n\nJoining, by = \"word\"\nJoining, by = \"word\"\n\n\n# A tibble: 2 × 4\n  neg_pos senti_avg senti_sd senti_n\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;int&gt;\n1 neg        -0.313    0.237    3576\n2 pos         0.112    0.145    5800\n\n\nScheint zu passen.\nWir könnten noch die beiden Funktionen in eine wrappen:\n\nprep_sentiments &lt;- function(tweets) {\n\n  tweets %&gt;% \n    prepare_tweets() %&gt;% \n    get_tweets_sentiments()\n}\n\n\ntweets_to_kl_raw %&gt;% \n  prep_sentiments()\n\nJoining, by = \"word\"\nJoining, by = \"word\"\n\n\n# A tibble: 2 × 4\n  neg_pos senti_avg senti_sd senti_n\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;int&gt;\n1 neg        -0.313    0.237    3576\n2 pos         0.112    0.145    5800\n\n\nOkay, jetzt werden wir die Funktion auf jede Screenname bzw. die Tweets jedes Screennames an.\n\ntweets_list &lt;-\n  list(\n    kl = tweets_to_kl_raw, \n    ms = tweets_to_ms_raw)\n\n\nsentis &lt;-\n  tweets_list %&gt;% \n  map_df(prep_sentiments, .id = \"id\")\n\nJoining, by = \"word\"\nJoining, by = \"word\"\nJoining, by = \"word\"\nJoining, by = \"word\"\n\n\n\nCategories:\n\ntextmining\ntwitter\nprogramming"
  },
  {
    "objectID": "posts/penguins-stan-02/penguins-stan-02.html",
    "href": "posts/penguins-stan-02/penguins-stan-02.html",
    "title": "penguins-stan-02",
    "section": "",
    "text": "Aufgabe\nWir untersuchen Einflussfaktoren bzw. Prädiktoren auf das Körpergewicht von Pinguinen. In dieser Aufgabe untersuchen wir den Zusammenhang von Schnabellänge (als UV) und Körpergewicht (als AV).\nWie groß ist der statistische Einfluss der UV auf die AV?\nGeben Sie die Breite eines 90%-HDI an (zum Effekt)!\nHinweise:\n\nNutzen Sie den Datensatz zu den Palmer Penguins.\nSie können den Datensatz z.B. hier beziehen oder über das R-Paket palmerpenguins.\nWeitere Hinweise\n\n         \n\n\nLösung\nZentrieren ist eigentlich immer nützlich, aber hier streng genommen nicht unbedingt nötig. Der Hauptgrund dafür ist, dass Stan für uns den Prior für den Intercept festlegt, und zwar auf Basis der Daten, wir uns also nicht um die komische Frage zu kümmern brauchen, welchen Prior wir für den unzentrierten Achsenabschnitt vergeben wollten: Wie schwer sind Pinguins der Schnabellänge Null? Mit zentrierten Prädiktoren ist die Frage nach dem Prior viel einfacher zu beantworten: Wie schwer ist ein Pinguin mit mittelgroßem Schnabel?\nSetup:\n\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(rstanarm)\n\ndata(\"penguins\", package = \"palmerpenguins\")\n\nEs wird in dieser Aufgabe vorausgesetzt, dass Sie den Datensatz selbständig importieren können. Tipp: Kurzes Googeln hilft ggf., den Datensatz zu finden.\nAlternativ könnten Sie den Datensatz als CSV-Datei importieren:\n\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\"\npenguins &lt;- data_read(d_path)\n\nEin Blick in die Daten zur Kontrolle, ob das Importieren richtig funktioniert hat:\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nVertrauen ist gut, aber - was Golems betrifft - ist Kontrolle eindeutig besser ;-)\n\nm1 &lt;- stan_glm(body_mass_g ~  bill_length_mm,  # Regressionsgleichung\n               data = penguins, #  Daten\n               seed = 42,  # Repro.\n               refresh = 0)  # nicht so viel Output\n\n\nparameters(m1, ci_method = \"hdi\", ci = .9, keep = \"bill_length_mm\")\n\nParameter      | Median |         90% CI |   pd |  Rhat |     ESS |                Prior\n----------------------------------------------------------------------------------------\nbill_length_mm |  87.45 | [76.24, 97.70] | 100% | 1.000 | 3216.00 | Normal (0 +- 367.22)\n\n\n\nUncertainty intervals (highest-density) and p-values (two-tailed)\n  computed using a MCMC distribution approximation.\n\n\nDie Lösung lautet also, wie aus der Ausgabe von parameters() ersichtlich, 21.47.\n\nCategories:\n\nbayes\nregression\nexam-22"
  },
  {
    "objectID": "posts/wrangle10/wrangle10.html",
    "href": "posts/wrangle10/wrangle10.html",
    "title": "wrangle10",
    "section": "",
    "text": "Aufgabe\nBetrachten Sie folgende Tabelle:\n\ndf &lt;- tibble(\n  groesse = c(180, 190, 160, 170),\n  geschlecht = c(\"m\", \"m\", \"f\", \"f\")\n)\ndf\n\n\n\n\ngroesse\ngeschlecht\n\n\n\n\n180\nm\n\n\n190\nm\n\n\n160\nf\n\n\n170\nf\n\n\n\n\n\nHinweis: Der Befehl tibble erstellt einen Tibble (Dataframe).\nWas ist er erste Wert, den der folgende Ausdruck zurückliefert?\n\ndf_grouped &lt;- group_by(df, geschlecht)\n\nsummarise(df_grouped, ergebnis = mean(groesse))\n\n         \n\n\nLösung\nDie Werte werden alphabetisch (bzw. alphanumerisch) sortiert. “f” kommt vor “m” im Alphabet.\nAntwort: 165\n\ndf_grouped &lt;- group_by(df, geschlecht)\n\nsummarise(df_grouped, ergebnis = mean(groesse))\n\n\n\n\ngeschlecht\nergebnis\n\n\n\n\nf\n165\n\n\nm\n185\n\n\n\n\n\n\nCategories:\n\neda\nlagemaße\nnum"
  },
  {
    "objectID": "posts/tidymodels-poly01/tidymodels-poly01.html",
    "href": "posts/tidymodels-poly01/tidymodels-poly01.html",
    "title": "tidymodels-poly01",
    "section": "",
    "text": "Aufgabe\nFitten Sie ein Polynomial-Modell für folgende Modellgleichung:\nbody_mass_g ~ bill_length_mm.\nGesucht ist der optimale Polynomgrad im Train-Sample (optimal hinsichtlich minimalem Prognosefehler).\nHinweise:\n\nDatensatz penguins (palmerpenguins)\nVerwenden Sie Tidymodels\nFitten Sie Polynome des Grades 1 bis 10.\nDefinieren Sie die Polynomegrade als Tuningparameter.\nBeziehen Sie sich auf RMSE als Kennzahl der Modellgüte.\nEntfernen Sie fehlende Werte in den Prädiktoren\n\n         \n\n\nLösung\nSetup:\n\nlibrary(tidymodels)\ndata(penguins, package = \"palmerpenguins\")\n\nRezept:\n\nrec1 &lt;- \n  recipe(body_mass_g ~ bill_length_mm, data = penguins) %&gt;% \n  step_naomit(all_predictors()) %&gt;% \n  step_poly(all_predictors(), degree = tune()) %&gt;% \n  update_role(contains(\"_poly_\"), new_role = \"predictor\")\n\nWarning: No columns were selected in `update_role()`.\n\n\nCheck:\n\nd_baked &lt;- bake(prep(rec1), new_data = NULL)\n\nWorkflow:\n\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(linear_reg()) %&gt;% \n  add_recipe(rec1)\n\nRezepte mit Tuningparametern kann man nicht preppen/backen.\nTuning:\n\ntune1 &lt;-\n  tune_grid(\n    wf1,\n    resamples = vfold_cv(data = penguins),\n    metrics = metric_set(rmse),\n    grid = grid_regular(degree(range = c(1, 10)),\n                               levels = 10)\n  )\n\n\nautoplot(tune1)\n\n\n\n\n\n\n\n\n\nshow_best(tune1)\n\n# A tibble: 5 × 7\n  degree .metric .estimator  mean     n std_err .config              \n   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1      2 rmse    standard    645.    10    18.6 Preprocessor02_Model1\n2      5 rmse    standard    647.    10    15.8 Preprocessor05_Model1\n3      1 rmse    standard    648.    10    22.7 Preprocessor01_Model1\n4      4 rmse    standard    649.    10    19.4 Preprocessor04_Model1\n5      3 rmse    standard    652.    10    20.9 Preprocessor03_Model1\n\n\n\nsol &lt;- show_best(tune1)$degree[1]\nsol\n\n[1] 2\n\n\nDie Antwort lautet: 2.\n\nCategories:\n\nR\nstat-learning\ntidymodels\nnum"
  },
  {
    "objectID": "posts/argumente/argumente.html",
    "href": "posts/argumente/argumente.html",
    "title": "argumente",
    "section": "",
    "text": "Welche der folgenden Syntax-Beispiele zeigt fehlerhaften Code?\nEs sei jeweils definiert:\n\nx &lt;- c(1, 2, 3)\n\n\n\n\nmean(x = x)\nmean(FALSE)\nmean(na.rm = FALSE, x = x)\nmean(x, 0, FALSE)"
  },
  {
    "objectID": "posts/argumente/argumente.html#answerlist",
    "href": "posts/argumente/argumente.html#answerlist",
    "title": "argumente",
    "section": "",
    "text": "mean(x = x)\nmean(FALSE)\nmean(na.rm = FALSE, x = x)\nmean(x, 0, FALSE)"
  },
  {
    "objectID": "posts/argumente/argumente.html#answerlist-1",
    "href": "posts/argumente/argumente.html#answerlist-1",
    "title": "argumente",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\nFalsch\nFalsch\n\n\nCategories:\n\nR\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/penguins-stan-05/penguins-stan-05.html",
    "href": "posts/penguins-stan-05/penguins-stan-05.html",
    "title": "penguins-stan-05",
    "section": "",
    "text": "Aufgabe\nWir untersuchen Einflussfaktoren bzw. Prädiktoren auf das Körpergewicht von Pinguinen. In dieser Aufgabe untersuchen wir den Zusammenhang von Schnabellänge (als UV) und Körpergewicht (als AV).\nAufgabe: Wie breit ist das 95%-ETI, wenn Sie nur die Spezies Adelie untersuchen?\nHinweise:\n\nSie können den Datensatz z.B. hier beziehen oder über das R-Paket palmerpenguins.\nWeitere Hinweise\n\n         \n\n\nLösung\nZentrieren ist eigentlich immer nützlich, aber hier streng genommen nicht unbedingt nötig. Der Hauptgrund dafür ist, dass Stan für uns den Prior für den Intercept festlegt, und zwar auf Basis der Daten, wir uns also nicht um die komische Frage zu kümmern brauchen, welchen Prior wir für den unzentrierten Achsenabschnitt vergeben wollten: Wie schwer sind Pinguins der Schnabellänge Null? Mit zentrierten Prädiktoren ist die Frage nach dem Prior viel einfacher zu beantworten: Wie schwer ist ein Pinguin mit mittelgroßem Schnabel?\nSetup:\n\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(rstanarm)\n\ndata(\"penguins\", package = \"palmerpenguins\")\n\nEs wird in dieser Aufgabe vorausgesetzt, dass Sie den Datensatz selbständig importieren können. Tipp: Kurzes Googeln hilft ggf., den Datensatz zu finden.\nAlternativ könnten Sie den Datensatz als CSV-Datei importieren:\n\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\"\n\nEin Blick in die Daten zur Kontrolle, ob das Importieren richtig funktioniert hat:\n\npenguins &lt;- data_read(d_path)\npenguins_adelie &lt;- \n  penguins %&gt;% \n  filter(species == \"Adelie\")\n\nglimpse(penguins)\n\nRows: 344\nColumns: 9\n$ V1                &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n$ species           &lt;chr&gt; \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"A…\n$ island            &lt;chr&gt; \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", …\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;chr&gt; \"male\", \"female\", \"female\", NA, \"female\", \"male\", \"f…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nVertrauen ist gut, aber - was Golems betrifft - ist Kontrolle eindeutig besser ;-)\n\nm1 &lt;- stan_glm(body_mass_g ~  bill_length_mm,  # Regressionsgleichung\n               data = penguins_adelie, #  Daten\n               seed = 42,  # Repro.\n               refresh = 0)  # nicht so viel Output\n\n\nparameters(m1, ci = .95, ci_method = \"eti\")\n\nParameter      | Median |            95% CI |     pd |  Rhat |     ESS |                       Prior\n----------------------------------------------------------------------------------------------------\n(Intercept)    |  15.91 | [-865.68, 948.08] | 51.45% | 1.000 | 3825.00 | Normal (3700.66 +- 1146.42)\nbill_length_mm |  95.01 | [  70.92, 117.66] |   100% | 1.000 | 3876.00 |     Normal (0.00 +- 430.43)\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a MCMC distribution approximation.\n\n\nDie Lösung lautet also, wie in der Ausgabe zu den Parametern ersichtlich, 46.74.\n\nCategories:\n\nbayes\nregression\nexam-22"
  },
  {
    "objectID": "posts/summarise01/summarise01.html",
    "href": "posts/summarise01/summarise01.html",
    "title": "summarise01",
    "section": "",
    "text": "Aufgabe\nIm Datensatz mariokart:\nFassen Sie die Spalte total_pr zusammen und zwar zum maximalwert!\nGeben Sie diese Zahl als Antwort zurück!\n         \n\n\nLösung\nPakete starten:\nDaten importieren:\nZusammenfassen:\n\n\n  max_preis\n1    326.51\n\n\nmin analog.\nDie Lösung lautet: 327 Euro\n\nCategories:\n\ndatawrangling\neda\ntidyverse\ndplyr\nnum"
  },
  {
    "objectID": "posts/Ridges-vergleichen/Ridges-vergleichen.html",
    "href": "posts/Ridges-vergleichen/Ridges-vergleichen.html",
    "title": "Ridges-vergleichen",
    "section": "",
    "text": "Warning: `stat(x)` was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(x)` instead.\n\n\n\n\n\n\n\n\n\n\n\n\nDie Mittelwerte der Histogramme sind identisch.\nDie Mediane der Histogramme sind identisch.\nDie Histogramme sind (alle) linksschief.\nDie Histogramme sind (alle) rechtsschief.\nDie Färbung (Füllfarbe) kodiert die Schliffart (cut).\nEinige Histogramme sind normalverteilt."
  },
  {
    "objectID": "posts/Ridges-vergleichen/Ridges-vergleichen.html#answerlist",
    "href": "posts/Ridges-vergleichen/Ridges-vergleichen.html#answerlist",
    "title": "Ridges-vergleichen",
    "section": "",
    "text": "Die Mittelwerte der Histogramme sind identisch.\nDie Mediane der Histogramme sind identisch.\nDie Histogramme sind (alle) linksschief.\nDie Histogramme sind (alle) rechtsschief.\nDie Färbung (Füllfarbe) kodiert die Schliffart (cut).\nEinige Histogramme sind normalverteilt."
  },
  {
    "objectID": "posts/Ridges-vergleichen/Ridges-vergleichen.html#answerlist-1",
    "href": "posts/Ridges-vergleichen/Ridges-vergleichen.html#answerlist-1",
    "title": "Ridges-vergleichen",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nRichtig\nFalsch\nFalsch\n\n\nCategories:\n\nvis\ndyn\nschoice"
  },
  {
    "objectID": "posts/log-y-regr2/log-y-regr2.html",
    "href": "posts/log-y-regr2/log-y-regr2.html",
    "title": "log-y-regr2",
    "section": "",
    "text": "Exercise\n\nlibrary(tidyverse)\nlibrary(easystats)\n\nIn dieser Aufgabe modellieren wir den (kausalen) Effekt von Schulbildung auf das Einkommen.\nImportieren Sie zunächst den Datensatz und verschaffen Sie sich einen Überblick.\n\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Treatment.csv\"\n\nd &lt;- data_read(d_path)\n\nDokumentation und Quellenangaben zum Datensatz finden sich hier.\n\nglimpse(d)\n\nRows: 2,675\nColumns: 11\n$ V1      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…\n$ treat   &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n$ age     &lt;int&gt; 37, 30, 27, 33, 22, 23, 32, 22, 19, 21, 18, 27, 17, 19, 27, 23…\n$ educ    &lt;int&gt; 11, 12, 11, 8, 9, 12, 11, 16, 9, 13, 8, 10, 7, 10, 13, 10, 12,…\n$ ethn    &lt;chr&gt; \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\",…\n$ married &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ re74    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ re75    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ re78    &lt;dbl&gt; 9930.05, 24909.50, 7506.15, 289.79, 4056.49, 0.00, 8472.16, 21…\n$ u74     &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n$ u75     &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n\n\nModellieren Sie den Effekt der Bildungsdauer auf das Einkommen! Gehen Sie von einem exponenziellen Zusammenhang der beiden Variablen aus. Wie verändert sich die Verteilung der abhängigen Variablen (Y) durch die Logarithmus-Transformation?\nHinweise:\n\nVerwenden Sie lm zur Modellierung.\nOperationalisieren Sie das Einkommen mit der Variable re74.\nFügen Sie keine weiteren Variablen dem Modell hinzu.\nGehen Sie von einem kausalen Effekt des Prädiktors aus.\n\n         \n\n\nSolution\n\nd2 &lt;-\n  d %&gt;% \n  filter(re74 &gt; 0) %&gt;% \n  mutate(re74_log = log(re74))\n\n\nm &lt;- lm(re74_log ~ educ, data = d2)\n\n\nggplot(d2) +\n  aes(x = re74) +\n  geom_density() +\n  labs(title = \"Income raw\")\n\n\nggplot(d2) +\n  aes(x = re74_log) +\n  geom_density() +\n  labs(title = \"Income log transformed\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBetrachten wir die deskriptiven Statistiken:\n\nd2 %&gt;% \n  select(re74, re74_log) %&gt;% \n  describe_distribution()\n\nVariable |     Mean |       SD |      IQR |             Range | Skewness | Kurtosis |    n | n_Missing\n------------------------------------------------------------------------------------------------------\nre74     | 20938.28 | 12631.52 | 15086.30 | [17.63, 1.37e+05] |     1.62 |     6.81 | 2329 |         0\nre74_log |     9.73 |     0.76 |     0.80 |     [2.87, 11.83] |    -1.67 |     6.01 | 2329 |         0\n\n\nDie Log-Transformation hat in diesem Fall nicht wirklich zu einer Normalisierung der Variablen beigetragen. Aber das war auch nicht unser Ziel.\n\nCategories:\n\nregression\nlm\nqm2\nstats-nutshell"
  },
  {
    "objectID": "posts/fat-tails-Artikel/fat-tails-Artikel.html",
    "href": "posts/fat-tails-Artikel/fat-tails-Artikel.html",
    "title": "fat-tails-Artikel",
    "section": "",
    "text": "Exercise\nIn diesem Diagramm sehen Sie etwas Nomenklatur für eine Verteilung: Gipfel (Peak), Schultern (shoulders) und Ränder (tails). Bitte klicken Sie den Link, um sich das Diagramm anzuschauen.\nQuelle: Taleb, N. N. (2019). The statistical consequences of fat tails, papers and commentaries. https://nassimtaleb.org/2020/01/final-version-fat-tails/\nZwar sind viele Daten in der Welt normalverteilt, aber längst nicht alle. In jüngerer Zeit sind sog. “Fat Tails” in die Aufmerksamkeit gerückt. Das sind Variablen, bei denen Werte in den Rändern (tails) wahrscheinlicher sind als bei einer Normalverteilung; ein Beispiel für eine Fat-Tail-Verteilung ist die t-Verteilung mit 1 Freiheitsgrad. Sie müssen diese Verteilung nicht weiter kennen, es ist aber nützlich, zu wissen, wozu diese Verteilung nützt\nRecherchieren Sie (Fach-)Artikel, die argumentieren, dass ein bestimmtes Phänomen Fat-Tails zeigt!\n         \n\n\nSolution\n\nKriege\nPandemien\nErfolg auf der Singlebörse Tinder\nKapitelmarkt\n\n\nCategories:\n\nprobability\ndistributions\nfat-tails"
  },
  {
    "objectID": "posts/kausal27/kausal27.html",
    "href": "posts/kausal27/kausal27.html",
    "title": "kausal27",
    "section": "",
    "text": "Gegeben sei der DAG g (s. u.). Der DAG verfügt über 7 Variablen, die als Knoten im Graph dargestellt sind (mit \\(x_1, x_2, \\ldots x_n\\) bezeichnet) und über Kanten verbunden sind.\nWelche minimale Variablenmenge muss kontrolliert werden, um den kausalen Effekt von der UV zur AV zu identifizieren?\nUV: x1.\nAV: x3.\nHinweise:\n\nMengen sind mittels geschweifter Klammern gekennzeichnet, z.B. {x8, x9} meint die Menge mit den zwei Elementen x8 und x9.\nDie leere Menge { } bedeutet, dass keine Variable kontrolliert werden muss, um den kausalen Effekt zu identifizieren.\nAlle Variablen werden als gemessen vorausgesetzt.\nEs ist möglich, dass es keine Lösung gibt, dass es also keine Adjustierungsmenge gibt, um den kausalen Effekt zu identifizieren. Wenn dies der Fall sein sollte, wählen Sie “keine Lösung”.\nEs ist möglich, dass einzelne Variablen keine Kanten besitzen, also keine Verbindung zu anderen Variablen (Knoten) haben.\n\n\n\n\n{ x2, x5 }\n{ x1, x5 }\n{ x2 }\n{ }\n{ x3 }"
  },
  {
    "objectID": "posts/kausal27/kausal27.html#answerlist",
    "href": "posts/kausal27/kausal27.html#answerlist",
    "title": "kausal27",
    "section": "",
    "text": "{ x2, x5 }\n{ x1, x5 }\n{ x2 }\n{ }\n{ x3 }"
  },
  {
    "objectID": "posts/kausal27/kausal27.html#answerlist-1",
    "href": "posts/kausal27/kausal27.html#answerlist-1",
    "title": "kausal27",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nRichtig\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/tidymodels-penguins04/tidymodels-penguins04.html",
    "href": "posts/tidymodels-penguins04/tidymodels-penguins04.html",
    "title": "tidymodels-penguins04",
    "section": "",
    "text": "Aufgabe\nBerechnen Sie ein kNN-Modell mit tidymodels und zwar anhand des penguins Datensatzes.\nModellgleichung: body_mass_g ~ bill_length_mm, data = d_train.\nGesucht ist R-Quadrat als Maß für die Modellgüte im TEST-Sample.\nHinweise:\n\nFixieren Sie die Zufallszahlen auf den Startwert 42.\nNutzen Sie eine v=5,r=2 CV.\nTunen Sie \\(K\\) (Default-Tuning)\nEntfernen Sie fehlende Werte in den Variablen.\nVerzichten Sie auf weitere Schritte der Vorverarbeitung.\n\n         \n\n\nLösung\nSetup:\n\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(tictoc)  # Rechenzeit messen, optional\n# data(penguins, package = \"palmerpenguins\")\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/modeldata/penguins.csv\"\nd &lt;- read_csv(d_path)\n\nDatensatz auf NAs prüfen:\n\nd2 &lt;-\n  d %&gt;% \n  drop_na() \n\nDatensatz aufteilen:\n\nset.seed(42)\nd_split &lt;- initial_split(d2)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\nWorkflow:\n\nrec1 &lt;-\n  recipe(body_mass_g ~ bill_length_mm, data = d_train) %&gt;% \n  step_naomit(all_numeric())\n\nknn_model &lt;-\n  nearest_neighbor(\n    mode = \"regression\",\n    neighbors = tune()\n  ) \n\nwflow &lt;-\n  workflow() %&gt;%\n  add_recipe(rec1) %&gt;%\n  add_model(knn_model)\n\nwflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_naomit()\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-Nearest Neighbor Model Specification (regression)\n\nMain Arguments:\n  neighbors = tune()\n\nComputational engine: kknn \n\n\nBacken:\n\nd_baked &lt;- prep(rec1) %&gt;% bake(new_data = NULL)\nd_baked %&gt;% head()\n\n# A tibble: 6 × 2\n  bill_length_mm body_mass_g\n           &lt;dbl&gt;       &lt;dbl&gt;\n1           34.5        2900\n2           52.2        3450\n3           45.4        4800\n4           42.1        4000\n5           50          5350\n6           41.5        4000\n\n\nAuf NA prüfen:\n\nsum(is.na(d_baked))\n\n[1] 0\n\n\nCV:\n\nset.seed(42)\nfolds &lt;- vfold_cv(d_train, v = 5, repeats = 2)\nfolds\n\n#  5-fold cross-validation repeated 2 times \n# A tibble: 10 × 3\n   splits           id      id2  \n   &lt;list&gt;           &lt;chr&gt;   &lt;chr&gt;\n 1 &lt;split [199/50]&gt; Repeat1 Fold1\n 2 &lt;split [199/50]&gt; Repeat1 Fold2\n 3 &lt;split [199/50]&gt; Repeat1 Fold3\n 4 &lt;split [199/50]&gt; Repeat1 Fold4\n 5 &lt;split [200/49]&gt; Repeat1 Fold5\n 6 &lt;split [199/50]&gt; Repeat2 Fold1\n 7 &lt;split [199/50]&gt; Repeat2 Fold2\n 8 &lt;split [199/50]&gt; Repeat2 Fold3\n 9 &lt;split [199/50]&gt; Repeat2 Fold4\n10 &lt;split [200/49]&gt; Repeat2 Fold5\n\n\nTunen:\n\nd_resamples &lt;-\n  tune_grid(\n    wflow,\n    resamples = folds,\n    control = control_grid(save_workflow = TRUE)\n  )\n\nd_resamples\n\n# Tuning results\n# 5-fold cross-validation repeated 2 times \n# A tibble: 10 × 5\n   splits           id      id2   .metrics          .notes          \n   &lt;list&gt;           &lt;chr&gt;   &lt;chr&gt; &lt;list&gt;            &lt;list&gt;          \n 1 &lt;split [199/50]&gt; Repeat1 Fold1 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [199/50]&gt; Repeat1 Fold2 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [199/50]&gt; Repeat1 Fold3 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [199/50]&gt; Repeat1 Fold4 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [200/49]&gt; Repeat1 Fold5 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [199/50]&gt; Repeat2 Fold1 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [199/50]&gt; Repeat2 Fold2 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [199/50]&gt; Repeat2 Fold3 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [199/50]&gt; Repeat2 Fold4 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [200/49]&gt; Repeat2 Fold5 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt;\n\n\nBester Kandidat:\n\nshow_best(d_resamples)\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\n\n# A tibble: 5 × 7\n  neighbors .metric .estimator  mean     n std_err .config              \n      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1        15 rmse    standard    676.    10    15.6 Preprocessor1_Model10\n2        13 rmse    standard    684.    10    15.7 Preprocessor1_Model09\n3        11 rmse    standard    692.    10    16.4 Preprocessor1_Model08\n4        10 rmse    standard    694.    10    16.5 Preprocessor1_Model07\n5         9 rmse    standard    702.    10    15.8 Preprocessor1_Model06\n\n\n\nfitbest &lt;- fit_best(d_resamples)\nfitbest\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_naomit()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nkknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(15L,     data, 5))\n\nType of response variable: continuous\nminimal mean absolute error: 526.1741\nMinimal mean squared error: 416047.6\nBest kernel: optimal\nBest k: 15\n\n\nLast Fit:\n\nfit_last &lt;- last_fit(fitbest, d_split)\nfit_last\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits           id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;           &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [249/84]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n\nModellgüte im Test-Sample:\n\nfit_last %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard     605.    Preprocessor1_Model1\n2 rsq     standard       0.385 Preprocessor1_Model1\n\n\nR-Quadrat:\n\nsol &lt;- collect_metrics(fit_last)[[\".estimate\"]][2]\nsol\n\n[1] 0.3849557\n\n\n\nCategories:\n\nds1\ntidymodels\nprediction\nyacsda\nstat-learning\nnum"
  },
  {
    "objectID": "posts/tidymodels-penguins03/tidymodels-penguins03.html",
    "href": "posts/tidymodels-penguins03/tidymodels-penguins03.html",
    "title": "tidymodels-penguins03",
    "section": "",
    "text": "Aufgabe\nBerechnen Sie ein kNN-Modell mit tidymodels und zwar anhand des penguins Datensatzes.\nModellgleichung: body_mass_g ~ bill_length_mm, data = d_train.\nGesucht ist R-Quadrat als Maß für die Modellgüte im TEST-Sample.\nHinweise:\n\nFixieren Sie die Zufallszahlen auf den Startwert 42.\nNutzen Sie eine v=5,r=1 CV.\nTunen Sie \\(K\\) (Default-Tuning)\nEntfernen Sie fehlende Werte in den Variablen.\nVerzichten Sie auf weitere Schritte der Vorverarbeitung.\n\n         \n\n\nLösung\nSetup:\n\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(tictoc)  # Rechenzeit messen, optional\n# data(penguins, package = \"palmerpenguins\")\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/modeldata/penguins.csv\"\nd &lt;- read_csv(d_path)\n\nDatensatz auf NAs prüfen:\n\nd2 &lt;-\n  d %&gt;% \n  drop_na() \n\nDatensatz aufteilen:\n\nset.seed(42)\nd_split &lt;- initial_split(d2)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\nWorkflow:\n\nrec1 &lt;-\n  recipe(body_mass_g ~ bill_length_mm, data = d_train) %&gt;% \n  step_naomit(all_numeric())\n\nknn_model &lt;-\n  nearest_neighbor(\n    mode = \"regression\",\n    neighbors = tune()\n  ) \n\nwflow &lt;-\n  workflow() %&gt;%\n  add_recipe(rec1) %&gt;%\n  add_model(knn_model)\n\nwflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_naomit()\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-Nearest Neighbor Model Specification (regression)\n\nMain Arguments:\n  neighbors = tune()\n\nComputational engine: kknn \n\n\nBacken:\n\nd_baked &lt;- prep(rec1) %&gt;% bake(new_data = NULL)\nd_baked %&gt;% head()\n\n# A tibble: 6 × 2\n  bill_length_mm body_mass_g\n           &lt;dbl&gt;       &lt;dbl&gt;\n1           34.5        2900\n2           52.2        3450\n3           45.4        4800\n4           42.1        4000\n5           50          5350\n6           41.5        4000\n\n\nAuf NA prüfen:\n\nsum(is.na(d_baked))\n\n[1] 0\n\n\nCV:\n\nset.seed(43)\nfolds &lt;- vfold_cv(d_train, v = 5)\nfolds\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits           id   \n  &lt;list&gt;           &lt;chr&gt;\n1 &lt;split [199/50]&gt; Fold1\n2 &lt;split [199/50]&gt; Fold2\n3 &lt;split [199/50]&gt; Fold3\n4 &lt;split [199/50]&gt; Fold4\n5 &lt;split [200/49]&gt; Fold5\n\n\nTunen:\n\nd_resamples &lt;-\n  tune_grid(\n    wflow,\n    resamples = folds,\n    control = control_grid(save_workflow = TRUE)\n  )\n\nd_resamples\n\n# Tuning results\n# 5-fold cross-validation \n# A tibble: 5 × 4\n  splits           id    .metrics          .notes          \n  &lt;list&gt;           &lt;chr&gt; &lt;list&gt;            &lt;list&gt;          \n1 &lt;split [199/50]&gt; Fold1 &lt;tibble [18 × 5]&gt; &lt;tibble [0 × 3]&gt;\n2 &lt;split [199/50]&gt; Fold2 &lt;tibble [18 × 5]&gt; &lt;tibble [0 × 3]&gt;\n3 &lt;split [199/50]&gt; Fold3 &lt;tibble [18 × 5]&gt; &lt;tibble [0 × 3]&gt;\n4 &lt;split [199/50]&gt; Fold4 &lt;tibble [18 × 5]&gt; &lt;tibble [0 × 3]&gt;\n5 &lt;split [200/49]&gt; Fold5 &lt;tibble [18 × 5]&gt; &lt;tibble [0 × 3]&gt;\n\n\nBester Kandidat:\n\nshow_best(d_resamples)\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\n\n# A tibble: 5 × 7\n  neighbors .metric .estimator  mean     n std_err .config             \n      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1        14 rmse    standard    664.     5    22.7 Preprocessor1_Model9\n2        12 rmse    standard    671.     5    23.2 Preprocessor1_Model8\n3        11 rmse    standard    675.     5    24.1 Preprocessor1_Model7\n4         9 rmse    standard    685.     5    22.3 Preprocessor1_Model6\n5         8 rmse    standard    688.     5    22.9 Preprocessor1_Model5\n\n\n\nfitbest &lt;- fit_best(d_resamples)\nfitbest\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_naomit()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nkknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(14L,     data, 5))\n\nType of response variable: continuous\nminimal mean absolute error: 526.4603\nMinimal mean squared error: 416216.1\nBest kernel: optimal\nBest k: 14\n\n\nLast Fit:\n\nfit_last &lt;- last_fit(fitbest, d_split)\nfit_last\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits           id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;           &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [249/84]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n\nModellgüte im Test-Sample:\n\nfit_last %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard     606.    Preprocessor1_Model1\n2 rsq     standard       0.382 Preprocessor1_Model1\n\n\nR-Quadrat:\n\nsol &lt;- collect_metrics(fit_last)[[\".estimate\"]][2]\nsol\n\n[1] 0.38246\n\n\n\nCategories:\n\nds1\ntidymodels\nprediction\nyacsda\nstat-learning\nnum"
  },
  {
    "objectID": "posts/kausal20/kausal20.html",
    "href": "posts/kausal20/kausal20.html",
    "title": "kausal20",
    "section": "",
    "text": "Gegeben sei der DAG g (s. u.). Der DAG verfügt über mehrere Variablen, die als Knoten im Graph dargestellt sind und mit \\(x_1, x_2, \\ldots x_n\\) bezeichnet sind.\nWelche minimale Variablenmenge muss kontrolliert werden, um den kausalen Effekt von der UV zur AV zu identifizieren?\nUV: x3.\nAV: x5.\nHinweise:\n\nMengen sind mittels geschweifter Klammern gekennzeichnet, z.B. {x8, x9} meint die Menge mit den zwei Elementen x8 und x9.\nDie leere Menge { } bedeutet, dass keine Variable kontrolliert werden muss, um den kausalen Effekt zu identifizieren.\nAlle Variablen werden als gemessen vorausgesetzt.\nEs ist möglich, dass es keine Lösung gibt, dass es also keine Adjustierungsmenge gibt, um den kausalen Effekt zu identifizieren. Wenn dies der Fall sein sollte, wählen Sie “keine Lösung”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n{ x2 }\n{ x1 }\n{ x1, x2 }\n{ x2, x5 }\n{ x2, x3 }"
  },
  {
    "objectID": "posts/kausal20/kausal20.html#answerlist",
    "href": "posts/kausal20/kausal20.html#answerlist",
    "title": "kausal20",
    "section": "",
    "text": "{ x2 }\n{ x1 }\n{ x1, x2 }\n{ x2, x5 }\n{ x2, x3 }"
  },
  {
    "objectID": "posts/kausal20/kausal20.html#answerlist-1",
    "href": "posts/kausal20/kausal20.html#answerlist-1",
    "title": "kausal20",
    "section": "Answerlist",
    "text": "Answerlist\n\nRichtig\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/adjustieren1/adjustieren1.html",
    "href": "posts/adjustieren1/adjustieren1.html",
    "title": "adjustieren1",
    "section": "",
    "text": "Exercise\nBetrachten Sie folgendes Modell, das den Zusammenhang von PS-Zahl und Spritverbrauch untersucht (Datensatz mtcars).\nAber zuerst zentrieren wir den metrischen Prädiktor hp, um den Achsenabschnitt besser interpretieren zu können.\n\nmtcars &lt;-\n  mtcars %&gt;% \n  mutate(hp_z = hp - mean(hp))\n\n\nlibrary(rstanarm)\nlm1 &lt;- stan_glm(mpg ~ hp_z, data = mtcars,\n                refresh = 0)\nsummary(lm1)\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 20.1    0.7 19.2  20.1  21.0 \nhp_z        -0.1    0.0 -0.1  -0.1  -0.1 \nsigma        4.0    0.5  3.4   3.9   4.7 \nJetzt können wir aus dem Achsenabschnitt (Intercept) herauslesen, dass ein Auto mit hp_z = 0 - also mit mittlerer PS-Zahl - vielleicht gut 20 Meilen weit mit einer Gallone Sprit kommt.\nZur Verdeutlichung ein Diagramm zum Modell:\n\nmtcars %&gt;% \n  ggplot() +\n  aes(x = hp_z, y = mpg) +\n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\n\n\nAdjustieren Sie im Modell die PS-Zahl um die Art des Schaltgetriebes (am), so dass das neue Modell den statistischen Effekt (nicht notwendig auch kausal) der PS-Zahl bereinigt bzw. unabhängig von der Art des Schaltgetriebes widerspiegelt!\nHinweise:\n\nam=0 ist ein Auto mit Automatikgetriebe.\nWir gehen davon aus, dass der Regressionseffekt gleich stark ist auf allen (beiden) Stufen von am. M.a.W.: Es liegt kein Interaktionseffekt vor.\n\n         \n\n\nSolution\n\nlibrary(rstanarm)\nlm2 &lt;- stan_glm(mpg ~ hp_z + am, data = mtcars,\n                refresh = 0)\nsummary(lm2)\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 26.6    1.5 24.7  26.6  28.5 \nhp          -0.1    0.0 -0.1  -0.1   0.0 \nam           5.3    1.1  3.8   5.3   6.6 \nsigma        3.0    0.4  2.5   3.0   3.5 \nDie Spalte mean gibt den mittleren geschätzten Wert für den jeweiligen Koeffizienten an, also den Schätzwert zum Koeffizienten.\nDie Koeffizienten zeigen, dass der Achsenabschnitt für Autos mit Automatikgetriebe um etwa 5 Meilen geringer ist als für Autos mit manueller Schaltung: Ein durchschnittliches Auto mit manueller Schaltung kommt also etwa 5 Meilen weiter als ein Auto mit Automatikschaltung, glaubt unser Modell.\n\nmtcars %&gt;% \n  mutate(am = factor(am)) %&gt;% \n  ggplot() +\n  aes(x = hp_z, y = mpg, color = am) +\n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\n\n\nMan könnte hier noch einen Interaktionseffekt ergänzen.\n\nCategories:\n\nqm2\nlm\nbayes\nstats-nutshell"
  },
  {
    "objectID": "posts/Interaktionseffekt1/Interaktionseffekt1.html",
    "href": "posts/Interaktionseffekt1/Interaktionseffekt1.html",
    "title": "Interaktionseffekt1",
    "section": "",
    "text": "Wählen Sie das Diagramm, in dem kein Interaktionseffekt (in der Population) vorhanden ist (bzw. wählen Sie Diagramm, dass dies am ehesten darstellt).\n\n\n\nDiagramm A\nDiagramm B\nDiagramm C\nDiagramm D\nDiagramm E"
  },
  {
    "objectID": "posts/Interaktionseffekt1/Interaktionseffekt1.html#answerlist",
    "href": "posts/Interaktionseffekt1/Interaktionseffekt1.html#answerlist",
    "title": "Interaktionseffekt1",
    "section": "",
    "text": "Diagramm A\nDiagramm B\nDiagramm C\nDiagramm D\nDiagramm E"
  },
  {
    "objectID": "posts/Interaktionseffekt1/Interaktionseffekt1.html#answerlist-1",
    "href": "posts/Interaktionseffekt1/Interaktionseffekt1.html#answerlist-1",
    "title": "Interaktionseffekt1",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ninteraction\nregression"
  },
  {
    "objectID": "posts/euro-bayes/euro-bayes.html",
    "href": "posts/euro-bayes/euro-bayes.html",
    "title": "euro-bayes",
    "section": "",
    "text": "Exercise\nIn Information Theory, Inference, and Learning Algorithms, stellt David MacKay folgendes Problem.\n“A statistical statement appeared in The Guardian on Friday January 4, 2002:\nWhen spun on edge 250 times, a Belgian one-euro coin came up heads 140 times and tails 110. ‘It looks very suspicious to me,’ said Barry Blight, a statistics lecturer at the London School of Economics. ‘If the coin were unbiased, the chance of getting a result as extreme as that would be less than 7%.’\nBut do these data give evidence that the coin is biased rather than fair?”\nWie wahrscheinlich ist es, dass die Münze (exakt) fair ist, im Lichte dieser Daten?\nHinweise:\n\nUntersuchen Sie die Hypothesen \\(\\pi_0 = 0, \\pi_1 = 0.1, \\pi_2 = 0.2, ..., \\pi_{10} = 1\\) für die Trefferwahrscheinlichkeit (Kopf; heads).\nErstellen Sie ein Bayes-Gitter zur Lösung dieser Aufgabe.\nGehen Sie davon aus, dass Sie indifferent gegenüber der Hypothesen zu den Parameterwerten der Münze sind.\nGeben Sie Prozentzahlen immer als Anteil an und lassen Sie die führende Null weg (z.B. .42).\n\n         \n\n\nSolution\n\n\n\n\n\np_Gitter\nPriori\nLikelihood\nunstd_Post\nPost\n\n\n\n\n0.0\n1\n0.00\n0.00\n0.00\n\n\n0.1\n1\n0.00\n0.00\n0.00\n\n\n0.2\n1\n0.00\n0.00\n0.00\n\n\n0.3\n1\n0.00\n0.00\n0.00\n\n\n0.4\n1\n0.00\n0.00\n0.00\n\n\n0.5\n1\n0.01\n0.01\n0.27\n\n\n0.6\n1\n0.02\n0.02\n0.73\n\n\n0.7\n1\n0.00\n0.00\n0.00\n\n\n0.8\n1\n0.00\n0.00\n0.00\n\n\n0.9\n1\n0.00\n0.00\n0.00\n\n\n1.0\n1\n0.00\n0.00\n0.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDie Wahrscheinlichkeit \\(Pr(\\pi = 1/2 \\, | \\, X=140)\\) wenn \\(X \\sim Bin(250, 1/2)\\) beträgt ca. 27% oder .27.\nAllerdings würden viele Statistiker:innen nicht (nur) fragen, wie wahrscheinlich 140 Treffer sind. Stattdessen könnte man von folgender Überlegung ausgehen.\nZuerst: Welcher Wert wäre am wahrscheinlichsten, wenn die Münze fair wäre?\n\ndbinom(x = 0:250, size = 250, prob = 1/2) %&gt;% which.max()\n\n[1] 126\n\n\nDer 126. Wert in der Liste 0:250 ist der wahrscheinlichste (also 125 Treffer).\nWenn die Münze fair ist, dann wären doch 15 Treffer mehr als 125 genauso so unwahrscheinlich wie 15 Treffer weniger als 125 Treffer. Beide Ereignisse - 110 und 140 Treffer - sind ja gleich weit entfernt von denjenigen Wert, der am wahrscheinlichsten ist, wenn die Münze fair ist.\nEi typischi Statistiki würde also eher fragen: “Wie wahrscheinlich ist es, dass man ein Ergebnis erhält, dass mind. 15 Treffer entfernt ist von der Trefferzahl, die bei einer fairen Münze zu erwarten ist?”. Aber genug davon für diese Aufgabe :-)\n\nCategories:\n\nprobability\nbayes-grid"
  },
  {
    "objectID": "posts/Ziele-Statistik/Ziele-Statistik.html",
    "href": "posts/Ziele-Statistik/Ziele-Statistik.html",
    "title": "Ziele-Statistik",
    "section": "",
    "text": "Welche von den folgenden Optionen gehört nicht zu den Zielen von Statistik bzw. einer Forschungsfrage mit statistischem Character?\n\n\n\nverstehen\nerklären\nvorhersagen\nbeschreiben"
  },
  {
    "objectID": "posts/Ziele-Statistik/Ziele-Statistik.html#answerlist",
    "href": "posts/Ziele-Statistik/Ziele-Statistik.html#answerlist",
    "title": "Ziele-Statistik",
    "section": "",
    "text": "verstehen\nerklären\nvorhersagen\nbeschreiben"
  },
  {
    "objectID": "posts/Ziele-Statistik/Ziele-Statistik.html#answerlist-1",
    "href": "posts/Ziele-Statistik/Ziele-Statistik.html#answerlist-1",
    "title": "Ziele-Statistik",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nbasics\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/Pupil-size/Pupil-size.html",
    "href": "posts/Pupil-size/Pupil-size.html",
    "title": "Pupil-size",
    "section": "",
    "text": "Exercise\nPupillendaten sind ein verbreiteter Analysegegenstand in Bereichen wie Psychologie, Marktforschung und Marketing.\nBetrachten wir dazu ein R-Paket (zum Vorverbarbeitung, preprocessing) und einen Datensatz der Uni Münster.\n\nlibrary(PupilPre)\ndata(\"Pupildat\")\nd &lt;-\n  Pupildat %&gt;% \n  select(size = RIGHT_PUPIL_SIZE,\n         time = TIMESTAMP) %&gt;% \n  mutate(size = size / 100) # in millimeter\n\nMit dem R-Paket easystats kann man sich bequem typische Statistiken ausgeben lassen. Aber natürlich können Sie auch mit summarise(mw = mean(size)) arbeiten.\n\nlibrary(easystats)\nd %&gt;% \n  describe_distribution()\n\nVariable |     Mean |       SD |      IQR |                Range | Skewness | Kurtosis |     n | n_Missing\n----------------------------------------------------------------------------------------------------------\nsize     |    10.01 |     5.11 |     3.88 |        [1.04, 25.01] |     1.25 |     0.32 | 45343 |      1607\ntime     | 2.99e+06 | 9.87e+05 | 1.95e+06 | [1.44e+06, 4.06e+06] |    -0.41 |    -1.70 | 46950 |         0\n\n\nWir verzichten hier auf eine Aufbereitung der Daten (was eigentlich nötig wäre, aber nicht Gegenstand dieser Übung ist). Stattdessen konzentrieren wir uns auf die Posteriori-Verteilung zur Pupillengröße.\nWir sind also interessiert an einem Modell zur Schätzung der (Verteilung der) Pupillengröße; die Posteriori-Verteilung bildet das ab.\n\nFormulieren Sie ein passendes Modell.\nVerteidigen Sie Ihre Modellspezifikation.\nSimulieren Sie Daten aus der Priori-Verteilung. Kritisieren Sie die Wahl der Priori-Werte.\nBerechnen Sie die Posteriori-Verteilung mit den Pupillendaten d. Geben Sie zentrale Statistiken an.\nGeben Sie ein 95%-Intervall für die mittlere Pupillengröße an auf Basis der Posteriori-Verteilung.\n\nHinweise:\n\nSpezifizieren Sie eine Gleichverteilung von 0 bis 20 mm als Prior für die Streuung \\(\\sigma\\).\n\n         \n\n\nSolution\n\nModelldefinition\n\n\\[\\begin{aligned}\ns_i &\\sim \\mathcal{N}(\\mu, \\sigma)\\qquad \\text{| s wie size }\\\\\n\\mu &\\sim \\mathcal{N}(10, 5)\\\\\n\\sigma &\\sim \\mathcal{U}(0, 20)\n\\end{aligned}\\]\n\nBegründung der Modellspezifikation\n\n\\(s_i\\): Pupillengrößen sind normalverteilt, da viele Gene additiv auf die Größe hin zusammenwirken\n\\(\\mu\\): Da wir nicht viel wissen über die mittlere Pupillengröße, entscheiden wir uns für Normalverteilung für diesen Parameter, da dies keine weiteren Annahmen (außer dass Mittelwert und Streuung endlich sind) hinzufügt. Ein Modell mit wenig Annahmen nennt man “sparsam” oder konservativ. Es ist wünschenswert, dass Modelle mit so wenig wie möglich Annahmen auskommt (aber so vielen wie nötig).\n\\(\\sigma\\): Die Streuung muss positiv sein, daher kommt keine Normalverteilung in Frage. Eine Gleichverteilung ist eine von mehreren denkabaren Verteilungen. Besser wäre vermutlich eine Verteilung, die extrem große Werte als zunehmen unwahrscheinlich beurteilt. Aus Gründen der Einfachheit bleiben wir hier bei der Gleichverteilung.\nDie große Stichprobe wird den Priori-Wert vermutlich überstimmen.\n\nPriori-Prädiktiv-Verteilung\n\n\nn &lt;- 1e4\nsim_prior_pred &lt;-\n  tibble(\n    mu = rnorm(n, mean = 10, sd = 5),\n    sigma = runif(n, min = 0, max = 20),\n    size = rnorm(n, mu, sigma)\n  )\n\nsim_prior_pred %&gt;% \n  ggplot(aes(x = size)) +\n  geom_density()\n\n\n\n\n\n\n\n\nDa es viele negative Pupillengröße-Werte gibt, sieht man deutlich, dass das Modell nicht gut spezifiziert ist. So könnte kleinere Streuungswerte zu einem realistischeren Modell führen. Oder man verwendet Verteilungen, die rein positiv sind (hier nicht weiter ausgeführt).\n\nBerechnen Sie die Posteriori-Verteilung.\n\nDie Modelle wie stan_glm() tun sich leichter, wenn man nur die relevanten Daten, ohne fehlende Werte und schon schön fertig vorverarbeitet, zur Analyse in die Modellberechnung gibt:\n\nd3 &lt;-\n  d %&gt;% \n  select(size) %&gt;% \n  drop_na()\n\nDie Posteriori-Verteilung kann man mit dem Paket {rstanarm} d.h. mit der Funktion stan_glm() berechnen:\n\nlibrary(rstanarm)\nm_pupil &lt;- stan_glm(size ~ 1,\n                    data = d3,\n                    refresh = 0)\n\nDie Daten sind groß, es kann ein paar Sekunden brauchen…\nHier ist eine nützliche Zusammenfassung der Post-Verteilung.\n\nparameters(m_pupil)\n\nParameter   | Median |        95% CI |   pd |  Rhat |     ESS |                   Prior\n---------------------------------------------------------------------------------------\n(Intercept) |  10.01 | [9.96, 10.05] | 100% | 1.000 | 1730.00 | Normal (10.01 +- 12.77)\n\n\nHier eine Visualisierung der Parameter:\n\nplot(parameters(m_pupil), show_intercept = TRUE)\n\n\n\n\n\n\n\n\nNatürlich kann man auch die Post-Verteilung plotten:\n\nm_hdi &lt;- hdi(m_pupil, ci = c(0.5, 0.95))\n\nplot(m_hdi, show_intercept = TRUE)\n\n\n\n\n\n\n\n\nHier zur Info die ersten paar Zeilen des Post-Verteilung:\n\n\n\n\n\n\n\n\n\n(Intercept)\nsigma\n\n\n\n\n10.01\n5.13\n\n\n10.03\n5.14\n\n\n10.02\n5.13\n\n\n9.99\n5.12\n\n\n10.05\n5.10\n\n\n\n\n\n\n\n\nGeben Sie ein 95%-Intervall für die mittlere Pupillengröße an auf Basis der Posteriori-Verteilung.\n\n\neti(m_pupil)\n\nEqual-Tailed Interval\n\nParameter   |       95% ETI | Effects |   Component\n---------------------------------------------------\n(Intercept) | [9.96, 10.05] |   fixed | conditional\n\n\nUnd dann erstellen wir ein 89%-PI:\n\neti(m_pupil, ci = .89)\n\nEqual-Tailed Interval\n\nParameter   |       89% ETI | Effects |   Component\n---------------------------------------------------\n(Intercept) | [9.97, 10.04] |   fixed | conditional\n\n\n\nCategories:\n\nprobability\nbayes\nregression"
  },
  {
    "objectID": "posts/Logikpruefung1/Logikpruefung1.html",
    "href": "posts/Logikpruefung1/Logikpruefung1.html",
    "title": "Logikpruefung1",
    "section": "",
    "text": "Aufgabe\nWir definieren x wie folgt:\n\nx &lt;- 42\n\nGeben Sie die Syntax an, für die Prüfung, ob x kleiner 100 und größer 0 ist.\nGeben Sie keine Leerzeichen in Ihre Lösung ein.\n         \n\n\nLösung\n\nx&lt;100&x&gt;0\n\n[1] TRUE\n\n\nMit Leerzeichen sieht es aber schöner aus:\n\nx &lt; 100 & x &gt; 0\n\n[1] TRUE\n\n\n\nCategories:\n\nR\n‘2023’\nLogikpruefung1"
  },
  {
    "objectID": "posts/kausal06/kausal06.html",
    "href": "posts/kausal06/kausal06.html",
    "title": "kausal06",
    "section": "",
    "text": "Im Rahmen einer Studie soll untersucht werden, ob eine Influenza-Infektion einen (kausalen) Einfluss auf eine Covid19-Infektion hat. Außerdem wird dabei der Nutzen des Medikaments Acetaminophen untersucht.\nIn Wahrheit (aber unbekannt) sei der DAG wie folgt (s.u.).\n\n\n\n\n\n\n\n\n\nIst es sinnvoll, die Einnahme von Fiebersenker (Acetaminophen) zu kontrollieren?\n\n\n\nNein, es ist nicht sinnvoll, da durch eine Kontrolle von Acetaminophen eine Verzerrung erzeugt wird (Kollision)\nJa, nur so ist ein kausaler Effekt identifizierbar\nJa, es ist nicht nötig, aber wird zu exakteren Ergebnissen führen\nNein, es ist nicht sinnvoll,da durch eine Kontrolle von Acetaminophen eine Verzerrung erzeugt wird (Konfundierung)\nNein, es ist nicht sinnvoll, da es nicht nötig ist (aber auch nicht schädlich)"
  },
  {
    "objectID": "posts/kausal06/kausal06.html#answerlist",
    "href": "posts/kausal06/kausal06.html#answerlist",
    "title": "kausal06",
    "section": "",
    "text": "Nein, es ist nicht sinnvoll, da durch eine Kontrolle von Acetaminophen eine Verzerrung erzeugt wird (Kollision)\nJa, nur so ist ein kausaler Effekt identifizierbar\nJa, es ist nicht nötig, aber wird zu exakteren Ergebnissen führen\nNein, es ist nicht sinnvoll,da durch eine Kontrolle von Acetaminophen eine Verzerrung erzeugt wird (Konfundierung)\nNein, es ist nicht sinnvoll, da es nicht nötig ist (aber auch nicht schädlich)"
  },
  {
    "objectID": "posts/kausal06/kausal06.html#answerlist-1",
    "href": "posts/kausal06/kausal06.html#answerlist-1",
    "title": "kausal06",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/import-mtcars/import-mtcars.html",
    "href": "posts/import-mtcars/import-mtcars.html",
    "title": "import-mtcars",
    "section": "",
    "text": "Aufgabe\nFinden Sie den Datensatz “mtcars” online! “mtcars.csv” Tipp: vincentarelbundock- das ist ein guter Ort. Importieren Sie dann den Datensatz in R.\nSagen Sie mir den Namen der letzten Spalte und dort den 1. Wert!\n         \n\n\nLösung\nAntwort: Die letzte Spalte heißt carb und der 1. Wert ist 4.\nAnstelle von data_read aus easystats könnte man auch read.csv verwenden, das ist ein “eingebauter” Befehl in R, für den man kein Paket gestartet haben muss.\n\nCategories:\n\nR\ndata\nnum"
  },
  {
    "objectID": "posts/Regr-Bayes-interpret03/Regr-Bayes-interpret03.html",
    "href": "posts/Regr-Bayes-interpret03/Regr-Bayes-interpret03.html",
    "title": "Regr-Bayes-interpret03",
    "section": "",
    "text": "Exercise\nBerechnen Sie das Modell und interpretieren Sie die Ausgabe des folgenden Regressionsmodells. Geben Sie für jeden Regressionskoeffizienten an, wie sein Wert zu verstehen ist!\nmpg_z ~ hp_z + am + hp_z:am\nHinweise:\n\nFixieren Sie die Zufallszahlen.\nVerwenden Sie Stan zur Berechnung.\nRunden Sie auf 2 Dezimalstellen.\nDas Suffix _z steht für z-standardisierte Variablen.\n\n         \n\n\nSolution\n\nlibrary(tidyverse)  # Datenjudo\nlibrary(rstanarm)  # Stan, komm her\nlibrary(easystats)  # Komfort\n\ndata(mtcars)\n\nZuerst standardisieren wir die Daten:\n\nmtcars2 &lt;-\n  mtcars %&gt;% \n  standardize(append = TRUE)\n\nmtcars2  %&gt;% \n  describe_distribution()\n\nVariable |      Mean |     SD |    IQR |           Range | Skewness | Kurtosis |  n | n_Missing\n-----------------------------------------------------------------------------------------------\nmpg      |     20.09 |   6.03 |   7.53 |  [10.40, 33.90] |     0.67 |    -0.02 | 32 |         0\ncyl      |      6.19 |   1.79 |   4.00 |    [4.00, 8.00] |    -0.19 |    -1.76 | 32 |         0\ndisp     |    230.72 | 123.94 | 221.53 | [71.10, 472.00] |     0.42 |    -1.07 | 32 |         0\nhp       |    146.69 |  68.56 |  84.50 | [52.00, 335.00] |     0.80 |     0.28 | 32 |         0\ndrat     |      3.60 |   0.53 |   0.84 |    [2.76, 4.93] |     0.29 |    -0.45 | 32 |         0\nwt       |      3.22 |   0.98 |   1.19 |    [1.51, 5.42] |     0.47 |     0.42 | 32 |         0\nqsec     |     17.85 |   1.79 |   2.02 |  [14.50, 22.90] |     0.41 |     0.86 | 32 |         0\nvs       |      0.44 |   0.50 |   1.00 |    [0.00, 1.00] |     0.26 |    -2.06 | 32 |         0\nam       |      0.41 |   0.50 |   1.00 |    [0.00, 1.00] |     0.40 |    -1.97 | 32 |         0\ngear     |      3.69 |   0.74 |   1.00 |    [3.00, 5.00] |     0.58 |    -0.90 | 32 |         0\ncarb     |      2.81 |   1.62 |   2.00 |    [1.00, 8.00] |     1.16 |     2.02 | 32 |         0\nmpg_z    |  7.11e-17 |   1.00 |   1.25 |   [-1.61, 2.29] |     0.67 |    -0.02 | 32 |         0\ncyl_z    | -1.47e-17 |   1.00 |   2.24 |   [-1.22, 1.01] |    -0.19 |    -1.76 | 32 |         0\ndisp_z   | -9.08e-17 |   1.00 |   1.79 |   [-1.29, 1.95] |     0.42 |    -1.07 | 32 |         0\nhp_z     |  1.04e-17 |   1.00 |   1.23 |   [-1.38, 2.75] |     0.80 |     0.28 | 32 |         0\ndrat_z   | -2.92e-16 |   1.00 |   1.57 |   [-1.56, 2.49] |     0.29 |    -0.45 | 32 |         0\nwt_z     |  4.68e-17 |   1.00 |   1.21 |   [-1.74, 2.26] |     0.47 |     0.42 | 32 |         0\nqsec_z   |  5.30e-16 |   1.00 |   1.13 |   [-1.87, 2.83] |     0.41 |     0.86 | 32 |         0\nvs_z     |  6.94e-18 |   1.00 |   1.98 |   [-0.87, 1.12] |     0.26 |    -2.06 | 32 |         0\nam_z     |  4.51e-17 |   1.00 |   2.00 |   [-0.81, 1.19] |     0.40 |    -1.97 | 32 |         0\ngear_z   | -3.47e-18 |   1.00 |   1.36 |   [-0.93, 1.78] |     0.58 |    -0.90 | 32 |         0\ncarb_z   |  3.17e-17 |   1.00 |   1.24 |   [-1.12, 3.21] |     1.16 |     2.02 | 32 |         0\n\n\n\nm1 &lt;- \n  stan_glm(mpg_z ~ hp_z + am + hp_z:am, \n           seed = 42,\n           refresh = 0,\n           data = mtcars2)\n\ncoef(m1)\n\n (Intercept)         hp_z           am      hp_z:am \n-0.357413145 -0.677859338  0.876342434  0.005465839 \n\n\n\nIntercept: Ein Auto mit 0 PS und Automatikantrieb (am=0, s. Hilfe zum Datensatz: help(mtcars)) kann laut Modell mit einer Gallone Sprit ca. -0.36 Meilen fahren. Dieser Wert ist ca. Null, da die AV z-standardisiert ist. Ein Wert von Null in einer z-standardisierten Variablen entspricht dem Mittelwert in den Rohwerten.\nhp: Pro zusätzlichem PS kann ein Auto mit Automatikantrieb pro Gallone Sprit ca. -0.68 Meilen weniger weit fahren.\nam: Ein Auto mit 0 PS und Schaltgetriebe (am=1) kommt pro Gallone Sprit ca. 0.88 Meilen weiter als ein Auto mit Automatikantrieb.\nhp:am: Der Interaktionseffekt ist praktisch Null (-0.36): Der Zusammenhang von PS-Zahl und Spritverbrauch unterscheidet sich nicht (wesentlich) zwischen Autos mit bzw. ohne Automatikantrieb.\n\n\nCategories:\n\nbayes\nregression"
  },
  {
    "objectID": "posts/kausal01/kausal01.html",
    "href": "posts/kausal01/kausal01.html",
    "title": "kausal01",
    "section": "",
    "text": "Gegeben sei der DAG g (s.u.). Welche Variable/n sind zu kontrollieren, um den kausalen Effekt von x auf y zu identifizieren?\n\n\n\n\n\n\n\n\n\nGegeben sei der DAG g (s.o.). Dabei ist zu beachten, dass die gebogene Kurve (keine Gerade) mit zwei Pfeilspitzen keinen Kausaleffekt beschreibt, sondern eine Assoziation. Die dahinterstehende kausale Struktur ist eine Konfundierung. Daher ist der “Doppelpfeil” als Abkürzung für eine Konfundierung zu verstehen.\nWelche Variable/n sind zu kontrollieren, um den kausalen Effekt von x auf y zu identifizieren?\n\n\n\nkeine, bereits identifiziert\nx\ny\nm\nkeine, nicht identifizierbar"
  },
  {
    "objectID": "posts/kausal01/kausal01.html#answerlist",
    "href": "posts/kausal01/kausal01.html#answerlist",
    "title": "kausal01",
    "section": "",
    "text": "keine, bereits identifiziert\nx\ny\nm\nkeine, nicht identifizierbar"
  },
  {
    "objectID": "posts/kausal01/kausal01.html#answerlist-1",
    "href": "posts/kausal01/kausal01.html#answerlist-1",
    "title": "kausal01",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal\nexam-22"
  },
  {
    "objectID": "posts/rope-regr/rope-regr.html",
    "href": "posts/rope-regr/rope-regr.html",
    "title": "rope-regr",
    "section": "",
    "text": "Exercise\nJohn Kruschke hat einen (Absolut-)Wert vorschlagen, als Grenze für Regressionskoeffizienten “vernachlässigbarer” Größe.\nNennen Sie diesen Wert!\nHinweise:\n\nGeben Sie nur Zahlen ein (und ggf. Dezimaltrennzeichen).\nFührende Nullen dürfen auch bei Zahlen kleiner als 1 nicht weggelassen werden.\n\n         \n\n\nSolution\n0.05\n\nCategories:\n\nbayes\nregression\nrope"
  },
  {
    "objectID": "posts/kekse01/kekse01.html",
    "href": "posts/kekse01/kekse01.html",
    "title": "kekse01",
    "section": "",
    "text": "Exercise\nIn Think Bayes stellt Allen Downey folgende Aufgabe:\n“Suppose there are two bowls of cookies.\nBowl 1 contains 30 vanilla cookies and 10 chocolate cookies.\nBowl 2 contains 20 vanilla cookies and 20 chocolate cookies.\nNow suppose you choose one of the bowls at random and, without looking, choose a cookie at random. If the cookie is vanilla, what is the probability that it came from Bowl 1?”\nHinweise:\n\nUntersuchen Sie die Hypothesen \\(\\pi_0 = 0, \\pi_1 = 0.1, \\pi_2 = 0.2, ..., \\pi_{10} = 1\\) für die Trefferwahrscheinlichkeit\nErstellen Sie ein Bayes-Gitter zur Lösung dieser Aufgabe.\nGehen Sie davon aus, dass Sie (apriori) indifferent gegenüber der Hypothesen zu den Parameterwerten sind.\nGeben Sie Prozentzahlen immer als Anteil an und lassen Sie die führende Null weg (z.B. .42).\n\n         \n\n\nSolution\n\n\n\n\n\np_Gitter\nPriori\nLikelihood\nunstd_Post\nPost\n\n\n\n\n1\n1\n0.75\n0.75\n0.6\n\n\n2\n1\n0.50\n0.50\n0.4\n\n\n\n\n\nDie Antwort lautet: .6\n\nCategories:\n\nprobability\nbayes-grid"
  },
  {
    "objectID": "posts/interpret-koeff/interpret-koeff.html",
    "href": "posts/interpret-koeff/interpret-koeff.html",
    "title": "interpret-koeff",
    "section": "",
    "text": "Exercise\nBetrachten Sie dieses Modell, das den Zusammenhang von PS-Zahl und Spritverbrauch untersucht (Datensatz mtcars):\n\ndata(mtcars)\nlibrary(rstanarm)\nlibrary(easystats)\nlm1 &lt;- stan_glm(mpg ~ hp, data = mtcars,\n                refresh = 0)\nparameters(lm1)\n\nParameter   | Median |         95% CI |   pd | % in ROPE |  Rhat |     ESS |                   Prior\n----------------------------------------------------------------------------------------------------\n(Intercept) |  30.11 | [26.80, 33.35] | 100% |        0% | 1.000 | 3623.00 | Normal (20.09 +- 15.07)\nhp          |  -0.07 | [-0.09, -0.05] | 100% |      100% | 1.000 | 3550.00 |   Normal (0.00 +- 0.22)\n\n\n\nWas bedeuten die Koeffizienten?\nWie ist der Effekt von \\(\\beta_1\\) zu interpretieren?\n\n         \n\n\nSolution\n\nIntercept (\\(\\beta_0\\)): Der Achsenabschnitt gibt den geschätzten mittleren Y-Wert (Spritverbrauch) an, wenn \\(x=0\\), also für ein Auto mit 0 PS (was nicht wirklich Sinn macht). hp (\\(\\beta_1\\)) ist der Regressionskoeffizient oder Regressionsgewicht und damit die Steigung der Regressionsgeraden.\nhp (\\(\\beta_1\\)) ist der Regressionskoeffizient oder Regressionsgewicht und gibt den statistischen “Effekt” der PS-Zahl auf den Spritverbrauch an. Vorsicht: Dieser “Effekt” darf nicht vorschnell als kausaler Effekt verstanden werden. Daher muss man vorsichtig sein, wenn man von einem “Effekt” spricht. Vorsichtiger wäre zu sagen: “Ein Auto mit einem PS mehr, kommt im Mittel 0,1 Meilen weniger weit mit einer Gallone Sprit, laut diesem Modell”.\n\n\nCategories:\n\nregression\nlm\nbayes\nstats-nutshell"
  },
  {
    "objectID": "posts/kausal08/kausal08.html",
    "href": "posts/kausal08/kausal08.html",
    "title": "kausal08",
    "section": "",
    "text": "Gegeben sei die Theorie (oder schlichter: das Modell), demzufolge eine Anlage zu Suchtverhalten die Ursache von sowohl Rauchen als auch Kaffeegewohnheit darstellt. Lungenkrebs wiederum hat als (alleinige) Ursache Rauchen (laut diesem Modell).\nDaten zeigen, dass Kaffeegenuss und Lungenkrebs assoziiert sind: Bei Kaffeetrinkern ist die Lungenkrebsrate höher als bei Nichttrinkern (von Kaffee). Ob Kaffeegebrauch Lungenkrebs erzeugt?\nEine alternative Erklärung bietet folgender DAG.\n\n\n\n\n\n\n\n\n\nWelche Variablenmenge muss mindestens kontrolliert werden, um Konfundierung auszuschließen und damit den kausalen Effekt von Kaffee auf Lungenkrebs zu identifizieren?\n\n\n\n{Addictive Behavior oder aber Rauchen}\n{Rauchen}\n{Addictive Behavior}\n{Addictive Behavior und Rauchen}\n{Addictive Behavior und Lungenkrebs}"
  },
  {
    "objectID": "posts/kausal08/kausal08.html#answerlist",
    "href": "posts/kausal08/kausal08.html#answerlist",
    "title": "kausal08",
    "section": "",
    "text": "{Addictive Behavior oder aber Rauchen}\n{Rauchen}\n{Addictive Behavior}\n{Addictive Behavior und Rauchen}\n{Addictive Behavior und Lungenkrebs}"
  },
  {
    "objectID": "posts/kausal08/kausal08.html#answerlist-1",
    "href": "posts/kausal08/kausal08.html#answerlist-1",
    "title": "kausal08",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/Bayesmod-bestimmen02/Bayesmod-bestimmen02.html",
    "href": "posts/Bayesmod-bestimmen02/Bayesmod-bestimmen02.html",
    "title": "Bayesmod-bestimmen02",
    "section": "",
    "text": "Exercise\nSie möchten, im Rahmen einer Studie, ein einfaches lineare Modell spezifizieren, d.h. den Likelihood und die Priori-Verteilungen benennen.\nFolgende Informationen sind gegeben:\n\nAV: einnahmen\nUV: werbebudget\nAlle empirischen Variablen sind z-standardisiert.\nAlle Variablen sollen als normalverteilt angegeben werden mit Ausnahme der Streuung der AV, diese ist exponenzialverteilt mit Rate 1 zu modellieren.\nStreuungen der Normalverteilung sind mit 2.5 SD anzugeben.\n\nSchreiben Sie in mathematischer Notation folgende Notation auf:\nPriori-Verteilung der Streuung der AV\nHinweise:\n\nVerzichten Sie auf Leerstellen in Ihrer Antwort. \nBenennen Sie \\(\\beta\\) mit b, \\(\\alpha\\) mit a und \\(\\sigma\\) mit s.\nNutzen Sie die Tilde ~ um stochastische Relationen (Verteilungen) anzuzeigen.\nGeben Sie Normalverteilungen als Normal(x;y) und Exponentialverteilung als Exp(x) an (jeweils mit den korrekten Argumenten in der allgemein üblichen Form).\n\n         \n\n\nSolution\ns~Exp(1)\n\nCategories:\n\nregression\nbayes\nprior"
  },
  {
    "objectID": "posts/stan_glm01/stan_glm01.html",
    "href": "posts/stan_glm01/stan_glm01.html",
    "title": "stan_glm01",
    "section": "",
    "text": "Exercise\nGegeben dem folgenden Modell, geben Sie den Befehl mit stan_glm() an, um die Posteriori-Verteilung zu berechnen.\nLikelihood: \\(h_i \\sim \\mathcal{N}(\\mu, \\sigma)\\)\nPrior für \\(\\mu\\): \\(\\mu \\sim \\mathcal{N}(0, 1)\\)\nPrior für \\(\\sigma\\): \\(\\sigma \\sim \\mathcal{U}(0, 10)\\)\n         \n\n\nSolution\n\nlibrary(rstanarm)\n\n\nmodel &lt;-\n  stan_glm(h ~ 1,\n           prior_intercept = normal(0,1),\n           prior_aux = exponential(0.1),\n           daten = meine_Daten\n  )\n\n\nCategories:\n\nprobability\nbayes"
  },
  {
    "objectID": "posts/ungewiss-arten-regr/ungewiss-arten-regr.html",
    "href": "posts/ungewiss-arten-regr/ungewiss-arten-regr.html",
    "title": "ungewiss-arten-regr",
    "section": "",
    "text": "Exercise\nEine statistische Analyse, wie eine Regression, ist mit mehreren Arten an Ungewissheit konfrontiert. Zum einen gibt es die Ungewissheit in den Modellparametern. Für die Regression bedeutet das: “Liegt die Regressionsgerade in”Wahrheit” (in der Population) genauso wie in der Stichprobe, sind Achsenabschnitt und Steigung in der Stichprobe also identisch zur Popuation?“. Zum anderen die Ungewissheit innerhalb des Modells. Auch wenn wir die”wahre” Regressionsgleichung kennen würden, wären (in aller Regel) die Vorhersagen trotzdem nicht perfekt. Auch wenn wir etwa wüssten, wieviel Klausurpunkte “in Wahrheit” pro Stunde Lernen herausspringen (und wenn wir den wahren Achsenabschnitt kennen würden), so würde das Modell trotzdem keine perfekten Vorhersagen zum Klausurerfolg liefern. Vermutlich fehlen dem Modell wichtige Informationen etwa zur Motivation der Studentis.\nVor diesem Hintergrund, betrachten Sie folgendes statistisches Modell, das mit den Methoden der Bayes-Statistik berechnet wurde. Dazu wurde die Funktion stan_glm() verwendet, die ähnlich zu lm() ein lineare Modell berechnet. Ein wichtiger Unterschied zu lm() ist, dass Ungewissheiten zu den Parameterschätzungen ausgegeben werden.\n\ndata(mtcars) \nlibrary(rstanarm) \nlibrary(easystats)\nlm1 &lt;- stan_glm(mpg ~ hp, data = mtcars,\n                refresh = 0)  # um nicht zu viel R-Ausgabe zu erhalten\n\nparameters(lm1)\n\nParameter   | Median |         95% CI |   pd | % in ROPE |  Rhat |     ESS |                   Prior\n----------------------------------------------------------------------------------------------------\n(Intercept) |  30.05 | [26.63, 33.67] | 100% |        0% | 1.001 | 3745.00 | Normal (20.09 +- 15.07)\nhp          |  -0.07 | [-0.09, -0.05] | 100% |      100% | 1.000 | 3733.00 |   Normal (0.00 +- 0.22)\n\n\nFür den Prädiktor hp ist das Regressionsgewicht (Punktschätzer) angegeben unter der Spalte Median. Dieser Wert entspricht der Punktschätzung in der Population und ist identisch zum Regressionsgewicht (“b”) der Stichprobe.\nDie Spalte 95% CI gibt das 95%-Konfidenzintervall (CI wie confidence interval) zur Schätzung der Ungewissheit der Koeffizienten (der entsprechenden Zeile) wieder.\n\nWie breit ist das Intervall, in dem mit 95% Gewissheit der Achsenabschnitt liegt (laut diesem Model)?\nWie breit ist das Intervall, in dem mit 95% Gewissheit das Regressionsgewicht liegt (laut diesem Model)?\n\nHinweise:\n\nRunden Sie auf zwei Dezimalstellen.\nIgnorieren Sie die Spalte zu ROPE, pd, Prior und Rhat! Goldene Regel der Statistik: Wenn du eine Information nicht brauchst, dann ignoriere sie erstmal ;-)\n\n         \n\n\nSolution\n\n7.04\n0.04\n\n\nCategories:\n\nqm2\ninference\nlm"
  },
  {
    "objectID": "posts/variation02/variation02.html",
    "href": "posts/variation02/variation02.html",
    "title": "variation02",
    "section": "",
    "text": "In welchem Datensatz gibt es mehr Variation?\nDatensatz A:\n\n\n\n\n\n\n\n\n\nDatensatz B:\n\n\n\n\n\n\n\n\n\nDatensatz C:\n\n\n\n\n\n\n\n\n\nDatensatz D:\n\n\n\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD"
  },
  {
    "objectID": "posts/variation02/variation02.html#answerlist",
    "href": "posts/variation02/variation02.html#answerlist",
    "title": "variation02",
    "section": "",
    "text": "A\nB\nC\nD"
  },
  {
    "objectID": "posts/variation02/variation02.html#answerlist-1",
    "href": "posts/variation02/variation02.html#answerlist-1",
    "title": "variation02",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nWahr\n\n\nCategories:\n\nvariation\nbasics\nschoice"
  },
  {
    "objectID": "posts/bike01/bike01.html",
    "href": "posts/bike01/bike01.html",
    "title": "bike01",
    "section": "",
    "text": "Kann man die Anzahl gerade verliehener Fahrräder eines entsprechenden Anbieters anhand der Temperatur vorhersagen?\nIn dieser Übung untersuchen wir diese Frage.\nSie können die Daten von der Webseite der UCI herunterladen.\nWir beziehen uns auf den Datensatz day.\nBerechnen Sie ein lineares Modell mit der Anzahl der aktuell vermieteten Räder als AV und der aktuellen Temperatur als UV!\nGeben Sie den MSE an!\nHinweise"
  },
  {
    "objectID": "posts/bike01/bike01.html#data-split",
    "href": "posts/bike01/bike01.html#data-split",
    "title": "bike01",
    "section": "Data split",
    "text": "Data split\n\nset.seed(42)\nsplit_vec &lt;- initial_split(d, strata = cnt)\n\nd_train &lt;- training(split_vec)\nd_test &lt;- testing(split_vec)"
  },
  {
    "objectID": "posts/bike01/bike01.html#define-recipe",
    "href": "posts/bike01/bike01.html#define-recipe",
    "title": "bike01",
    "section": "Define recipe",
    "text": "Define recipe\n\nrec1 &lt;- \n  recipe(cnt ~ temp, data = d)"
  },
  {
    "objectID": "posts/bike01/bike01.html#define-model",
    "href": "posts/bike01/bike01.html#define-model",
    "title": "bike01",
    "section": "Define model",
    "text": "Define model\n\nm1 &lt;-\n  linear_reg()"
  },
  {
    "objectID": "posts/bike01/bike01.html#workflow",
    "href": "posts/bike01/bike01.html#workflow",
    "title": "bike01",
    "section": "Workflow",
    "text": "Workflow\n\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(m1) %&gt;% \n  add_recipe(rec1)"
  },
  {
    "objectID": "posts/bike01/bike01.html#fit",
    "href": "posts/bike01/bike01.html#fit",
    "title": "bike01",
    "section": "Fit",
    "text": "Fit\n\nfit1 &lt;- last_fit(wf1, split_vec)\nfit1\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits            id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;            &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [547/184]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;"
  },
  {
    "objectID": "posts/bike01/bike01.html#model-performance-metrics-in-test-set",
    "href": "posts/bike01/bike01.html#model-performance-metrics-in-test-set",
    "title": "bike01",
    "section": "Model performance (metrics) in test set",
    "text": "Model performance (metrics) in test set\n\nfit1 %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    1509.    Preprocessor1_Model1\n2 rsq     standard       0.411 Preprocessor1_Model1\n\n\n\nMSE &lt;- fit1 %&gt;% collect_metrics() %&gt;% pluck(3, 1)\nMSE\n\n[1] 1509.477\n\n\nSolution: 1509.4768321\n\nCategories:\n\nstat-learning\ntidymodels\nnum"
  },
  {
    "objectID": "posts/vorhersageintervall1/vorhersageintervall1.html",
    "href": "posts/vorhersageintervall1/vorhersageintervall1.html",
    "title": "vorhersageintervall1",
    "section": "",
    "text": "Vorhersagen, etwa in einem Regressionsmodell, sind mit mehreren Arten von Unsicherheit konfrontiert.\nBerechnen Sie dazu ein Regressionsmodell, Datensatz mtcars, mit hp als Prädiktor (UV) und mpg als AV (Kriterium)!\nDann sagen Sie bitte den Wert der AV für eine Beobachtungseinheit mit mittlerer Ausprägung im Präktor vorher:\nEinmal nur unter Berücksichtigung der Unsicherheit innerhalb des Modells (“Konfidenzintervall”); einmal unter Berücksichtigung der Unsicherheit innerhalb des Modells sowie die Unsicherheit durch die Koffizienten (“Vohersageintervall”).\nHinweise:\n\npredict() ist eine Funktion, die Sie zur Vorhersage von Regressionsmodellen verwenden können.\nVerwenden Sie lm() zur Berechnung eines Regressionsmodells.\nDas Argument type von predict() erlaubt Ihnen die Wahl der Art der Vorhersage, betrachten Sie Hilfe der Funktion z.B. hier.\n\nBei welchem Intervall ist die Ungewissheit in der Vorhersage größer?\n\n\n\nKonfidenzintervall\nVohersageintervall\nGleich groß\nKommt auf weitere Faktoren an, keine pauschale Antwort möglich"
  },
  {
    "objectID": "posts/vorhersageintervall1/vorhersageintervall1.html#answerlist",
    "href": "posts/vorhersageintervall1/vorhersageintervall1.html#answerlist",
    "title": "vorhersageintervall1",
    "section": "",
    "text": "Konfidenzintervall\nVohersageintervall\nGleich groß\nKommt auf weitere Faktoren an, keine pauschale Antwort möglich"
  },
  {
    "objectID": "posts/vorhersageintervall1/vorhersageintervall1.html#answerlist-1",
    "href": "posts/vorhersageintervall1/vorhersageintervall1.html#answerlist-1",
    "title": "vorhersageintervall1",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\nFalsch\nFalsch\n\n\nCategories:\n\nlm\ninference\nqm2"
  },
  {
    "objectID": "posts/Wertzuweisen/Wertzuweisen.html",
    "href": "posts/Wertzuweisen/Wertzuweisen.html",
    "title": "Wertzuweisen",
    "section": "",
    "text": "Aufgabe\nWeisen Sie dem Objekt loesung den Wert 42 zu. Geben Sie den korrekten R-Code dafür ein.\nHinweis: Verzichten Sie jegliche Leerzeichen in Ihrer Eingabe, da sonst die Eingabe nicht als korrekt erkannt werden kann.\n         \n\n\nLösung\nloesung&lt;-42\n\nCategories:\n\nR\n‘2023’\nstring"
  },
  {
    "objectID": "posts/voll-normal/voll-normal.html",
    "href": "posts/voll-normal/voll-normal.html",
    "title": "voll-normal",
    "section": "",
    "text": "Exercise\nNehmen wir an, \\(k=10\\) voneinander unabhängige Eigenschaften \\(E_1, E_2, \\ldots, E_{10}\\) bestimmen, ob eine Person als “normal” angesehen wird. Jede dieser Eigenschaften kann entweder mit “normal” (n) oder aber “nichtnormal” (nn) ausgeprägt sein, wobei wir nicht genau vorhersagen können, wie diese Eigenschaften bei einer Person bestellt sein werden.\nAls Zufallsexperiment ausgedrückt: \\(\\Omega_E := \\{n, nn\\}\\) mit den zwei Ergebnissen \\(n\\) und \\(nn\\).\nMit der Wahrscheinlichkeit \\(Pr_{E_i} = 0.9\\) treffe das Ereignis \\(N_i := E_i = \\{n\\}\\) (für alle \\(i = 1, \\ldots, k\\)) zu.\nNehmen wir weiter an, als “voll normal” (\\(VN\\)) wird eine Person genau dann angesehen, wenn sie in allen \\(k\\) Eigenschaften “normal” ausgeprägt ist, das Ereignis \\(N\\) also für alle \\(k\\) Eigenschaften auftritt.\n\nNennen Sie Beispiele für mögliche Eigenschaften \\(E\\)!\nWie groß ist die Wahrscheinlichkeit - unter den hier geschilderten Annahmen -, dass eine Person “voll normal” ist?\nDiskutieren Sie die Plausibilität der Annahmen!\n\n         \n\n\nSolution\n\nIntelligenz, Aussehen, Gesundheit, Herkunft, Hautfarbe, sexuelle Identität oder Neigung, …\nFür unabhängige Ereignisse ist die Wahrscheinlichkeit, dass sie alle eintreten, gleich dem Produkt ihrer Einzelwahrscheinlichkeiten:\n\n\\(VN = Pr(E_i)^{10} = 0.9^{10} \\approx 0.3486784\\)\nDie Wahrscheinlichkeit, dass \\(VN\\) nicht eintritt (Nicht-Voll-Normal, NVN), ist dann die Gegenwahrscheinlichkeit: \\(NVN = 1- VN\\).\n\nMehrere der Annahmen sind diskutabel. So könnten die Eigenschaften nicht unabhängig sein, dann wäre der hier gezeigte Rechenweg nicht anwendbar. Die Wahrscheinlichkeit für “normal” könnte höher oder niedriger sein, wobei 90% nicht ganz unplausibel ist. Schließlich unterliegt das Ereignis \\(E_N\\) mit den Ergebnissen \\(n\\) bzw. \\(nn\\) sozialpsychologischen bzw. soziologischen Einflüssen und kann variieren.\n\n\nCategories:\n\nprobability\nmeta"
  },
  {
    "objectID": "posts/MWberechnen/MWberechnen.html",
    "href": "posts/MWberechnen/MWberechnen.html",
    "title": "MWberechnen",
    "section": "",
    "text": "Question\n\nAufgabe\nBerechnen Sie den Mittelwert folgender Zahlenreihe; ignorieren sie etwaige fehlende Werte. Runden Sie auf zwei Dezimalstellen.\n\n\n[1] -0.77 -0.74  1.94  1.02  0.91    NA\n\n\n         \n\n\nLösung\nDer Mittelwert liegt bei 0.47.\nDie Antwort lautet 0.47.\nIn R kann man den Mittelwert z.B. so berechnen:\n\nmean(zahlenreihe, na.rm = TRUE)\n\n[1] 0.472\n\n\nDas Argument na.rm = TRUE sorgt dafür, dass R auch bei Vorhandensein fehlender Werte ein Ergebnis ausgibt. Ohne dieses Argument würde R ein sprödes NA zurückgeben, falls fehlende Werte vorliegen. Dieses Verhalten von R ist recht defensiv, getreu dem Motto: Wenn es ein Problem gibt, sollte man so früh wie möglich darüber deutlich informiert werden (und nicht erst, wenn die Marsrakete gestartet ist…).\n\nCategories:\n\neda\ndatawrangling\nnum\ndyn"
  },
  {
    "objectID": "posts/kausal09/kausal09.html",
    "href": "posts/kausal09/kausal09.html",
    "title": "kausal09",
    "section": "",
    "text": "Ein Forschungsteam aus Epidemiologen untersucht den (möglicherweise kausalen) Zusammenhang von Erziehung (education) und Diabetes (diabetes). Das Team schlägt folgendes Modell zur Erklärung des Zusammenhangs vor (s. DAG).\n\n\n\n\n\n\n\n\n\nNochmal den gleich DAG ohne “Schilder”, damit man die Pfeilspitzen besser sieht:\n\n\n\n\n\n\n\n\n\nSollte die Krankengeschichte der Mutter hinsichtlich Diabetes kontrolliert werden, um den kausalen Effekt von Erziehung auf Diabetes zu identifizieren?\n\n\n\nNein, Mother's Diabetes sollte nicht kontrolliert werden, da so ein Collider Bias (Kollisionsverzerrung) resultiert.\nNein, Mother's Diabetes sollte nicht kontrolliert werden, da so eine Konfundierung resultiert.\nNein, Mother's Diabetes sollte nicht kontrolliert werden, da zwar keine Verzerrung entsteht, es aber auch nicht nötig ist.\nJa, Mother's Diabetes sollte kontrolliert werden, da so ein Collider Bias (Kollisionsverzerrung) vermieden wird.\nJa, Mother's Diabetes sollte kontrolliert werden, da so eine Konfundierung vermieden wird."
  },
  {
    "objectID": "posts/kausal09/kausal09.html#answerlist",
    "href": "posts/kausal09/kausal09.html#answerlist",
    "title": "kausal09",
    "section": "",
    "text": "Nein, Mother's Diabetes sollte nicht kontrolliert werden, da so ein Collider Bias (Kollisionsverzerrung) resultiert.\nNein, Mother's Diabetes sollte nicht kontrolliert werden, da so eine Konfundierung resultiert.\nNein, Mother's Diabetes sollte nicht kontrolliert werden, da zwar keine Verzerrung entsteht, es aber auch nicht nötig ist.\nJa, Mother's Diabetes sollte kontrolliert werden, da so ein Collider Bias (Kollisionsverzerrung) vermieden wird.\nJa, Mother's Diabetes sollte kontrolliert werden, da so eine Konfundierung vermieden wird."
  },
  {
    "objectID": "posts/kausal09/kausal09.html#answerlist-1",
    "href": "posts/kausal09/kausal09.html#answerlist-1",
    "title": "kausal09",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/mutate01/mutate01.html",
    "href": "posts/mutate01/mutate01.html",
    "title": "mutate01",
    "section": "",
    "text": "Aufgabe\nIm Datensatz mariokart:\nErzeugen Sie eine Spalte zu_teuer, die folgende Prüfung durchführt: total_pr &gt; 100.\n         \n\n\nLösung\nPakete starten:\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.6.0 (red = needs update)\n✔ bayestestR  0.13.0   ✔ correlation 0.8.3 \n✖ datawizard  0.6.5    ✔ effectsize  0.8.3 \n✖ insight     0.19.0   ✔ modelbased  0.8.6 \n✔ performance 0.10.2   ✔ parameters  0.20.2\n✖ report      0.5.6    ✖ see         0.7.4 \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`.\n\nlibrary(tidyverse)  # startet das Paket tidyverse\n\nDaten importieren:\n\nmariokart &lt;- data_read(\"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\")\n\n\nmariokart &lt;- \n  mutate(mariokart, zu_teuer = total_pr &gt; 100)\n\nmariokart2 &lt;-\n  select(mariokart, total_pr, zu_teuer)\n\nhead(mariokart2)\n\n  total_pr zu_teuer\n1    51.55    FALSE\n2    37.04    FALSE\n3    45.50    FALSE\n4    44.00    FALSE\n5    71.00    FALSE\n6    45.00    FALSE\n\n\n\nCategories:\n\ndatawrangling\neda\ntidyverse\ndplyr\nnum"
  },
  {
    "objectID": "posts/Diamonds-Histogramm-Vergleich/Diamonds-Histogramm-Vergleich.html",
    "href": "posts/Diamonds-Histogramm-Vergleich/Diamonds-Histogramm-Vergleich.html",
    "title": "Diamonds-Histogramm-Vergleich",
    "section": "",
    "text": "Die Daten beziehen sich auf den Datensatz diamonds und sind hier einzusehen bzw. können bei Interesse dort heruntergeladen werden.\n\n\n\n\n\n\n\n\n\n\n\n\nDer vertikale Strich passt nicht auf den Median.\nEs ist nicht sinnvoll, die Gesamtverteilung zusätzlich zur Verteilung pro Gruppe in jeder Facette darzustellen.\nDen globalen Median (für den gesamten Datensatz, also über alle Gruppen hinweg) in jeder Facette darzustellen, ist redundant. Daher ist es besser, in jeder Facetten den Median pro Gruppe darzustellen.\nDie Verwendung einer Füllfarbe (Diagramm B) ist hier nicht sinnvoll."
  },
  {
    "objectID": "posts/Diamonds-Histogramm-Vergleich/Diamonds-Histogramm-Vergleich.html#answerlist",
    "href": "posts/Diamonds-Histogramm-Vergleich/Diamonds-Histogramm-Vergleich.html#answerlist",
    "title": "Diamonds-Histogramm-Vergleich",
    "section": "",
    "text": "Der vertikale Strich passt nicht auf den Median.\nEs ist nicht sinnvoll, die Gesamtverteilung zusätzlich zur Verteilung pro Gruppe in jeder Facette darzustellen.\nDen globalen Median (für den gesamten Datensatz, also über alle Gruppen hinweg) in jeder Facette darzustellen, ist redundant. Daher ist es besser, in jeder Facetten den Median pro Gruppe darzustellen.\nDie Verwendung einer Füllfarbe (Diagramm B) ist hier nicht sinnvoll."
  },
  {
    "objectID": "posts/Diamonds-Histogramm-Vergleich/Diamonds-Histogramm-Vergleich.html#answerlist-1",
    "href": "posts/Diamonds-Histogramm-Vergleich/Diamonds-Histogramm-Vergleich.html#answerlist-1",
    "title": "Diamonds-Histogramm-Vergleich",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nWahr\nFalsch\n\n\nCategories:\n\nvis\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/max-corr2/max-corr2.html",
    "href": "posts/max-corr2/max-corr2.html",
    "title": "max-corr2",
    "section": "",
    "text": "Aufgabe\nWelches Diagramm zeigt den stärksten (absoluten) linearen negativen Zusammenhang (Korrelation)?\nGeben Sie die Nummer ein, die in der Kopfzeile jedes Teildiagramms angezeigt wird.\n         \n\n\nLösung\n\n\n\n\n\n\n\n\n\n\nCategories:\n\nvis\n‘2023’\nnum"
  },
  {
    "objectID": "posts/kausal07/kausal07.html",
    "href": "posts/kausal07/kausal07.html",
    "title": "kausal07",
    "section": "",
    "text": "Eine Forscherin untersucht den Zusammenhang von Rauchen smo (smoking, UV, exposure) und Herzstillstand ca (cardiac arrest, AV, outcome). Sie hegt die Hypothese, dass Rauchen einen Einfluss auf den Cholesterolspiegel cho (cholestorol) hat, was wiederum Herzstillstand auslösen könnte.\n\n\n\n\n\n\n\n\n\nHier sehen Sie die Definition des DAGs:\n\n\ndag {\nca [outcome]\ncho\nsmo [exposure]\nunh\nwei\ncho -&gt; ca\nsmo -&gt; cho\nunh -&gt; smo\nunh -&gt; wei\nwei -&gt; cho\n}\n\n\nDie Forscherin überlegt, Cholestorol zu kontrollieren. Ist diese Idee sinnvoll?\n\n\n\nNein, da die Assoziation zwischen UV und AV unterbrochen wird.\nJa, so wird der kausale Effekt identifiziert.\nJa, nur so wird der kausale Effekt identifiziert.\nEs schadet nicht, aber es ist auch nicht nötig.\nNein, da eine Kollision erzeugt wird."
  },
  {
    "objectID": "posts/kausal07/kausal07.html#answerlist",
    "href": "posts/kausal07/kausal07.html#answerlist",
    "title": "kausal07",
    "section": "",
    "text": "Nein, da die Assoziation zwischen UV und AV unterbrochen wird.\nJa, so wird der kausale Effekt identifiziert.\nJa, nur so wird der kausale Effekt identifiziert.\nEs schadet nicht, aber es ist auch nicht nötig.\nNein, da eine Kollision erzeugt wird."
  },
  {
    "objectID": "posts/kausal07/kausal07.html#answerlist-1",
    "href": "posts/kausal07/kausal07.html#answerlist-1",
    "title": "kausal07",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/stan_glm_parameterzahl/stan_glm_parameterzahl.html",
    "href": "posts/stan_glm_parameterzahl/stan_glm_parameterzahl.html",
    "title": "stan_glm_parameterzahl",
    "section": "",
    "text": "Exercise\nBerechnet man eine Posteriori-Verteilung mit stan_glm(), so kann man entweder die schwach informativen Prioriwerte der Standardeinstellung verwenden, oder selber Prioriwerte definieren.\nBetrachten Sie dazu dieses Modell:\nstan_glm(price ~ cut, data = diamonds, \n                   prior = normal(location = c(100, 100, 100, 100),\n                                  scale = c(100, 100, 100, 100)),\n                   prior_intercept = normal(3000, 500))\nWie viele Parameter gibt es in diesem Modell?\nHinweise:\n\nGeben Sie nur eine (ganze) Zahl ein.\n\n         \n\n\nSolution\nBerechnet man das Modell, so kann man sich auch Infos über die Prioris ausgeben lassen:\n\nm1 &lt;- stan_glm(price ~ cut, data = diamonds, \n               prior = normal(location = c(100, 100, 100, 100),\n                              scale = c(100, 100, 100, 100)),\n               prior_intercept = normal(3000, 500),\n               refresh = 0)\n\nprior_summary(m1)\n\nPriors for model 'm1' \n------\nIntercept (after predictors centered)\n ~ normal(location = 3000, scale = 500)\n\nCoefficients\n ~ normal(location = [100,100,100,...], scale = [100,100,100,...])\n\nAuxiliary (sigma)\n  Specified prior:\n    ~ exponential(rate = 1)\n  Adjusted prior:\n    ~ exponential(rate = 0.00025)\n------\nSee help('prior_summary.stanreg') for more details\n\n\nWie man sieht, wird für die Streuung im Standard eine Exponentialverteilung verwendet von stan_glm(). Gibt man also nicht an - wie im Beispiel m1 oben, so wird stan_glm() für die Streuung, d.h. prior_aux eine Exponentialverteilung verwenden. Zu beachten ist, dass stan_glm() ein automatische Skalierung vornimmt.\nS. hier für weitere Erläuterung.\nMöchte man den Prior für die Streuung direkt ansprechen, so kann man das so formulieren:\n\nm2 &lt;- stan_glm(price ~ cut, data = diamonds, \n               prior = normal(location = c(100, 100, 100, 100),\n                              scale = c(100, 100, 100, 100)),\n               prior_intercept = normal(3000, 500),\n               prior_aux = exponential(1),\n               refresh = 0)\n\nprior_summary(m1)\n\nPriors for model 'm1' \n------\nIntercept (after predictors centered)\n ~ normal(location = 3000, scale = 500)\n\nCoefficients\n ~ normal(location = [100,100,100,...], scale = [100,100,100,...])\n\nAuxiliary (sigma)\n  Specified prior:\n    ~ exponential(rate = 1)\n  Adjusted prior:\n    ~ exponential(rate = 0.00025)\n------\nSee help('prior_summary.stanreg') for more details\n\n\nZu beachten ist beim selber definieren der Prioris, dass dann keine Auto-Skalierung von stan_glm() vorgenommen wird, es sei denn, man weist es explizit an:\n\nm3 &lt;- stan_glm(price ~ cut, data = diamonds, \n               prior = normal(location = c(100, 100, 100, 100),\n                              scale = c(100, 100, 100, 100),\n                              autoscale = TRUE),\n               prior_intercept = normal(3000, 500, autoscale = TRUE),\n               prior_aux = exponential(1, autoscale = TRUE),\n               chain = 1,  # nur 1 mal Stichproben ziehen, um Zeit zu sparen\n               refresh = 0)\n\nprior_summary(m3)\n\nPriors for model 'm3' \n------\nIntercept (after predictors centered)\n  Specified prior:\n    ~ normal(location = 3000, scale = 500)\n  Adjusted prior:\n    ~ normal(location = 3000, scale = 2e+06)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = [100,100,100,...], scale = [100,100,100,...])\n  Adjusted prior:\n    ~ normal(location = [100,100,100,...], scale = [1129833.17, 868199.02, 936606.47,...])\n\nAuxiliary (sigma)\n  Specified prior:\n    ~ exponential(rate = 1)\n  Adjusted prior:\n    ~ exponential(rate = 0.00025)\n------\nSee help('prior_summary.stanreg') for more details\n\n\nGrundsätzlich ist es nützlich für die numerische Stabilität, dass die Zahlen (hier die Parameterwerte) etwa die gleiche Größenordnung haben, am besten um die 0-1 herum. Daher bietet sich oft eine z-Standardisierung an.\nUnabhängig von der der Art der Parameter ist die Anzahl immer gleich.\nDie Anzahl der geschätzten Parameter werden im Modell-Summary unter Estimates gezeigt:\n\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      price ~ cut\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 53940\n predictors:   5\n\nEstimates:\n              mean   sd     10%    50%    90% \n(Intercept) 4026.0   21.8 3998.0 4025.9 4053.7\ncut.L       -244.4   51.6 -310.9 -243.2 -178.6\ncut.Q       -283.2   46.4 -343.0 -283.2 -224.7\ncut.C       -576.9   44.5 -633.5 -576.2 -520.7\ncut^4       -263.9   38.7 -313.7 -263.3 -214.6\nsigma       3964.1   11.6 3949.2 3964.3 3979.0\n\nFit Diagnostics:\n           mean   sd     10%    50%    90% \nmean_PPD 3931.7   23.8 3900.9 3931.8 3961.8\n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.4  1.0  3185 \ncut.L         1.1  1.0  2254 \ncut.Q         0.9  1.0  2417 \ncut.C         0.9  1.0  2568 \ncut^4         0.7  1.0  2744 \nsigma         0.1  1.0  7543 \nmean_PPD      0.4  1.0  3766 \nlog-posterior 0.0  1.0  1554 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\nDas sind:\n\n1 Intercept (Achsenabschnitt) - prior_intercept\n4 Gruppen (zusätzlich zur Referenzgruppe, die mit dem Achsenabschnitt dargestellt ist) - prior_normal\n1 Sigma (Ungewissheit “innerhalb des Modells”) - prior_aux\n\nDie Anzahl der Parameter in diesem Modell ist also: 6\n\nCategories:\n~"
  },
  {
    "objectID": "posts/Regr-Bayes-interpret02/Regr-Bayes-interpret02.html",
    "href": "posts/Regr-Bayes-interpret02/Regr-Bayes-interpret02.html",
    "title": "Regr-Bayes-interpret02",
    "section": "",
    "text": "Exercise\nBerechnen Sie das Modell und interpretieren Sie die Ausgabe des folgenden Regressionsmodells. Geben Sie für jeden Regressionskoeffizienten an, wie sein Wert zu verstehen ist!\nmpg ~ hp_z + am + hp_z:am\nHinweise:\n\nFixieren Sie die Zufallszahlen.\nVerwenden Sie Stan zur Berechnung.\nRunden Sie auf 2 Dezimalstellen.\nDas Suffix _z steht für z-standardisierte Variablen.\n\n         \n\n\nSolution\n\nlibrary(tidyverse)  # Datenjudo\nlibrary(rstanarm)  # Stan, komm her\nlibrary(easystats)  # Komfort\n\ndata(mtcars)\n\nZuerst standardisieren wir die Daten:\n\nmtcars2 &lt;-\n  mtcars %&gt;% \n  standardize(append = TRUE)\n\nmtcars2  %&gt;% \n  describe_distribution()\n\nVariable |      Mean |     SD |    IQR |           Range | Skewness | Kurtosis |  n | n_Missing\n-----------------------------------------------------------------------------------------------\nmpg      |     20.09 |   6.03 |   7.53 |  [10.40, 33.90] |     0.67 |    -0.02 | 32 |         0\ncyl      |      6.19 |   1.79 |   4.00 |    [4.00, 8.00] |    -0.19 |    -1.76 | 32 |         0\ndisp     |    230.72 | 123.94 | 221.53 | [71.10, 472.00] |     0.42 |    -1.07 | 32 |         0\nhp       |    146.69 |  68.56 |  84.50 | [52.00, 335.00] |     0.80 |     0.28 | 32 |         0\ndrat     |      3.60 |   0.53 |   0.84 |    [2.76, 4.93] |     0.29 |    -0.45 | 32 |         0\nwt       |      3.22 |   0.98 |   1.19 |    [1.51, 5.42] |     0.47 |     0.42 | 32 |         0\nqsec     |     17.85 |   1.79 |   2.02 |  [14.50, 22.90] |     0.41 |     0.86 | 32 |         0\nvs       |      0.44 |   0.50 |   1.00 |    [0.00, 1.00] |     0.26 |    -2.06 | 32 |         0\nam       |      0.41 |   0.50 |   1.00 |    [0.00, 1.00] |     0.40 |    -1.97 | 32 |         0\ngear     |      3.69 |   0.74 |   1.00 |    [3.00, 5.00] |     0.58 |    -0.90 | 32 |         0\ncarb     |      2.81 |   1.62 |   2.00 |    [1.00, 8.00] |     1.16 |     2.02 | 32 |         0\nmpg_z    |  7.11e-17 |   1.00 |   1.25 |   [-1.61, 2.29] |     0.67 |    -0.02 | 32 |         0\ncyl_z    | -1.47e-17 |   1.00 |   2.24 |   [-1.22, 1.01] |    -0.19 |    -1.76 | 32 |         0\ndisp_z   | -9.08e-17 |   1.00 |   1.79 |   [-1.29, 1.95] |     0.42 |    -1.07 | 32 |         0\nhp_z     |  1.04e-17 |   1.00 |   1.23 |   [-1.38, 2.75] |     0.80 |     0.28 | 32 |         0\ndrat_z   | -2.92e-16 |   1.00 |   1.57 |   [-1.56, 2.49] |     0.29 |    -0.45 | 32 |         0\nwt_z     |  4.68e-17 |   1.00 |   1.21 |   [-1.74, 2.26] |     0.47 |     0.42 | 32 |         0\nqsec_z   |  5.30e-16 |   1.00 |   1.13 |   [-1.87, 2.83] |     0.41 |     0.86 | 32 |         0\nvs_z     |  6.94e-18 |   1.00 |   1.98 |   [-0.87, 1.12] |     0.26 |    -2.06 | 32 |         0\nam_z     |  4.51e-17 |   1.00 |   2.00 |   [-0.81, 1.19] |     0.40 |    -1.97 | 32 |         0\ngear_z   | -3.47e-18 |   1.00 |   1.36 |   [-0.93, 1.78] |     0.58 |    -0.90 | 32 |         0\ncarb_z   |  3.17e-17 |   1.00 |   1.24 |   [-1.12, 3.21] |     1.16 |     2.02 | 32 |         0\n\n\n\nm1 &lt;- \n  stan_glm(mpg ~ hp_z + am + hp_z:am, \n           seed = 42,\n           refresh = 0,\n           data = mtcars2)\n\ncoef(m1)\n\n(Intercept)        hp_z          am     hp_z:am \n17.95232456 -4.07311028  5.26504242  0.06037871 \n\n\n\nIntercept: Ein Auto mit 0 PS und Automatikantrieb (am=0, s. Hilfe zum Datensatz: help(mtcars)) kann laut Modell mit einer Gallone Sprit ca. 17.95 Meilen fahren.\nhp: Pro zusätzlichem PS kann ein Auto mit Automatikantrieb pro Gallone Sprit ca. -4.07 Meilen weniger weit fahren.\nam: Ein Auto mit 0 PS und Schaltgetriebe (am=1) kommt pro Gallone Sprit ca. 5.27 Meilen weiter als ein Auto mit Automatikantrieb.\nhp:am: Der Interaktionseffekt ist praktisch Null (17.95): Der Zusammenhang von PS-Zahl und Spritverbrauch unterscheidet sich nicht (wesentlich) zwischen Autos mit bzw. ohne Automatikantrieb.\n\n\nCategories:\n\nbayes\nregression"
  },
  {
    "objectID": "posts/iq07/iq07.html",
    "href": "posts/iq07/iq07.html",
    "title": "iq07",
    "section": "",
    "text": "Exercise\nIntelligenz wird häufig mittels einem IQ-Test ermittelt.\nIn einer Population gebe es zwei Subgruppen, für die gilt:\n\\(IQ_1 \\sim N(85, 15)\\) \\(IQ_2 \\sim N(115, 15)\\)\nWie groß ist die Wahrscheinlichkeit, dass eine zufällig gezogene Person einen IQ-Wert von mind. 115 Punkten hat?\nHinweise:\n\nNutzen Sie Simulationsmethoden.\nGehen Sie von folgender IQ-Verteilung aus: \\(IQ \\sim N(100,15)\\)\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\nSimulieren Sie \\(n=10^3\\) Stichproben pro Subpopulation.\nNutzen Sie die Zahl 42 als Startwert für Ihre Zufallszahlen (um die Reproduzierbarkeit zu gewährleisten)\n\n         \n\n\nSolution\n\nlibrary(tidyverse)\n\nWir simulieren die Daten; Subpopulation 1:\n\nset.seed(42)\n\nd1 &lt;- tibble(\n  id = 1:10^3,\n  iq = rnorm(n = 10^3, mean = 85, sd = 15))\n\nSubpopulation 2:\n\nset.seed(42)\n\nd2 &lt;- tibble(\n  id = 1:10^3,\n  iq = rnorm(n = 10^3, mean = 115, sd = 15))\n\nDann kombinieren wir die Daten zu einer Tabelle:\n\nd &lt;-\n  d1 %&gt;% \n  bind_rows(d2)\n\nDann filtern wir wie in der Angabe gefragt:\n\nsolution_d &lt;-\n  d %&gt;% \n  count(iq &gt; 115) %&gt;% \n  mutate(prop = n / sum(n))\n\nsolution_d\n\n# A tibble: 2 × 3\n  `iq &gt; 115`     n  prop\n  &lt;lgl&gt;      &lt;int&gt; &lt;dbl&gt;\n1 FALSE       1494 0.747\n2 TRUE         506 0.253\n\n\nDie Lösung lautet also 0.253.\nWenn Sie die Zufallszahlen mit set.seed fixiert haben, sollten Sie den exakt gleichen Wert gefunden haben.\nInteressant ist es vielleicht, die Gesamtpopulation zu visualisieren:\n\nggplot(d) +\n  aes(x = iq) +\n  geom_density()\n\n\n\n\n\n\n\n\nIm Vergleich dazu eine Normalverteilung mit MW=100 und SD=15:\n\n\n\n\n\n\n\n\n\nWir sehen, dass unsere Population über eine (deutlich) höhere Streuung verfügt:\n\nd %&gt;% \n  summarise(sd(iq))\n\n# A tibble: 1 × 1\n  `sd(iq)`\n     &lt;dbl&gt;\n1     21.2\n\n\n\nCategories:\n\nprobability\nsimulation\nnormal-distribution"
  },
  {
    "objectID": "posts/lm-Standardfehler/lm-Standardfehler.html",
    "href": "posts/lm-Standardfehler/lm-Standardfehler.html",
    "title": "lm-Standardfehler",
    "section": "",
    "text": "Man kann angeben, wie genau eine Schätzung von Regressionskoeffizienten die Grundgesamtheit widerspiegelt. Zumeist wird dazu der Standardfehler (engl. standard error, SE) verwendet.\nIn dieser Übung untersuchen wir, wie sich der SE als Funktion der Stichprobengröße, \\(n\\), verhält.\nErstellen Sie dazu folgenden Datensatz:\n\nlibrary(tidyverse)\n\nn &lt;- 2^4\n\nd &lt;-\n  tibble(x = rnorm(n = n),  # im Default: mean = 0, sd = 1\n         y = x + rnorm(n, mean = 0, sd = .5))\n\nHier ist das Ergebnis. Uns interessiert v.a. Std. Error für den Prädiktor x:\n\nlm(y ~ x, data = d) %&gt;% \nsummary()\n\n\nCall:\nlm(formula = y ~ x, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.60191 -0.42922  0.09198  0.32313  0.59878 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.1226     0.1061  -1.156    0.267    \nx             1.0385     0.0888  11.694  1.3e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4223 on 14 degrees of freedom\nMultiple R-squared:  0.9071,    Adjusted R-squared:  0.9005 \nF-statistic: 136.8 on 1 and 14 DF,  p-value: 1.302e-08\n\n\nHier haben wir eine Tabelle mit zwei Variablen, x und y, definiert mit n=16.\nVerdoppeln Sie die Stichprobengröße 5 Mal und betrachten Sie, wie sich die Schätzgenauigkeit, gemessen über den SE, verändert. Berechnen Sie dazu für jedes n eine Regression mit x als Prädiktor und y als AV!\nBei welcher Stichprobengröße ist SE am kleinsten?\n\n\n\n\\(2^5\\)\n\\(2^6\\)\n\\(2^7\\)\n\\(2^8\\)\n\\(2^9\\)"
  },
  {
    "objectID": "posts/lm-Standardfehler/lm-Standardfehler.html#answerlist",
    "href": "posts/lm-Standardfehler/lm-Standardfehler.html#answerlist",
    "title": "lm-Standardfehler",
    "section": "",
    "text": "\\(2^5\\)\n\\(2^6\\)\n\\(2^7\\)\n\\(2^8\\)\n\\(2^9\\)"
  },
  {
    "objectID": "posts/lm-Standardfehler/lm-Standardfehler.html#answerlist-1",
    "href": "posts/lm-Standardfehler/lm-Standardfehler.html#answerlist-1",
    "title": "lm-Standardfehler",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nFalsch\nWahr. Die größte Stichprobe impliziert den kleinsten SE, ceteris paribus.\n\n\nCategories:\n\ninference\nlm\nqm2"
  },
  {
    "objectID": "posts/mariokart-max1/mariokart-max1.html",
    "href": "posts/mariokart-max1/mariokart-max1.html",
    "title": "mariokart-max1",
    "section": "",
    "text": "Aufgabe\nImportieren Sie den Datensatz mariokart in R. Berechnen Sie die maximale Verkaufspreise (total_pr) für Spiele, die mit 0, 1, 2, … Lenkräder (wheels) gekauft werden. Bilden Sie davon den Mittelwert und geben Sie diesen an.\nHinweise:\n\nRunden Sie auf 1 Dezimalstelle.\n\n         \n\n\nLösung\nPakete starten:\n\nlibrary(tidyverse)\nlibrary(easystats)\n\nDaten importieren:\n\nd_url &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\"\nd &lt;- data_read(d_url)\n\n\nsolution &lt;-\nd  %&gt;% \n  group_by(wheels) %&gt;% \n  summarise(pr_max = max(total_pr)) %&gt;% \n  summarise(pr_max_mean = mean(pr_max))\n\nsolution\n\n# A tibble: 1 × 1\n  pr_max_mean\n        &lt;dbl&gt;\n1        128.\n\n\nLösung: 127.95.\n\nCategories:\n\ndatawrangling\ndplyr\neda\nnum"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-01/Verteilungen-Quiz-01.html",
    "href": "posts/Verteilungen-Quiz-01/Verteilungen-Quiz-01.html",
    "title": "Verteilungen-Quiz-01",
    "section": "",
    "text": "Beziehen Sie sich auf den Standard-Globusversuch mit \\(N=9\\) Würfen und \\(W=6\\) Wassertreffern (binomialverteilt), vgl. hier.\nDie Stichproben-Postverteilung sieht so aus:\n\n\n\n\n\n\n\n\n\nIst sich das Modell auf Basis dieser Post-Verteilung sicher sein, dass der Wasseranteil \\(\\pi=.7\\) beträgt?\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-01/Verteilungen-Quiz-01.html#answerlist",
    "href": "posts/Verteilungen-Quiz-01/Verteilungen-Quiz-01.html#answerlist",
    "title": "Verteilungen-Quiz-01",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-01/Verteilungen-Quiz-01.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-01/Verteilungen-Quiz-01.html#answerlist-1",
    "title": "Verteilungen-Quiz-01",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/Wertpruefen/Wertpruefen.html",
    "href": "posts/Wertpruefen/Wertpruefen.html",
    "title": "Wertpruefen",
    "section": "",
    "text": "Aufgabe\nGeben Sie die R-Syntax ein, um zu prüfen, dass die Variable loesung den Wert 42 hat.\nHinweis: Geben Sie Ihre Lösung ohne Leerzeichen an, da sonst eine richtige Lösung nicht erkannt werden kann.\n         \n\n\nLösung\nloesung==42\n\nCategories:\n\nR\n‘2023’\nstring"
  },
  {
    "objectID": "posts/punktschaetzer-reicht-nicht/punktschaetzer-reicht-nicht.html",
    "href": "posts/punktschaetzer-reicht-nicht/punktschaetzer-reicht-nicht.html",
    "title": "punktschaetzer-reicht-nicht",
    "section": "",
    "text": "Exercise\nZwei Modelle, m1 und m2 produzieren jeweils die gleiche Vorhersage (den gleichen Punktschätzer).\nm1:\n\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.201112 -0.063837  0.004139  0.053888  0.214878 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.008410   0.008621  -0.975    0.332    \nx            0.998655   0.007949 125.640   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08603 on 98 degrees of freedom\nMultiple R-squared:  0.9938,    Adjusted R-squared:  0.9938 \nF-statistic: 1.579e+04 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nm2:\n\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.1669 -0.7398  0.0997  0.6694  2.5358 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.01371    0.10070   0.136    0.892    \nx            1.11902    0.10962  10.208   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.006 on 98 degrees of freedom\nMultiple R-squared:  0.5153,    Adjusted R-squared:  0.5104 \nF-statistic: 104.2 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nDie Modelle unterscheiden sich aber in ihrer Ungewissheit bezüglich \\(\\beta\\), wie in der Spalte Std. Error ausgedrückt.\nWelches der beiden Modelle ist zu bevorzugen? Begründen Sie.\n         \n\n\nSolution\nModell m1 hat eine kleinere Ungewissheit im Hinblick auf die Modellkoeffizienten \\(\\beta_0, \\beta_1\\) und ist daher gegenüber m2 zu bevorzugen.\n\nCategories:\n\nregresssion\nen\nbayes\nfrequentist\nqm1\nstats-nutshell\nqm2\nstats-nutshell"
  },
  {
    "objectID": "posts/Lose-Nieten-Binomial-Grid/Lose-Nieten-Binomial-Grid.html",
    "href": "posts/Lose-Nieten-Binomial-Grid/Lose-Nieten-Binomial-Grid.html",
    "title": "Lose-Nieten-Binomial-Grid",
    "section": "",
    "text": "Exercise\nIn einer Lostrommel befinden sich “sehr viele” Lose, davon ein Anteil \\(p\\) Treffer (und \\(1-p\\) Nieten), mit zunächst \\(p=0.01\\).\nSie kaufen \\(n=10\\) Lose.\n\nWie groß ist die Wahrscheinlichkeit für genau \\(k=0,1,...,10\\) Treffer?\nSagen wir, Sie haben 3 Treffer in den 10 Losen. Yeah! Jetzt sei \\(p\\) unbekannt und Sie sind indifferent zu den einzelnen Werten von \\(p\\). Visualisieren Sie die Posteriori-Wahrscheinlichkeitsverteilung mit ca. 100 Gridwerten. Was beobachten Sie?\nVariieren Sie \\(n\\), aber halten Sie die Trefferquote bei 1/3. Was beobachten Sie?\n\nNutzen Sie die Gittermethode. Treffen Sie Annahmen, wo nötig.\n         \n\n\nSolution\n\nWie groß ist die Wahrscheinlichkeit für genau \\(k=0,1,...,10\\) Treffer?\n\n\nd_a &lt;- \n  tibble(\n    k = 0:10,\n    wskt = dbinom(k, size = 10, prob = .01))\n\nd_a %&gt;% \n  ggplot() +\n  aes(x = k, y = wskt) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(breaks = 1:10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nk\nwskt\n\n\n\n\n0\n9.04 × 10−1\n\n\n1\n9.14 × 10−2\n\n\n2\n4.15 × 10−3\n\n\n3\n1.12 × 10−4\n\n\n4\n1.98 × 10−6\n\n\n5\n2.40 × 10−8\n\n\n6\n2.02 × 10−10\n\n\n7\n1.16 × 10−12\n\n\n8\n4.41 × 10−15\n\n\n9\n9.90 × 10−18\n\n\n10\n1.00 × 10−20\n\n\n\n\n\n\n\n\nSagen wir, Sie haben 3 Treffer in den 10 Losen. Yeah! Jetzt sei \\(p\\) unbekannt und Sie sind indifferent zu den einzelnen Werten von \\(p\\). Visualisieren Sie die Posteriori-Wahrscheinlichkeitsverteilung mit ca. 100 Gridwerten. Was beobachten Sie?\n\n\nd2 &lt;-\n  tibble(\n    p_grid = seq(0, 1, by = 0.01),\n    prior = 1,\n    Likelihood = dbinom(x = 3, size = 10, prob = p_grid),\n    unstand_post = prior * Likelihood,\n    std_post = unstand_post / sum(unstand_post)\n  )\n\nd2 %&gt;% \n  ggplot() +\n  aes(x = p_grid, y = std_post) +\n  geom_point() +\n  geom_line()\n\n\n\n\n\n\n\n\nDer Modus liegt bei ca 1/3. Der Bereich plausibler Werte für \\(p\\) liegt ca. zwischen 0.1 und und 0.7, grob visuell geschätzt. Mehr dazu später.\n\nVariieren Sie \\(n\\), aber halten Sie die Trefferquote bei 1/3. Was beobachten Sie?\n\n\n# n = 2\nd3 &lt;-\n  tibble(\n    p_grid = seq(0,1, by = 0.01),\n    prior = 1,\n    Likelihood = dbinom(x = 2, size = 6, prob = p_grid),\n    unstand_post = prior * Likelihood,\n    std_post = unstand_post / sum(unstand_post)\n  )\n\nd3 %&gt;% \n  ggplot() +\n  aes(x = p_grid, y = std_post) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"n=20\")\n\n\n# n = 20\nd4 &lt;-\n  tibble(\n    p_grid = seq(0,1, by = 0.01),\n    prior = 1,\n    Likelihood = dbinom(x = 20, size = 60, prob = p_grid),\n    unstand_post = prior * Likelihood,\n    std_post = unstand_post / sum(unstand_post)\n  )\n\nd4 %&gt;% \n  ggplot() +\n  aes(x = p_grid, y = std_post) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"n = 20\")\n\n# n = 200\nd5 &lt;-\n  tibble(\n    p_grid = seq(0,1, by = 0.01),\n    prior = 1,\n    Likelihood = dbinom(x = 200, size = 600, prob = p_grid),\n    unstand_post = prior * Likelihood,\n    std_post = unstand_post / sum(unstand_post)\n  )\n\nd5 %&gt;% \n  ggplot() +\n  aes(x = p_grid, y = std_post) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"n = 20\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDer Modus und andere Maße der zentralen Tendenz bleiben gleich; die Streuung wird geringer.\n\nCategories:\n\nprobability\nbinomial"
  },
  {
    "objectID": "posts/fattails01/fattails01.html",
    "href": "posts/fattails01/fattails01.html",
    "title": "fattails01",
    "section": "",
    "text": "Exercise\nIn seinem Buch “Statistical Consequences of Fat Tails” schreibt der Autor, Nassim Taleb (S. 53):\n\nIn the summer of 1998, the hedge fund called “Long Term Capital Management” (LTCM) proved to have a very short life; it went bust from some deviations in the markets –those “of an unexpected nature”. The loss was a yuuuge deal because two of the partners received the Swedish Riksbank Prize, marketed as the “Nobel” in economics. (…) At least two of the partners made the statement that it was a “10 sigma” event (10 standard deviations), hence they should be absolved of all accusations of incompetence (I was ﬁrst hand witness of two such statements).\n\nWir testen in diesem Zusammenhang zwei Hypothesen: \\(H_N\\), dass der Finanzmarkt normalverteilt ist und \\(H_F\\), dass die Variable fat tailed ist, also nicht normalverteilt, sondernn einer Verteilung entspringt, in der “Extremereignisse” üblicher sind als in einer Normalverteilung.\nUm die Fat-Tails-Verteilung mit \\(n=10\\) zu simulieren, nutzen wir hier folgende Funktion:\n\nfat_tail_data &lt;- rt(n = 100, df = 2)\n\nDabei bedeutet df = 2, dass die Verteilung sehr randlastig (fat tailed) sein soll (genauer gesagt eine t-Verteilung mit zwei Freiheitsgraden). Details dazu sollen uns hier nicht interessieren. Nur für diejenigen, die neugierig sind: r steht für random, also eine Zufallszahl. Diese soll aus der sog. t-Verteilung mit df=1 stammen. Das ist, einfach gesagt, eine “plattgedrückte” Normalverteilung.\nBerechnen wir die Wahrscheinlichkeit, dass die Daten einer Normalverteilung entspringen (und nicht der Fat-Tail-Verteilung).\nDie Wahrscheinlichkeit eines 10-Sigma-Events ist übrigens … klein. Taleb berichtet sie mit \\(1.31 \\cdot 10^{-23}\\):\n\nL_norm &lt;- 1.31e-23\n\nFür die t-Verteilung ist der entsprechende Wert:\n\nL_fat &lt;- 1 - pt(q = 10, df = 2)\n\nAuch hier soll der Befehl pt nicht interessieren. Nur für die Neugierigen: p steht für probability, t für die t-Verteilung. Der Befehl gibt uns also die Wahrscheintlichkeit, \\(p\\), für ein bestimmten Quartil, \\(q\\), aus einer t-Verteilung mit 2 Freiheitsgraden.\nWie hoch ist die Post-Wahrscheinlichkeit, dass die Variable normalverteilt ist?\nHinweise:\n\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\nApriori sollen uns beide Hypothesen gleich plausibel sein.\n\n\n\nAnswerlist\n\nkleiner als 50%\nkleiner als 5%\nkleiner als 0.5%\nkleiner als 0.05%\nkleiner als 0.005%\n\n         \n\n\nSolution\n\nlibrary(tidyverse)\n\nErstellen wir erstmal den ersten Teil einer Bayes-Box:\n\nd &lt;-\n  tibble(H = c(\"Normalverteilt\", \"Randlastig verteilt\"),\n         Prior = c(1,1))\n\nd\n\n# A tibble: 2 × 2\n  H                   Prior\n  &lt;chr&gt;               &lt;dbl&gt;\n1 Normalverteilt          1\n2 Randlastig verteilt     1\n\n\nDann fügen wir den Likelihood jeder Hypothese dazu:\n\nd &lt;-\n  d %&gt;% \n  mutate(L = c(L_norm, L_fat))\n\nd\n\n# A tibble: 2 × 3\n  H                   Prior        L\n  &lt;chr&gt;               &lt;dbl&gt;    &lt;dbl&gt;\n1 Normalverteilt          1 1.31e-23\n2 Randlastig verteilt     1 4.93e- 3\n\n\nDann berechnen wir die Post-Wahrscheinlichkeit:\n\nd &lt;-\n  d %&gt;% \n  mutate(Post_unstand = Prior * L,\n         Post = Post_unstand / sum(Post_unstand))\nd\n\n# A tibble: 2 × 5\n  H                   Prior        L Post_unstand     Post\n  &lt;chr&gt;               &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 Normalverteilt          1 1.31e-23     1.31e-23 2.66e-21\n2 Randlastig verteilt     1 4.93e- 3     4.93e- 3 1   e+ 0\n\n\nDie Wahrscheinlichkeit, dass die Variable normalverteilt ist, ist seeeeehr klein, ca. \\(10^{-21}\\).\n\n\nAnswerlist\n\nFALSE\nFALSE\nFALSE\nFALSE\nTRUE\n\n\nCategories:\n\nprobability\nsimulation\nfat-tails\nnormal-distribution\nfat-tails"
  },
  {
    "objectID": "posts/Gem-Wskt1/Gem-Wskt1.html",
    "href": "posts/Gem-Wskt1/Gem-Wskt1.html",
    "title": "Gem-Wskt1",
    "section": "",
    "text": "Exercise\nProf. Süß-Sauer untersucht eine seiner Lieblingsfragen: Wie viel bringt das Lernen auf eine Klausur? Dabei konzentriert er sich auf das Fach Statistik (es gefällt ihm gut). In einer aktuellen Untersuchung hat er \\(n=70\\) Studierende untersucht (s. Tabelle und Diagramm) und jeweils erfasst, ob die Person die Klausur bestanden (\\(B\\)) hat oder durchgefallen (\\(\\neg B\\)) ist. Im Hinblick auf das Lernen, \\(L\\) (wie viel die Person gelernt hat) hat er zwei Gruppen unterschieden: Die “Viel-Lerner” (VL) und die “Wenig-Lerner” (WL).\nBerechnen Sie die folgende: gemeinsame Wahrscheinlichkeit: p(Durchfallen UND Viellerner).\nBeispiel: Wenn Sie ausrechnen, dass die Wahrscheinlichkeit bei 42 Prozentpunkten liegt, so geben Sie ein: 0,42 bzw. 0.42 (das Dezimalzeichen ist abhängig von Ihren Spracheinstellungen).\n\nGeben Sie nur eine Zahl ein (ohne Prozentzeichen o.Ä.), z.B. 0,42.\nAndere Angaben können u.U. nicht gewertet werden.\nRunden Sie auf zwei Dezimalstellen.\nAchten Sie darauf, das korrekte Dezimaltrennzeichen einzugeben; auf Geräten mit deutscher Spracheinstellung ist dies oft ein Komma.\n\nDas folgende Diagramm zeigt die Häufigkeiten pro Gruppe:\n\n\n\n\n\n\n\n\n\nHier ist die Kontingenztabelle mit den Häufigkeiten pro Gruppe:\n\n\n\n\n\n\n\n\nLerntyp\nBestehen\nDurchfallen\n\n\n\n\nViellerner\n37\n5\n\n\nWeniglerner\n27\n1\n\n\n\n\n\n\n         \n\n\nSolution\nDie gemeinsame Wahrscheinlichkeit beträgt 0.07.\n\n\n\n\n\n\n\n\n\nLerntyp\nKlausurergebnis\nn\nn_group\nprop_conditional_group\njoint_prob\n\n\n\n\nViellerner\nDurchfallen\n5\n42\n0.1190476\n0.07142857\n\n\n\n\n\n\n\nDie gemeinsame Wahrscheinlichkeit berechnet sich hier als der Quotient der Zellenhäufigkeit und der Gesamthäufigkeit.\nMan kann auch die Formel für gemeinsame Wahrscheinlichkeiten anwenden: \\(Pr(A) \\cdot \\Pr(B)\\).\nDazu berechnet man die Anteile für jedes der beiden Ereignisse und multipliziert diese beiden Anteile:\n`Klausurergebnis_selected * Lerntyp_selected*\n\nCategories:\n\nprobability\n‘2022’"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-06/Verteilungen-Quiz-06.html",
    "href": "posts/Verteilungen-Quiz-06/Verteilungen-Quiz-06.html",
    "title": "Verteilungen-Quiz-06",
    "section": "",
    "text": "Ist folgende Aussage wahr?\nSei \\(X \\sim N(100,15)\\), dann ist \\(Pr(X \\ge 100) \\ne 1/2\\).\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-06/Verteilungen-Quiz-06.html#answerlist",
    "href": "posts/Verteilungen-Quiz-06/Verteilungen-Quiz-06.html#answerlist",
    "title": "Verteilungen-Quiz-06",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-06/Verteilungen-Quiz-06.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-06/Verteilungen-Quiz-06.html#answerlist-1",
    "title": "Verteilungen-Quiz-06",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/Typ-Fehler-R-02/Typ-Fehler-R-02.html",
    "href": "posts/Typ-Fehler-R-02/Typ-Fehler-R-02.html",
    "title": "Typ-Fehler-R-02",
    "section": "",
    "text": "R gibt folgende Fehlermeldung aus:\n(Fehler in library(XXX): es gibt kein Paket namens 'XXX'),\nwobei für XXX ein Paketname wie tidyverse angeführt wird.\nWählen Sie die plausibelste Ursache aus!\n\n\n\nDas Paket XXX ist nicht installiert auf dem aktuellen Rechner.\nDas Paket XXX ist nicht verfügbar genau für dieses Betriebssystem.\nEs existiert kein Paket mit Namen XXX.\nDas Paket XXX ist nicht geladen.\nDas Paket XXX ist defekt."
  },
  {
    "objectID": "posts/Typ-Fehler-R-02/Typ-Fehler-R-02.html#answerlist",
    "href": "posts/Typ-Fehler-R-02/Typ-Fehler-R-02.html#answerlist",
    "title": "Typ-Fehler-R-02",
    "section": "",
    "text": "Das Paket XXX ist nicht installiert auf dem aktuellen Rechner.\nDas Paket XXX ist nicht verfügbar genau für dieses Betriebssystem.\nEs existiert kein Paket mit Namen XXX.\nDas Paket XXX ist nicht geladen.\nDas Paket XXX ist defekt."
  },
  {
    "objectID": "posts/Typ-Fehler-R-02/Typ-Fehler-R-02.html#answerlist-1",
    "href": "posts/Typ-Fehler-R-02/Typ-Fehler-R-02.html#answerlist-1",
    "title": "Typ-Fehler-R-02",
    "section": "Answerlist",
    "text": "Answerlist\n\nRichtig.\nFalsch.\nFalsch.\nFalsch.\nFalsch.\n\n\nCategories:\n\nR\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/wuerfel04/wuerfel04.html",
    "href": "posts/wuerfel04/wuerfel04.html",
    "title": "wuerfel04",
    "section": "",
    "text": "Exercise\nWas ist die Wahrscheinlichkeit, mit zwei fairen Würfeln genau 10 Augen zu werfen?\nHinweise:\n\nNutzen Sie Simulationsmnethoden der Wahrscheinlichkeitsrechnung, keine exakten Rechnung auf Basis der Wahrscheinlichkeitsrechnung.\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\nSetzen Sie bei Simulationsaufgaben immer die Zufallszahlen mit set.seed(). Sofern kein anderer Wert für set.seed() genannt, verwenden Sie die Zahl 42.\nDa es bei dieser Aufgabe nötig ist, zwei Mal Zufallszahlen zu berechnen (für zwei Würfel nämlich), verwenden Sie beim ersten Würfel die Zahl 42 und beim zweiten Würfel die Zahl 43.\n\n         \n\n\nSolution\n\nlibrary(tidyverse)\n\nEinen Würfelwurf in R kann man so simulieren:\n\nwuerfel &lt;- sample(x = c(1,2,3,4,5,6), size = 1, prob = c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6))\nwuerfel\n\n[1] 5\n\n\nBei sample gibt x den Ereignisraum, \\(\\Omega\\), an, size die Stichprobengröße und prob gibt für jedes Element von x die Wahrscheinlichkeit an.\nDas machen wir jetzt 1000 Mal. Viel Spaß beim Tippen…\n… … …\nOkay, das sollten wir einfacher hinkriegen. Man kann R sagen, dass sie eine Funktion (wie sample) oft ausführen soll. Damit können wir viele Würfelwürfe simulieren. Diese “Wiederholungsfunktion” heißt replicate(n, expr); dabei gibt n an, wie oft die Funktion wiederholt werden soll, und expr ist der Ausdruck (die Funktion), die wiederholt werden soll, das ist bei uns die Funktion sample, wie oben dargestellt.\n\nzehn_wuerfel &lt;- replicate(n = 10, expr = sample(x = c(1,2,3,4,5,6), size = 1, prob = c(1/6, 1/6,1/6,1/6,1/6,1/6)))\nzehn_wuerfel\n\n [1] 5 3 2 1 3 1 4 6 3 3\n\n\nKönnen wir natürlich auch zich Mal wiederholen, nicht nur 10 Mal, sagen wir \\(10^4\\) Mal:\n\nset.seed(42)\nwuerfel1_oft &lt;- replicate(n = 10^4, expr = sample(x = c(1,2,3,4,5,6), size = 1, prob = c(1/6, 1/6,1/6,1/6,1/6,1/6)))\n\nmean(wuerfel1_oft)\n\n[1] 3.4968\n\n\nAh, interessant: Der Mittelwert ist etwa 3.5…\nJetzt werfen wir noch einen zweiten Würfel genau so oft:\n\nset.seed(43)\nwuerfel2_oft &lt;- replicate(n = 10^4, expr = sample(x = c(1,2,3,4,5,6), size = 1, prob = c(1/6, 1/6,1/6,1/6,1/6,1/6)))\n\nmean(wuerfel2_oft)\n\n[1] 3.4983\n\n\nDas packen wir jetzt in eine Tabelle und ergänzen die Augensumme für jede Wiederholung des Doppelwurfes:\n\nd &lt;-\n  tibble(w1 = wuerfel1_oft,\n         w2 = wuerfel2_oft,\n         w_sum = w1+w2)\n\nhead(d)\n\n# A tibble: 6 × 3\n     w1    w2 w_sum\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     4     5\n2     1     1     2\n3     3     2     5\n4     6     6    12\n5     5     3     8\n6     5     5    10\n\n\nJetzt ist es einfach:\nWir zählen einfach, wie oft das Ergebnis 10 vorkommt in der Tabelle.\n\nd %&gt;% \n  count(w_sum == 10)\n\n# A tibble: 2 × 2\n  `w_sum == 10`     n\n  &lt;lgl&gt;         &lt;int&gt;\n1 FALSE          9148\n2 TRUE            852\n\n\nErgänzen wir die Anteile dieser Anzahl:\n\nd %&gt;% \n  count(w_sum == 10) %&gt;% \n  mutate(Anteil = n/sum(n))\n\n# A tibble: 2 × 3\n  `w_sum == 10`     n Anteil\n  &lt;lgl&gt;         &lt;int&gt;  &lt;dbl&gt;\n1 FALSE          9148 0.915 \n2 TRUE            852 0.0852\n\n\nDie Lösung lautet also: 0.08 (gerundet auf zwei Dezimalen)\nAuf einfache Weise können wir entsprechend die Wahrscheinlichkeit für mindestens \\(k\\) Augen (bei zwei Würfelwürfen) ermitteln, mit \\(k\\) ist die gesuchte Augensumme, hier 10.\n\nd %&gt;% \n  count(w_sum &gt;= 10) %&gt;% \n  mutate(Anteil = n/sum(n))\n\n# A tibble: 2 × 3\n  `w_sum &gt;= 10`     n Anteil\n  &lt;lgl&gt;         &lt;int&gt;  &lt;dbl&gt;\n1 FALSE          8316  0.832\n2 TRUE           1684  0.168\n\n\nOder höchstens 10, ganz analog:\n\nd %&gt;% \n  count(w_sum &lt;= 10) %&gt;% \n  mutate(Anteil = n/sum(n))\n\n# A tibble: 2 × 3\n  `w_sum &lt;= 10`     n Anteil\n  &lt;lgl&gt;         &lt;int&gt;  &lt;dbl&gt;\n1 FALSE           832 0.0832\n2 TRUE           9168 0.917 \n\n\n\nCategories:\n\nprobability\ndice\nsimulation"
  },
  {
    "objectID": "posts/Schiefe1/Schiefe1.html",
    "href": "posts/Schiefe1/Schiefe1.html",
    "title": "Schiefe1",
    "section": "",
    "text": "Welche der Abbildungen zeigt am deutlichsten eine bimodale Verteilung?\n\n\nWarning: `stat(density)` was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\nA\nB\nC\nD"
  },
  {
    "objectID": "posts/Schiefe1/Schiefe1.html#answerlist",
    "href": "posts/Schiefe1/Schiefe1.html#answerlist",
    "title": "Schiefe1",
    "section": "",
    "text": "A\nB\nC\nD"
  },
  {
    "objectID": "posts/Schiefe1/Schiefe1.html#answerlist-1",
    "href": "posts/Schiefe1/Schiefe1.html#answerlist-1",
    "title": "Schiefe1",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nRichtig\nFalsch\n\n\nCategories:\nschoice"
  },
  {
    "objectID": "posts/wuerfel03/wuerfel03.html",
    "href": "posts/wuerfel03/wuerfel03.html",
    "title": "wuerfel03",
    "section": "",
    "text": "Exercise\nWas ist die Wahrscheinlichkeit, mit zwei fairen Würfeln höchstens 10 Augen zu werfen?\nHinweise:\n\nNutzen Sie exakte Methoden der Wahrscheinlichkeitsrechnung, keine Simulation.\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\n\n         \n\n\nSolution\nErstellen wir uns eine Tabelle, die alle Permutationen der beiden Würfelergebnisse fasst, das sind 36 Paare: (1,1), (1,2), …, (1,6), …, (6,6).\nDas kann man von Hand erstellen, halbautomatisch in Excel oder z.B. so:\n\nlibrary(tidyverse)\nd &lt;- expand_grid(wuerfel1 = 1:6,\n         wuerfel2 = 1:6)\n\nd\n\n# A tibble: 36 × 2\n   wuerfel1 wuerfel2\n      &lt;int&gt;    &lt;int&gt;\n 1        1        1\n 2        1        2\n 3        1        3\n 4        1        4\n 5        1        5\n 6        1        6\n 7        2        1\n 8        2        2\n 9        2        3\n10        2        4\n# … with 26 more rows\n\n\nJetzt ergänzen wir eine Spalte für die Wahrscheinlichkeit jeder Kombination, das ist einfach, denn \\(p(A \\cap B) = p(A) \\cdot p(B) = 1/36\\) gilt.\n\nd2 &lt;-\n  d %&gt;% \n  mutate(prob = 1/36)\n\nhead(d2)\n\n# A tibble: 6 × 3\n  wuerfel1 wuerfel2   prob\n     &lt;int&gt;    &lt;int&gt;  &lt;dbl&gt;\n1        1        1 0.0278\n2        1        2 0.0278\n3        1        3 0.0278\n4        1        4 0.0278\n5        1        5 0.0278\n6        1        6 0.0278\n\n\nAußerdem ergänzen wir die Summe der Augenzahlen, weil die Frage ja nach einer bestimmten Summe an Augenzahlen abzielt.\n\nd3 &lt;-\n  d2 %&gt;% \n  mutate(augensumme = wuerfel1 + wuerfel2)\n\nhead(d3)\n\n# A tibble: 6 × 4\n  wuerfel1 wuerfel2   prob augensumme\n     &lt;int&gt;    &lt;int&gt;  &lt;dbl&gt;      &lt;int&gt;\n1        1        1 0.0278          2\n2        1        2 0.0278          3\n3        1        3 0.0278          4\n4        1        4 0.0278          5\n5        1        5 0.0278          6\n6        1        6 0.0278          7\n\n\nFür manche Augensummen gibt es mehrere Möglichkeiten:\n\nd3 %&gt;% \n  filter(augensumme == 7)\n\n# A tibble: 6 × 4\n  wuerfel1 wuerfel2   prob augensumme\n     &lt;int&gt;    &lt;int&gt;  &lt;dbl&gt;      &lt;int&gt;\n1        1        6 0.0278          7\n2        2        5 0.0278          7\n3        3        4 0.0278          7\n4        4        3 0.0278          7\n5        5        2 0.0278          7\n6        6        1 0.0278          7\n\n\n… für andere weniger:\n\nd3 %&gt;% \n  filter(augensumme == 12)\n\n# A tibble: 1 × 4\n  wuerfel1 wuerfel2   prob augensumme\n     &lt;int&gt;    &lt;int&gt;  &lt;dbl&gt;      &lt;int&gt;\n1        6        6 0.0278         12\n\n\nJetzt summieren wir (nach dem Additionssatz der Wahrscheinlichkeit) die Wahrscheinlichkeiten pro Augenzahl:\n\nd4 &lt;- \n  d3 %&gt;% \n  group_by(augensumme) %&gt;% \n  summarise(totale_w_pro_augenzahl = sum(prob))\n\nd4\n\n# A tibble: 11 × 2\n   augensumme totale_w_pro_augenzahl\n        &lt;int&gt;                  &lt;dbl&gt;\n 1          2                 0.0278\n 2          3                 0.0556\n 3          4                 0.0833\n 4          5                 0.111 \n 5          6                 0.139 \n 6          7                 0.167 \n 7          8                 0.139 \n 8          9                 0.111 \n 9         10                 0.0833\n10         11                 0.0556\n11         12                 0.0278\n\n\nTest: Die Summe der Wahrscheinlichkeit muss insgesamt 1 sein.\n\nd4 %&gt;% \n  summarise(sum(totale_w_pro_augenzahl))\n\n# A tibble: 1 × 1\n  `sum(totale_w_pro_augenzahl)`\n                          &lt;dbl&gt;\n1                             1\n\n\nUnd:\n\nd2 %&gt;% \n  summarise(sum(prob))\n\n# A tibble: 1 × 1\n  `sum(prob)`\n        &lt;dbl&gt;\n1           1\n\n\nPasst!\nDie Wahrscheinlichkeit für die Augensumme von höchstens 10 beträgt also:\n\nloesung &lt;-\n  d4 %&gt;% \n  filter(augensumme &lt;= 10) %&gt;% \n  summarise(prob_sum = sum(totale_w_pro_augenzahl)) %&gt;% \n  pull(prob_sum)\n\nloesung\n\n[1] 0.9166667\n\n\n\nCategories:\n\nprobability\ndice"
  },
  {
    "objectID": "posts/Rethink_2E4/Rethink_2E4.html",
    "href": "posts/Rethink_2E4/Rethink_2E4.html",
    "title": "Rethink_2E4",
    "section": "",
    "text": "Exercise\nThis question is taken from McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Ed.). Taylor and Francis, CRC Press.\n2E4. The Bayesian statistician Bruno de Finetti (1906–1985) began his 1973 book on probability theory with the dedication: “PROBABILITY DOES NOT EXIST.” The capitals appeared in the original, so I imagine de Finetti wanted us to shout this statement. What he meant is that probability is a device for describing uncertainty from the perspective of an observer with limited knowledge; it has no objective reality. Discuss the globe tossing example from the chapter, in light of this statement. What does it mean to say “the probability of water is 0.7”?\n         \n\n\nSolution\nThe solution is taken from this source.\nThe idea is that probability is only a subjective perception of the likelihood that something will happen. In the globe tossing example, the result will always be either “land” or “water” (i.e., 0 or 1). When we toss the globe, we don’t know what the result will be, but we know it will always be “land” or “water.” To express our uncertainty in the outcome, we use probability. Because we know that water is more likely than land, we may say that the probability of “water” is 0.7; however, we’ll never actually observe a result of 0.7 waters, or observe any probability. We will only ever observe the two results of “land” and “water.”\n\nCategories:\n\nprobability\nphilosophy"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-08/Verteilungen-Quiz-08.html",
    "href": "posts/Verteilungen-Quiz-08/Verteilungen-Quiz-08.html",
    "title": "Verteilungen-Quiz-08",
    "section": "",
    "text": "Ist folgende Aussage wahr?\nSei \\(X \\sim N(100,15)\\), dann ist \\(Pr(X \\ge 115) \\approx 0.16\\).\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-08/Verteilungen-Quiz-08.html#answerlist",
    "href": "posts/Verteilungen-Quiz-08/Verteilungen-Quiz-08.html#answerlist",
    "title": "Verteilungen-Quiz-08",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-08/Verteilungen-Quiz-08.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-08/Verteilungen-Quiz-08.html#answerlist-1",
    "title": "Verteilungen-Quiz-08",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/iq01/iq01.html",
    "href": "posts/iq01/iq01.html",
    "title": "iq01",
    "section": "",
    "text": "Aufgabe\nIntelligenz wird häufig mittels einem IQ-Test ermittelt. Ab einem Testwert von 130 Punkten nennt man die getestete Person hochbegabt.\nWie groß ist die Wahrscheinlichkeit, dass die nächste Person, die Sie treffen, hochbetagthochbegabt ist? Geben Sie die Wahrscheinlichkeit (als Anteil) an.\nHinweise:\n\nNutzen Sie Simulationsmethoden.\nGehen Sie von folgender IQ-Verteilung aus: \\(IQ \\sim N(100,15)\\).\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\nSimulieren Sie \\(n=10^3\\) Stichproben.\nNutzen Sie die Zahl 42 als Startwert für Ihre Zufallszahlen (um die Reproduzierbarkeit zu gewährleisten)\nWir wollen hier keine Post-Verteilung berechnen, sondern lediglich Werte simulieren.\nGeben Sie keine Prozentzahlen, sondern stets Anteile an.\n\n         \n\n\nLösung\n\nlibrary(tidyverse)\n\nWir simulieren die Daten:\n\nset.seed(42)\nd &lt;- tibble(\n  id = 1:10^3,\n  iq = rnorm(n = 10^3, mean = 100, sd= 15))\n\nWir filtern wie in der Angabe gewünscht:\n\nd %&gt;% \n  count(iq &gt;= 130)\n\n# A tibble: 2 × 2\n  `iq &gt;= 130`     n\n  &lt;lgl&gt;       &lt;int&gt;\n1 FALSE         979\n2 TRUE           21\n\n\nCa. 20 von 1000 Personen erfüllen diese Bedingung (IQ &gt;= 130).\nLösung: Die gesuchte Wahrscheinlichkeit beträgt ca. 2% bzw. 0.02\n\n\n[1] 0.021\n\n\n\nCategories:\n\nprobability\nsimulation\nnormal-distribution\nexam-22"
  },
  {
    "objectID": "posts/iq06/iq06.html",
    "href": "posts/iq06/iq06.html",
    "title": "iq06",
    "section": "",
    "text": "Exercise\nIntelligenz wird häufig mittels einem IQ-Test ermittelt.\nWie wahrscheinlich ist es, zur Gruppe der “durchschnittlich intelligenten” Menschen gehören?\nDabei sei “durchschnittlich intelligent” definiert als der Intelligenzwert \\(X\\), für den gilt \\(x-\\sigma &lt; x &lt; x + \\sigma\\).\nHinweise:\n\nNutzen Sie Simulationsmethoden.\nGehen Sie von folgender IQ-Verteilung aus: \\(IQ \\sim N(100,15)\\)\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\nSimulieren Sie \\(n=10^3\\) Stichproben.\nNutzen Sie die Zahl 42 als Startwert für Ihre Zufallszahlen (um die Reproduzierbarkeit zu gewährleisten)\n\n         \n\n\nSolution\n\nlibrary(tidyverse)\n\nWir simulieren die Daten:\n\nset.seed(42)\nk &lt;- 3\nd &lt;- tibble(\n  id = 1:10^3,\n  iq = rnorm(n = 10^3, mean = 100, sd = 15))\n\nWir filtern die schlauesten 0,1 Prozent:\n\nd %&gt;% \n  count(iq &gt; 85 & iq &lt; 115) \n\n# A tibble: 2 × 2\n  `iq &gt; 85 & iq &lt; 115`     n\n  &lt;lgl&gt;                &lt;int&gt;\n1 FALSE                  327\n2 TRUE                   673\n\n\nDie Antwort auf die Frage\nWie wahrscheinlich ist es, zur Gruppe der “durchschnittlich intelligenten” Menschen gehören?,\nlautet also 0.673.\n\nCategories:\n\nprobability\nsimulation\nnormal-distribution"
  },
  {
    "objectID": "posts/iq08/iq08.html",
    "href": "posts/iq08/iq08.html",
    "title": "iq08",
    "section": "",
    "text": "Exercise\nAn einer Elite-Hochschule wird man nur zugelassen, wenn man entweder schön oder schlau ist.\n“Schön” sei definiert als eine SD-Einheit über dem mittleren Aussehen, unter der Annahme, dass Aussehen normalverteilt ist.\n“Schlau” sei definiert als eine SD-Einheit über dem mittleren Wert, unter der Annahme, dass die Variable normalverteilt ist.\nWie hoch ist die Wahrscheinlichkeit, an dieser Elite-Uni zugelassen zu werden?\nHinweise:\n\nNutzen Sie Simulationsmethoden.\nGehen Sie von folgender Verteilung für Schönheit und für Schlauheit aus: \\(X \\sim N(0,1)\\)\nIntelligenz und Schönheit sollen als unabhängig angenommen werden.\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\nSimulieren Sie \\(n=10^4\\) Stichproben.\nNutzen Sie die Zahl 42 als Startwert für Ihre Zufallszahlen (um die Reproduzierbarkeit zu gewährleisten).\n\n         \n\n\nSolution\nDie Wahrscheinlichkeit für “schön”, \\(S1\\) ist gleich der Wahrscheinlichkeit für “Schlau”, \\(S2\\).\n\nlibrary(tidyverse)\n\nWir simulieren die Daten:\n\nset.seed(42)\n\nd &lt;- tibble(\n  id = 1:10^4,\n  schoenheit = rnorm(n = 10^4, mean = 0, sd = 1),\n  schlauheit = rnorm(n = 10^4, mean = 0, sd = 1))\n\nDa es nur um Anteile (bzw. Wahrscheinlichkeiten) der Population geht, können wir mit z-Werten arbeiten.\nZur Erinnerung: Ein z-Wert von 1 bedeutet, dass der Messwert eine SD-Einheit größer ist als der Mittelwert der Verteilung.\nDann filtern wir wie in der Angabe gefragt:\n\nd2 &lt;-\n  d %&gt;% \n  count(schoenheit &gt; 1, schlauheit &gt; 1) %&gt;% \n  mutate(prop = n / sum(n))\n\nd2\n\n# A tibble: 4 × 4\n  `schoenheit &gt; 1` `schlauheit &gt; 1`     n  prop\n  &lt;lgl&gt;            &lt;lgl&gt;            &lt;int&gt; &lt;dbl&gt;\n1 FALSE            FALSE             7082 0.708\n2 FALSE            TRUE              1364 0.136\n3 TRUE             FALSE             1314 0.131\n4 TRUE             TRUE               240 0.024\n\n\nWieder nehmen wir den Anteil her und bezeichnen ihn als Wahrscheinlichkeit. Das ist eine schöne Sache dieser Simulationsmethoden: Es vereinfacht die Angelegenheit, denn mit Häufigkeiten lässt sich einfacher hantieren als mit Wahrscheinlichkeiten. Und die Anteile erfüllen die Kolmogorov-Axiome, wir können also beruhigt rechnen. Falls Sie also vor Sorge um die Reinheit der Mathematik nicht schlafen konnten, kann ich Sie beruhigen :-)\nDie Lösung lautet also 0.024.\nInteressant ist es vielleicht, die Gesamtpopulation zu visualisieren:\n\nd %&gt;% \n  mutate(ist_schoen = if_else(schoenheit &gt; 1, TRUE, FALSE),\n         ist_schlau = if_else(schlauheit &gt; 1, TRUE, FALSE),\n         ist_schoen_schlau = if_else(ist_schoen & ist_schlau, TRUE, FALSE)) %&gt;% \n  ggplot() +\n  aes(x = schoenheit, y = schlauheit, color = ist_schoen_schlau, alpha = .1) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nCategories:\n\nprobability\nsimulation\nnormal-distribution"
  },
  {
    "objectID": "posts/pigs2/pigs2.html",
    "href": "posts/pigs2/pigs2.html",
    "title": "pigs2",
    "section": "",
    "text": "Aufgabe\n\nlibrary(tidyverse)  # Datenjudo\nlibrary(rstanarm)  # Bayes-Inferenz\nlibrary(easystats)  # Komfort\n\nLaden Sie den Datensatz toothgrowth, z.B. von dieser Quelle. In dem Datensatz sind die Daten eines einfaches Experiments berichtet.\nIn dem Experiment wird der (kausale) Effekt verschiedener Darreichungsformen und Konzentrationen von Vitamin C auf das Zahnwachstum von Meerschweinchen untersucht.\nForschungsfrage:\nHat die Dosis (dose) einen (kausalen) Effekt auf die AV, Zahnlänge (len)?\nWir gehen mal einfach davon aus, dass der Faktor experimentell (also randomisiert und auf Störeffekte hin kontrollliert) veraeicht wurde. Sonst wäre eine Kausalinterpretation nicht (ohne Weiteres) möglich.\nBerechnen Sie die Breite eines 95%-HDI für den Effekt!\nHinweise\n         \n\n\nLösung\n\nSchritt: Modell berechnen\n\n\nlm2 &lt;- stan_glm(len ~ dose, data = d,\n                seed = 42,\n                refresh = 0)\n\nZur Erinnerung: Bei der Regressionsformel gilt immer av ~  uv.\n\nSchritt: Posteriori-Verteilung betrachten\n\nMit parameters() kriegt man einen guten Überblick über die Modellparameter:\n\nparameters(lm2)\n\nParameter   | Median |        95% CI |   pd |  Rhat |     ESS |                   Prior\n---------------------------------------------------------------------------------------\n(Intercept) |   7.40 | [4.84,  9.96] | 100% | 0.999 | 3881.00 | Normal (18.81 +- 19.12)\ndose        |   9.76 | [7.81, 11.76] | 100% | 0.999 | 4144.00 |  Normal (0.00 +- 30.41)\n\n\nDas Modell zeigt einen positiven Effekt für dose:\nPro Einheit von dose steigt die Zahnlänge (len) um ca. 8-12 mm im Schnitt (laut unserem Modell).\nNull ist nicht im Intervall enthalten; die Nullhypothese ist demnach auszuschließen (falls das jemanden interessiert). Man sagt, man verwirft die Nullhypothese (oder weist sie zurück).\nDas können wir auch plotten:\n\nplot(parameters(lm2))\n\n\n\n\n\n\n\n\nMan kann sich auch ein HDI direkt ausgeben, ohne die sonstigen Informationen, die parameters() ausgibt:\n\nhdi(lm2, ci = .95)\n\nHighest Density Interval\n\nParameter   |       95% HDI\n---------------------------\n(Intercept) | [4.96, 10.06]\ndose        | [7.77, 11.70]\n\n\nWie wir sehen, wird im Standard ein 95%-Intervall berichtet, wie in der Aufgabenstellung verlangt.\nWieder sehen wir, die Null ist nicht im Intervall enthalten. Null ist also kein plausibler Wert für den gesuchten Effekt (laut unserem Modell).\nSchauen wir uns mal zum Vergleich die Stichproben-Daten an:\n\nd %&gt;% \n  ggplot(aes(y = len, x = dose)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nMan sieht deutlich einen positiven Effekt: Die Regressionsgerade steigt.\nDie Breite des Schätzintervalls für den Effekt beträgt also:\n\nsol &lt;- 11.70 - 7.77\nsol\n\n[1] 3.93\n\n\n\nCategories:\n\nbayes\n~\nregression\nexam-22"
  },
  {
    "objectID": "posts/Typ-Fehler-R-04/Typ-Fehler-R-04.html",
    "href": "posts/Typ-Fehler-R-04/Typ-Fehler-R-04.html",
    "title": "Typ-Fehler-R-04",
    "section": "",
    "text": "Aufgabe\nGegeben sei diese Syntax, die einen Fehlermeldung ausgibt:\n\nmean(c(1,2,3,4). na.rm = TRUE)\n\nError: &lt;text&gt;:1:16: unexpected symbol\n1: mean(c(1,2,3,4).\n                   ^\n\n\nGeben Sie die korrekte Syntax ein, die nicht zu einer Fehlermeldung führt!\nBitte verwenden Sie keine Leerzeichen bei Ihrer Eingabe.\n         \n\n\nLösung\n\nmean(c(1,2,3,4), na.rm = TRUE)\n\n[1] 2.5\n\n\n\nsol &lt;- \"mean(c(1,2,3,4),na.rm=TRUE)\"\n\nDie Antwort lautet: mean(c(1,2,3,4),na.rm=TRUE).\n\nCategories:\n\nR\n‘2023’\nstring"
  },
  {
    "objectID": "posts/wuerfel02/wuerfel02.html",
    "href": "posts/wuerfel02/wuerfel02.html",
    "title": "wuerfel02",
    "section": "",
    "text": "Exercise\nWas ist die Wahrscheinlichkeit, mit zwei fairen Würfeln mindestens 10 Augen zu werfen?\nHinweise:\n\nNutzen Sie exakte Methoden der Wahrscheinlichkeitsrechnung, keine Simulation.\nGeben Sie Anteile oder Wahrscheinlichkeiten stets mit zwei Dezimalstellen an (sofern nicht anders verlangt).\n\n         \n\n\nSolution\nErstellen wir uns eine Tabelle, die alle Permutationen der beiden Würfelergebnisse fasst, das sind 36 Paare: (1,1), (1,2), …, (1,6), …, (6,6).\nDas kann man von Hand erstellen, halbautomatisch in Excel oder z.B. so:\n\nlibrary(tidyverse)\nd &lt;- expand_grid(wuerfel1 = 1:6,\n         wuerfel2 = 1:6)\n\nd\n\n# A tibble: 36 × 2\n   wuerfel1 wuerfel2\n      &lt;int&gt;    &lt;int&gt;\n 1        1        1\n 2        1        2\n 3        1        3\n 4        1        4\n 5        1        5\n 6        1        6\n 7        2        1\n 8        2        2\n 9        2        3\n10        2        4\n# … with 26 more rows\n\n\nJetzt ergänzen wir eine Spalte für die Wahrscheinlichkeit jeder Kombination, das ist einfach, denn \\(p(A \\cap B) = p(A) \\cdot p(B) = 1/36\\) gilt.\n\nd2 &lt;-\n  d %&gt;% \n  mutate(prob = 1/36)\n\nhead(d2)\n\n# A tibble: 6 × 3\n  wuerfel1 wuerfel2   prob\n     &lt;int&gt;    &lt;int&gt;  &lt;dbl&gt;\n1        1        1 0.0278\n2        1        2 0.0278\n3        1        3 0.0278\n4        1        4 0.0278\n5        1        5 0.0278\n6        1        6 0.0278\n\n\nAußerdem ergänzen wir die Summe der Augenzahlen, weil die Frage ja nach einer bestimmten Summe an Augenzahlen abzielt.\n\nd3 &lt;-\n  d2 %&gt;% \n  mutate(augensumme = wuerfel1 + wuerfel2)\n\nhead(d3)\n\n# A tibble: 6 × 4\n  wuerfel1 wuerfel2   prob augensumme\n     &lt;int&gt;    &lt;int&gt;  &lt;dbl&gt;      &lt;int&gt;\n1        1        1 0.0278          2\n2        1        2 0.0278          3\n3        1        3 0.0278          4\n4        1        4 0.0278          5\n5        1        5 0.0278          6\n6        1        6 0.0278          7\n\n\nFür manche Augensummen gibt es mehrere Möglichkeiten:\n\nd3 %&gt;% \n  filter(augensumme == 7)\n\n# A tibble: 6 × 4\n  wuerfel1 wuerfel2   prob augensumme\n     &lt;int&gt;    &lt;int&gt;  &lt;dbl&gt;      &lt;int&gt;\n1        1        6 0.0278          7\n2        2        5 0.0278          7\n3        3        4 0.0278          7\n4        4        3 0.0278          7\n5        5        2 0.0278          7\n6        6        1 0.0278          7\n\n\n… für andere weniger:\n\nd3 %&gt;% \n  filter(augensumme == 12)\n\n# A tibble: 1 × 4\n  wuerfel1 wuerfel2   prob augensumme\n     &lt;int&gt;    &lt;int&gt;  &lt;dbl&gt;      &lt;int&gt;\n1        6        6 0.0278         12\n\n\nJetzt summieren wir (nach dem Additionssatz der Wahrscheinlichkeit) die Wahrscheinlichkeiten pro Augenzahl:\n\nd4 &lt;- \n  d3 %&gt;% \n  group_by(augensumme) %&gt;% \n  summarise(totale_w_pro_augenzahl = sum(prob))\n\nd4\n\n# A tibble: 11 × 2\n   augensumme totale_w_pro_augenzahl\n        &lt;int&gt;                  &lt;dbl&gt;\n 1          2                 0.0278\n 2          3                 0.0556\n 3          4                 0.0833\n 4          5                 0.111 \n 5          6                 0.139 \n 6          7                 0.167 \n 7          8                 0.139 \n 8          9                 0.111 \n 9         10                 0.0833\n10         11                 0.0556\n11         12                 0.0278\n\n\nTest: Die Summe der Wahrscheinlichkeit muss insgesamt 1 sein.\n\nd4 %&gt;% \n  summarise(sum(totale_w_pro_augenzahl))\n\n# A tibble: 1 × 1\n  `sum(totale_w_pro_augenzahl)`\n                          &lt;dbl&gt;\n1                             1\n\n\nUnd:\n\nd2 %&gt;% \n  summarise(sum(prob))\n\n# A tibble: 1 × 1\n  `sum(prob)`\n        &lt;dbl&gt;\n1           1\n\n\nPasst!\nDie Wahrscheinlichkeit für die Augensumme von mind. 10 beträgt also:\n\nloesung &lt;-\n  d4 %&gt;% \n  filter(augensumme &gt;= 10) %&gt;% \n  summarise(prob_sum = sum(totale_w_pro_augenzahl)) %&gt;% \n  pull(prob_sum)\n\nloesung\n\n[1] 0.1666667\n\n\n\nCategories:\n\nprobability\ndice"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-09/Verteilungen-Quiz-09.html",
    "href": "posts/Verteilungen-Quiz-09/Verteilungen-Quiz-09.html",
    "title": "Verteilungen-Quiz-09",
    "section": "",
    "text": "Ist folgende Aussage wahr?\nSei \\(X \\sim N(100,15)\\), dann ist \\(Pr(X \\ge \\bar{x} + \\sigma) \\approx 0.84\\).\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-09/Verteilungen-Quiz-09.html#answerlist",
    "href": "posts/Verteilungen-Quiz-09/Verteilungen-Quiz-09.html#answerlist",
    "title": "Verteilungen-Quiz-09",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-09/Verteilungen-Quiz-09.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-09/Verteilungen-Quiz-09.html#answerlist-1",
    "title": "Verteilungen-Quiz-09",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/Typ-Fehler-R-03/Typ-Fehler-R-03.html",
    "href": "posts/Typ-Fehler-R-03/Typ-Fehler-R-03.html",
    "title": "Typ-Fehler-R-03",
    "section": "",
    "text": "Aufgabe\nGegeben sei diese Syntax:\n\nx &lt;- 42\nY &lt;- 1\n\nLässt man folgende Syntax laufen, so kommt eine Fehlermeldung:\n\nX + Y\n\nError in eval(expr, envir, enclos): object 'X' not found\n\n\nGeben Sie die korrekte Syntax ein (zur Berechnung der Summe), die nicht zu einer Fehlermeldung führt!\nBitte verwenden Sie keine Leerzeichen bei Ihrer Eingabe.\n         \n\n\nLösung\n\nx+Y\n\n[1] 43\n\n\nDie Antwort lautet: x+Y.\n\nCategories:\nstring"
  },
  {
    "objectID": "posts/posterior_interval/posterior_interval.html",
    "href": "posts/posterior_interval/posterior_interval.html",
    "title": "posterior_interval",
    "section": "",
    "text": "Welches Ergebnis hat der R-Befehl posterior_interval() (R-Paket rstanarm)?\nWählen Sie die (am besten) passende Antwort aus.\nHinweis:\n\nSoweit nicht anders benannt, ist immer die Voreinstellung der betreffenden Funktion gemeint.\n\n\n\n\nEr liefert einen Vorhersagewert aus der Posteriori-Verteilung.\nEr liefert ein Vorhersageintervall aus der Posteriori-Verteilung.\nEr liefert ein 90%-Vorhersageintervall aus der Posteriori-Verteilung.\nEr liefert ein 95%-Vorhersageintervall aus der Posteriori-Verteilung.\nEr liefert ein HDI-Vorhersageinterval aus der Posteriori-Verteilung."
  },
  {
    "objectID": "posts/posterior_interval/posterior_interval.html#answerlist",
    "href": "posts/posterior_interval/posterior_interval.html#answerlist",
    "title": "posterior_interval",
    "section": "",
    "text": "Er liefert einen Vorhersagewert aus der Posteriori-Verteilung.\nEr liefert ein Vorhersageintervall aus der Posteriori-Verteilung.\nEr liefert ein 90%-Vorhersageintervall aus der Posteriori-Verteilung.\nEr liefert ein 95%-Vorhersageintervall aus der Posteriori-Verteilung.\nEr liefert ein HDI-Vorhersageinterval aus der Posteriori-Verteilung."
  },
  {
    "objectID": "posts/posterior_interval/posterior_interval.html#answerlist-1",
    "href": "posts/posterior_interval/posterior_interval.html#answerlist-1",
    "title": "posterior_interval",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nWahr\nFalsch\nFalsch\n\n\nCategories:\n\nbayes\nregression\npost"
  },
  {
    "objectID": "posts/mtcars-rope1/mtcars-rope1.html",
    "href": "posts/mtcars-rope1/mtcars-rope1.html",
    "title": "mtcars-rope1",
    "section": "",
    "text": "Exercise\nIm Datensatz mtcars: Ist der (mittlere) Unterschied im Spritverbrauch zwischen den beiden Stufen von vs vernachlässigbar klein?\nDefinieren Sie “vernachlässigbar klein” mit “höchstens eine Meile”.\n\nGeben Sie die Breite des 95% PI an (im Bezug zur gesuchten Größe).\nGeben Sie das 95% HDI an (im Bezug zur gesuchten Größe).\nIm Hinblick auf die Rope-Methode: Ist der Unterschied vernachlässigbar klein? (ja/nein/unentschieden)\n\nHinweise:\n\nVerwenden Sie ansonsten die Standardwerte (Defaults) der typischen (im Unterricht verwendeten) R-Funktionen.\nRunden Sie auf 2 Dezimalstellen.\nVerwenden Sie Methoden der Bayes-Statistik.\n\n         \n\n\nSolution\nSetup:\n\ndata(mtcars)\nlibrary(tidyverse)\nlibrary(rstanarm)\nlibrary(bayesplot)  # Histogramm-Plots für Post-Vert.\nlibrary(bayestestR)  # rope\n\nModell berechnen:\n\nm1 &lt;- stan_glm(mpg ~ vs, data = mtcars,\n               refresh = 0)\n\n\ncoef(m1)\n\n(Intercept)          vs \n  16.620952    7.921868 \n\n\nzu a)\n95%-PI:\n\npost_m1_vs &lt;- posterior_interval(m1, prob = .95,\n                   pars = \"vs\")\npost_m1_vs[1]\n\n[1] 4.623953\n\npost_m1_vs[2]\n\n[1] 11.14271\n\n\nBreite des Intervalls:\n\nbreite &lt;- post_m1_vs[2] - post_m1_vs[1]\nbreite &lt;- breite %&gt;% round(2)\nbreite\n\n[1] 6.52\n\n\nDie Antwort für a) lautet also 6.52.\n\nmcmc_areas(m1)\n\n\n\n\n\n\n\n\nzu b)\nWir nutzen den Befehl hdi() aus {bayestestR}.\n\nhdi(m1)\n\nHighest Density Interval\n\nParameter   |        95% HDI\n----------------------------\n(Intercept) | [14.35, 18.67]\nvs          | [ 4.68, 11.19]\n\n\nMit dem Schalter ci = .89 bekäme man bspw. ein 89%-Intervall (s. Hilfe für den Befehl).\n“hdi” und “hdpi” und “hpdi” sind synonym.\n\nggplot(mtcars) +\n  aes(x = vs, y = mpg) +\n  geom_point()+\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nzu c)\n\nrope(m1,range = c(-1,1))\n\n# Proportion of samples inside the ROPE [-1.00, 1.00]:\n\nParameter   | inside ROPE\n-------------------------\n(Intercept) |      0.00 %\nvs          |      0.00 %\n\n\n\nplot(rope(m1, range = c(-1,1)))\n\n\n\n\n\n\n\n\nWir verwerfen also die H0-Rope.\n\nCategories:\n\nbayes\nlm"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-07/Verteilungen-Quiz-07.html",
    "href": "posts/Verteilungen-Quiz-07/Verteilungen-Quiz-07.html",
    "title": "Verteilungen-Quiz-07",
    "section": "",
    "text": "Ist folgende Aussage wahr?\nSei \\(X \\sim N(100,15)\\), dann ist \\(Pr(X \\le 100) \\ne 1/2\\).\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-07/Verteilungen-Quiz-07.html#answerlist",
    "href": "posts/Verteilungen-Quiz-07/Verteilungen-Quiz-07.html#answerlist",
    "title": "Verteilungen-Quiz-07",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-07/Verteilungen-Quiz-07.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-07/Verteilungen-Quiz-07.html#answerlist-1",
    "title": "Verteilungen-Quiz-07",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/stan_glm_prioriwerte/stan_glm_prioriwerte.html",
    "href": "posts/stan_glm_prioriwerte/stan_glm_prioriwerte.html",
    "title": "stan_glm_prioriwerte",
    "section": "",
    "text": "Berechnet man eine Posteriori-Verteilung mit stan_glm(), so kann man entweder die schwach informativen Prioriwerte der Standardeinstellung verwenden, oder selber Prioriwerte definieren.\nBetrachten Sie dazu dieses Modell:\nstan_glm(price ~ cut, data = diamonds, \n                   prior = normal(location = c(100, 100, 100, 100),\n                                  scale = c(10, 10, 10, 10)),\n                   prior_intercept = normal(3000, 500))\nBeziehen Sie sich auf den Datensatz diamonds.\nHinweise:\n\nGehen Sie davon aus, dass die Post-Verteilung von Intercept und Gruppeneffekte normalverteilt sind.\n\nWelche Aussage dazu passt (am besten)?\n\n\n\nEs wird für (genau) einen Parameter eine Priori-Verteilung definiert.\nFür das Regressionsgewicht \\(\\beta_1\\) sind negative Werte apriori plausibel.\nMit prior = normal() werden Gruppenmittelwerte definiert.\nAlle Parameter des Modells sind normalverteilt."
  },
  {
    "objectID": "posts/stan_glm_prioriwerte/stan_glm_prioriwerte.html#answerlist",
    "href": "posts/stan_glm_prioriwerte/stan_glm_prioriwerte.html#answerlist",
    "title": "stan_glm_prioriwerte",
    "section": "",
    "text": "Es wird für (genau) einen Parameter eine Priori-Verteilung definiert.\nFür das Regressionsgewicht \\(\\beta_1\\) sind negative Werte apriori plausibel.\nMit prior = normal() werden Gruppenmittelwerte definiert.\nAlle Parameter des Modells sind normalverteilt."
  },
  {
    "objectID": "posts/stan_glm_prioriwerte/stan_glm_prioriwerte.html#answerlist-1",
    "href": "posts/stan_glm_prioriwerte/stan_glm_prioriwerte.html#answerlist-1",
    "title": "stan_glm_prioriwerte",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch. Es gibt mehrere Parameter im Modell (Achsenabschnitt, 4 Prädiktoren, sigma)\nWahr. Für cutGood sind negative Werte plausibel.\nFalsch. prior = normal() werden Regressionskoeffizienten in ihren Prioris definiert.\nFalsch. sigma ist in Stans Voreinstellung exponentialverteilt.\n\n\nCategories:\n\nbayes\nregression"
  },
  {
    "objectID": "posts/nasa03/nasa03.html",
    "href": "posts/nasa03/nasa03.html",
    "title": "nasa03",
    "section": "",
    "text": "Exercise\nViele Quellen berichten Klimadaten unserer Erde, z.B. auch National Aeronautics and Space Administration - Goddard Institute for Space Studies.\nVon dieser Quelle beziehen wir diesen Datensatz.\nDie Datensatz sind auf der Webseite wie folgt beschrieben:\nTables of Global and Hemispheric Monthly Means and Zonal Annual Means\nCombined Land-Surface Air and Sea-Surface Water Temperature Anomalies (Land-Ocean Temperature Index, L-OTI)\nThe following are plain-text files in tabular format of temperature anomalies, i.e. deviations from the corresponding 1951-1980 means.\n\nGlobal-mean monthly, seasonal, and annual means, 1880-present, updated through most recent month: TXT, CSV\n\nStarten Sie zunächst das R-Paket tidyverse falls noch nicht geschehen.\n\nlibrary(tidyverse)\n\nImportieren Sie dann die Daten:\n\ndata_path &lt;- \"https://data.giss.nasa.gov/gistemp/tabledata_v4/GLB.Ts+dSST.csv\"\nd &lt;- read_csv(data_path, skip = 1)\n\nWir lassen die 1. Zeile des Datensatzes aus (Argument skip), da dort Metadaten stehen, also keine Daten, sondern Informationen (Daten) zu den eigentlichen Daten.\nAufgaben\n\nErstellen Sie für jeden Januar eine Variable (temp_is_above), die ausgibt, ob die Temperatur über oder unter dem Durchschnitt liegt (d.h. eine negative oder positive Abweichung ist). Nutzen Sie als Ausprägungen die Werte “yes” und “no”.\nErstellen Sie dann eine Variable, die das Jahrhundert angibt (19. JH. vs. 20 JH).\nZählen Sie dann wie oft temp_is_above “yes” aufweist pro Jahrhundert (“erhöhter Temperatur”).\nBerechnen Sie das Odds Ratio (Chancenverhältnis) von erhöhter Temperatur (vs. nicht erhöhter Temperatur) zwischen dem 19. und dem 20. Jahrhundert.\n\nFür “Wenn-Dann-Abfragen” eignet sich folgender R-Befehl (als “Pseudocode” dargestellt):\n\nd %&gt;% \n  mutate(neue_spalte = case_when(\n    erste_bedingung_bzw_wenn_teil ~ dann_teil1,\n    zweite_bedingung_bzw_zweiter_wenn_teil ~ dann_teil2\n  ))\n\n         \n\n\nSolution\ntemp_is_above erstellen:\n\nd &lt;-\n  d %&gt;% \n  mutate(temp_is_above = case_when(\n    Jan &gt; 0 ~ \"yes\",\n    Jan &lt;= 0 ~ \"no\"\n  ))\n\nJahrhundert berechnen:\n\nd &lt;-\n  d %&gt;% \n  mutate(century = case_when(\n    Year &lt; 1900 ~ \"19th\",\n    Year &gt;= 1900 ~ \"20th\"\n  ))\n\nErhöhte Werte der Januar-Temperatur pro Jahrhundert berechnen:\n\nd_summarized &lt;- \nd %&gt;% \n  group_by(century) %&gt;% \n  count(temp_is_above)\n\nd_summarized\n\n# A tibble: 4 × 3\n# Groups:   century [2]\n  century temp_is_above     n\n  &lt;chr&gt;   &lt;chr&gt;         &lt;int&gt;\n1 19th    no               19\n2 19th    yes               1\n3 20th    no               56\n4 20th    yes              67\n\n\nDer Befehl count() zählt aus, wie häufig die Ausprägungen der angegebenen Variablen X sind, m.a.W. er gibt die Verteilung von X wieder.\nEs macht vermutlich Sinn, noch die Anteile (relative Häufigkeiten) zu den absoluten Häufigkeiten zu ergänzen:\n\nd_summarized %&gt;% \n  mutate(prop = n / sum(n))\n\n# A tibble: 4 × 4\n# Groups:   century [2]\n  century temp_is_above     n  prop\n  &lt;chr&gt;   &lt;chr&gt;         &lt;int&gt; &lt;dbl&gt;\n1 19th    no               19 0.95 \n2 19th    yes               1 0.05 \n3 20th    no               56 0.455\n4 20th    yes              67 0.545\n\n\nOdds Ratio berechnen:\nWir bezeichnen mit c19 (für “Chance 1”) das Verhältnis von erhöhter Temperatur zu nicht erhöhter Temperatur im 19. Jahrhundert.\n\nc19 &lt;- 1 / 19\n\nMit c20 bezeichnen wir die analoge Chance für das 20. Jahrhundert:\n\nc20 &lt;- 56 / 67\n\nDas Verhältnis der beiden Chancen gibt das Chancenverhältnis (Odds Ratio, OR):\n\nc19 / c20\n\n[1] 0.06296992\n\n\nGenauso gut kann man das OR von c20 zu c19 ausrechnen, der Effekt bleibt identisch:\n\nc20 / c19\n\n[1] 15.8806\n\n\nIn beiden Fällen ist es ein Faktor von knapp 16.\n\nCategories:\n\ndata\neda"
  },
  {
    "objectID": "posts/Pfad/Pfad.html",
    "href": "posts/Pfad/Pfad.html",
    "title": "Pfad",
    "section": "",
    "text": "Aufgabe\nRecherchieren Sie den Datensatz “Palmer Penguins” als CSV-Datei im Internet.\n\nImportieren Sie die Datendatei in R von einer geeigneten Online-Quelle.\nLaden Sie die Datendatei herunter, speichern Sie Sie in den Ordner Ihres aktuellen RStudio-Projekts. Dann importieren Sie die Datendatei in R von diesem Ort.\n\n         \n\n\nLösung\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nAd 1)\n\npenguins_url &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\"\n\nd &lt;- read_csv(penguins_url)\n\n\n\n\n\n\n\n\n\n\n...1\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n1\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\n2\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\n4\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n2007\n\n\n5\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\n6\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007\n\n\n\n\n\n\n\nAlternativ (hier aber nicht verlangt) können Sie den Datensatz penguins auch über ein R-Paket beziehen:\n\ndata(penguins, package = \"palmerpenguins\")\n\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n2 Adelie  Torgersen           39.5          17.4         186    3800 fema…  2007\n3 Adelie  Torgersen           40.3          18           195    3250 fema…  2007\n4 Adelie  Torgersen           NA            NA            NA      NA &lt;NA&gt;   2007\n5 Adelie  Torgersen           36.7          19.3         193    3450 fema…  2007\n6 Adelie  Torgersen           39.3          20.6         190    3650 male   2007\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n\nSynonym:\n\nlibrary(palmerpenguins)\ndata(penguins)\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n2 Adelie  Torgersen           39.5          17.4         186    3800 fema…  2007\n3 Adelie  Torgersen           40.3          18           195    3250 fema…  2007\n4 Adelie  Torgersen           NA            NA            NA      NA &lt;NA&gt;   2007\n5 Adelie  Torgersen           36.7          19.3         193    3450 fema…  2007\n6 Adelie  Torgersen           39.3          20.6         190    3650 male   2007\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n\nAchtung:\nWenn Sie das Paket palmerpenguins nicht mit library() gestartet haben, dann wird data(penguins) nicht funktionieren.\nAd 2)\nWenn Sie die Datei heruntergeladen haben und in Ihrem (aktuellen) RStudio-Projektordner abgespeichert haben, dann (und nur dann) können Sie sie ohne Angabe eines Pfades in R importieren:\n\nd &lt;- read_csv(\"penguins.csv\")  # die Datei muss im aktuellen Verzeichnis liegen\n\n\nCategories:\n\nR\npath\ndatawrangling\nqm1\nqm2\nstring"
  },
  {
    "objectID": "posts/rope4/rope4.html",
    "href": "posts/rope4/rope4.html",
    "title": "rope4",
    "section": "",
    "text": "Einer der (bisher) größten Studien der Untersuchung psychologischer Konsequenzen (oder Korrelate) der Covid-Zeit ist die Studie COVIDiStress.\nIm Folgenden sollen Sie folgende Forschungsfrage untersuchen:\nForschungsfrage:\nIst der Unterschied zwischen Männern und Frauen (Dem_gender) im Hinblick zum Zusammenhang von Stress (PSS10_avg, AV) und Neurotizismus (neu, UV) vernachlässigbar klein?\nDen Datensatz können Sie so herunterladen (Achtung, groß):\n\nosf_d_path &lt;- \"https://osf.io/cjxua/?action=download\"\n\nd &lt;- read_csv(osf_d_path)\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nHinweise:\n\nSie benötigen einen Computer, um diese Aufgabe zu lösen.\nVerwenden Sie die statistischen Methoden, die im Unterricht behandelt wurden.\nVerwenden Sie Ansätze aus der Bayes-Statistik zur Lösung dieser Aufgabe.\nBei der Variable für Geschlecht können Sie sich auf Fälle begrenzen, die Männer und Frauen umfassen.\nWandeln Sie die die Variable für Geschlecht in eine binäre Variable - also Werte mit 0 und 1 - um.\nAlle Daten (und weitere Informationen) zum Projekt sind hier abgelegt.\nEine Beschreibung der Variablen der Studie finden Sie hier.\nFixieren Sie die Zufallszahlen auf den Startwert 42.\n\nAntwortoptionen:\n\n\n\nJa\nNein\nDie Daten sind nicht konkludent; es ist keine Entscheidung möglich.\nAuf Basis der bereitgestellten Informationen ist keine Entscheidung möglich."
  },
  {
    "objectID": "posts/rope4/rope4.html#answerlist",
    "href": "posts/rope4/rope4.html#answerlist",
    "title": "rope4",
    "section": "",
    "text": "Ja\nNein\nDie Daten sind nicht konkludent; es ist keine Entscheidung möglich.\nAuf Basis der bereitgestellten Informationen ist keine Entscheidung möglich."
  },
  {
    "objectID": "posts/rope4/rope4.html#answerlist-1",
    "href": "posts/rope4/rope4.html#answerlist-1",
    "title": "rope4",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\nFalsch\nFalsch\n\n\nCategories:\n\nrope\nbayes"
  },
  {
    "objectID": "posts/Histogramm-in-Boxplot/Histogramm-in-Boxplot.html",
    "href": "posts/Histogramm-in-Boxplot/Histogramm-in-Boxplot.html",
    "title": "Histogramm-in-Boxplot",
    "section": "",
    "text": "Histogramm:\n\n\n\n\n\n\n\n\n\nBoxplots:\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n[[4]]\n\n\n\n\n\n\n\n\n\n\n[[5]]\n\n\n\n\n\n\n\n\n\n\n\n\nBoxplot A\nBoxplot B\nBoxplot C\nBoxplot D\nBoxplot E"
  },
  {
    "objectID": "posts/Histogramm-in-Boxplot/Histogramm-in-Boxplot.html#answerlist",
    "href": "posts/Histogramm-in-Boxplot/Histogramm-in-Boxplot.html#answerlist",
    "title": "Histogramm-in-Boxplot",
    "section": "",
    "text": "Boxplot A\nBoxplot B\nBoxplot C\nBoxplot D\nBoxplot E"
  },
  {
    "objectID": "posts/Histogramm-in-Boxplot/Histogramm-in-Boxplot.html#answerlist-1",
    "href": "posts/Histogramm-in-Boxplot/Histogramm-in-Boxplot.html#answerlist-1",
    "title": "Histogramm-in-Boxplot",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nWahr\nFalsch\nFalsch\n\n\nCategories:\n\nvis\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/Regression6/Regression6.html",
    "href": "posts/Regression6/Regression6.html",
    "title": "Regression6",
    "section": "",
    "text": "Gegeben sei ein Datensatz mit folgenden Prädiktoren, wobei Studierende die Beobachtungseinheit darstellen:\n\n\\(X_1\\): Geschlecht_Frau (0: nein, 1: ja)\n\\(X_2\\): Letzte Mathenote (z-Wert)\n\\(X_3\\): Motivation-Testwert (z-Wert)\n\\(X_4\\): Interaktion von \\(X1\\) und \\(X2\\)\n\nDie vorherzusagende Variable (\\(Y\\); Kriterium) ist Gehalt nach Studienabschluss.\nFolgende Modellparameter einer Regression (Least Squares, mit lm()) seien gegeben:\n\n\\(\\beta_0: 40\\)\n\\(\\beta_1: 10\\)\n\\(\\beta_2: 1\\)\n\\(\\beta_3: 20\\)\n\\(\\beta_4: 7\\)\n\nWelche der Aussagen ist korrekt?\n\n\n\nFür einen bestimmten (festen) Wert von \\(X_2=\\) Letzte Mathenote (z-Wert) und \\(X_3=\\) Motivation-Testwert (z-Wert) gilt, dass das erwartete Gehalt im Mittel höher ist bei \\(X_1=1\\) im Vergleich zu \\(X_1=0\\), laut dem Modell.\nFür einen bestimmten (festen) Wert von \\(X_2\\)= Letzte Mathenote (z-Wert) und \\(X_3\\)= Motivation-Testwert (z-Wert) gilt, dass das erwartete Gehalt im Mittel höher ist bei \\(X_1=0\\) im Vergleich zu \\(X_1=1\\), laut dem Modell.\nDer mittlere erwartete Gehaltsunterschied \\(Y\\) zweier Personen \\(a\\) und \\(b\\), wobei bei Person \\(a\\) gilt \\(X_1=0\\) und bei Person \\(b\\) gilt \\(X_1=1\\), beträgt stets \\(40\\), laut dem Modell.\nDer mittlere erwartete Gehaltsunterschied \\(Y\\) zweier Personen \\(a\\) und \\(b\\), wobei bei Person \\(a\\) gilt \\(X_2=0\\) und bei Person \\(b\\) gilt \\(X_2=1\\), beträgt stets \\(40\\), laut dem Modell.\nDer mittlere erwartete Gehaltsunterschied von Menschen ist eine Wirkung von genau drei Ursachen: Geschlecht_Frau (0: nein, 1: ja), Letzte Mathenote (z-Wert), Motivation-Testwert (z-Wert), laut dem Modell."
  },
  {
    "objectID": "posts/Regression6/Regression6.html#answerlist",
    "href": "posts/Regression6/Regression6.html#answerlist",
    "title": "Regression6",
    "section": "",
    "text": "Für einen bestimmten (festen) Wert von \\(X_2=\\) Letzte Mathenote (z-Wert) und \\(X_3=\\) Motivation-Testwert (z-Wert) gilt, dass das erwartete Gehalt im Mittel höher ist bei \\(X_1=1\\) im Vergleich zu \\(X_1=0\\), laut dem Modell.\nFür einen bestimmten (festen) Wert von \\(X_2\\)= Letzte Mathenote (z-Wert) und \\(X_3\\)= Motivation-Testwert (z-Wert) gilt, dass das erwartete Gehalt im Mittel höher ist bei \\(X_1=0\\) im Vergleich zu \\(X_1=1\\), laut dem Modell.\nDer mittlere erwartete Gehaltsunterschied \\(Y\\) zweier Personen \\(a\\) und \\(b\\), wobei bei Person \\(a\\) gilt \\(X_1=0\\) und bei Person \\(b\\) gilt \\(X_1=1\\), beträgt stets \\(40\\), laut dem Modell.\nDer mittlere erwartete Gehaltsunterschied \\(Y\\) zweier Personen \\(a\\) und \\(b\\), wobei bei Person \\(a\\) gilt \\(X_2=0\\) und bei Person \\(b\\) gilt \\(X_2=1\\), beträgt stets \\(40\\), laut dem Modell.\nDer mittlere erwartete Gehaltsunterschied von Menschen ist eine Wirkung von genau drei Ursachen: Geschlecht_Frau (0: nein, 1: ja), Letzte Mathenote (z-Wert), Motivation-Testwert (z-Wert), laut dem Modell."
  },
  {
    "objectID": "posts/Regression6/Regression6.html#answerlist-1",
    "href": "posts/Regression6/Regression6.html#answerlist-1",
    "title": "Regression6",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr. \\(X_1\\) ist positiv. Daher hat Gruppe \\(X_1=1\\) höhere erwartete Werte als \\(X_1=0\\).\nFalsch. Diese Option sagt das Gegenteil wie die fast gleich lautende (aber richtige) Antwortoption.\nFalsch. Der Unterschied in der AV ist von mehreren UV abhängig. Bei Kenntnis des Wertes nur einer UV kann nicht sicher auf den erwarteten Wert der AV geschlossen werden.\nFalsch. Der Unterschied in der AV ist von mehreren UV abhängig. Bei Kenntnis des Wertes nur einer UV kann nicht sicher auf den erwarteten Wert der AV geschlossen werden.\nFalsch. Ein Regressionsmodell ist nicht automatisch ein Kausalmodell.\n\n\nCategories:\n\ndyn\nregression\nexam-22"
  },
  {
    "objectID": "posts/dplyr-uebersetzen/dplyr-uebersetzen.html",
    "href": "posts/dplyr-uebersetzen/dplyr-uebersetzen.html",
    "title": "dplyr-uebersetzen",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\nAufgabe\nImportieren Sie den folgenden Datensatz in R:\n\nmtcars &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/datasets/mtcars.csv\")\n\nÜbersetzen Sie dann die folgende R-Sequenz ins Deutsche:\n\nmtcars %&gt;% \n  drop_na() %&gt;% \n  select(mpg, hp, cyl) %&gt;% \n  filter(hp &gt; 100, cyl &gt;= 6) %&gt;% \n  group_by(cyl) %&gt;% \n  summarise(mpg_mean = mean(mpg))\n\n# A tibble: 2 × 2\n    cyl mpg_mean\n  &lt;dbl&gt;    &lt;dbl&gt;\n1     6     19.7\n2     8     15.1\n\n\n         \n\n\nLösung\nHey R:\n\nNimm den Datensatz mtcars UND DANN\nhau alle Zeilen raus, in denen es fehlende Werte gibt UND DANN\nwähle (selektiere) die folgenden Spalten: Spritverbrauch, PS, Zylinder UND DANN\nfilter Autos mit mehr als 100 PS und mit mindestens 6 Zylindern UND DANN\ngruppiere nach der Zahl der Zylinder UND DANN\nfasse den Verbrauch zum Mittelwert zusammen.\n\n\nCategories:\n\ndatawrangling\ntidyverse\nstring"
  },
  {
    "objectID": "posts/rope3/rope3.html",
    "href": "posts/rope3/rope3.html",
    "title": "rope3",
    "section": "",
    "text": "Einer der (bisher) größten Studien der Untersuchung psychologischer Konsequenzen (oder Korrelate) der Covid-Zeit ist die Studie COVIDiStress.\nIm Folgenden sollen Sie folgende Forschungsfrage untersuchen:\nIst der Zusammenhang von Stress (PSS10_avg, AV) und Neurotizismus (neu, UV) vernachlässigbar klein?\nDen Datensatz können Sie so herunterladen (Achtung, groß):\n\nosf_d_path &lt;- \"https://osf.io/cjxua/?action=download\"\n\nd &lt;- read_csv(osf_d_path)\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nHinweise:\n\nSie benötigen einen Computer, um diese Aufgabe zu lösen.\nVerwenden Sie die statistischen Methoden, die im Unterricht behandelt wurden.\nVerwenden Sie Ansätze aus der Bayes-Statistik zur Lösung dieser Aufgabe.\nBei der Variable für Geschlecht können Sie sich auf Fälle begrenzen, die Männer und Frauen umfassen.\nWandeln Sie die die Variable für Geschlecht in eine binäre Variable - also Werte mit 0 und 1 - um.\nAlle Daten (und weitere Informationen) zum Projekt sind hier abgelegt.\nEine Beschreibung der Variablen der Studie finden Sie hier.\nDas Codebook findet sich hier.\nDer Datensatz ist recht groß (ca. 150 MB).\nFixieren Sie die Zufallszahlen auf den Startwert 42.\nBerechnen Sie 89%-PIs, wenn Sie Ungewissheit quantifizieren.\n\nAntwortoptionen\n\n\n\nJa\nNein\nDie Daten sind nicht konkludent; es ist keine Entscheidung möglich.\nAuf Basis der bereitgestellten Informationen ist keine Entscheidung möglich."
  },
  {
    "objectID": "posts/rope3/rope3.html#answerlist",
    "href": "posts/rope3/rope3.html#answerlist",
    "title": "rope3",
    "section": "",
    "text": "Ja\nNein\nDie Daten sind nicht konkludent; es ist keine Entscheidung möglich.\nAuf Basis der bereitgestellten Informationen ist keine Entscheidung möglich."
  },
  {
    "objectID": "posts/rope3/rope3.html#answerlist-1",
    "href": "posts/rope3/rope3.html#answerlist-1",
    "title": "rope3",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr. ROPE ist zu verwerfen, damit sind Werte um die Null herum nicht wahrscheinlich.\nFalsch\nFalsch\n\n\nCategories:\n\nrope\nbayes"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-13/Verteilungen-Quiz-13.html",
    "href": "posts/Verteilungen-Quiz-13/Verteilungen-Quiz-13.html",
    "title": "Verteilungen-Quiz-13",
    "section": "",
    "text": "Ist folgende Aussage wahr?\nBei rechtsschiefen Verteilungen gilt \\(\\bar{x} \\gt Md\\).\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-13/Verteilungen-Quiz-13.html#answerlist",
    "href": "posts/Verteilungen-Quiz-13/Verteilungen-Quiz-13.html#answerlist",
    "title": "Verteilungen-Quiz-13",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-13/Verteilungen-Quiz-13.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-13/Verteilungen-Quiz-13.html#answerlist-1",
    "title": "Verteilungen-Quiz-13",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/Schiefe-erkennen/Schiefe-erkennen.html",
    "href": "posts/Schiefe-erkennen/Schiefe-erkennen.html",
    "title": "Schiefe-erkennen",
    "section": "",
    "text": "Wählen Sie das Histogramm, welches am deutlichsten die Eigenschaft “linksschief” aufweist!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE"
  },
  {
    "objectID": "posts/Schiefe-erkennen/Schiefe-erkennen.html#answerlist",
    "href": "posts/Schiefe-erkennen/Schiefe-erkennen.html#answerlist",
    "title": "Schiefe-erkennen",
    "section": "",
    "text": "A\nB\nC\nD\nE"
  },
  {
    "objectID": "posts/Schiefe-erkennen/Schiefe-erkennen.html#answerlist-1",
    "href": "posts/Schiefe-erkennen/Schiefe-erkennen.html#answerlist-1",
    "title": "Schiefe-erkennen",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\neda\ndistributions\nschoice"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-14/Verteilungen-Quiz-14.html",
    "href": "posts/Verteilungen-Quiz-14/Verteilungen-Quiz-14.html",
    "title": "Verteilungen-Quiz-14",
    "section": "",
    "text": "Ist folgende Aussage wahr?\nBeim Perzentilintervall (PI) werden “links” und “rechts” die gleiche Wahrscheinlichkeitsmasse von einer Verteilung “abgeschnitten”.\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-14/Verteilungen-Quiz-14.html#answerlist",
    "href": "posts/Verteilungen-Quiz-14/Verteilungen-Quiz-14.html#answerlist",
    "title": "Verteilungen-Quiz-14",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-14/Verteilungen-Quiz-14.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-14/Verteilungen-Quiz-14.html#answerlist-1",
    "title": "Verteilungen-Quiz-14",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/Rethink_2m7/Rethink_2m7.html",
    "href": "posts/Rethink_2m7/Rethink_2m7.html",
    "title": "Rethink_2m7",
    "section": "",
    "text": "Exercise\nThis question is taken from McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Ed.). Taylor and Francis, CRC Press.\n2M7. Assume again the original card problem, with a single card showing a black side face up. Before looking at the other side, we draw another card from the bag and lay it face up on the table. The face that is shown on the new card is white. Show that the probability that the first card, the one showing a black side, has black on its other side is now 0.75. Use the counting method, if you can. Hint: Treat this like the sequence of globe tosses, counting all the ways to see each observation, for each possiible first card.\n         \n\n\nSolution\nLet’s label the cards bb (black on both sides), bw (black on one, white on the other), and ww (both sides are white), respectively.\nTo keep things straigt, here’s visualization of our data.\n\n\n\n\n\n\n\n\n\nWanted is the probability \\(Pr(c1=bb|1b,2w)\\), the probability of drawing (as card 1) a bb card, given that we observerd b in the first draw, denoted as 1b, and a white card in the second draw, denoted as 2w.\nLet’s draw a tree diagram for easier comprehension.\n\n\n\n\n\n\nIn the diagram, the symbol “_b_w” means that black face of a the bw-card (one black, one white face) was drawn. Similarly, “_b_b” means that one (of the two) black faces of the bb-card (two black faces) was drawn.\nHere, we have to consider two cards. Let’s use this notation ww-bb for the sequence “first card is white on both sides, second card is black on both sides”.\nThe data observed is: first card has one black side, the second card has one white side, i.,e b-w.\nLooking at the tree, we realize that out of all 8 paths, 6 feature the bb card as first card:\n\\(Pr(1bb|b,w) = 6/8 = 3/4 = 0.75\\)\nwhere 1bb means “card 1 is black on both sides”, and b,w means “first draw showed a black face, and second card showed a white face”.\nIn other words, there are 8 valid paths in the tree diagram, out of which 6 belong the the hypothesis that the first card is all black.\nAs a Bayes-Grid (or “Bayes-Box”) we can depict the situation like this:\n\n\n\n\n\nHyp\nPrior\nL\nunstand_Post\nPost\n\n\n\n\nbb\n1\n6\n6\n3/4\n\n\nbw\n1\n2\n2\n1/4\n\n\n\n\n\nOr, equally valid, realizihg that there are the events of the card “bb”:\n\n\n\n\n\nHyp\nPrior\nL\nunstand_Post\nPost\n\n\n\n\nbb\n2\n3\n6\n3/4\n\n\nbw\n1\n2\n2\n1/4\n\n\n\n\n\nOr, using probability, and not counts:\n\n\n\n\n\nHyp\nPrior\nL\nunstand_Post\nPost\n\n\n\n\nbb\n2\n3/4\n6/4\n3/4\n\n\nbw\n1\n2/4\n2/4\n1/4\n\n\n\n\n\nWhenever the probability of all paths (in a tree diagram) is the same, as it is the case in the present example, we do not need to write down the probability of the path for the likelihood. It is enough to write the number of paths (of course we can if we want).\n\nCategories:\n\nprobability\nbayes"
  },
  {
    "objectID": "posts/ReThink3m2/ReThink3m2.html",
    "href": "posts/ReThink3m2/ReThink3m2.html",
    "title": "ReThink3m2",
    "section": "",
    "text": "Exercise\nNehmen wir an, wir haben 8 (Wasser-)“Treffer” (\\(W=8\\)) bei 15 Würfen (\\(N=15\\)) erhalten (wieder im Globusversuch).\n\nZiehen Sie \\(10^4\\) Stichproben aus der Posteriori-Verteilung basierend auf der Gittermethode. Gehen Sie von einer gleichverteilung Priori-Wahrscheinlichkeit aus.\nVisualisieren Sie die Verteilung der Stichproben.\nBerechnen Sie ds 90%-HDI.\n\nHinweise:\n\nBerechnen Sie eine Bayes-Box (Gittermethode).\nVerwenden Sie 1000 Gitterwerte.\nFixieren Sie die Zufallszahlen mit dem Startwert 42, d.h. set.seed(42).\nGehen Sie von einem gleichverteilten Prior aus.\n\nQuelle: McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Aufl.). Taylor and Francis, CRC Press.\n         \n\n\nSolution\n\nlibrary(tidyverse)\n\n\n\n\nPost-Verteilung berechnen:\n\np_grid &lt;- seq(from = 0, to = 1, length.out = 1000)\nprior &lt;- rep(1, 1000)\nlikelihood &lt;- dbinom(8, size = 15, prob = p_grid)\nposterior &lt;- likelihood * prior\nposterior &lt;- posterior / sum(posterior)\n\nStichproben-Postverteilung erstellen:\n\nsamples &lt;- \n  tibble(anteil_wasser = sample(p_grid, prob = posterior, size = 1e4, replace = TRUE))\n\nhead(samples)\n\n# A tibble: 6 × 1\n  anteil_wasser\n          &lt;dbl&gt;\n1         0.458\n2         0.290\n3         0.427\n4         0.610\n5         0.687\n6         0.484\n\n\n\n\n\n\nsamples %&gt;% \n  ggplot() +\n  aes(x = anteil_wasser) +\n  geom_histogram() + \n  labs(title = \"Stichproben aus der Posteriori-Verteilung\")\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(easystats)\nhdi(samples, prob = 0.9)\n\nHighest Density Interval\n\nParameter     |      95% HDI\n----------------------------\nanteil_wasser | [0.31, 0.76]\n\n\n\nCategories:\n\nbayes\npost\nprobability"
  },
  {
    "objectID": "posts/Aussagen-einfache-Regr/Aussagen-einfache-Regr.html",
    "href": "posts/Aussagen-einfache-Regr/Aussagen-einfache-Regr.html",
    "title": "Aussagen-einfache-Regr",
    "section": "",
    "text": "Im Hinblick auf die lineare Regression: Welche der folgenden Aussage passt am besten?\n\n\n\nDie einfache Regression - \\(y=\\alpha + \\beta_1x_1 + \\epsilon\\) - prüft, inwieweit zwei Variablen zusammenhängen (linear oder anderweitig).\nObwohl statistische Zusammenhänge nicht ohne Weiteres Kausalschlüsse erlauben, kann man die Regression für Vorhersagen gut nutzen.\nRegressionskoeffizienten kann man so interpretieren: “Erhöht man X um eine 1 Einheit, so steigt daraufhin Y um \\(\\beta_1\\) Einheiten” (\\(\\beta_1\\) sei der entsprechende Regressionskoeffizient).\n“Lineare Regression” bedeutet, dass z.B. keine Polynome wie \\(y= \\alpha + \\beta_1 x_1^2 + \\beta_2 x_1 + \\epsilon\\) berechnet werden dürfen, bzw. nicht zur linearen Regression zählen.\nZentrieren der Prädiktoren ist bei der linearen Regression nicht zulässig."
  },
  {
    "objectID": "posts/Aussagen-einfache-Regr/Aussagen-einfache-Regr.html#answerlist",
    "href": "posts/Aussagen-einfache-Regr/Aussagen-einfache-Regr.html#answerlist",
    "title": "Aussagen-einfache-Regr",
    "section": "",
    "text": "Die einfache Regression - \\(y=\\alpha + \\beta_1x_1 + \\epsilon\\) - prüft, inwieweit zwei Variablen zusammenhängen (linear oder anderweitig).\nObwohl statistische Zusammenhänge nicht ohne Weiteres Kausalschlüsse erlauben, kann man die Regression für Vorhersagen gut nutzen.\nRegressionskoeffizienten kann man so interpretieren: “Erhöht man X um eine 1 Einheit, so steigt daraufhin Y um \\(\\beta_1\\) Einheiten” (\\(\\beta_1\\) sei der entsprechende Regressionskoeffizient).\n“Lineare Regression” bedeutet, dass z.B. keine Polynome wie \\(y= \\alpha + \\beta_1 x_1^2 + \\beta_2 x_1 + \\epsilon\\) berechnet werden dürfen, bzw. nicht zur linearen Regression zählen.\nZentrieren der Prädiktoren ist bei der linearen Regression nicht zulässig."
  },
  {
    "objectID": "posts/Aussagen-einfache-Regr/Aussagen-einfache-Regr.html#answerlist-1",
    "href": "posts/Aussagen-einfache-Regr/Aussagen-einfache-Regr.html#answerlist-1",
    "title": "Aussagen-einfache-Regr",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch. Die lineare Regression \\(y=\\alpha + \\beta_1x_1 + \\epsilon\\) untersucht, wie die Korrelation, den Grad des linearen Zusammenhangs. Allerdings sind auch nicht-lineare Zusammenhänge von \\(y\\) und den Prädiktoren erlaubt, etwa \\(y=\\alpha + \\beta_1x_1^2 + \\beta_2x_2 + \\epsilon\\). Linear ist dabei so zu verstehen, dass \\(y\\) eine additive Funktion der Prädiktoren ist. Vielleicht wäre es daher besser, anstelle von “linearen” Modellen von “additiven” Modellen zu sprechen.\nRichtig. Für Vorhersagen ist Kenntnis einer Kausalstruktur nicht unbedingt nötig, kann aber sehr hilfreich sein.\nFalsch. Diese Interpretation suggeriert einen Kausaleffekt. Besser ist die Interpretation “Vergleicht man zwei Beobachtungen, die sich um 1 Einheit in X unterscheiden, so findet man im Durchschnitt einen Unterschied von \\(\\beta_1\\) in Y”.\nFalsch.Die Gleichung \\(y= \\alpha + \\beta_1 x_1^2 + \\beta_2 x_2 + \\epsilon\\) ist linear in ihren Summanden.\nFalsch. Zentrieren der Prädiktoren ist bei der linearen Regression zulässig und oft sinnvoll.\n\n\nCategories:\n\nregression\n‘2022’"
  },
  {
    "objectID": "posts/ReThink3m5/ReThink3m5.html",
    "href": "posts/ReThink3m5/ReThink3m5.html",
    "title": "ReThink3m5",
    "section": "",
    "text": "Exercise\nNehmen wir an, wir haben 8 (Wasser-)“Treffer” (\\(W=8\\)) bei 15 Würfen (\\(N=15\\)) erhalten (wieder im Globusversuch).\nNehmen Sie dieses Mal keine gleichverteilte Priori-Verteilung an. Stattdessen verwenden Sie einen Priori-Wert von Null solange \\(p &lt; 0.5\\) und einen konstanten Wert für \\(p \\ge 0.5\\). Diese Priori-Verteilung kodiert die Information, dass mindestens die Hälfte der Erdoberfläche mit Sicherheit aus Wasser besteht.\nFür alle folgenden Berechnungen, vergleichen Sie Ihre Ergebnisse zu der analogen Analyse mit einem konstanten (gleichverteilten) Priori-Wert!\n\nBerechnen Sie die Posteriori-Verteilung und visualisieren Sie sie. Nutzen Sie die Gittermethode.\nZiehen Sie \\(10^4\\) Stichproben aus der Posteriori-Verteilung, die Sie mit der Gittermethode erhalten haben. Berechnen Sie auf dieser Grundlage das 90%-HDI.\nBerechnen Sie die PPV für dieses Modell. Was ist die Wahrscheinlichkeit 8 von 15 Treffer zu erzielen laut dieser PPV?\nAuf Basis der aktuellen Posteriori-Wahrscheinlichkeit: Was ist die Wahrscheinlichkeit für 6 Wasser bei 9 Würfen?\n\nHinweise:\n\nBerechnen Sie eine Bayes-Box (Gittermethode).\nVerwenden Sie 1000 Gitterwerte.\nFixieren Sie die Zufallszahlen mit dem Startwert 42, d.h. set.seed(42).\nGehen Sie von einem gleichverteilten Prior aus.\n\nQuelle: McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Aufl.). Taylor and Francis, CRC Press.\n         \n\n\nSolution\n\nBerechnen Sie die Posteriori-Verteilung und visualisieren Sie sie. Nutzen Sie die Gittermethode.\n\n\nset.seed(42)\np_grid &lt;- seq(from = 0, to = 1, length.out = 1000)\nprior &lt;- case_when(\n  p_grid &lt;  0.5 ~ 0,\n  p_grid &gt;= 0.5 ~ 1)\nlikelihood &lt;- dbinom(8, size = 15, prob = p_grid)\nunstand_posterior &lt;- likelihood * prior\nposterior &lt;- unstand_posterior / sum(unstand_posterior)\n\n\ntibble(p = p_grid, \n       posterior = posterior) %&gt;%\n  ggplot(aes(x = p, y = posterior)) +\n # geom_point() +\n  geom_line() +\n  labs(x = \"Proportion Water (p)\", y = \"Posterior Density\")\n\n\n\n\n\n\n\n\n\nZiehen Sie \\(10^4\\) Stichproben aus der Posteriori-Verteilung, die Sie mit der Gittermethode erhalten haben. Berechnen Sie auf dieser Grundlage das 90%-HDI.\n\n\nlibrary(easystats)\n# Stichproben (samples) aus der Posteriori-Verteilung:\nsamples &lt;- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE)\nhdi(samples, prob = 0.9)\n\n95% HDI: [0.50, 0.75]\n\n\n\nBerechnen Sie die PPV für dieses Modell. Was ist die Wahrscheinlichkeit 8 von 15 Treffer zu erzielen laut dieser PPV?\n\n\nPPV &lt;-\n  tibble(w = rbinom(1e4, size = 15, prob = samples))  # w wie Wasser\n\nPPV %&gt;% \n  count(w == 8) %&gt;% \n  mutate(prop = n/sum(n))\n\n# A tibble: 2 × 3\n  `w == 8`     n  prop\n  &lt;lgl&gt;    &lt;int&gt; &lt;dbl&gt;\n1 FALSE     8508 0.851\n2 TRUE      1492 0.149\n\n\n\nAuf Basis der aktuellen Posteriori-Wahrscheinlichkeit: Was ist die Wahrscheinlichkeit für 6 Wasser bei 9 Würfen?\n\n\nPPV &lt;-\n  PPV %&gt;% \n  mutate(w2 = rbinom(1e4, size = 9, prob = samples))\n\nPPV %&gt;% \n  count(w2 == 6) %&gt;% \n  mutate(prop = n/sum(n))\n\n# A tibble: 2 × 3\n  `w2 == 6`     n  prop\n  &lt;lgl&gt;     &lt;int&gt; &lt;dbl&gt;\n1 FALSE      7738 0.774\n2 TRUE       2262 0.226\n\n\n\nCategories:\n\nbayes\nppv\nprobability"
  },
  {
    "objectID": "posts/wrangle4/wrangle4.html",
    "href": "posts/wrangle4/wrangle4.html",
    "title": "wrangle4",
    "section": "",
    "text": "Welche Variante der folgenden Syntax-Beispiele ist richtig (formal korrekt)?\n\n\n\nfilter(flights, month = 1, day = 1)\nfilter(flights, day == 1)\nfilter(month == 1, day == 1)\nfilter(month = 1, day == 1)\nfilter(flights, month == 1, day == 1)"
  },
  {
    "objectID": "posts/wrangle4/wrangle4.html#answerlist",
    "href": "posts/wrangle4/wrangle4.html#answerlist",
    "title": "wrangle4",
    "section": "",
    "text": "filter(flights, month = 1, day = 1)\nfilter(flights, day == 1)\nfilter(month == 1, day == 1)\nfilter(month = 1, day == 1)\nfilter(flights, month == 1, day == 1)"
  },
  {
    "objectID": "posts/wrangle4/wrangle4.html#answerlist-1",
    "href": "posts/wrangle4/wrangle4.html#answerlist-1",
    "title": "wrangle4",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nFalsch\nWahr\n\n\nCategories:\n\neda\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/diamonds-nullhyp-mws/diamonds-nullhyp-mws.html",
    "href": "posts/diamonds-nullhyp-mws/diamonds-nullhyp-mws.html",
    "title": "diamonds-nullhyp-mws",
    "section": "",
    "text": "Betrachten Sie folgende Ausgabe eines Bayesmodells, das mit rstanarm “gefittet” wurde:\nstan_glm\n family:       gaussian [identity]\n formula:      price ~ cut\n observations: 53940\n predictors:   5\n------\n             Median MAD_SD\n(Intercept)  4358.6  100.7\ncutGood      -431.4  112.4\ncutIdeal     -901.9  104.3\ncutPremium    226.7  105.4\ncutVery Good -375.2  103.9\n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 3964.2   11.8\nWelche Aussage passt (am besten)?\nHinweise:\n\nMit “Nullhypothese” ist im Folgenden dieser Ausdruck gemeint: \\(\\mu_1 = \\mu_2 = \\ldots = \\mu_k\\).\nGehen Sie davon aus, dass die Posteriori-Verteilungen der Regressionskoeffizienten normalverteilt sind.\nBeziehen Sie sich bei den Antworten auf die oben dargestellten Daten.\n\n\n\n\nDie Nullhypothese ist (sicher) falsch und muss daher verworfen werden.\nDie Nullhypothese ist (sicher) wahr und muss daher beibehalten werden.\nMan kann schließen, dass beim Parameter von cutGood der Wert Null außerhalb des 95%-PI der Posteriori-Verteilung liegt.\nMan kann schließen, dass alle Parameter positiv sind."
  },
  {
    "objectID": "posts/diamonds-nullhyp-mws/diamonds-nullhyp-mws.html#answerlist",
    "href": "posts/diamonds-nullhyp-mws/diamonds-nullhyp-mws.html#answerlist",
    "title": "diamonds-nullhyp-mws",
    "section": "",
    "text": "Die Nullhypothese ist (sicher) falsch und muss daher verworfen werden.\nDie Nullhypothese ist (sicher) wahr und muss daher beibehalten werden.\nMan kann schließen, dass beim Parameter von cutGood der Wert Null außerhalb des 95%-PI der Posteriori-Verteilung liegt.\nMan kann schließen, dass alle Parameter positiv sind."
  },
  {
    "objectID": "posts/diamonds-nullhyp-mws/diamonds-nullhyp-mws.html#answerlist-1",
    "href": "posts/diamonds-nullhyp-mws/diamonds-nullhyp-mws.html#answerlist-1",
    "title": "diamonds-nullhyp-mws",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch. Streng genommen können wir nicht ganz sicher sein, ob eine Hypothese auf Basis eines Modells richtig oder falsch ist.\nFalsch. Streng genommen können wir nicht ganz sicher sein, ob eine Hypothese auf Basis eines Modells richtig oder falsch ist.\nRichtig. Mittelwert plus/minus 2 SD-Einheiten gibt bei einer Normalverteilung das 95%-ETI an.\nFalsch. cutGood hat z.B. negative Werte in seinem 95%-ETI der Postverteilung.\n\n\nCategories:\n\nbayes\nregression\nnullhypothesis"
  },
  {
    "objectID": "posts/wrangle3/wrangle3.html",
    "href": "posts/wrangle3/wrangle3.html",
    "title": "wrangle3",
    "section": "",
    "text": "Welche Aussage zu der Funktionsweise folgender Funktionen im R-Paket dplyr ist richtig?\n\nfilter\nselect\nsummarise\ncount\ngroup_by\n\n\n\n\nDas erste Argument darf nie ein Dataframe sein.\nDas erste Argument ist immer die zu analysierende Variable.\nSpaltennamen müssen mit Anführungsstrichen benannt werden.\nEs wird immer eine Tabelle ausgegeben.\nFunktionsnamen sind (zumeist) nicht als Verben formuliert."
  },
  {
    "objectID": "posts/wrangle3/wrangle3.html#answerlist",
    "href": "posts/wrangle3/wrangle3.html#answerlist",
    "title": "wrangle3",
    "section": "",
    "text": "Das erste Argument darf nie ein Dataframe sein.\nDas erste Argument ist immer die zu analysierende Variable.\nSpaltennamen müssen mit Anführungsstrichen benannt werden.\nEs wird immer eine Tabelle ausgegeben.\nFunktionsnamen sind (zumeist) nicht als Verben formuliert."
  },
  {
    "objectID": "posts/wrangle3/wrangle3.html#answerlist-1",
    "href": "posts/wrangle3/wrangle3.html#answerlist-1",
    "title": "wrangle3",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nWahr\nFalsch\n\n\nCategories:\n\ndatawrangling\neda\nschoice"
  },
  {
    "objectID": "posts/Bsp-Binomial/Bsp-Binomial.html",
    "href": "posts/Bsp-Binomial/Bsp-Binomial.html",
    "title": "Bsp-Binomial",
    "section": "",
    "text": "Exercise\nDie Binomialverteilung wird in Lehrbüchern häufig mit Münzwürfen motiviert. Im Buch Statistical Rethinking muss etwa ein Globus herhalten (also ein Zufallsexperiment mit den Ergebnissen Wasser und Land unter dem Zeigefinger). Dahinter steht als theoretisches Konzept die Binomialverteilung.\nDie Beispiele sind ja gut und schön. Aber was hat das mit der Praxis zu tun? Gute Frage. Nennen Sie Beispiele aus Berufsfeldern der Sozialwissenschaften, für die die Binomialverteilung relevant ist.\nSie müssen nichts rechnen, nur Beispiele nennen.\n         \n\n\nSolution\nZur Erinnerung: Die Inferenzstatistik macht Aussagen bzgl. einer Population, nicht einer Stichprobe. Solche Aussagen sind ungewiss, also mit einer Unsicherheit behaftet, da wir nicht die ganze Population kennen. Aber die Daten der Stichprobe werden als Grundlage der Schätzung herangezogen.\n\nAuswahl geeigneter Kandidatis in einem Assessment-Verfahren. Man hat \\(n=40\\) Bewerbis, und die Wahrscheinlichkeit geeigneter Kandidatis liege bei \\(p=10%\\). Welche Spannweite an geeigneten Bewerbis kann man erwarten?\nSocial Influencing. Sie posten 100 Videoclips; davon werden 9 viral. Welche Spannweite plausibler Werte für eine Erfolgsquote kann man zugrunde legen?\nApp-Wartung. Sie prüfen eine Anzahl (\\(n=42\\)) alter Apps, aus einer früheren Kampagne. Sie finden, dass \\(k=19\\) noch funktionieren. Welche Quote an “technisch veraltet” muss man in der Population erwarten, und in welchem Bereich könnte sich diese Quote bewegen?\nSchulungsprogramm. Sie entwickeln ein Schulungsprogramm, das im großen Stil in einer Firma eingesetzt werden soll; mehrere Tausend Personen sollen das Programm durchlaufen. In einer Pilotstudie mit \\(n=90\\) Personen erreichen \\(k=42\\) nicht das Lernziel. Welche Parameterwerte für \\(p\\) (Lernziel erreicht) sind plausibel?\n\n\nCategories:\n\nprobability\nbinomial\nexample"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-15/Verteilungen-Quiz-15.html",
    "href": "posts/Verteilungen-Quiz-15/Verteilungen-Quiz-15.html",
    "title": "Verteilungen-Quiz-15",
    "section": "",
    "text": "Ist folgende Aussage wahr?\nDas HDI schneidet auf beiden Seiten des Intervalls die gleiche Wahrscheinlichkeitsmasse ab.\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-15/Verteilungen-Quiz-15.html#answerlist",
    "href": "posts/Verteilungen-Quiz-15/Verteilungen-Quiz-15.html#answerlist",
    "title": "Verteilungen-Quiz-15",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-15/Verteilungen-Quiz-15.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-15/Verteilungen-Quiz-15.html#answerlist-1",
    "title": "Verteilungen-Quiz-15",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/randomdag1/randomdag1.html",
    "href": "posts/randomdag1/randomdag1.html",
    "title": "randomdag1",
    "section": "",
    "text": "Gegeben sei der DAG g (s. u.). Der DAG verfügt über \\(n = 6\\) Variablen, die als Knoten im Graph dargestellt sind und mit \\(x_1, x_2, \\ldots x_n\\) bezeichnet sind.\nWelche minimale Variablenmenge muss kontrolliert werden, um den kausalen Effekt von der UV zur AV zu identifizieren?\nUV: x2.\nAV: x4.\nHinweise:\n\nMengen sind mittels geschweifter Klammern gekennzeichnet, z.B. {x1, x2} meint die Menge mit den zwei Elementen x1 und x2.\nDie leere Menge { } bedeutet, dass keine Variable kontrolliert werden muss, um den kausalen Effekt zu identifizieren.\nAlle Variablen werden als gemessen vorausgesetzt.\nEs ist möglich, dass es keine Lösung gibt, dass es also keine Adjustierungsmenge gibt, um den kausalen Effekt zu identifizieren. Wenn dies der Fall sein sollte, wählen Sie “/”.\nEs ist möglich, dass einzelne Variablen keine Kanten besitzen, also keine Verbindung zu anderen Variablen (Knoten) haben. Diese Variablen sind dann kausal unabhängig von den übrigen Variablen.\n\n\n\n\n\n\n\n\n\n\n\n\n\n{ x1 }\n{ x1, x2 }\n{ x2, x3 }\n{ x1, x5 }\n{ x2, x6 }"
  },
  {
    "objectID": "posts/randomdag1/randomdag1.html#answerlist",
    "href": "posts/randomdag1/randomdag1.html#answerlist",
    "title": "randomdag1",
    "section": "",
    "text": "{ x1 }\n{ x1, x2 }\n{ x2, x3 }\n{ x1, x5 }\n{ x2, x6 }"
  },
  {
    "objectID": "posts/randomdag1/randomdag1.html#answerlist-1",
    "href": "posts/randomdag1/randomdag1.html#answerlist-1",
    "title": "randomdag1",
    "section": "Answerlist",
    "text": "Answerlist\n\nRichtig\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ncausal\ndag"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-12/Verteilungen-Quiz-12.html",
    "href": "posts/Verteilungen-Quiz-12/Verteilungen-Quiz-12.html",
    "title": "Verteilungen-Quiz-12",
    "section": "",
    "text": "Ist folgende Aussage wahr?\nBei einer symmetrischen Verteilung gilt: \\(\\bar{x} = Md = \\text{Modus}\\).\n\n\n\nJa\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-12/Verteilungen-Quiz-12.html#answerlist",
    "href": "posts/Verteilungen-Quiz-12/Verteilungen-Quiz-12.html#answerlist",
    "title": "Verteilungen-Quiz-12",
    "section": "",
    "text": "Ja\nNein"
  },
  {
    "objectID": "posts/Verteilungen-Quiz-12/Verteilungen-Quiz-12.html#answerlist-1",
    "href": "posts/Verteilungen-Quiz-12/Verteilungen-Quiz-12.html#answerlist-1",
    "title": "Verteilungen-Quiz-12",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\n\n\nCategories:\n\ndistributions\nVerteilungen-Quiz\nprobability\nbayes\nsimulation"
  },
  {
    "objectID": "posts/ReThink3e1-7/ReThink3e1-7.html",
    "href": "posts/ReThink3e1-7/ReThink3e1-7.html",
    "title": "ReThink3e1-7",
    "section": "",
    "text": "Exercise\nErstellen Sie die Posteriori-Verteilung für den Globusversuch. Nutzen Sie dafür diese Syntax:\n\np_grid &lt;- seq( from=0 , to=1 , length.out=1000 )  # Gitterwerte\n\nprior &lt;- rep( 1 , 1000 )  # Priori-Gewichte\n\nlikelihood &lt;- dbinom( 6 , size=9 , prob=p_grid ) \n\nunstandardisierte_posterior &lt;- likelihood * prior \n\nposterior &lt;- unstandardisierte_posterior / sum(unstandardisierte_posterior)\n\n# um die Zufallszahlen festzulegen, damit wir alle die gleichen Zahlen bekommen zum Schnluss: \nset.seed(42) \n\n# Stichproben ziehen aus der Posteriori-Verteilung\nsamples &lt;- \n  tibble(\n    p = sample( p_grid , prob=posterior, size=1e4, replace=TRUE)) \n\n\nWie viel Wahrscheinlichkeitsmasse liegt unter \\(p=0.2\\)?\nWie viel Wahrscheinlichkeitsmasse liegt über \\(p=0.8\\)?\nWelcher Anteil der Posteriori-Verteilung liegt zwischen \\(p=0.2\\) und \\(p=0.8\\)?\nUnter welchem Wasseranteil \\(p\\) liegen 10% der Posteriori-Verteilung?\nÜber welchem Wasseranteil \\(p\\) liegen 10% der Posteriori-Verteilung?\nWelches schmälstes Intervall von \\(p\\) enthält 66% der Posteriori-Wahrscheinlichkeit?\nWelcher Wertebereich (synonym: Welches Intervall) von \\(p\\) enthält 66% der Posteriori-Wahrscheinlichkeit (hier wird Posteriori-Wahrscheinlichkeit synonym gebraucht zu Posteriori-Verteilung)? Wie nennt man diese Arten von Intervall?\n\nQuelle: McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Aufl.). Taylor and Francis, CRC Press.\n         \n\n\nSolution\nEs finden sich auch Lösungsvorschläge online, z.B. hier\n\nWie viel Wahrscheinlichkeitsmasse liegt unter \\(p=0.2\\)?\n\n\nsamples %&gt;% \n  count(p &lt; 0.2)\n\n# A tibble: 2 × 2\n  `p &lt; 0.2`     n\n  &lt;lgl&gt;     &lt;int&gt;\n1 FALSE      9993\n2 TRUE          7\n\n\nFast nix!\n\nWie viel Wahrscheinlichkeitsmasse liegt über \\(p=0.8\\)?\n\n\nsamples %&gt;% \n  count(p &gt; 0.8)\n\n# A tibble: 2 × 2\n  `p &gt; 0.8`     n\n  &lt;lgl&gt;     &lt;int&gt;\n1 FALSE      8842\n2 TRUE       1158\n\n\nNaja, so gut 10%!\n\nWelcher Anteil der Posteriori-Verteilung liegt zwischen \\(p=0.2\\) und \\(p=0.8\\)?\n\n\nsamples %&gt;% \n  count(p &gt; 0.2 & p &lt; 0.8) \n\n# A tibble: 2 × 2\n  `p &gt; 0.2 & p &lt; 0.8`     n\n  &lt;lgl&gt;               &lt;int&gt;\n1 FALSE                1165\n2 TRUE                 8835\n\n\nKnapp 90%!\n\nUnter welchem Wasseranteil \\(p\\) liegen 20% der Posteriori-Verteilung?\n\nEine Möglichkeit: Wir sortieren \\(p\\) der Größe nach (aufsteigend), filtern dann so, dass wir nur die ersten 20% der Zeilen behalten und schauen dann, was der größte Wert ist.\n\nsamples %&gt;% \n  arrange(p) %&gt;% \n  slice_head(prop = 0.2) %&gt;% \n  summarise(quantil_20 = max(p))\n\n# A tibble: 1 × 1\n  quantil_20\n       &lt;dbl&gt;\n1      0.517\n\n\nAndererseits: Das, was wir gerade gemacht haben, nennt man auch ein Quantil berechnen, s. auch hier. Dafür gibt’s fertige Funktionen in R, wie quantile():\n\nsamples %&gt;% \n  summarise(q_20 = quantile(p, 0.2))\n\n# A tibble: 1 × 1\n   q_20\n  &lt;dbl&gt;\n1 0.517\n\n\n\nÜber welchem Wasseranteil \\(p\\) liegen 10% der Posteriori-Verteilung?\n\n\nsamples %&gt;% \n  summarise(quantile(p, 0.9))\n\n# A tibble: 1 × 1\n  `quantile(p, 0.9)`\n               &lt;dbl&gt;\n1              0.810\n\n\nMit 90% Wahrscheinlichkeit ist der Wasseranteil höchstns bei 81%.\n\nWelches schmälstes Intervall von \\(p\\) enthält 66% der Posteriori-Wahrscheinlichkeit?\n\n\nlibrary(easystats)\nhdi(samples, ci = 0.66)\n\nHighest Density Interval\n\nParameter |      66% HDI\n------------------------\np         | [0.52, 0.79]\n\n\n\nWelcher Wertebereich von \\(p\\) enthält 66% der Posteriori-Wahrscheinlichkeit (hier wird Posteriori-Wahrscheinlichkeit syonyom gebraucht zu Posteriori-Verteilung)?\n\nWir nutzen hier die Equal-Tail-Intervall (oder Perzentilintervall genannt), da die Aufgabe keine genauen Angaben macht.\n\neti(samples, ci = 0.66)\n\nEqual-Tailed Interval\n\nParameter |      66% ETI\n------------------------\np         | [0.50, 0.77]\n\n\nEin “mittleres” 2/3-Intervall lässt 1/3 der Wahrscheinlichkeitsmasse außen vor, und zwar gleichmäßig in zwei Hälften links und rechts, also jeweils 1/6 (17%). So ein Intervall heißt Perzentilintervall. Daher synonym:\n\nsamples %&gt;% \n  summarise(PI_66 = quantile(p, prob = c(0.17, .84)))\n\n# A tibble: 2 × 1\n  PI_66\n  &lt;dbl&gt;\n1 0.501\n2 0.779\n\n\n\nCategories:\n\nbayes\nprobability\npost"
  },
  {
    "objectID": "posts/min-corr1/min-corr1.html",
    "href": "posts/min-corr1/min-corr1.html",
    "title": "min-corr1",
    "section": "",
    "text": "Aufgabe\nWelches Diagramm zeigt den schwächsten (absoluten) linearen Zusammenhang (Korrelation)?\nGeben Sie die Nummer ein, die in der Kopfzeile jedes Teildiagramms angezeigt wird.\n         \n\n\nLösung\n\n\n\n\n\n\n\n\n\n\nCategories:\n\nvis\n‘2023’\nnum"
  },
  {
    "objectID": "posts/rope2/rope2.html",
    "href": "posts/rope2/rope2.html",
    "title": "rope2",
    "section": "",
    "text": "Im Datensatz mtcars: Ist der (mittlere) Unterschied im Spritverbrauch zwischen den beiden Gruppen Automatik vs. Schaltgetriebe vernachlässigbar?\nDefinieren Sie selber, was “vernachlässigbar klein” bedeutet. Oder greifen Sie auf die Definition “höchstens eine Meile” zurück.\nPrüfen Sie rechnerisch, anhand des angegebenen Datensatzes, folgende Behauptung:\nBehauptung: “Der Unterschied ist vernachlässigbar klein!”\nWählen Sie die Antwortoption, die am besten zu der obigen Behauptung passt!\nHinweise\nAntwortoptionen:\n\n\n\nJa, die Behauptung ist korrekt.\nNein, die Behauptung ist falsch.\nDie Daten sind bzw. das Modell nicht konkludent; es ist keine Entscheidung über die Behauptung möglich.\nAuf Basis der bereitgestellten Informationen ist keine Antwort möglich."
  },
  {
    "objectID": "posts/rope2/rope2.html#answerlist",
    "href": "posts/rope2/rope2.html#answerlist",
    "title": "rope2",
    "section": "",
    "text": "Ja, die Behauptung ist korrekt.\nNein, die Behauptung ist falsch.\nDie Daten sind bzw. das Modell nicht konkludent; es ist keine Entscheidung über die Behauptung möglich.\nAuf Basis der bereitgestellten Informationen ist keine Antwort möglich."
  },
  {
    "objectID": "posts/rope2/rope2.html#answerlist-1",
    "href": "posts/rope2/rope2.html#answerlist-1",
    "title": "rope2",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nRichtig\nFalsch\nFalsch\n\n\nCategories:\n\nrope\nbayes\nregression\nexam-22"
  },
  {
    "objectID": "posts/nasa02/nasa02.html",
    "href": "posts/nasa02/nasa02.html",
    "title": "nasa02",
    "section": "",
    "text": "Aufgabe\nViele Quellen berichten Klimadaten unserer Erde, z.B. auch National Aeronautics and Space Administration - Goddard Institute for Space Studies.\nVon dieser Quelle beziehen wir diesen Datensatz.\nDie Datensatz sind auf der Webseite wie folgt beschrieben:\nTables of Global and Hemispheric Monthly Means and Zonal Annual Means\nCombined Land-Surface Air and Sea-Surface Water Temperature Anomalies (Land-Ocean Temperature Index, L-OTI)\nThe following are plain-text files in tabular format of temperature anomalies, i.e. deviations from the corresponding 1951-1980 means.\n\nGlobal-mean monthly, seasonal, and annual means, 1880-present, updated through most recent month: TXT, CSV\n\nStarten Sie zunächst das R-Paket tidyverse falls noch nicht geschehen.\n\nlibrary(tidyverse)\n\nImportieren Sie dann die Daten:\n\ndata_path &lt;- \"https://data.giss.nasa.gov/gistemp/tabledata_v4/GLB.Ts+dSST.csv\"\nd &lt;- read_csv(data_path, skip = 1)\n\nWir lassen die 1. Zeile des Datensatzes aus (Argument skip), da dort Metadaten stehen, also keine Daten, sondern Informationen (Daten) zu den eigentlichen Daten.\nAufgaben\n\nBerechnen Sie die die Korelation der Temperatur von Januar und Februar\nBerechnen Sie die die Korelation der Temperatur von Januar und Februar pro Dekade\n\nHinweise:\n\nSie müssen zuerst die Dekade als neue Spalte berechnen.\n\n         \n\n\nLösung\nDekade berechnen:\n\nd &lt;-\n  d %&gt;% \n  mutate(decade = round(Year/10))\n\nKorrelation:\n\nd %&gt;% \n  summarise(temp_cor = cor(Jan, Feb))\n\n# A tibble: 1 × 1\n  temp_cor\n     &lt;dbl&gt;\n1    0.942\n\n\nKorrelation pro Dekade:\n\nd_summarized &lt;- \n  d %&gt;% \n  group_by(decade) %&gt;% \n  summarise(temp_cor = cor(Jan, Feb))\n\n\n\n\n\n\n\n  \n    \n    \n      decade\n      temp_cor\n    \n  \n  \n    188\n0.87\n    189\n0.79\n    190\n0.51\n    191\n0.84\n    192\n0.83\n    193\n0.72\n    194\n0.51\n    195\n0.78\n    196\n0.55\n    197\n0.78\n    198\n0.80\n    199\n0.54\n    200\n0.56\n    201\n0.66\n    202\n0.93\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nAlternativ können Sie zum Visualisieren der Daten z.B. das Paket ggpubr nutzen:\n\nlibrary(ggpubr)\nggscatter(d_summarized, x = \"decade\", y = \"temp_cor\", add = \"reg.line\")\n\n\n\n\n\n\n\n\nDie Korrelation der Temperaturen und damit die Ähnlichkeit der Muster hat im Laufe der Dekaden immer mal wieder geschwankt.\nFalls Sie Teile der R-Syntax nicht kennen: Machen Sie sich nichts daraus. 😄\n\nCategories:\n\ndata\neda\nlagemaße\nstring"
  },
  {
    "objectID": "posts/Likelihood-identifizieren/Likelihood-identifizieren.html",
    "href": "posts/Likelihood-identifizieren/Likelihood-identifizieren.html",
    "title": "Likelihood-identifizieren",
    "section": "",
    "text": "Welche Zeile der folgenden Modellspezifikation zeigt den Likelihood?\n\\[\n\\begin{align}\n\\text{height}_i &\\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta \\cdot  \\text{weight}_i\\\\\n\\alpha &\\sim \\operatorname{Normal}(178, 20)\\\\\n\\beta &\\sim \\operatorname{Normal}(5,3)\\\\\n\\sigma &\\sim \\operatorname{Exp}(0.1)\n\\end{align}\n\\]\nZeile …\n\n\n\n1\n2\n3\n4\n5"
  },
  {
    "objectID": "posts/Likelihood-identifizieren/Likelihood-identifizieren.html#answerlist",
    "href": "posts/Likelihood-identifizieren/Likelihood-identifizieren.html#answerlist",
    "title": "Likelihood-identifizieren",
    "section": "",
    "text": "1\n2\n3\n4\n5"
  },
  {
    "objectID": "posts/Likelihood-identifizieren/Likelihood-identifizieren.html#answerlist-1",
    "href": "posts/Likelihood-identifizieren/Likelihood-identifizieren.html#answerlist-1",
    "title": "Likelihood-identifizieren",
    "section": "Answerlist",
    "text": "Answerlist\n\nRichtig\nFalsch. Lineares Modell.\nFalsch. Prior Achsenabschnitt.\nFalsch. Prior Regressiongewicht.\nFalsch. Prior Streuung der AV.\n\n\nCategories:\n\nregression\nbayes\nlikelihood"
  },
  {
    "objectID": "posts/tidymodels-ames-01/tidymodels-ames-01.html",
    "href": "posts/tidymodels-ames-01/tidymodels-ames-01.html",
    "title": "tidymodels-ames-01",
    "section": "",
    "text": "Aufgabe\nBerechnen Sie ein lineares Modell mit tidymodels und zwar anhand des ames Datensatzes.\nModellgleichung: Sale_Price ~ Gr_Liv_Area, data = ames.\nBerechnen Sie ein multiplikatives (exponenzielles) Modell.\nGesucht ist R-Quadrat als Maß für die Modellgüte im Train-Sample.\nHinweise:\n\nFixieren Sie die Zufallszahlen auf den Startwert 42.\n\n         \n\n\nLösung\n\nlibrary(tidymodels)\ndata(ames)\n\nMultiplikatives Modell:\n\names &lt;- \n  ames %&gt;% \n  mutate(Sale_Price = log10(Sale_Price))\n\nDatensatz aufteilen:\n\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\nModell definieren:\n\nm1 &lt;-\n  linear_reg() # engine ist \"lm\" im Default\n\nModell fitten:\n\nfit1 &lt;-\n  m1 %&gt;% \n  fit(Sale_Price ~ Gr_Liv_Area, data = ames)\n\n\nfit1 %&gt;% pluck(\"fit\") \n\n\nCall:\nstats::lm(formula = Sale_Price ~ Gr_Liv_Area, data = data)\n\nCoefficients:\n(Intercept)  Gr_Liv_Area  \n  4.8552133    0.0002437  \n\n\nModellgüte im Train-Sample:\n\nfit1_performance &lt;-\n  fit1 %&gt;% \n  extract_fit_engine()  # identisch zu pluck(\"fit\")\n\nModellgüte:\n\nfit1_performance %&gt;% summary()\n\n\nCall:\nstats::lm(formula = Sale_Price ~ Gr_Liv_Area, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.02587 -0.06577  0.01342  0.07202  0.39231 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 4.855e+00  7.355e-03  660.12   &lt;2e-16 ***\nGr_Liv_Area 2.437e-04  4.648e-06   52.43   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1271 on 2928 degrees of freedom\nMultiple R-squared:  0.4842,    Adjusted R-squared:  0.484 \nF-statistic:  2749 on 1 and 2928 DF,  p-value: &lt; 2.2e-16\n\n\nR-Quadrat via easystats:\n\nlibrary(easystats)\nfit1_performance %&gt;% r2()  # rmse()\n\n# R2 for Linear Regression\n       R2: 0.484\n  adj. R2: 0.484\n\n\n\ntidy(fit1_performance)  # ähnlich zu parameters()\n\n# A tibble: 2 × 5\n  term        estimate  std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept) 4.86     0.00736        660.        0\n2 Gr_Liv_Area 0.000244 0.00000465      52.4       0\n\n\n\nsol &lt;- 0.484\n\n\nCategories:\n\nds1\ntidymodels\nprediction\nyacsda\nstat-learning\nnum"
  },
  {
    "objectID": "posts/wrangle5/wrangle5.html",
    "href": "posts/wrangle5/wrangle5.html",
    "title": "wrangle5",
    "section": "",
    "text": "Wie prüft man in R auf Gleichheit zweier Ausdrücke?\nWählen Sie die korrekte Aussage.\n\n\n\n!= – “definiert als”\n== Prüfung auf Gleichheit\n= – Prüfung auf Gleichheit\n&lt;- – Prüfung auf Gleichheit\n!= – Prüfung auf Gleichheit"
  },
  {
    "objectID": "posts/wrangle5/wrangle5.html#answerlist",
    "href": "posts/wrangle5/wrangle5.html#answerlist",
    "title": "wrangle5",
    "section": "",
    "text": "!= – “definiert als”\n== Prüfung auf Gleichheit\n= – Prüfung auf Gleichheit\n&lt;- – Prüfung auf Gleichheit\n!= – Prüfung auf Gleichheit"
  },
  {
    "objectID": "posts/wrangle5/wrangle5.html#answerlist-1",
    "href": "posts/wrangle5/wrangle5.html#answerlist-1",
    "title": "wrangle5",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\neda\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/wozu-balkendiagramm/wozu-balkendiagramm.html",
    "href": "posts/wozu-balkendiagramm/wozu-balkendiagramm.html",
    "title": "wozu-balkendiagramm",
    "section": "",
    "text": "Zu welchem Zweck ist ein Balkendiagramm am besten geeignet?\n\n\n\nUm Mittelwerte zwischen zwei oder mehr Gruppen darzustellen.\nUm Häufigkeiten darzustellen.\nUm metrische Verteilungen darzustellen.\nUm metrische Zusammenhänge darzustellen."
  },
  {
    "objectID": "posts/wozu-balkendiagramm/wozu-balkendiagramm.html#answerlist",
    "href": "posts/wozu-balkendiagramm/wozu-balkendiagramm.html#answerlist",
    "title": "wozu-balkendiagramm",
    "section": "",
    "text": "Um Mittelwerte zwischen zwei oder mehr Gruppen darzustellen.\nUm Häufigkeiten darzustellen.\nUm metrische Verteilungen darzustellen.\nUm metrische Zusammenhänge darzustellen."
  },
  {
    "objectID": "posts/wozu-balkendiagramm/wozu-balkendiagramm.html#answerlist-1",
    "href": "posts/wozu-balkendiagramm/wozu-balkendiagramm.html#answerlist-1",
    "title": "wozu-balkendiagramm",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\nFalsch\nFalsch\n\n\nCategories:\n\nvis\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/Kung-height/Kung-height.html",
    "href": "posts/Kung-height/Kung-height.html",
    "title": "Kung-height",
    "section": "",
    "text": "Exercise\nBetrachten Sie den Datensatz zur Größe der !Kung:\n\nlibrary(tidyverse)\nurl_kung &lt;- \"https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/Howell1.csv\"\nd &lt;-\n  read_delim(url_kung, delim = \";\")  # Strichpunkt als Trennzeichen in der CSV-Datei\n\n\nUntersuchen Sie mit Hilfe eines Diagramms, ob bzw. inwieweit sich die Größe der erwachsenen Personen normalverteilt.\nKennzahlen, die angegeben, inwieweit sich eine Größe normalverteilt, sind Schiefe und Kurtosis. Die Schiefe gibt an, wie symmetrische eine Verteilung ist.\n\nNormalverteilungen sind symmetrisch und haben daher einen Wert von 0 für Schiefe. Kurtosis gibt die “Wölbung”, also wie “spitz” oder “plattgedrückt” eine Verteilung ist. Eine Normalverteilung hat eine Wert von 3 für Kurtosis.\nEntsprechende R-Funktionen finden Sie z.B. im Paket moments. Berechnen Sie die beiden Kennzahlen für die Gruppe der Erwachsenen sowie aufgeteilt nach dem Geschlecht. Interpretieren Sie das Ergebnis.\n\nDiskutieren Sie, inwieweit man aus biologisch fundierten Sachverhalten (also ontologisch) eine Normalverteilung der Körpergröße annehmen kann.\n\n         \n\n\nSolution\n\nVisuelle Prüfung der Normalverteilung\n\n\nd2 &lt;- d %&gt;% \n  filter(age &gt;= 18)\n\nd3 &lt;- d2 %&gt;% \n  select(-male)\n\nggplot(d2, aes(x = height)) +\n  geom_density()\n\nggplot(d2, aes(x = height )) +\n  facet_wrap(~ male) +\n    geom_density()\n\nggplot(d2, aes(x = height)) +\n  facet_wrap(~ male) +\n  geom_histogram(data = d3, fill = \"grey60\", alpha = .6) +\n    geom_histogram() +\n  labs(caption = \"Grau hinterlegt ist das Histogramm für die Daten über beide Geschlechter\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSchiefe und Kurtosis\n\n\nlibrary(easystats)\nd2 %&gt;%  skewness()\n\nParameter | Skewness |    SE\n----------------------------\nheight    |    0.151 | 0.129\nweight    |    0.132 | 0.129\nage       |    0.665 | 0.129\nmale      |    0.126 | 0.129\n\nd2 %&gt;% kurtosis()\n\nParameter | Kurtosis |    SE\n----------------------------\nheight    |   -0.483 | 0.256\nweight    |   -0.506 | 0.256\nage       |   -0.213 | 0.256\nmale      |   -1.996 | 0.256\n\n\n\nNormalverteilung, Begründung\n\nEs ist plausibel anzunehmen, dass der Phänotyp Körpergröße das Resultat des (kausalen) Einflusses vieler Gene ist, vieler Gene, die über einen vergleichbar starken Einfluss verfügen.\nEine besondere Situation stellt das X- bzw. Y-Chromosom dar, das Gene zum Geschlecht bereitstellt. Das Geschlecht ist ein einzelner Faktor, der (erfahrungsgemäß) einen relativ großen Einfluss auf die Körpergröße hat (in Anbetracht, dass vielleicht Tausende Gene additiv die Größe bestimmen). Insofern ist eine klarere Annäherung an die Normalverteilung zu erwarten, wenn man die Geschlechter einzeln betrachtet.\n\nCategories:\n\nbayes\nppv\nprobability"
  },
  {
    "objectID": "posts/ReThink3m4/ReThink3m4.html",
    "href": "posts/ReThink3m4/ReThink3m4.html",
    "title": "ReThink3m4",
    "section": "",
    "text": "Exercise\nNehmen wir an, wir haben 8 (Wasser-)“Treffer” (\\(W=8\\)) bei 15 Würfen (\\(N=15\\)) erhalten (wieder im Globusversuch).\nBerechnen Sie auf Basis dieser Posteriori-Verteilung (8 Treffer bei 15 Würfen) die Wahrscheinlichkeit für 6 Wasser bei 9 Würfen (\\(W=6, N=9\\)).\nHinweise:\n\nBerechnen Sie eine Bayes-Box (Gittermethode).\nVerwenden Sie 1000 Gitterwerte.\nFixieren Sie die Zufallszahlen mit dem Startwert 42, d.h. set.seed(42).\nGehen Sie von einem gleichverteilten Prior aus.\n\nQuelle: McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Aufl.). Taylor and Francis, CRC Press.\n         \n\n\nSolution\nErstellen wir zuerst wieder die Posteriori-Verteilung für den Globusversuch.\n\np_grid &lt;- seq( from=0 , to=1 , length.out=1000 )  # Gitterwerte\n\nprior &lt;- rep( 1 , 1000 )  # Priori-Gewichte\n\nset.seed(42)\nlikelihood &lt;- dbinom( 6 , size=9 , prob=p_grid ) \n\nunstandardisierte_posterior &lt;- likelihood * prior \n\nposterior &lt;- unstandardisierte_posterior / sum(unstandardisierte_posterior)\n\nDann ziehen wir unsere Stichproben daraus:\n\n# um die Zufallszahlen festzulegen, damit alle die gleichen Zufallswerte bekommen: \nset.seed(100) \n\n# Stichproben ziehen aus der Posteriori-Verteilung\nsamples &lt;- \n  tibble(\n    p = sample( p_grid , prob=posterior, size=1e4, replace=TRUE)) \n\nJetzt erstellen wir die PPV für einen anderen Versuch, nämlich mit 9 Zügen:\n\nPPV &lt;-\n  samples %&gt;% \n  mutate(anzahl_wasser2 = rbinom(1e4, size = 9, prob = p))\n\nSchließlich zählen wir, wie oft 6 Treffer beobachtet werden:\n\nPPV %&gt;% \n  count(anzahl_wasser2 == 6) \n\n# A tibble: 2 × 2\n  `anzahl_wasser2 == 6`     n\n  &lt;lgl&gt;                 &lt;int&gt;\n1 FALSE                  8059\n2 TRUE                   1941\n\n\nQuelle\n\nCategories:\n\nbayes\nppv\nprobability"
  },
  {
    "objectID": "posts/Rethink_2m1/Rethink_2m1.html",
    "href": "posts/Rethink_2m1/Rethink_2m1.html",
    "title": "Rethink_2m1",
    "section": "",
    "text": "Exercise\nThis question is taken from McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Ed.). Taylor and Francis, CRC Press.\n2M1. Recall the globe tossing model from the chapter. Compute and plot the grid approximate posterior distribution for each of the following sets of observations. In each case, assume a uniform prior for p.\n\nWWW\nWWWL\nLWWLWWW\n\n         \n\n\nSolution\nThe solution is taken from this source.\n\nlibrary(tidyverse)\n\ndist &lt;- \n  tibble(\n    # Gridwerte bestimmen:\n    p_grid = seq(from = 0, to = 1, length.out = 20),\n    # Priori-Wskt bestimmen:\n    prior = rep(1, times = 20)) %&gt;%\n  mutate(\n    # Likelihood berechnen:\n    likelihood_1 = dbinom(3, size = 3, prob = p_grid),  # WWW\n    likelihood_2 = dbinom(3, size = 4, prob = p_grid),  # WWWL\n    likelihood_3 = dbinom(5, size = 7, prob = p_grid),  # LWWLWWW\n    # unstand. Posterior-Wskt:\n    unstand_post_1 = likelihood_1 * prior,\n    unstand_post_2 = likelihood_2 * prior,\n    unstand_post_3 = likelihood_3 * prior,\n    # stand. Post-Wskt:\n    std_post_1 = unstand_post_1 / sum(unstand_post_1),\n    std_post_2 = unstand_post_2 / sum(unstand_post_2),\n    std_post_3 = unstand_post_3 / sum(unstand_post_3)\n    ) \n\nJetzt können wir das Diagramm zeichnen:\n\nggplot(dist) +\n  aes(x = p_grid, y= std_post_1) +\n  geom_line()+\n  geom_point() +\n  labs(x = \"p(W)\",\n       y = \"Posteriori-Wahrscheinlichkeit\",\n       title = \"Daten: WWW\")\n\n\n\n\n\n\n\nggplot(dist) +\n  aes(x = p_grid, y= std_post_2) +\n  geom_line()+\n  geom_point() +\n  labs(x = \"p(W)\",\n       y = \"Posteriori-Wahrscheinlichkeit\",\n       title = \"Daten: WWWL\")\n\n\n\n\n\n\n\nggplot(dist) +\n  aes(x = p_grid, y= std_post_3) +\n  geom_line()+\n  geom_point() +\n  labs(x = \"p(W)\",\n       y = \"Posteriori-Wahrscheinlichkeit\",\n       title = \"Daten: LWWLWWW\")\n\n\n\n\n\n\n\n\nEtwas eleganter (und komplizierter) kann man es auch so in R schreiben (Quelle):\n\nlibrary(tidyverse)\n\ndist &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = 20),\n               prior = rep(1, times = 20)) %&gt;%\n  mutate(likelihood_1 = dbinom(3, size = 3, prob = p_grid),\n         likelihood_2 = dbinom(3, size = 4, prob = p_grid),\n         likelihood_3 = dbinom(5, size = 7, prob = p_grid),\n         across(starts_with(\"likelihood\"), ~ .x * prior),\n         across(starts_with(\"likelihood\"), ~ .x / sum(.x))) %&gt;%\n  pivot_longer(cols = starts_with(\"likelihood\"), names_to = \"pattern\",\n               values_to = \"posterior\") %&gt;%\n  separate(pattern, c(NA, \"pattern\"), sep = \"_\", convert = TRUE) %&gt;%\n  mutate(obs = case_when(pattern == 1L ~ \"W, W, W\",\n                         pattern == 2L ~ \"W, W, W, L\",\n                         pattern == 3L ~ \"L, W, W, L, W, W, W\"))\n\nggplot(dist, aes(x = p_grid, y = posterior)) +\n  facet_wrap(vars(fct_inorder(obs)), nrow = 1) +\n  geom_line() +\n  geom_point() +\n  labs(x = \"Proportion Water (p)\", y = \"Posterior Density\")\n\n\n\n\n\n\n\n\n\nCategories:\n\nprobability\nbayes-grid"
  },
  {
    "objectID": "posts/movies-vis1/movies-vis1.html",
    "href": "posts/movies-vis1/movies-vis1.html",
    "title": "movies-vis1",
    "section": "",
    "text": "Aufgabe\nImportieren Sie bitte für diese Aufgabe den Datensatz movies (aus dem R-Paket ggplot2movies). Ein Data-Dictionary findet sich hier.\nErstellen Sie folgende Visualisierung:\n\nStreudiagramme mit rating als Y-Variable, und alle übrigen metrischen Variablen als X-Variable.\nLassen Sie aber folgende Variablen außen vor: etwaige ID-Variablen, die Variablen, die die Perzentile der Bewertungen angeben (rX, mit X von 1 bis 10)\nBerücksichtigen Sie nur Actionfilme ab 2000\nVerzichten Sie auf Filme mit einer unterdurchschnittlichen Zahl an Bewertungen (votes; gemessen an allen Filmen, gerundet zur nächsten ganzen Zahl)\n\n         \n\n\nLösung\nPakete starten:\n\nlibrary(tidyverse)\nlibrary(DataExplorer)\n\nDaten importieren:\n\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/ggplot2movies/movies.csv\"\nd &lt;- read.csv(d_path)\n\nDurchschnittliche Zahl an Bewertungen:\n\nd %&gt;% \n  summarise(votes_mean = mean(votes))\n\n  votes_mean\n1   632.1304\n\n\nDie durchschnittliche Zahl an Bewertungen beträgt also 632.\n\nd %&gt;% \n  select(length, budget, rating, year, votes, Action) %&gt;% \n  filter(year &gt;= 2000) %&gt;% \n  filter(Action == 1) %&gt;% \n  filter(votes &gt;= 632) %&gt;% \n  select(-Action) %&gt;% \n  plot_scatterplot(by = \"rating\")\n\nWarning: Removed 66 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nCategories:\n\nvis\neda\nstring"
  },
  {
    "objectID": "posts/kausal_corona_glatze/kausal_corona_glatze.html",
    "href": "posts/kausal_corona_glatze/kausal_corona_glatze.html",
    "title": "kausal_corona_glatze",
    "section": "",
    "text": "Exercise\nVor einiger Zeit war in der Presse Folgendes zu lesen:\n\nRecent studies linking male sex hormones to severe coronavirus infections point to a potential predictor of disease severity: baldness. International researchers studying global data on COVID-19 patients have found that in general, the more male hormones called androgens someone has, the easier it is for SARS-CoV-2 to enter and take over their immune systems. And bald men have more of these hormones than men with full manes, and women.\n\nQuelle\nAllerdings versäumten es einige (viele) der Pressemeldungen, eine belastbare Quelle, also den Forschungsartikel, auf dem dieses Befund beruhen muss, zu zitieren.\nEine mögliche Kausalhypothese für die obige Pressemitteilung ist, dass männliche Sexualhormone eine gemeinsame Ursache für Haarausfall und auch die Schwere eine Covid19-Infektion darstellen.\nAllerdings sind auch andere, skeptischere, Hypothesen denkbar. Skeptisch meint dabei, dass auf komplexere Erklärungen zugunsten einfachere verzichtet werden kann.\nSo benötigt etwa die Hypothese “Störche bringen Babies” komplexe Erkärungsmodelle; eine skeptischere (einfachere) Erklärung wäre, dass die (geringe) Urbanisierung eines Landstrichs die gemeinsame Ursache für sowohl die Häufigkeit von Störchen als auch von Babies darstellt.\nGeben Sie eine skeptische Kausalerkärung an für den Befund, dass Haarausfall und Schwere eines Coronaverlaufs assoziiert sind!.\n         \n\n\nSolution\nAlter ist sowohl die Ursache von Haarausfall (bei Männern) als auch von der Schwere eines Corona-Verlaufs. Daher sind Haarausfall (bei Männern) und die Schwerer eines Corona-Verlaufs durch das Alter konfundiert. Ohne Berücksichtigung der gemeinsamen Ursache erscheint Haarausfall mit Corona-Schwere korreliert zu sein. Nach Kontrolle der konfundierenden Variablen verschwindet die Korrelation.\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/Rethink_2m6/Rethink_2m6.html",
    "href": "posts/Rethink_2m6/Rethink_2m6.html",
    "title": "Rethink_2m6",
    "section": "",
    "text": "Exercise\nThis question is taken from McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Ed.). Taylor and Francis, CRC Press.\n2M6. Imagine that black ink is heavy, and so cards with black sides are heavier than cards with white sides. As a result, it’s less likely that a card with black sides is pulled from the bag. So again assume there are three cards: B/B, B/W, and W/W. After experimenting a number of times, you conclude that for every way to pull the B/B card from the bag, there are 2 ways to pull the B/W card and 3 ways to pull the W/W card. Again suppose that a card is pulled and a black side appears face up. Show that the probability the other side is black is now 0.5. Use the counting method, as before.\n         \n\n\nSolution\nLet’s label the cards bb (black on both sides), bw (black on one, white on the other), and ww (both sides are white), respectively.\nWanted is the probability that the second side of the card is black (2b), given one side is black (1b): \\(Pr(2b|1b)\\).\n\nd &lt;-\n  tibble::tribble(\n  ~Hyp, ~Prior,\n  \"bb\",     1L, \n  \"bw\",     2L,   \n  \"ww\",     3L, \n  ) %&gt;% \n  mutate(Likelihood = c(2,1,0),\n         unstand_post = Prior*Likelihood,\n         std_post = unstand_post / sum(unstand_post))\n\nd %&gt;% \n  gt() %&gt;% \n  fmt_number(columns = 5)\n\n\n\n\n\n\n\n\nHyp\nPrior\nLikelihood\nunstand_post\nstd_post\n\n\n\n\nbb\n1\n2\n2\n0.50\n\n\nbw\n2\n1\n2\n0.50\n\n\nww\n3\n0\n0\n0.00\n\n\n\n\n\n\n\nWhenever the probability of all paths (in a tree diagram) is the same, we do not need to write down the probability of the path for the likelihood. It is enough to write the number of paths.\n\nCategories:\n\nprobability\nbayes"
  },
  {
    "objectID": "posts/ReThink3m3/ReThink3m3.html",
    "href": "posts/ReThink3m3/ReThink3m3.html",
    "title": "ReThink3m3",
    "section": "",
    "text": "Exercise\nNehmen wir an, wir haben 8 (Wasser-)“Treffer” (\\(W=8\\)) bei 15 Würfen (\\(N=15\\)) erhalten (wieder im Globusversuch).\n\nFühren Sie einen Posteriori-Prädiktiv-Check durch: Erstellen Sie also eine Posteriori-Prädiktiv-Verteilung (PPV). Mit anderen Worten: Erstellen Sie die Stichprobenverteilung, gemittelt über die Posteriori-Wahrscheinlichkeiten des Wasseranteils \\(p\\)!\nVisualisieren Sie die PPV!\nWas ist die Wahrscheinlichkeit laut PPV 8 von 15 Treffer zu erzielen (also 8 Wasser in 15 Würfen)?\n\nHinweise:\n\nBerechnen Sie eine Bayes-Box (Gittermethode).\nVerwenden Sie 1000 Gitterwerte.\nGehen Sie von einem gleichverteilten Prior aus.\nFixieren Sie die Zufallszahlen mit dem Startwert 42, d.h. set.seed(42).\n\nQuelle: McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Aufl.). Taylor and Francis, CRC Press.\n         \n\n\nSolution\n\nlibrary(tidyverse)\n\n\n\n\nErstellen wir zuerst wieder die Posteriori-Verteilung für den Globusversuch.\n\np_grid &lt;- seq( from=0 , to=1 , length.out=1000 )  # Gitterwerte\n\nprior &lt;- rep(1, 1000 )  # Priori-Gewichte\n\nlikelihood &lt;- dbinom(8 , size= 15, prob=p_grid ) \n\nunstandardisierte_posterior &lt;- likelihood * prior \n\nposterior &lt;- unstandardisierte_posterior / sum(unstandardisierte_posterior)\n\nDann ziehen wir unsere Stichproben daraus:\n\n# um die Zufallszahlen festzulegen, damit alle die gleichen Zufallswerte bekommen: \nset.seed(42) \n\n# Stichproben ziehen aus der Posteriori-Verteilung\nsamples &lt;- \n  tibble(\n    p = sample(p_grid , prob=posterior, size=1e4, replace=TRUE))\n\n\nPPV &lt;- \n  samples %&gt;% \n  mutate( anzahl_wasser = rbinom(1e4, size = 15, prob = p))\n\nDurch prob = p gewichten wir die Wahrscheinlichkeit an den Werten der Posteriori-Verteilung.\nSo sehen die ersten paar Zeilen von PPV aus:\n\n\n\n\n\n\n\n\n\np\nanzahl_wasser\n\n\n\n\n0.4304304\n4\n\n\n0.5575576\n11\n\n\n0.6516517\n4\n\n\n0.6156156\n9\n\n\n0.6716717\n6\n\n\n\n\n\n\n\n\n\n\n\nPPV %&gt;% \n  ggplot() +\n  aes(x = anzahl_wasser) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n\nPPV %&gt;% \n  count(anzahl_wasser == 8)\n\n# A tibble: 2 × 2\n  `anzahl_wasser == 8`     n\n  &lt;lgl&gt;                &lt;int&gt;\n1 FALSE                 8536\n2 TRUE                  1464\n\n\nAlternativer R-Code:\n\nw &lt;- rbinom(1e4, size = 15, prob = samples$p)\nmean(w == 8)\n\n[1] 0.1504\n\n\nQuelle\n\nCategories:\n\nbayes\nppv\nprobability"
  },
  {
    "objectID": "posts/kausal25/kausal25.html",
    "href": "posts/kausal25/kausal25.html",
    "title": "kausal25",
    "section": "",
    "text": "Gegeben sei der DAG g (s. u.). Der DAG verfügt über 6 Variablen, die als Knoten im Graph dargestellt sind und mit \\(x_1, x_2, \\ldots x_n\\) bezeichnet sind.\nWelche minimale Variablenmenge muss kontrolliert werden, um den kausalen Effekt von der UV zur AV zu identifizieren?\nUV: x4.\nAV: x5.\nHinweise:\n\nMengen sind mittels geschweifter Klammern gekennzeichnet, z.B. {x8, x9} meint die Menge mit den zwei Elementen x8 und x9.\nDie leere Menge { } bedeutet, dass keine Variable kontrolliert werden muss, um den kausalen Effekt zu identifizieren.\nAlle Variablen werden als gemessen vorausgesetzt.\nEs ist möglich, dass es keine Lösung gibt, dass es also keine Adjustierungsmenge gibt, um den kausalen Effekt zu identifizieren. Wenn dies der Fall sein sollte, wählen Sie “/”.\nEs ist möglich, dass einzelne Variablen keine Kanten besitzen, also keine Verbindung zu anderen Variablen (Knoten) haben.\n\n\n\n\n\n\n\n\n\n\n\n\n\n{ x1, x4 }\n{ x4, x5 }\n{ }\n/\n{ x3, x4 }"
  },
  {
    "objectID": "posts/kausal25/kausal25.html#answerlist",
    "href": "posts/kausal25/kausal25.html#answerlist",
    "title": "kausal25",
    "section": "",
    "text": "{ x1, x4 }\n{ x4, x5 }\n{ }\n/\n{ x3, x4 }"
  },
  {
    "objectID": "posts/kausal25/kausal25.html#answerlist-1",
    "href": "posts/kausal25/kausal25.html#answerlist-1",
    "title": "kausal25",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nRichtig\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/zwert-berechnen/zwert-berechnen.html",
    "href": "posts/zwert-berechnen/zwert-berechnen.html",
    "title": "zwert-berechnen",
    "section": "",
    "text": "Exercise\nSei \\(X \\sim \\mathcal{N}(42, 7)\\) und \\(x_1 = 28\\).\nBerechnen Sie den z-Wert für \\(x_1\\)!\nHinweis:\n\nRunden Sie ggf. auf die nächste ganze Zahl.\n\n         \n\n\nSolution\n\nx1_z = (x1 - x_mw) / x_sd\n\n-2\n\nCategories:\n\nz-value\nR\nmath"
  },
  {
    "objectID": "posts/Skalenniveau1b/Skalenniveau1b.html",
    "href": "posts/Skalenniveau1b/Skalenniveau1b.html",
    "title": "Skalenniveau1b",
    "section": "",
    "text": "Geben Sie für jede Variable an, ob sie über ein metrisches Skalenniveau verfügt!\n\n\n\nTemperatur in Celcius\nHaarfarbe\nNationalität\nAugenfarbe"
  },
  {
    "objectID": "posts/Skalenniveau1b/Skalenniveau1b.html#answerlist",
    "href": "posts/Skalenniveau1b/Skalenniveau1b.html#answerlist",
    "title": "Skalenniveau1b",
    "section": "",
    "text": "Temperatur in Celcius\nHaarfarbe\nNationalität\nAugenfarbe"
  },
  {
    "objectID": "posts/Skalenniveau1b/Skalenniveau1b.html#answerlist-1",
    "href": "posts/Skalenniveau1b/Skalenniveau1b.html#answerlist-1",
    "title": "Skalenniveau1b",
    "section": "Answerlist",
    "text": "Answerlist\n\nRichtig\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndyn\nscale-level\nvariable-niveau\nmchoice"
  },
  {
    "objectID": "posts/tidymodels-penguins01/tidymodels-penguins01.html",
    "href": "posts/tidymodels-penguins01/tidymodels-penguins01.html",
    "title": "tidymodels-penguins01",
    "section": "",
    "text": "Aufgabe\nBerechnen Sie ein lineares Modell mit tidymodels und zwar anhand des penguins Datensatzes.\nModellgleichung: body_mass_g ~ bill_length_mm, data = d_train.\nGesucht ist R-Quadrat als Maß für die Modellgüte im TEST-Sample.\nHinweise:\n\nFixieren Sie die Zufallszahlen auf den Startwert 42.\nNutzen Sie eine v=5,r=1 CV.\nEntfernen Sie fehlende Werte in den Variablen.\nVerzichten Sie auf weitere Schritte der Vorverarbeitung.\n\n         \n\n\nLösung\nSetup:\n\nlibrary(tidymodels)\nlibrary(tictoc)  # Rechenzeit messen, optional\n# data(penguins, package = \"palmerpenguins\")\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/modeldata/penguins.csv\"\nd &lt;- read.csv(d_path)\n\nDatensatz aufteilen:\n\nset.seed(42)\nd_split &lt;- initial_split(penguins)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\nWorkflow:\n\nrec1 &lt;-\n  recipe(body_mass_g ~ bill_length_mm, data = d_train) %&gt;% \n  step_naomit(all_numeric())\n\nlm_mod &lt;-\n  linear_reg()\n\nwflow &lt;-\n  workflow() %&gt;%\n  add_recipe(rec1) %&gt;%\n  add_model(lm_mod)\n\nwflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_naomit()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nBacken:\n\nd_baked &lt;- prep(rec1) %&gt;% bake(new_data = NULL)\nd_baked %&gt;% head()\n\n# A tibble: 6 × 2\n  bill_length_mm body_mass_g\n           &lt;dbl&gt;       &lt;int&gt;\n1           36          3450\n2           50.9        3675\n3           46.1        4500\n4           45.8        4150\n5           48.6        5800\n6           39          3650\n\n\nAuf NA prüfen:\n\nsum(is.na(d_baked))\n\n[1] 0\n\n\nCV:\n\nset.seed(42)\nfolds &lt;- vfold_cv(d_train, v = 5)\nfolds\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits           id   \n  &lt;list&gt;           &lt;chr&gt;\n1 &lt;split [206/52]&gt; Fold1\n2 &lt;split [206/52]&gt; Fold2\n3 &lt;split [206/52]&gt; Fold3\n4 &lt;split [207/51]&gt; Fold4\n5 &lt;split [207/51]&gt; Fold5\n\n\nResampling:\n\npenguins_resamples &lt;-\n  fit_resamples(\n    wflow,\n    resamples = folds\n  )\npenguins_resamples\n\n# Resampling results\n# 5-fold cross-validation \n# A tibble: 5 × 4\n  splits           id    .metrics         .notes          \n  &lt;list&gt;           &lt;chr&gt; &lt;list&gt;           &lt;list&gt;          \n1 &lt;split [206/52]&gt; Fold1 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n2 &lt;split [206/52]&gt; Fold2 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n3 &lt;split [206/52]&gt; Fold3 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n4 &lt;split [207/51]&gt; Fold4 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n5 &lt;split [207/51]&gt; Fold5 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n\n\nLast Fit:\n\npenguins_last &lt;- last_fit(wflow, d_split)\n\nModellgüte im Test-Sample:\n\npenguins_last %&gt;% collect_metrics()\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard     652.    Preprocessor1_Model1\n2 rsq     standard       0.385 Preprocessor1_Model1\n\n\nR-Quadrat:\n\nsol &lt;-  collect_metrics(penguins_last)[[\".estimate\"]][2]\nsol\n\n[1] 0.3850608\n\n\n\nCategories:\n\nds1\ntidymodels\nprediction\nyacsda\nstat-learning\nnum"
  },
  {
    "objectID": "posts/Priorwahl2/Priorwahl2.html",
    "href": "posts/Priorwahl2/Priorwahl2.html",
    "title": "Priorwahl2",
    "section": "",
    "text": "Betrachten wir den biologisch fundierten Zusammenhang von Gewicht (UV) und Körpergröße (AV).\nWelche der folgenden Priori-Verteilungen passt am besten für \\(\\beta\\)?\nGehen Sie von z-standardisierten Variablen aus.\n\n\n\n\\(N(0,1)\\)\n\\(N(0,100)\\)\n\\(N(1,0)\\)\n\\(N(0,0)\\)\n\\(N(-1,1)\\)"
  },
  {
    "objectID": "posts/Priorwahl2/Priorwahl2.html#answerlist",
    "href": "posts/Priorwahl2/Priorwahl2.html#answerlist",
    "title": "Priorwahl2",
    "section": "",
    "text": "\\(N(0,1)\\)\n\\(N(0,100)\\)\n\\(N(1,0)\\)\n\\(N(0,0)\\)\n\\(N(-1,1)\\)"
  },
  {
    "objectID": "posts/Priorwahl2/Priorwahl2.html#answerlist-1",
    "href": "posts/Priorwahl2/Priorwahl2.html#answerlist-1",
    "title": "Priorwahl2",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr. Plausibler Prior. Bei z-standardisierten Werten sind die Koeffizienten meist kleiner 1. Noch sinnvoller wäre vermutlich, wenn \\(\\mu &gt; 0\\) und nicht \\(\\mu=0\\).\nFalsch. Zu weit.\nFalsch. Keine Streuung.\nFalsch. Keine Streuung.\nFalsch. Negativer Mittelwert ist nicht sehr plausibel.\n\n\nCategories:\n\nregression\nbayes\ndistribution"
  },
  {
    "objectID": "posts/kausal22/kausal22.html",
    "href": "posts/kausal22/kausal22.html",
    "title": "kausal22",
    "section": "",
    "text": "Gegeben sei der DAG g (s. u.). Der DAG verfügt über mehrere Variablen, die als Knoten im Graph dargestellt sind und mit \\(x_1, x_2, \\ldots x_n\\) bezeichnet sind.\nWelche minimale Variablenmenge muss kontrolliert werden, um den kausalen Effekt von der UV zur AV zu identifizieren?\nUV: x3.\nAV: x5.\nHinweise:\n\nMengen sind mittels geschweifter Klammern gekennzeichnet, z.B. {x8, x9} meint die Menge mit den zwei Elementen x8 und x9.\nDie leere Menge { } bedeutet, dass keine Variable kontrolliert werden muss, um den kausalen Effekt zu identifizieren.\nAlle Variablen werden als gemessen vorausgesetzt.\nEs ist möglich, dass es keine Lösung gibt, dass es also keine Adjustierungsmenge gibt, um den kausalen Effekt zu identifizieren. Wenn dies der Fall sein sollte, wählen Sie “keine Lösung”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n{ x6 }\n{ x2 }\n{ x1, x2 }\n{ x5, x6 }\n{ x3, x6 }"
  },
  {
    "objectID": "posts/kausal22/kausal22.html#answerlist",
    "href": "posts/kausal22/kausal22.html#answerlist",
    "title": "kausal22",
    "section": "",
    "text": "{ x6 }\n{ x2 }\n{ x1, x2 }\n{ x5, x6 }\n{ x3, x6 }"
  },
  {
    "objectID": "posts/kausal22/kausal22.html#answerlist-1",
    "href": "posts/kausal22/kausal22.html#answerlist-1",
    "title": "kausal22",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nRichtig\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/knn-ames01/knn-ames01.html",
    "href": "posts/knn-ames01/knn-ames01.html",
    "title": "knn-ames01",
    "section": "",
    "text": "Aufgabe\nBerechnen Sie ein knn-Modell für den Datensatz ames!\nNutzen Sie diese Modellformel: Sale_Price ~ Lot_Area + Fireplaces + Longitude + Latitude.\nBerichten Sie die Modellgüte.\nHinweise:\n\nTunen Sie \\(k\\) mit den Werten 1 bis 10.\nTeilen Sie in Train- und Test-Sample auf.\nVerwenden Sie Defaults der Funktionen, wo nicht anders angegeben.\nz-Transformieren Sie die Prädiktoren.\nVerwenden Sie den RSME als Kennzahl der Modellgüte.\n\n         \n\n\nLösung\n\nlibrary(tidymodels)\ndata(ames)\n\nDaten aufteilen:\n\nd_split &lt;- initial_split(ames)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\nModell definieren:\n\nmod1 &lt;-\n  nearest_neighbor(\n    mode = \"regression\",\n    neighbors = tune())  # k-Wert zum Tunen taggen\n\nRezept definieren:\n\nrec1 &lt;-\n  recipe(Sale_Price ~ Lot_Area + Fireplaces + Longitude + Latitude, data = d_split) %&gt;% \n  step_normalize(all_predictors())\n\nWorkflow definieren:\n\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(mod1) %&gt;% \n  add_recipe(rec1)\n\nResampling definieren:\n\ncv1 &lt;- vfold_cv(d_train)\n\nTuning definieren:\n\nk_grid &lt;-\n  tibble(neighbors = 1:10)\n\nFitting:\n\nfit1 &lt;-\n  tune_grid(wf1,\n            resamples = vfold_cv(d_train),\n            metrics = metric_set(rmse),  # nur RMSE als Modellgüte, Default ist RMSE und R2\n            grid = k_grid,\n            control = control_grid(save_workflow = TRUE)  # nur nötig für \"fit_best\", s.u.\n            )\n\nMetriken im Train-Sample (genauer: im Assessment-Sample):\n\nshow_best(fit1)\n\n# A tibble: 5 × 7\n  neighbors .metric .estimator   mean     n std_err .config              \n      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1        10 rmse    standard   44209.    10   1612. Preprocessor1_Model10\n2         9 rmse    standard   44356.    10   1605. Preprocessor1_Model09\n3         8 rmse    standard   44547.    10   1600. Preprocessor1_Model08\n4         7 rmse    standard   44774.    10   1595. Preprocessor1_Model07\n5         6 rmse    standard   45105.    10   1610. Preprocessor1_Model06\n\n\n(Komplettes) Train-Sample mit bestem Tuning-Kandidat fitten:\n\ntune1_best &lt;- fit_best(fit1)\n\nIm Test-Sample predicten:\n\nfit_test &lt;- last_fit(tune1_best, d_split)\n\nMetriken einsammeln:\n\ncollect_metrics(fit_test)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   40819.    Preprocessor1_Model1\n2 rsq     standard       0.730 Preprocessor1_Model1\n\n\nDie Lösung lautet 4.081919^{4}.\n\nCategories:\n\nstat-learning\ntidymodels\nnum"
  },
  {
    "objectID": "posts/Boxplot-Aussagen/Boxplot-Aussagen.html",
    "href": "posts/Boxplot-Aussagen/Boxplot-Aussagen.html",
    "title": "Boxplot-Aussagen",
    "section": "",
    "text": "Hinweis: Female steht für Frauen; Male für Männer.\nDiese Aufgabe bezieht sich auf den Datensatz tips (aus dem R-Paket reshape2), den Sie ggf. hier herunterladen können. Ein Data-Dictionary findet sich hier.\n\n\n\n\n\n\n\n\n\n\n\n\nDer IQR der Männer ist größer als der der Frauen.\nDer Boxplot ist schlecht geeignet, um die Verteilung mehrerer Gruppen prägnant und übersichtlich zu visualisieren.\nDer Mittelwert der Männer ist kleiner als der der Frauen.\nDie Streuung in den RANDbereichen der Frauen ist größer als die der Männer.\nBei den Männern gibt es mehr Ausreißer als bei den Frauen.\nDie Streuung in den beiden äußeren Quartilen ist bei den Frauen größer als bei den Männern.\nDie Verteilungen sind beide nicht schief.\nDie Verteilungen sind beide symmetrisch.\nAuf der X-Achse steht eine metrische (quantitative) Variable.\nAuf der Y-Achse steht eine nominale (qualitative) Variable.\nAuf der Y-Achse steht eine metrische (quantitative) Variable.\nAuf der X-Achse steht eine nominale (qualitative) Variable."
  },
  {
    "objectID": "posts/Boxplot-Aussagen/Boxplot-Aussagen.html#answerlist",
    "href": "posts/Boxplot-Aussagen/Boxplot-Aussagen.html#answerlist",
    "title": "Boxplot-Aussagen",
    "section": "",
    "text": "Der IQR der Männer ist größer als der der Frauen.\nDer Boxplot ist schlecht geeignet, um die Verteilung mehrerer Gruppen prägnant und übersichtlich zu visualisieren.\nDer Mittelwert der Männer ist kleiner als der der Frauen.\nDie Streuung in den RANDbereichen der Frauen ist größer als die der Männer.\nBei den Männern gibt es mehr Ausreißer als bei den Frauen.\nDie Streuung in den beiden äußeren Quartilen ist bei den Frauen größer als bei den Männern.\nDie Verteilungen sind beide nicht schief.\nDie Verteilungen sind beide symmetrisch.\nAuf der X-Achse steht eine metrische (quantitative) Variable.\nAuf der Y-Achse steht eine nominale (qualitative) Variable.\nAuf der Y-Achse steht eine metrische (quantitative) Variable.\nAuf der X-Achse steht eine nominale (qualitative) Variable."
  },
  {
    "objectID": "posts/Boxplot-Aussagen/Boxplot-Aussagen.html#answerlist-1",
    "href": "posts/Boxplot-Aussagen/Boxplot-Aussagen.html#answerlist-1",
    "title": "Boxplot-Aussagen",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\nFalsch\nFalsch\nFalsch\nFalsch\nFalsch\nWahr\nWahr\n\n\nCategories:\n\nvis\neda\ndyn\nschoice"
  },
  {
    "objectID": "posts/haeufigkeit01/haeufigkeit01.html",
    "href": "posts/haeufigkeit01/haeufigkeit01.html",
    "title": "haeufigkeit01",
    "section": "",
    "text": "Aufgabe\nWerten Sie die Häufigkeiten (der Stufen) folgender Variablen aus wie unten beschrieben.\nDatensatz: mtcars.\nVariablen:\n\nam\ncyl\nvs\n\n\nErstellen Sie für jede der genannten Variablen eine univariate Häufigkeitsanalyse (also nur eine Variable).\nErstellen Sie dann für die ersten beiden genannten Variablen eine gemeinsame Häufigkeitsanalyse (bivariat).\nErstellen Sie dann für alle genannten Variablen eine gemeinsame Häufigkeitsanalyse.\nWie viele Gruppen (also Häufigkeitswerte) ergeben sich (theoretisch) in der letzten Auswertung?\n\n         \n\n\nLösung\n\nlibrary(\"tidyverse\")\ndata(\"mtcars\")\n\n\nunivariate Häufigkeitsanalyse\n\n\nmtcars %&gt;% \n  group_by(am) %&gt;% \n  summarise(zeilen_n = n())\n\n# A tibble: 2 × 2\n     am zeilen_n\n  &lt;dbl&gt;    &lt;int&gt;\n1     0       19\n2     1       13\n\n\nDer Befehle n() gibt die Anzahl der Zeilen (Reihen) zurück. Da in einem Dataframe alle Zeilen gleich lang sind, brauchen wir keine Spalte anzugeben.\nAlternativ könnte man auch schreiben:\n\nmtcars %&gt;% \n  count(am)\n\n  am  n\n1  0 19\n2  1 13\n\n\nDas ist haargenau der gleiche Effekt wie die vorherige Syntax.\nÜblich ist auch, eine Kontingenztabelle so darzustellen:\n\ngemeinsame Häufigkeitsanalyse (bivariat)\n\n\nmtcars %&gt;% \n  count(am, cyl)\n\n  am cyl  n\n1  0   4  3\n2  0   6  4\n3  0   8 12\n4  1   4  8\n5  1   6  3\n6  1   8  2\n\n\n\ntable(mtcars$am, mtcars$cyl)\n\n   \n     4  6  8\n  0  3  4 12\n  1  8  3  2\n\n\nWir sehen, dass wir \\(2\\cdot3=6\\) Gruppen haben, in denen sich die \\(n=32\\) Beobachtungen aufteilen.\n\nHäufigkeitsanalyse mit 3 Variablen\n\n\nmtcars %&gt;% \n  count(am, cyl, vs)\n\n  am cyl vs  n\n1  0   4  1  3\n2  0   6  1  4\n3  0   8  0 12\n4  1   4  0  1\n5  1   4  1  7\n6  1   6  0  3\n7  1   8  0  2\n\n\nDas sind drei Variablen mit \\(2 \\cdot 3 \\cdot 2 = 12\\) Gruppen.\nDa einige der 12 Gruppen in den Daten nicht vorkommen, sind sie in der Auszählung der Häufigkeiten nicht aufgenommen; in den Daten gibt es nur 7 der 12 Gruppen.\n\nCategories:\n\ndatawrangling\neda\ncount\nstring"
  },
  {
    "objectID": "posts/wozu-streudiagramm/wozu-streudiagramm.html",
    "href": "posts/wozu-streudiagramm/wozu-streudiagramm.html",
    "title": "wozu-streudiagramm",
    "section": "",
    "text": "Zu welchem Zweck ist ein Streudiagramm am besten geeignet?\n\n\n\nUm Verteilungen einer nominalen Variablen darzustellen.\nUm Verteilungen einer metrischen Variablen darzustellen.\nUm Verteilungen einer stetigen, metrischen Variablen darzustellen.\nUm Zusammenhänge zwischen zwei nominalen Variablen darzustellen.\nUm Zusammenhänge zwischen zwei metrischen Variablen darzustellen."
  },
  {
    "objectID": "posts/wozu-streudiagramm/wozu-streudiagramm.html#answerlist",
    "href": "posts/wozu-streudiagramm/wozu-streudiagramm.html#answerlist",
    "title": "wozu-streudiagramm",
    "section": "",
    "text": "Um Verteilungen einer nominalen Variablen darzustellen.\nUm Verteilungen einer metrischen Variablen darzustellen.\nUm Verteilungen einer stetigen, metrischen Variablen darzustellen.\nUm Zusammenhänge zwischen zwei nominalen Variablen darzustellen.\nUm Zusammenhänge zwischen zwei metrischen Variablen darzustellen."
  },
  {
    "objectID": "posts/wozu-streudiagramm/wozu-streudiagramm.html#answerlist-1",
    "href": "posts/wozu-streudiagramm/wozu-streudiagramm.html#answerlist-1",
    "title": "wozu-streudiagramm",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nFalsch\nWahr\n\n\nCategories:\n\nvis\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/griech-buchstaben/griech-buchstaben.html",
    "href": "posts/griech-buchstaben/griech-buchstaben.html",
    "title": "Griech-Buchstaben-Inferenz",
    "section": "",
    "text": "Exercise\nFür Statistiken (Stichprobe) verwendet man meist lateinische Buchstaben; für Parameter (Population) verwendet man meist (die entsprechenden) griechischen Buchstaben.\nVervollständigen Sie folgende Tabelle entsprechend!\n\n\n\n\n\nKennwert\nStatistik\nParameter\n\n\n\n\nMittelwert\n\\(\\bar{X}\\)\nNA\n\n\nMittelwertsdifferenz\n\\(\\bar{X}_1-\\bar{X}_2\\)\nNA\n\n\nStreuung\nsd\nNA\n\n\nAnteil\np\nNA\n\n\nKorrelation\nr\nNA\n\n\nRegressionsgewicht\nb\nNA\n\n\n\n\n\n         \n\n\nSolution\n\n\n\n\n\nKennwert\nStatistik\nParameter\n\n\n\n\nMittelwert\n\\[\\bar{X}\\]\n\\[\\mu\\]\n\n\nMittelwertsdifferenz\n\\[d=\\bar{X}_1-\\bar{X}_2\\]\n\\[\\mu_1\\]- \\[\\mu_2\\]\n\n\nStreuung\nsd\n\\[\\sigma\\]\n\n\nAnteil\np\n\\[\\pi\\]\n\n\nKorrelation\nr\n\\[\\rho\\]\n\n\nRegressionsgewicht\nb\n\\[\\beta\\]\n\n\n\n\n\n\nCategories:\n\nqm2\nqm2-thema01"
  },
  {
    "objectID": "posts/mtcars-simple2/mtcars-simple2.html",
    "href": "posts/mtcars-simple2/mtcars-simple2.html",
    "title": "mtcars-simple2",
    "section": "",
    "text": "Exercise\nWe will use the dataset mtcars in this exercise.\nAssume your causal model of your research dictates that fuel economy is a linear function of horse power, cylinder count and displacement of the engine.\nCompute the explained variance (point estimate) for the above model!\nNotes:\n\nUse can either use frequentist or bayesian modeling.\nUse R for all computations.\nThere are multiple ways to find a solution.\n\n         \n\n\nSolution\nCompute Model:\n\nlm1_freq &lt;- lm(mpg ~ hp + cyl + disp, data = mtcars)\n\nlibrary(rstanarm)\nlm1_bayes &lt;- stan_glm(mpg ~ hp + cyl + disp, data = mtcars, refresh = 0)\n\nGet R2:\n\nlibrary(easystats)\n\n\nr2(lm1_freq)\n\n# R2 for Linear Regression\n       R2: 0.768\n  adj. R2: 0.743\n\n\n\nr2(lm1_bayes)\n\n# Bayesian R2 with Compatibility Interval\n\n  Conditional R2: 0.746 (95% CI [0.601, 0.859])\n\n\nThe coefficient is estimated as about 0.77.\n\nCategories:\n\nregresssion\nen\nbayes\nfrequentist\nqm1\nstats-nutshell"
  },
  {
    "objectID": "posts/Zwielichter-Dozent-Bayes/Zwielichter-Dozent-Bayes.html",
    "href": "posts/Zwielichter-Dozent-Bayes/Zwielichter-Dozent-Bayes.html",
    "title": "Zwielichter-Dozent-Bayes",
    "section": "",
    "text": "options(digits = 2)\n\n\nExercise\nNach einem langen Unitag machen Sie sich auf den Weg nach Hause; ihr Weg führt Sie durch eine dunkle Ecke. Just dort regt sich auf einmal eine Gestalt in den Schatten. Die Person spricht Sie an: „Na, Lust auf ein Spielchen?“. Sie willigen sofort ein. Die Person stellt sich als ein Statistiker vor, dessen Namen nichts zur Sache tue; das Gesicht kommt Ihnen vage bekannt vor. „Pass auf“, erklärt der Statistiker, „wir werfen eine Münze, ich setze auf Zahl“. Dass er auf Zahl setzt, überrascht Sie nicht. „Wenn ich gewinne“, fährt der Statistiker fort, „bekomme ich 10 Euro von Dir, wenn Du gewinnst, bekommst Du 11 Euro von mir. Gutes Spiel, oder?“. Sie einigen sich auf 10 Durchgänge, in denen der Statistiker jedes Mal eine Münze wirft, fängt und dann die oben liegende Seite prüft. Erster Wurf: Zahl! Der Statistiker gewinnt. Pech für Sie. Zweiter Wurf: Zahl! Schon wieder 10 Euro für den Statistiker. Hm. Dritter Wurf: . . . Zahl! Schon wieder. Aber kann ja passieren, bei einer fairen Münze, oder? Vierter Wurf: Zahl! Langsam regen sich Zweifel bei Ihnen. Kann das noch mit rechten Dingen zugehen? Ist die Münze fair? Insgesamt gewinnt der zwielichte Statistiker 8 von 10 Durchgängen.\nUnter leisem Gelächter des Statistikers (und mit leeren Taschen) machen Sie sich von dannen. Hat er falsch gespielt? Wie plausibel ist es, bei 10 Würfen 8 Treffer zu erhalten, wenn die Münze fair ist? Ist das ein häufiges, ein typisches Ereignis oder ein seltenes, untypisches Ereignis bei einer fairen Münze? Wenn es ein einigermaßen häufiges Ereignis sein sollte, dann spricht das für die Fairness der Münze. Zumindest spricht ein Ereignis, welches von einer Hypothese als häufig vorausgesagt wird und schließlich eintritt, nicht gegen eine Hypothese. Zuhause angekommen, denken Sie sich, jetzt müssen Sie erstmal in Ruhe die Posteriori-Verteilung und die PPV ausrechnen!\n\nBerechnen Sie die Posteriori-Verteilung mit der Gittermethode! Gehen Sie von einer gleichverteilten Priori-Wahrscheinlichkeit aus. Visualisieren Sie sie. Alle folgenden Teil-Fragen bauen auf der Post-Verteilung auf.\nWie groß ist die Wahrscheinlichkeit, auf Basis der Post-Verteilung, dass die Münze zugunsten des Dozenten gezinkt ist?\nGeben Sie das 50%-PI und 50%-HDPI zum Parameterwert (\\(p\\) der Münze) an!\nMit welcher Wahrscheinlichkeit liegt die Trefferchance der Münze zwischen \\(p=.45\\) und \\(p=.55\\), ist also nicht “nennenswert” gezinkt?\nWas ist der wahrscheinlichste Parameterwert (Trefferchance der Münze)?\nGeben Sie das 90%-PI und 90%-HDI zu Parameterwert (\\(p\\) der Münze) an!\nBerechnen Sie die PPV! Visualisieren Sie sie. Interpretieren Sie die PPV.\nDiskutieren Sie die Annahme einer Gleichverteilung des Priori-Wertes von \\(p\\)!\n\n         \n\n\nSolution\n\nBerechnen Sie die Posteriori-Verteilung mit der Gittermethode! Visualisieren Sie sie. Alle folgenden Teil-Fragen bauen auf der Post-Verteilung auf.\n\n\np_grid &lt;- seq( from=0 , to=1 , length.out=1000 )  # Gitterwerte\n\nprior &lt;- rep( 1 , 1000 )  # Priori-Gewichte\n\nlikelihood &lt;- dbinom(8, size = 10, prob=p_grid) \n\nunstandardisierte_posterior &lt;- likelihood * prior \n\nposterior &lt;- unstandardisierte_posterior / sum(unstandardisierte_posterior)\n\n# Stichproben ziehen aus der Posteriori-Verteilung:\nsamples &lt;- \n  tibble(\n    gewinnchance_muenze = sample(p_grid , prob=posterior, size=1e4, replace=TRUE))\n\nVisualisierung:\n\nsamples %&gt;% \n  ggplot() +\n  aes(x = gewinnchance_muenze) +\n  geom_histogram() +\n  labs(title = \"Posterior-Verteilung\",\n       x = \"Gewinnchance der Münze (50%: faire Münze)\")\n\n\n\n\n\n\n\n\n\nWie groß ist die Wahrscheinlichkeit, auf Basis der Post-Verteilung, dass die Münze zugunsten des Dozenten gezinkt ist?\n\n\nsamples %&gt;% \n  count(gewinnchance_muenze &gt; .5) %&gt;% \n  mutate(prop = n / sum(n))\n\n# A tibble: 2 × 3\n  `gewinnchance_muenze &gt; 0.5`     n   prop\n  &lt;lgl&gt;                       &lt;int&gt;  &lt;dbl&gt;\n1 FALSE                         322 0.0322\n2 TRUE                         9678 0.968 \n\n\n\nGeben Sie das 50%-PI (Perzentilintervall) und 50%-HDI zum Parameterwert (\\(p\\) der Münze) an!\n\n\nlibrary(easystats)\neti(samples, ci = .5)\n\nEqual-Tailed Interval\n\nParameter           |      50% ETI\n----------------------------------\ngewinnchance_muenze | [0.67, 0.84]\n\nhdi(samples, ci = .5)\n\nHighest Density Interval\n\nParameter           |      50% HDI\n----------------------------------\ngewinnchance_muenze | [0.72, 0.88]\n\n\nEin PI wird auch equal tail interval genannt, weil die beiden “abgeschnitten Randbereiche” links und rechts die gleichen Flächenanteil (Wahrscheinlichkeitsmasse) aufweisen.\nInteresant ist, dass das PI und das HDI zu unterschiedlichen Ergebnissen kommen. Das lässt auf eine schiefe Verteilung schließen. Außerdem eröffnet es den Raum zur Diskussion, welches Intervall man berichtet. Um diese Frage besser zu verstehen, können wir die Intervalle visualisieren.\nBonus: Visualisieren wir die Intervalle:\nPI:\n\neti(samples, ci = .5) %&gt;% plot()\n\n\n\n\n\n\n\n\nHDI:\n\nhdi(samples, ci = .5) %&gt;% plot()\n\n\n\n\n\n\n\n\nDas HDI ist schmäler und liegt näher am Modus. Vermutlich ist das HDI zu bevorzugen.\n\nMit welcher Wahrscheinlichkeit liegt die Trefferchance der Münze zwischen \\(p=.45\\) und \\(p=.55\\), ist also nicht “nennenswert” gezinkt (auf Basis unserer Modellannahmen)?\n\n\nsamples %&gt;% \n  count(gewinnchance_muenze &gt;= 0.45 & gewinnchance_muenze &lt;= .55) %&gt;% \n  mutate(prop = n/sum(n))\n\n# A tibble: 2 × 3\n  `gewinnchance_muenze &gt;= 0.45 & gewinnchance_muenze &lt;= 0.55`     n   prop\n  &lt;lgl&gt;                                                       &lt;int&gt;  &lt;dbl&gt;\n1 FALSE                                                        9534 0.953 \n2 TRUE                                                          466 0.0466\n\n\nDie Wahrscheinlichkeit, dass die Münze nicht nennenswert gezinkt ist (nach unserer Definition), ist gering. Man sollte vielleicht erwähnen, dass unsere Definition von “nicht nennenswert gezinkt” plausibel ist, und andere (vernünftige) Definitionen zu einem sehr ähnlichen Ergebnis kämen.\n\nWas ist der wahrscheinlichste Parameterwert (Trefferchance der Münze)?\n\n\nsamples %&gt;% \n   map_estimate()\n\nMAP Estimate\n\nParameter           | MAP_Estimate\n----------------------------------\ngewinnchance_muenze |         0.78\n\n\nmap_estimate steht für …\n\nFind the Highest Maximum A Posteriori probability estimate (MAP) of a posterior, i.e., the value associated with the highest probability density (the “peak” of the posterior distribution). In other words, it is an estimation of the mode for continuous parameters.\n\n(aus der Hilfeseite der Funktion)\n\nGeben Sie das 90%-PI und 90%-HDI zum Parameterwert (\\(p\\) der Münze) an!\n\n\nlibrary(easystats)\neti(samples, ci = .9)\n\nEqual-Tailed Interval\n\nParameter           |      90% ETI\n----------------------------------\ngewinnchance_muenze | [0.53, 0.92]\n\nhdi(samples, ci = .9)\n\nHighest Density Interval\n\nParameter           |      90% HDI\n----------------------------------\ngewinnchance_muenze | [0.56, 0.94]\n\n\n\nBerechnen Sie die PPV! Visualisieren Sie sie. Interpretieren Sie die PPV.\n\n\nPPV &lt;-\n  samples %&gt;% \n  mutate(anzahl_kopf = rbinom(n = 1e4, size = 10, prob = gewinnchance_muenze))\n\nVisualisierung:\n\nPPV %&gt;% \n  ggplot() +\n  aes(x = anzahl_kopf) +\n  labs(title = \"PPV\") +\n  geom_bar()  # geom_bar() ginge auch, sieht aber bei wenig Balken nicht so gut aus.\n\n\n\n\n\n\n\n\nLaut der PPV sind 8 von 10 Treffern der Wert, der mit der höchsten Wahrscheinlichkeit zu beobachten sein wird. Allerdings sind 7 oder 9 Treffer fast genauso wahrscheinlich. Etwas genauer:\n\nPPV %&gt;% \n  count(between(anzahl_kopf, 7,9))   # \"zähle mir, wie oft ein Wert ZWISCHEN (between) 7 und 9 vorkommt\"\n\n# A tibble: 2 × 2\n  `between(anzahl_kopf, 7, 9)`     n\n  &lt;lgl&gt;                        &lt;int&gt;\n1 FALSE                         3815\n2 TRUE                          6185\n\n\nMit dieser Wahrscheinlichkeit ist ein Wert zwischen 7 und 9 zu beobachten, wenn man den Versuch wiederholt, laut dem Modell.\n\nPPV %&gt;% \n  eti(anzahl_kopf, ci = .9)\n\nEqual-Tailed Interval\n\nParameter           |       90% ETI\n-----------------------------------\ngewinnchance_muenze | [0.53,  0.92]\nanzahl_kopf         | [4.00, 10.00]\n\n\nUnser Modell sieht einen “Passungsbereich” (ein Perzentilintervall) von 4 bis 10 Treffern als mit 90% Wahrscheinlichkeit passend an.\n\nDiskutieren Sie die Annahme einer Gleichverteilung des Priori-Wertes von \\(p\\)!\n\nZwar hat eine Gleichverteilung der Priori-Werte den Vorteil, dass sie “objektiv” ist in dem Sinne, dass kein Wert “bevorteilt” wird; alle gelten als gleich wahrscheinlich. Aber das ist hochgradig unplausibel: So ist z.B. der Wert \\(p=1\\) logisch unmöglich, da wir nicht nur Treffer beobachtet haben. Ein Wert von z.B. \\(p=0.999\\) erscheint uns ebenfalls sehr unwahrscheinlich. Nützlicher erscheint daher vielleicht doch eine Priori-Verteilung, die extreme Werte von \\(p\\) als unwahrscheinlich bemisst.\n\nCategories:\n\nbayes\nprobability\nppv"
  },
  {
    "objectID": "posts/Streudiagramm/Streudiagramm.html",
    "href": "posts/Streudiagramm/Streudiagramm.html",
    "title": "Streudiagramm",
    "section": "",
    "text": "-.90\n0\n+.90\n1\n-1"
  },
  {
    "objectID": "posts/Streudiagramm/Streudiagramm.html#answerlist",
    "href": "posts/Streudiagramm/Streudiagramm.html#answerlist",
    "title": "Streudiagramm",
    "section": "",
    "text": "-.90\n0\n+.90\n1\n-1"
  },
  {
    "objectID": "posts/Streudiagramm/Streudiagramm.html#answerlist-1",
    "href": "posts/Streudiagramm/Streudiagramm.html#answerlist-1",
    "title": "Streudiagramm",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nvis\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/purrr-map05/purrr-map05.html",
    "href": "posts/purrr-map05/purrr-map05.html",
    "title": "purrr-map05",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\nExercise\nErstellen Sie eine Tabelle mit mit folgenden Spalten:\n\nID-Spalte: \\(1,2,..., 10\\)\nEine Spalte, in der jede Zelle eine Tabelle mit einem Vektor \\(x\\), einer standardnormalverteilten Zufallszahlen (n=1000), enthält\n\nBerechnen Sie den Mittelwert von jedem \\(x\\)! Diese Ergebnisse sollen als weitere Spalte der Tabelle hinzugefügt werden.\n         \n\n\nSolution\n\nd &lt;- tibble(\n  id = 1:10) %&gt;% \n  mutate(x = map(id, ~ rnorm(n = 1e3))\n) \n\nstr(d)\n\ntibble [10 × 2] (S3: tbl_df/tbl/data.frame)\n $ id: int [1:10] 1 2 3 4 5 6 7 8 9 10\n $ x :List of 10\n  ..$ : num [1:1000] 0.454 -0.379 0.838 0.576 -0.452 ...\n  ..$ : num [1:1000] 0.0403 0.0899 -1.9857 -1.6792 -0.3218 ...\n  ..$ : num [1:1000] 0.5553 0.1963 -0.4612 -0.0934 1.2162 ...\n  ..$ : num [1:1000] -0.5209 -0.2741 -0.4908 -0.1389 -0.0498 ...\n  ..$ : num [1:1000] -0.0841 0.5579 1.8881 -0.0544 1.0364 ...\n  ..$ : num [1:1000] -0.85 -1.036 -0.307 0.109 -1.692 ...\n  ..$ : num [1:1000] 0.349 -0.724 2.254 -1.578 -1.254 ...\n  ..$ : num [1:1000] -0.335 -0.351 0.173 0.47 0.954 ...\n  ..$ : num [1:1000] -1.42 0.981 -0.541 -1.375 0.274 ...\n  ..$ : num [1:1000] -0.411 0.388 -1.259 -1.037 1.545 ...\n\n\nSo kann man sich die Mittelwerte ausgeben lassen:\n\nd$x %&gt;% \n  map(mean)\n\n[[1]]\n[1] -0.03530719\n\n[[2]]\n[1] -0.010507\n\n[[3]]\n[1] 0.02497519\n\n[[4]]\n[1] 0.04412629\n\n[[5]]\n[1] -0.04437515\n\n[[6]]\n[1] -0.01892384\n\n[[7]]\n[1] 0.006194343\n\n[[8]]\n[1] 0.007741509\n\n[[9]]\n[1] 0.0105266\n\n[[10]]\n[1] 0.0165064\n\n\nJetzt fügen wir den letzten Schritt als Spalte hinzu:\n\nd2 &lt;-\n  d %&gt;% \n  mutate(x_mean = map_dbl(x, ~ mean(.x))) \n\nhead(d2)\n\n# A tibble: 6 × 3\n     id x              x_mean\n  &lt;int&gt; &lt;list&gt;          &lt;dbl&gt;\n1     1 &lt;dbl [1,000]&gt; -0.0353\n2     2 &lt;dbl [1,000]&gt; -0.0105\n3     3 &lt;dbl [1,000]&gt;  0.0250\n4     4 &lt;dbl [1,000]&gt;  0.0441\n5     5 &lt;dbl [1,000]&gt; -0.0444\n6     6 &lt;dbl [1,000]&gt; -0.0189\n\n\nHier hätten wir auch schreiben können:\n\nd %&gt;% \n  mutate(x_mean = map(x, mean)) %&gt;% \n  unnest(x_mean) %&gt;% \n  head()\n\n# A tibble: 6 × 3\n     id x              x_mean\n  &lt;int&gt; &lt;list&gt;          &lt;dbl&gt;\n1     1 &lt;dbl [1,000]&gt; -0.0353\n2     2 &lt;dbl [1,000]&gt; -0.0105\n3     3 &lt;dbl [1,000]&gt;  0.0250\n4     4 &lt;dbl [1,000]&gt;  0.0441\n5     5 &lt;dbl [1,000]&gt; -0.0444\n6     6 &lt;dbl [1,000]&gt; -0.0189\n\n\n\nCategories:\n\nprogramming\nloop"
  },
  {
    "objectID": "posts/anova-skalenniveau/anova-skalenniveau.html",
    "href": "posts/anova-skalenniveau/anova-skalenniveau.html",
    "title": "anova-skalenniveau",
    "section": "",
    "text": "Die Varianzanalyse (ANOVA) ist ein inferenzstatistisches Verfahren des Frequentismus. Welches Skalenniveau passt zu diesem Verfahren?\n\n\n\nUV: nominal (mehrstufig), AV: metrisch\nUV: metrisch, AV: nominal (zweistufig)\nUV: nominal (mehrstufig), AV: nominal (mehrstufig)\nUV: metrisch, AV: nominal (zweistufig)\nUV: nominal (zweistufig), AV: metrisch"
  },
  {
    "objectID": "posts/anova-skalenniveau/anova-skalenniveau.html#answerlist",
    "href": "posts/anova-skalenniveau/anova-skalenniveau.html#answerlist",
    "title": "anova-skalenniveau",
    "section": "",
    "text": "UV: nominal (mehrstufig), AV: metrisch\nUV: metrisch, AV: nominal (zweistufig)\nUV: nominal (mehrstufig), AV: nominal (mehrstufig)\nUV: metrisch, AV: nominal (zweistufig)\nUV: nominal (zweistufig), AV: metrisch"
  },
  {
    "objectID": "posts/anova-skalenniveau/anova-skalenniveau.html#answerlist-1",
    "href": "posts/anova-skalenniveau/anova-skalenniveau.html#answerlist-1",
    "title": "anova-skalenniveau",
    "section": "Answerlist",
    "text": "Answerlist\n\nWahr\nFalsch\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nvariable-levels\nanova"
  },
  {
    "objectID": "posts/purrr-map02/purrr-map02.html",
    "href": "posts/purrr-map02/purrr-map02.html",
    "title": "purrr-map02",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\nExercise\nBestimmen Sie die häufigsten Worte im Grundatzprogramm der Partei AfD (in der aktuellsten Version).\n         \n\n\nSolution\nText aus PDF-Dateien kann man mit dem Paket pdftools einlesen:\n\nlibrary(pdftools)\n\nUsing poppler version 22.02.0\n\nd_path &lt;- \"/Users/sebastiansaueruser/Google Drive/Literatur/_Div/Politik/afd-grundsatzprogramm-2022.pdf\"\n\nd &lt;- tibble(text = pdf_text(d_path))\n\nDann erstellen wir eine Tidy-Version und tokenisieren nach Wörtern:\n\nlibrary(tidytext)\nd2 &lt;-\n  d %&gt;% \n  unnest_tokens(output = word, input = text)\n\nhead(d2)\n\n# A tibble: 6 × 1\n  word             \n  &lt;chr&gt;            \n1 programm         \n2 für              \n3 deutschland      \n4 das              \n5 grundsatzprogramm\n6 der              \n\n\nDann zählen wir die Wörter:\n\nd2 %&gt;% \n  count(word, sort = TRUE) %&gt;% \n  head(20)\n\n# A tibble: 20 × 2\n   word            n\n   &lt;chr&gt;       &lt;int&gt;\n 1 die          1151\n 2 und          1147\n 3 der           870\n 4 zu            435\n 5 für           392\n 6 in            392\n 7 den           271\n 8 von           257\n 9 ist           251\n10 das           225\n11 werden        214\n12 eine          211\n13 nicht         196\n14 ein           191\n15 deutschland   190\n16 sind          187\n17 wir           176\n18 afd           171\n19 des           169\n20 sich          158\n\n\n\nCategories:\n\nr\nmap\ntidyverse"
  },
  {
    "objectID": "posts/summarise03/summarise03.html",
    "href": "posts/summarise03/summarise03.html",
    "title": "summarise03",
    "section": "",
    "text": "Aufgabe\nIm Datensatz mariokart:\n\nGruppieren Sie danach, wie viele Lenkräder bei der Auktion dabei waren.\nFassen Sie die Spalte total_pr zusammen und zwar zum Mittelelwert - pro Gruppe!\nBerechnen Sie den Mittelwert dieser Zahlen!\n\nGeben Sie diese Zahl als Antwort zurück!\n         \n\n\nLösung\nPakete starten:\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.6.0 (red = needs update)\n✖ bayestestR  0.13.0   ✔ correlation 0.8.4 \n✔ datawizard  0.7.1    ✔ effectsize  0.8.3 \n✔ insight     0.19.1   ✔ modelbased  0.8.6 \n✖ performance 0.10.2   ✖ parameters  0.20.3\n✔ report      0.5.7    ✔ see         0.7.5 \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`.\n\nlibrary(tidyverse)  # startet das Paket tidyverse\n\nDaten importieren:\n\nmariokart &lt;- data_read(\"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\")\n\nZusammenfassen:\n\nmariokart_gruppiert &lt;- group_by(mariokart, wheels)  # Gruppieren\nmariokart_klein &lt;- summarise(mariokart_gruppiert, pr_mean = mean(total_pr))  # zusammenfassen\nmariokart_klein\n\n# A tibble: 5 × 2\n  wheels pr_mean\n   &lt;int&gt;   &lt;dbl&gt;\n1      0    41.1\n2      1    44.2\n3      2    61.0\n4      3    69.8\n5      4    65.0\n\n\n\nsummarise(mariokart_klein, pr_mean = mean(pr_mean))\n\n# A tibble: 1 × 1\n  pr_mean\n    &lt;dbl&gt;\n1    56.2\n\n\nmin analog.\nDie Lösung lautet: 56\n\nCategories:\n\ndatawrangling\neda\ntidyverse\ndplyr\nnum"
  },
  {
    "objectID": "posts/diamonds-histogram/diamonds-histogram.html",
    "href": "posts/diamonds-histogram/diamonds-histogram.html",
    "title": "diamonds-histogram",
    "section": "",
    "text": "Die Daten beziehen sich auf den Datensatz diamonds und sind hier einzusehen bzw. können bei Interesse dort heruntergeladen werden.\n\n\n\n\n\n\n\n\n\n\n\n\nAuf der X-Achse ist eine nominalskalierte Variable abgetragen.\nDer vertikale Strich in jedem Bild passt gut zur Position des insgesamten Medians.\nDie Variable cut ist eine intervallskalierte Variable.\nAuf der x-Achse werden Häufigkeiten abgetragen.\nDie Gruppierungsvariable cut wird hier als ordinale Variable, also mit Ordnungsstruktur, verwendet."
  },
  {
    "objectID": "posts/diamonds-histogram/diamonds-histogram.html#answerlist",
    "href": "posts/diamonds-histogram/diamonds-histogram.html#answerlist",
    "title": "diamonds-histogram",
    "section": "",
    "text": "Auf der X-Achse ist eine nominalskalierte Variable abgetragen.\nDer vertikale Strich in jedem Bild passt gut zur Position des insgesamten Medians.\nDie Variable cut ist eine intervallskalierte Variable.\nAuf der x-Achse werden Häufigkeiten abgetragen.\nDie Gruppierungsvariable cut wird hier als ordinale Variable, also mit Ordnungsstruktur, verwendet."
  },
  {
    "objectID": "posts/diamonds-histogram/diamonds-histogram.html#answerlist-1",
    "href": "posts/diamonds-histogram/diamonds-histogram.html#answerlist-1",
    "title": "diamonds-histogram",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\nvis\n‘2023’\nschoice"
  },
  {
    "objectID": "posts/twitter03/twitter03.html",
    "href": "posts/twitter03/twitter03.html",
    "title": "twitter03",
    "section": "",
    "text": "Exercise\nLaden Sie die neuesten Tweets an karl_lauterbach herunter, die mindestens 100 Likes oder 100 Retweets haben.\n         \n\n\nSolution\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(rtweet)\n\n\nAttaching package: 'rtweet'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\n\nEinloggen bei Twitter; zuerst die Credentials bereithalten:\n\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\n\nDann anmelden:\n\nauth &lt;- rtweet_app(bearer_token = Bearer_Token)\n\nTweets an Karl Lauterbach suchen:\n\nkarl1 &lt;- search_tweets(\"@karl_lauterbach min_faves:100 OR min_retweets:100\", n = 10)\n\n\nkarl1 %&gt;% \n  select(retweet_count, favorite_count)\n\n# A tibble: 10 × 2\n   retweet_count favorite_count\n           &lt;int&gt;          &lt;int&gt;\n 1            56            210\n 2            56            229\n 3            44           1626\n 4            60            225\n 5            30            494\n 6             5            148\n 7            27            435\n 8            12            178\n 9            13            162\n10            46            375\n\nCategories:\n\ntextmining\ntwitter"
  },
  {
    "objectID": "posts/mariokart-mean2/mariokart-mean2.html",
    "href": "posts/mariokart-mean2/mariokart-mean2.html",
    "title": "mariokart-mean2",
    "section": "",
    "text": "Aufgabe\nImportieren Sie den Datensatz mariokart in R. Berechnen Sie den mittleren Verkaufspreis (total_pr) für neue Spiele.\nHinweise:\n\nRunden Sie auf 1 Dezimalstelle.\n\n         \n\n\nLösung\nPakete starten:\n\nlibrary(tidyverse)\nlibrary(easystats)\n\nDaten importieren:\n\nd_url &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\"\nd &lt;- data_read(d_url)\n\n\nsolution &lt;-\nd  %&gt;% \n  filter(cond == \"new\") %&gt;% \n  summarise(pr_mean = mean(total_pr))\n\nsolution\n\n   pr_mean\n1 53.77068\n\n\nLösung: 53.77.\n\nCategories:\n\ndatawrangling\ndplyr\neda\nnum"
  },
  {
    "objectID": "posts/twitter04/twitter04.html",
    "href": "posts/twitter04/twitter04.html",
    "title": "twitter04",
    "section": "",
    "text": "Exercise\nLaden Sie \\(n=10^k\\) Tweets von Twitter herunter (mit \\(k=2\\)) via der Twitter API; Suchterm soll sein “@karl_lauterbach”. Bereiten Sie die Textdaten mit grundlegenden Methoden des Textminings auf (Tokenisieren, Stopwörter entfernen, Zahlen entfernen, …). Berichten Sie dann die 10 häufigsten Wörter als Schätzer für die Dinge, die an Karl Lauterbach getweetet werden.\n         \n\n\nSolution\n\nlibrary(rtweet)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()  masks stats::filter()\n✖ purrr::flatten() masks rtweet::flatten()\n✖ dplyr::lag()     masks stats::lag()\n\nlibrary(tidytext)\nlibrary(lsa)  # Stopwörter\n\nLoading required package: SnowballC\n\nlibrary(SnowballC)  # Stemming\n\n\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\n\n\nauth &lt;- rtweet_app(bearer_token = Bearer_Token)\n\n\nkarl1 &lt;- search_tweets(\"@karl_lauterbach\", n = 1e2, include_rts = FALSE)\n#write_rds(karl1, file = \"karl1.rds\", compress = \"gz\")\n\n\nkarl2 &lt;- \n  karl1 %&gt;% \n  select(full_text)\n\n\nkarl3 &lt;- \n  karl2 %&gt;% \n  unnest_tokens(output = word, input = full_text)\n\n\nkarl4 &lt;- \nkarl3 %&gt;% \n  anti_join(tibble(word = lsa::stopwords_de)) \n\nJoining, by = \"word\"\n\n\n\nkarl5 &lt;- \n  karl4 %&gt;% \n  mutate(word = str_replace_na(word, \"^[:digit:]+$\")) %&gt;% \n  mutate(word = str_replace_na(word, \"hptts?://\\\\w+\")) %&gt;% \n  mutate(word = str_replace_na(word, \" +\")) %&gt;% \n  drop_na()\n\n\nkarl6 &lt;-\n  karl5 %&gt;% \n  mutate(word = wordStem(word))\n\n\nkarl6 %&gt;% \n  count(word, sort = TRUE) %&gt;% \n  slice_head(n=10)\n\n# A tibble: 10 × 2\n   word                       n\n   &lt;chr&gt;                  &lt;int&gt;\n 1 karl_lauterbach          100\n 2 rt                        60\n 3 ultrakaerl                19\n 4 corona                    16\n 5 wirwollenmaskenpflicht    16\n 6 länder                    12\n 7 gesundheitsminist         11\n 8 polarstern64              11\n 9 schon                     11\n10 shomburg                  11\n\n\n\nCategories:\n\ntextmining\ntwitter"
  },
  {
    "objectID": "posts/PPV1a-mtcars/PPV1a-mtcars.html",
    "href": "posts/PPV1a-mtcars/PPV1a-mtcars.html",
    "title": "PPV1a-mtcars",
    "section": "",
    "text": "Im Folgenden ist der Datensatz mtcars zu analysieren.\nEine Möglichkeit, den Datensatz zu beziehen, ist diese Sammlung an Datensätzen. Suchen Sie dort nach dem Namen des Datensatzes. Importieren Sie dann die Daten in R.\nHilfe zum Datensatz ist auf dieser Webseite abrufbar.\nBerechnen Sie das folgende lineare Modell:\nAV: mpg.\nUVs: hp, am.\nAufgabe: Was ist der Wert des Punktschätzers für eine Beobachtung, bei der alle Prädiktoren den Wert 0 aufweisen?\nHinweise\nWählen Sie die am besten passende Antwortoption!\n\n\n\n-27\n-17\n-7\n17\n27"
  },
  {
    "objectID": "posts/PPV1a-mtcars/PPV1a-mtcars.html#answerlist",
    "href": "posts/PPV1a-mtcars/PPV1a-mtcars.html#answerlist",
    "title": "PPV1a-mtcars",
    "section": "",
    "text": "-27\n-17\n-7\n17\n27"
  },
  {
    "objectID": "posts/PPV1a-mtcars/PPV1a-mtcars.html#answerlist-1",
    "href": "posts/PPV1a-mtcars/PPV1a-mtcars.html#answerlist-1",
    "title": "PPV1a-mtcars",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nFalsch\nWahr\n\n\nCategories:\n\nbayes\nregression\nexam-22"
  },
  {
    "objectID": "posts/n-vars-diagram/n-vars-diagram.html",
    "href": "posts/n-vars-diagram/n-vars-diagram.html",
    "title": "n-vars-diagram",
    "section": "",
    "text": "Aufgabe\nWie viele Variablen sind in folgendem Diagramm dargestellt?\nDie Daten beziehen sich au den Datensatz mtcars; hier finden Sie Informationen zu dem Datensatz. Er ist in R “fest eingebaut”, also direkt verfügbar.\n         \n\n\nLösung\n\n\n\n\n\n\n\n\n\nEs sind 5 Variablen abgebildet:\n\nhp\nmpg\nam\ncyl\nvs\n\n\nCategories:\n\nvis\n‘2023’\nnum"
  },
  {
    "objectID": "posts/mtcars-simple3/mtcars-simple3.html",
    "href": "posts/mtcars-simple3/mtcars-simple3.html",
    "title": "mtcars-simple3",
    "section": "",
    "text": "We will use the dataset mtcars in this exercise.\nAssume your causal model of your research dictates that fuel economy is a linear function of horse power, cylinder count and displacement of the engine.\nWhich of the predictors in the above model has the weakest causal impact on the output variable?\nNotes:\n\nUse can either use frequentist or bayesian modeling.\nUse R for all computations.\nThere are multiple ways to find a solution.\n\n\n\n\ncyl\nhp\ndisp\nAll are equally strong\nnone of the above"
  },
  {
    "objectID": "posts/mtcars-simple3/mtcars-simple3.html#answerlist",
    "href": "posts/mtcars-simple3/mtcars-simple3.html#answerlist",
    "title": "mtcars-simple3",
    "section": "",
    "text": "cyl\nhp\ndisp\nAll are equally strong\nnone of the above"
  },
  {
    "objectID": "posts/mtcars-simple3/mtcars-simple3.html#answerlist-1",
    "href": "posts/mtcars-simple3/mtcars-simple3.html#answerlist-1",
    "title": "mtcars-simple3",
    "section": "Answerlist",
    "text": "Answerlist\n\nwrong\ncorrect\nwrong\nwrong\nwrong\n\n\nCategories:\n\nregresssion\nen\nbayes\nfrequentist\nqm1\nstats-nutshell"
  },
  {
    "objectID": "posts/ttest-als-regr/ttest-als-regr.html",
    "href": "posts/ttest-als-regr/ttest-als-regr.html",
    "title": "ttest-als-regr",
    "section": "",
    "text": "Der t-Test kann als Spezialfall der Regressionsanalyse gedeutet werden.\nHierbei ist es wichtig, sich das Skalenniveau der Variablen, die ein t-Test verarbeitet, vor Augen zu führen.\nHinweisse:\n\nDie folgende Abbildung gibt Tipps.\nInformationen, die zur Lösung einer Aufgabe nicht nötig sind, sollte man ignorieren.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBenennen Sie die Skalenniveaus der UV eines t-Tests! Geben Sie nur ein Wort ein. Verwenden Sie nur Kleinbuchstaben (z.B. regression).\nBenennen Sie die Skalenniveaus der AV eines t-Tests! Geben Sie nur ein Wort ein. Verwenden Sie nur Kleinbuchstaben (z.B. regression).\nNennen Sie eine beispielhafte Forschungsfrage für einen t-Test.\nSkizzieren Sie ein Diagramm einer Regression, die analytisch identisch (oder sehr ähnlich) zu einem t-Test ist!"
  },
  {
    "objectID": "posts/ttest-als-regr/ttest-als-regr.html#answerlist",
    "href": "posts/ttest-als-regr/ttest-als-regr.html#answerlist",
    "title": "ttest-als-regr",
    "section": "",
    "text": "Benennen Sie die Skalenniveaus der UV eines t-Tests! Geben Sie nur ein Wort ein. Verwenden Sie nur Kleinbuchstaben (z.B. regression).\nBenennen Sie die Skalenniveaus der AV eines t-Tests! Geben Sie nur ein Wort ein. Verwenden Sie nur Kleinbuchstaben (z.B. regression).\nNennen Sie eine beispielhafte Forschungsfrage für einen t-Test.\nSkizzieren Sie ein Diagramm einer Regression, die analytisch identisch (oder sehr ähnlich) zu einem t-Test ist!"
  },
  {
    "objectID": "posts/ttest-als-regr/ttest-als-regr.html#answerlist-1",
    "href": "posts/ttest-als-regr/ttest-als-regr.html#answerlist-1",
    "title": "ttest-als-regr",
    "section": "Answerlist",
    "text": "Answerlist\n\nUV: binär\nAV: metrisch\nUnterscheiden sich die mittleren Einparkzeiten von Frauen und Männern?\nAus dem Datensatz mtcars:\n\n\ndata(mtcars)\nmtcars %&gt;% \n  ggplot() +\n  aes(x = am, y = mpg) +\n  geom_point(alpha = .5) +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nCategories:\n\nregression\nttest\nvariable-levels"
  },
  {
    "objectID": "posts/mtcars-post/mtcars-post.html",
    "href": "posts/mtcars-post/mtcars-post.html",
    "title": "mtcars-post",
    "section": "",
    "text": "Aufgabe\nIm Datensatz mtcars: Berichten Sie die Breite eines Schätzintervalls (89%, HDI) zum mittleren Spritverbrauch! Nutzen Sie Methoden der Bayes-Statistik.\nHinweise\n         \n\n\nLösung\nSetup:\n\ndata(mtcars)\nlibrary(tidyverse)\nlibrary(rstanarm)\nlibrary(easystats)\n\nModell berechnen:\n\nm1 &lt;- stan_glm(mpg ~ 1, \n               data = mtcars,\n               seed = 42,\n               refresh = 0)\n\nModellparameter auslesen, wobei wir als CI-Methode ein HDI auswählen, und als CI-Größe 89%:\n\nparameters(m1, ci = .89, ci_method = \"hdi\")\n\nParameter   | Median |         89% CI |   pd |  Rhat |     ESS |                   Prior\n----------------------------------------------------------------------------------------\n(Intercept) |  20.10 | [18.26, 21.64] | 100% | 1.001 | 2838.00 | Normal (20.09 +- 15.07)\n\n\nIm Standard wird ein 95%-Perzentilintervall berechnet, s. die Dokumentation zur Funktion hier.\nDie Lösung lautet also:\n\nsolution &lt;- 21.64 - 18.26\nsolution\n\n[1] 3.38\n\n\n\nCategories:\n\nbayes\npost\nestimation\nexam-22"
  },
  {
    "objectID": "posts/log-y-regr1/log-y-regr1.html",
    "href": "posts/log-y-regr1/log-y-regr1.html",
    "title": "log-y-regr1",
    "section": "",
    "text": "Exercise\n\nlibrary(tidyverse)\nlibrary(easystats)\n\nIn dieser Aufgabe modellieren wir den (kausalen) Effekt von Schulbildung auf das Einkommen.\nImportieren Sie zunächst den Datensatz und verschaffen Sie sich einen Überblick.\n\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Treatment.csv\"\n\nd &lt;- data_read(d_path)\n\nDokumentation und Quellenangaben zum Datensatz finden sich hier.\n\nglimpse(d)\n\nRows: 2,675\nColumns: 11\n$ V1      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…\n$ treat   &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n$ age     &lt;int&gt; 37, 30, 27, 33, 22, 23, 32, 22, 19, 21, 18, 27, 17, 19, 27, 23…\n$ educ    &lt;int&gt; 11, 12, 11, 8, 9, 12, 11, 16, 9, 13, 8, 10, 7, 10, 13, 10, 12,…\n$ ethn    &lt;chr&gt; \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\",…\n$ married &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ re74    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ re75    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ re78    &lt;dbl&gt; 9930.05, 24909.50, 7506.15, 289.79, 4056.49, 0.00, 8472.16, 21…\n$ u74     &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n$ u75     &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n\n\nModellieren Sie den Effekt der Bildungsdauer auf das Einkommen! Gehen Sie von einem exponenziellen Zusammenhang der beiden Variablen aus. Um welchen Faktor steigt das Einkommen pro Jahr Bildung (laut Modell)?\nHinweise:\n\nVerwenden Sie lm zur Modellierung.\nOperationalisieren Sie das Einkommen mit der Variable re74.\nFügen Sie keine weiteren Variablen dem Modell hinzu.\nGehen Sie von einem kausalen Effekt des Prädiktors aus.\n\n         \n\n\nSolution\n\nd2 &lt;-\n  d %&gt;% \n  filter(re74 &gt; 0) %&gt;% \n  mutate(re74_log = log(re74))\n\n\nm &lt;- lm(re74_log ~ educ, data = d2)\n\nHier sind die parameters des Modells.\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(2327)\np\n\n\n\n\n(Intercept)\n8.83\n0.06\n(8.70, 8.95)\n142.91\n&lt; .001\n\n\neduc\n0.07\n4.94e-03\n(0.07, 0.08)\n15.16\n&lt; .001\n\n\n\n\nFür jedes Jahr Bildung steigt das Einkommen also ca. um den Faktor 1.07.\nEtwas genauer:\n\\(\\hat{\\beta_1} = 0.07\\) bedeutet, dass ein Jahr Bildung zu einen erwarteten Unterschied im Einkommen in Höhe von 0.07 in Log-Einkommen führt. Anders gesagt wird das Einkommen um exp(0.07) erhöht. Dabei gilt \\(e^{0.07} \\approx 1.07\\):\n\nexp(0.07)\n\n[1] 1.072508\n\n\nDie Lösung lautet also: “Pro Jahr Bildung steigt das Einkommen - laut Modell um den Faktor ca. 1.07”.\nMan darf dabei nicht vergessen, dass wir wir uns hier auf die Schnelle ein Modell ausgedacht haben. Ob es in Wirklichkeit so ist, wie unser Modell meint, ist eine andere Sache!\n\nCategories:\n\nregression\nlm\nqm2\nstats-nutshell"
  },
  {
    "objectID": "posts/subjektiv-Bayes/subjektiv-Bayes.html",
    "href": "posts/subjektiv-Bayes/subjektiv-Bayes.html",
    "title": "subjektiv-Bayes",
    "section": "",
    "text": "Exercise\nNennen Sie einen Aspekte der bayesianischen Analyse, der (in Teilen) subjektiv ist - abgesehen von der Wahl der Priori-Verteilung.\n         \n\n\nSolution\n\nLinearitätsannahme in (linearen) Modellen\nWahl des Likelihoods\nWahl der Daten\nMethoden der Modellprüfung\nGeneralisierung des Modells auf andere Situationen\nWahl der Prädiktoren\n\n\nCategories:\n~"
  },
  {
    "objectID": "posts/Mediterran-Alk/Mediterran-Alk.html",
    "href": "posts/Mediterran-Alk/Mediterran-Alk.html",
    "title": "Mediterran-Alk",
    "section": "",
    "text": "Exercise\nAlkohol ist ein weit verbreites Genussmittel in vielen Gesellschaften. Insgesamt sind die negativen (kausalen) Konsequenzen für die Gesundheit unstrittig. So findet man etwa in dieser Studie:\n\nThis meta-analysis found that alcohol most strongly increased the risks for cancers of the oral cavity, pharynx, esophagus, and larynx. Statistically significant increases in risk also existed for cancers of the stomach, colon, rectum, liver, female breast, and ovaries.\n\nAllerdings gibt es auch Stimmen, die Alkohol mit gesundheitlich wünschenswerten Effekten in Verbindung bringen. Dabei wird in einigen Fällen die “mediterrane Ernährung” als Erkärungsnarrativ ins Spiel gebracht. So kann man etwa hier lesen:\n\nAdhering to a Mediterranean diet (…) were associated with a lower risk of all-cause mortality (…).\n\nSolche Befunde wurden von der Breiten- oder Boulevardpresse dankbar aufgenommen, wie man z.B. hier nachlesen kann:\n\nSmall Amounts of Alcohol in Mediterranean Diet Could Boost Brain Health, Claims Study\n\nMan beachte, dass “boost your health” eine kausale Aussage ist, die über einen reinen Zusammenhang hinausgeht. Nach dieser Lesart heißt es: Trink etwas Alkohol (A), das macht dich gesünder (G).\nIhre Aufgabe: Zeigen Sie ein alternatives Kausalmodell auf, das erklärt, warum ein Zusammenhang (wie eine Korrelation) zwischen A und G zu beobachten ist, aber ohne dass es einen (kausalen) Effekt zwischen beiden Größen gäbe!\n         \n\n\nSolution\nEine Erklärung lautet - frei erfunden! -, dass die Lebenszufriedenheit (L) jeweils einen (positiven, kausalen) Effekt auf Alkoholkonsum (A) und auf die Gesundheit (G) ausübt.\n\n\n\n\n\n\n\n\n\nÜbrigens: Eine Art von Diagramm, das Kausalbeziehungen zwischen Variablen aufzeigt, ist ein sog. Directed Acyclic Graph, oder kurz ein DAG. Hier ist so ein DAG gezeichnet.\n\nCategories:\n~"
  },
  {
    "objectID": "posts/kausal23/kausal23.html",
    "href": "posts/kausal23/kausal23.html",
    "title": "kausal23",
    "section": "",
    "text": "Gegeben sei der DAG g (s. u.). Der DAG verfügt über mehrere Variablen, die als Knoten im Graph dargestellt sind und mit \\(x_1, x_2, \\ldots x_n\\) bezeichnet sind.\nWelche minimale Variablenmenge muss kontrolliert werden, um den kausalen Effekt von der UV zur AV zu identifizieren?\nUV: x6.\nAV: x5.\nHinweise:\n\nMengen sind mittels geschweifter Klammern gekennzeichnet, z.B. {x8, x9} meint die Menge mit den zwei Elementen x8 und x9.\nDie leere Menge { } bedeutet, dass keine Variable kontrolliert werden muss, um den kausalen Effekt zu identifizieren.\nAlle Variablen werden als gemessen vorausgesetzt.\nEs ist möglich, dass es keine Lösung gibt, dass es also keine Adjustierungsmenge gibt, um den kausalen Effekt zu identifizieren. Wenn dies der Fall sein sollte, wählen Sie “/”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n{ x4, x5 }\n{ x1, x2 }\n/\n{ x3, x5 }\n{ x3, x6 }"
  },
  {
    "objectID": "posts/kausal23/kausal23.html#answerlist",
    "href": "posts/kausal23/kausal23.html#answerlist",
    "title": "kausal23",
    "section": "",
    "text": "{ x4, x5 }\n{ x1, x2 }\n/\n{ x3, x5 }\n{ x3, x6 }"
  },
  {
    "objectID": "posts/kausal23/kausal23.html#answerlist-1",
    "href": "posts/kausal23/kausal23.html#answerlist-1",
    "title": "kausal23",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nRichtig\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/adjustieren2/adjustieren2.html",
    "href": "posts/adjustieren2/adjustieren2.html",
    "title": "adjustieren2",
    "section": "",
    "text": "Betrachten Sie folgendes Modell, das den Zusammenhang des Preises (price) und dem Gewicht (carat) von Diamanten untersucht (Datensatz diamonds).\n\nlibrary(tidyverse)\ndiamonds &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/ggplot2/diamonds.csv\")\n\nNew names:\nRows: 53940 Columns: 11\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(3): cut, color, clarity dbl (8): ...1, carat, depth, table, price, x, y, z\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\nAber zuerst zentrieren wir den metrischen Prädiktor carat, um den Achsenabschnitt besser interpretieren zu können.\n\ndiamonds &lt;-\n  diamonds %&gt;% \n  mutate(carat_z = carat - mean(carat, na.rm = TRUE))\n\nDann berechnen wir ein (bayesianisches) Regressionsmodell, wobei wir auf die Standardwerte der Prior zurückgreifen.\n\nlibrary(rstanarm)\nlibrary(easystats)\nlm1 &lt;- stan_glm(price ~ carat_z, data = diamonds,\n                chains = 1,  # nur ein Mal Stichproben ziehen, spart Zeit (auf Kosten der Genauigkeit)\n                refresh = 0)\nparameters(lm1)\n\nParameter   |  Median |             95% CI |   pd | % in ROPE |  Rhat |     ESS |                       Prior\n-------------------------------------------------------------------------------------------------------------\n(Intercept) | 3933.11 | [3919.67, 3946.04] | 100% |        0% | 1.000 |  289.00 | Normal (3932.80 +- 9973.60)\ncarat_z     | 7757.01 | [7731.08, 7786.08] | 100% |        0% | 1.000 | 1181.00 |   Normal (0.00 +- 21040.85)\n\n\nZur Verdeutlichung ein Diagramm zum Modell:\n\ndiamonds %&gt;% \n  ggplot() +\n  aes(x = carat_z, y = price) +\n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nWas kostet in Diamant mittlerer Größe laut Modell lm1? Runden Sie auf eine Dezimale. Geben Sie nur eine Zahl ein.\nGeben Sie eine Regressionsformel an, die lm1 ergänzt, so dass die Schliffart (cut) des Diamanten kontrolliert (adjustiert) wird. Anders gesagt: Das Modell soll die mittleren Preise für jede der fünf Schliffarten angeben. Geben Sie nur die Regressionsformel an. Lassen Sie zwischen Termen jeweils ein Leerzeichen Abstand.\n\nHinweis: Es gibt (laut Datensatz) folgende Schliffarten (und zwar in der folgenden Reihenfolge):\n\ndiamonds %&gt;% \n  distinct(cut)\n\n# A tibble: 5 × 1\n  cut      \n  &lt;chr&gt;    \n1 Ideal    \n2 Premium  \n3 Good     \n4 Very Good\n5 Fair"
  },
  {
    "objectID": "posts/adjustieren2/adjustieren2.html#answerlist",
    "href": "posts/adjustieren2/adjustieren2.html#answerlist",
    "title": "adjustieren2",
    "section": "",
    "text": "Was kostet in Diamant mittlerer Größe laut Modell lm1? Runden Sie auf eine Dezimale. Geben Sie nur eine Zahl ein.\nGeben Sie eine Regressionsformel an, die lm1 ergänzt, so dass die Schliffart (cut) des Diamanten kontrolliert (adjustiert) wird. Anders gesagt: Das Modell soll die mittleren Preise für jede der fünf Schliffarten angeben. Geben Sie nur die Regressionsformel an. Lassen Sie zwischen Termen jeweils ein Leerzeichen Abstand.\n\nHinweis: Es gibt (laut Datensatz) folgende Schliffarten (und zwar in der folgenden Reihenfolge):\n\ndiamonds %&gt;% \n  distinct(cut)\n\n# A tibble: 5 × 1\n  cut      \n  &lt;chr&gt;    \n1 Ideal    \n2 Premium  \n3 Good     \n4 Very Good\n5 Fair"
  },
  {
    "objectID": "posts/Griech-Buchstaben-Inferenz/Griech-Buchstaben-Inferenz.html",
    "href": "posts/Griech-Buchstaben-Inferenz/Griech-Buchstaben-Inferenz.html",
    "title": "Griech-Buchstaben-Inferenz",
    "section": "",
    "text": "Exercise\nFür Statistiken (Stichprobe) verwendet man meist lateinische Buchstaben; für Parameter (Population) verwendet man meist (die entsprechenden) griechischen Buchstaben.\nVervollständigen Sie folgende Tabelle entsprechend!\n\n\n\n\n\nKennwert\nStatistik\nParameter\n\n\n\n\nMittelwert\n\\(\\bar{X}\\)\nNA\n\n\nMittelwertsdifferenz\n\\(\\bar{X}_1-\\bar{X}_2\\)\nNA\n\n\nStreuung\nsd\nNA\n\n\nAnteil\np\nNA\n\n\nKorrelation\nr\nNA\n\n\nRegressionsgewicht\nb\nNA\n\n\n\n\n\n         \n\n\nSolution\n\n\n\n\n\nKennwert\nStatistik\nParameter\n\n\n\n\nMittelwert\n\\[\\bar{X}\\]\n\\[\\mu\\]\n\n\nMittelwertsdifferenz\n\\[d=\\bar{X}_1-\\bar{X}_2\\]\n\\[\\mu_1\\]- \\[\\mu_2\\]\n\n\nStreuung\nsd\n\\[\\sigma\\]\n\n\nAnteil\np\n\\[\\pi\\]\n\n\nKorrelation\nr\n\\[\\rho\\]\n\n\nRegressionsgewicht\nb\n\\[\\beta\\]\n\n\n\n\n\n\nCategories:\n\nqm2\ninference\nparameters"
  },
  {
    "objectID": "posts/kausal24/kausal24.html",
    "href": "posts/kausal24/kausal24.html",
    "title": "kausal24",
    "section": "",
    "text": "Gegeben sei der DAG g (s. u.). Der DAG verfügt über mehrere Variablen, die als Knoten im Graph dargestellt sind und mit \\(x_1, x_2, \\ldots x_n\\) bezeichnet sind.\nWelche minimale Variablenmenge muss kontrolliert werden, um den kausalen Effekt von der UV zur AV zu identifizieren?\nUV: x7.\nAV: x8.\nHinweise:\n\nMengen sind mittels geschweifter Klammern gekennzeichnet, z.B. {x8, x9} meint die Menge mit den zwei Elementen x8 und x9.\nDie leere Menge { } bedeutet, dass keine Variable kontrolliert werden muss, um den kausalen Effekt zu identifizieren.\nAlle Variablen werden als gemessen vorausgesetzt.\nEs ist möglich, dass es keine Lösung gibt, dass es also keine Adjustierungsmenge gibt, um den kausalen Effekt zu identifizieren. Wenn dies der Fall sein sollte, wählen Sie “keine Lösung”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n{ x3, x8 }\n{ x1 , x2 , x3 }\n{ x1 }\n{ x5, x8 }\n{ x3, x5 }"
  },
  {
    "objectID": "posts/kausal24/kausal24.html#answerlist",
    "href": "posts/kausal24/kausal24.html#answerlist",
    "title": "kausal24",
    "section": "",
    "text": "{ x3, x8 }\n{ x1 , x2 , x3 }\n{ x1 }\n{ x5, x8 }\n{ x3, x5 }"
  },
  {
    "objectID": "posts/kausal24/kausal24.html#answerlist-1",
    "href": "posts/kausal24/kausal24.html#answerlist-1",
    "title": "kausal24",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nRichtig\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n\ndag\ncausal"
  },
  {
    "objectID": "posts/Wertberechnen/Wertberechnen.html",
    "href": "posts/Wertberechnen/Wertberechnen.html",
    "title": "Wertberechnen",
    "section": "",
    "text": "Aufgabe\nWelchen Wert bzw. welches Ergebnis liefert folgende R-Syntax für ergebnis zurück?\nx hat zu Beginn den Wert 8.\nHinweis: sqrt(x) liefert die (positive) Quadratwurzel von x zurück.\n         \n\n\nLösung\nEs wird 3 zurückgeliefert.\n\nCategories:\n\nR\ndyn\nnum"
  },
  {
    "objectID": "posts/twitter05/twitter05.html",
    "href": "posts/twitter05/twitter05.html",
    "title": "twitter05",
    "section": "",
    "text": "Exercise\nLaden Sie \\(n=10^k\\) Tweets von Twitter herunter (mit \\(k=2\\)) via der Twitter API; Suchterm soll sein “@karl_lauterbach”. Bereiten Sie die Textdaten mit grundlegenden Methoden des Textminings auf (Tokenisieren, Stopwörter entfernen, Zahlen entfernen, …).\nNutzen Sie die Daten, um eine Sentimentanalyse zu erstellen.\n         \n\n\nSolution\nNutzen Sie die Daten der letzten Aufgabe, um eine Sentimentanalyse zu erstellen.\nZuerst muss man sich anmelden und die Tweets herunterladen; dieser Teil ist hier nicht aufgeführt (s. andere Aufgaben).\n\nlibrary(rtweet)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()  masks stats::filter()\n✖ purrr::flatten() masks rtweet::flatten()\n✖ dplyr::lag()     masks stats::lag()\n\nlibrary(tidytext)\nlibrary(lsa)  # Stopwörter\n\nLoading required package: SnowballC\n\nlibrary(SnowballC)  # Stemming\n\nBeachten Sie, dass die Spalten je nach Funktion, die Sie zum Herunterladen der Tweets verwenden, unterschiedlich heißen können.\n\nkarl2 &lt;- \n  karl1 %&gt;% \n  select(contains(\"text\"))\n\n\nkarl3 &lt;- \n  karl2 %&gt;% \n  unnest_tokens(output = word, input = text)\n\n\nkarl4 &lt;- \nkarl3 %&gt;% \n  anti_join(tibble(word = lsa::stopwords_de)) \n\nJoining, by = \"word\"\n\n\n\nkarl5 &lt;- \n  karl4 %&gt;% \n  mutate(word = str_replace_na(word, \"^[:digit:]+$\")) %&gt;% \n  mutate(word = str_replace_na(word, \"hptts?://\\\\w+\")) %&gt;% \n  mutate(word = str_replace_na(word, \" +\")) %&gt;% \n  drop_na()\n\n\ndata(sentiws, package = \"pradadata\")\n\n\nkarl7 &lt;-\n  karl5 %&gt;% \n  inner_join(sentiws)\n\nJoining, by = \"word\"\n\n\n\nkarl7 %&gt;% \n  group_by(neg_pos) %&gt;% \n  summarise(senti_avg = mean(value, na.rm = TRUE),\n            senti_sd = sd(value, na.rm = TRUE),\n            senti_n = n())\n\n# A tibble: 2 × 4\n  neg_pos senti_avg senti_sd senti_n\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;int&gt;\n1 neg        -0.347    0.186       2\n2 pos         0.167    0.283       3\n\n\nAchtung, Sentimentanalyse sollte vor dem Stemming kommen.\n\nCategories:\n\ntextmining\ntwitter"
  },
  {
    "objectID": "posts/mariokart-mean4/mariokart-mean4.html",
    "href": "posts/mariokart-mean4/mariokart-mean4.html",
    "title": "mariokart-mean4",
    "section": "",
    "text": "Aufgabe\nImportieren Sie den Datensatz mariokart in R. Berechnen Sie den mittleren Verkaufspreis (total_pr) für Spiele, die neu sind oder (auch) über Lenkräder (wheels) verfügen.\nHinweise:\n\nRunden Sie auf 1 Dezimalstelle.\n\n         \n\n\nLösung\nPakete starten:\n\nlibrary(tidyverse)\nlibrary(easystats)\n\nDaten importieren:\n\nd_url &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\"\nd &lt;- data_read(d_url)\n\n\nsolution &lt;-\nd  %&gt;% \n  filter(cond == \"new\" | wheels &gt; 0) %&gt;% \n  summarise(pr_mean = mean(total_pr))\n\nsolution\n\n   pr_mean\n1 52.73218\n\n\nLösung: 52.73.\n\nCategories:\n\ndatawrangling\ndplyr\neda\nnum"
  },
  {
    "objectID": "posts/twitter02/twitter02.html",
    "href": "posts/twitter02/twitter02.html",
    "title": "twitter02",
    "section": "",
    "text": "Exercise\nLaden Sie die neuesten Tweets an karl_lauterbach herunter - und zwar so viele wie auf einmal möglich.\n         \n\n\nSolution\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(rtweet)\n\n\nAttaching package: 'rtweet'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\n\nEinloggen bei Twitter; zuerst die Credentials bereithalten:\n\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\n\n\nauth &lt;- rtweet_app(bearer_token = Bearer_Token)\n\nAus der Hilfe zu search_tweets:\nDescription\nReturns Twitter statuses matching a user provided search query. ONLY RETURNS DATA FROM THE PAST 6-9 DAYS.\nTweets an Karl Lauterbach suchen:\n\nkarl_tweets &lt;- search_tweets(q = \"@karl_lauterbach\", n = 150000, retryonratelimit = TRUE)\n\nWir könnten n auch auf Inf setzen, aber, da wir auf das Refreshen des Rate Limits warten müssen, könnte sehr lange dauern. Daher nehmen wir hier nur einen kürzeren Wert.\n\ndim(karl_tweets)\n\n[1] 18000    43\n\nhead(karl_tweets)\n\n# A tibble: 6 × 43\n  created_at               id id_str      full_…¹ trunc…² displ…³ entities     metad…⁴\n  &lt;dttm&gt;                &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;   &lt;lgl&gt;     &lt;dbl&gt; &lt;list&gt;       &lt;list&gt; \n1 2022-10-23 13:30:18 1.58e18 1584145185… \"Bei ⁦@… FALSE       122 &lt;named list&gt; &lt;df&gt;   \n2 2022-10-22 18:34:37 1.58e18 1583859379… \"Es is… FALSE       263 &lt;named list&gt; &lt;df&gt;   \n3 2022-10-22 17:56:39 1.58e18 1583849826… \"Die S… FALSE       215 &lt;named list&gt; &lt;df&gt;   \n4 2022-10-24 08:10:35 1.58e18 1584427113… \"Zu we… FALSE       219 &lt;named list&gt; &lt;df&gt;   \n5 2022-10-24 08:10:35 1.58e18 1584427113… \"RT @K… FALSE       140 &lt;named list&gt; &lt;df&gt;   \n6 2022-10-24 08:10:25 1.58e18 1584427072… \"RT @U… FALSE       139 &lt;named list&gt; &lt;df&gt;   \n# … with 35 more variables: source &lt;chr&gt;, in_reply_to_status_id &lt;dbl&gt;,\n#   in_reply_to_status_id_str &lt;chr&gt;, in_reply_to_user_id &lt;dbl&gt;,\n#   in_reply_to_user_id_str &lt;chr&gt;, in_reply_to_screen_name &lt;chr&gt;, geo &lt;list&gt;,\n#   coordinates &lt;list&gt;, place &lt;list&gt;, contributors &lt;lgl&gt;, is_quote_status &lt;lgl&gt;,\n#   retweet_count &lt;int&gt;, favorite_count &lt;int&gt;, favorited &lt;lgl&gt;, retweeted &lt;lgl&gt;,\n#   possibly_sensitive &lt;lgl&gt;, lang &lt;chr&gt;, quoted_status_id &lt;dbl&gt;,\n#   quoted_status_id_str &lt;chr&gt;, quoted_status &lt;list&gt;, retweeted_status &lt;list&gt;, …\n# ℹ Use `colnames()` to see all variable names\n\nCategories:\n\ntextmining\ntwitter"
  },
  {
    "objectID": "posts/ReThink4e1/ReThink4e1.html",
    "href": "posts/ReThink4e1/ReThink4e1.html",
    "title": "ReThink4e1",
    "section": "",
    "text": "Welche der folgenden Zeilen zeigt den Likelihood?\n\n\n\n\\(\\mu \\sim \\mathcal{N}(0, 10)\\)\n\\(\\sigma \\sim \\mathcal{U}(0, 1)\\)\n\\(y_i = \\beta_0 + \\beta_1\\cdot x\\)\n\\(y_i \\sim \\mathcal{N}(\\mu, \\sigma)\\)\n\nQuelle: McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Aufl.). Taylor and Francis, CRC Press."
  },
  {
    "objectID": "posts/ReThink4e1/ReThink4e1.html#answerlist",
    "href": "posts/ReThink4e1/ReThink4e1.html#answerlist",
    "title": "ReThink4e1",
    "section": "",
    "text": "\\(\\mu \\sim \\mathcal{N}(0, 10)\\)\n\\(\\sigma \\sim \\mathcal{U}(0, 1)\\)\n\\(y_i = \\beta_0 + \\beta_1\\cdot x\\)\n\\(y_i \\sim \\mathcal{N}(\\mu, \\sigma)\\)\n\nQuelle: McElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (2. Aufl.). Taylor and Francis, CRC Press."
  },
  {
    "objectID": "posts/ReThink4e1/ReThink4e1.html#answerlist-1",
    "href": "posts/ReThink4e1/ReThink4e1.html#answerlist-1",
    "title": "ReThink4e1",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch. Priori-Verteilung.\nFalsch. Priori-Verteilung.\nFalsch. Regressionsformel.\nWahr. Likelihood.\n\nMan könnte den Likelihood auch so schreiben:\n$y_i| , (, ) $,\nwas noch deutlicher macht, dass die Likelihood die Wahrscheinlichkeit der Daten (y) ausdrückt, gegeben der Modellparameter (\\(\\mu, \\sigma)\\).\n\nCategories:\n\nprobability\nbayes"
  },
  {
    "objectID": "posts/mariokart-mean3/mariokart-mean3.html",
    "href": "posts/mariokart-mean3/mariokart-mean3.html",
    "title": "mariokart-mean3",
    "section": "",
    "text": "Aufgabe\nImportieren Sie den Datensatz mariokart in R. Berechnen Sie den mittleren Verkaufspreis (total_pr) für Spiele, die sowohl neu sind als auch über Lenkräder (wheels) verfügen.\nHinweise:\n\nRunden Sie auf 1 Dezimalstelle.\n\n         \n\n\nLösung\nPakete starten:\n\nlibrary(tidyverse)\nlibrary(easystats)\n\nDaten importieren:\n\nd_url &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\"\nd &lt;- data_read(d_url)\n\n\nsolution &lt;-\nd  %&gt;% \n  filter(cond == \"new\" & wheels &gt; 0) %&gt;% \n  summarise(pr_mean = mean(total_pr))\n\nsolution\n\n   pr_mean\n1 54.28418\n\n\nLösung: 54.28.\n\nCategories:\n\ndatawrangling\ndplyr\neda\nnum"
  },
  {
    "objectID": "posts/purrr-map03/purrr-map03.html",
    "href": "posts/purrr-map03/purrr-map03.html",
    "title": "purrr-map03",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\nExercise\nImportieren Sie das Grundatzprogramm der Partei AfD (in der aktuellsten Version). Tokenisieren Sie nach Sätzen. Dann entfernen Sie alle Zahlen. Dann zählen Sie die Anzahl der Wörter pro Satz und berichten gängige deskriptive Statistiken dazu.\n         \n\n\nSolution\nText aus PDF-Dateien kann man mit dem Paket pdftools einlesen:\n\nlibrary(pdftools)\n\nUsing poppler version 22.02.0\n\nd_path &lt;- \"/Users/sebastiansaueruser/Google Drive/Literatur/_Div/Politik/afd-grundsatzprogramm-2022.pdf\"\n\nd &lt;- tibble(text = pdf_text(d_path))\n\nDann erstellen wir eine Tidy-Version und tokenisieren nach Sätzen:\n\nlibrary(tidytext)\nd2 &lt;-\n  d %&gt;% \n  unnest_sentences(output = word, input = text)\n\nhead(d2)\n\n# A tibble: 6 × 1\n  word                                                                          \n  &lt;chr&gt;                                                                         \n1 programm für deutschland.                                                     \n2 das grundsatzprogramm der alternative für deutschland.                        \n3 2   programm für deutschland | inhalt         präambel                       …\n4 familien stärken        43             und parteiferne rechnungshöfe         …\n5 3   programm für deutschland | inhalt         7 | kultur, sprache und identit…\n6 förder- und                         10.10.3 deutsche literatur im inland digi…\n\n\nDann entfernen wir die Zahlen:\n\nd3 &lt;- \n  d2 %&gt;% \n  mutate(word = str_remove_all(word, pattern = \"[:digit:]+\"))\n\nPrüfen wir, ob es geklappt hat:\n\nd2$word[10]\n\n[1] \"weniger subventionen    88      13.7 fischerei, forst und jagd: im einklang mit der natur     88      13.8 flächenkonkurrenz:           nicht zu lasten der land- und forstwirtschaft            88\"\n\nd3$word[10]\n\n[1] \"weniger subventionen          . fischerei, forst und jagd: im einklang mit der natur           . flächenkonkurrenz:           nicht zu lasten der land- und forstwirtschaft            \"\n\n\nOk.\nDann zählen wir die Wörter pro Satz:\n\nd4 &lt;- \n  d3 %&gt;% \n  summarise(word_count_per_sentence = str_count(word, \"\\\\w+\"))\n\nhead(d4)\n\n# A tibble: 6 × 1\n  word_count_per_sentence\n                    &lt;int&gt;\n1                       3\n2                       6\n3                     196\n4                      40\n5                     254\n6                      15\n\n\nVisualisierung:\n\nd4 %&gt;% \n  ggplot(aes(x = word_count_per_sentence)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.5.2 (red = needs update)\n✖ insight     0.18.5   ✖ datawizard  0.6.2 \n✔ bayestestR  0.13.0   ✔ performance 0.10.0\n✔ parameters  0.19.0   ✖ effectsize  0.8.1 \n✔ modelbased  0.8.5    ✔ correlation 0.8.3 \n✔ see         0.7.3    ✔ report      0.5.5 \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`.\n\ndescribe_distribution(d4)\n\nVariable                |  Mean |    SD | IQR |          Range | Skewness | Kurtosis |    n | n_Missing\n-------------------------------------------------------------------------------------------------------\nword_count_per_sentence | 21.86 | 17.24 |  19 | [0.00, 254.00] |     3.84 |    37.52 | 1208 |         0\n\n\n\nCategories:\n\nr\nmap\ntidyverse"
  },
  {
    "objectID": "posts/summarise02/summarise02.html",
    "href": "posts/summarise02/summarise02.html",
    "title": "summarise02",
    "section": "",
    "text": "Aufgabe\nIm Datensatz mariokart:\n\nGruppieren Sie danach, ob ein Foto bei der Auktion dabei war (stock_photo).\nFassen Sie die Spalte total_pr zusammen und zwar zum maximalwert - pro Gruppe!\nBerechnen Sie den Mittelwert dieser beiden Zahlen!\n\nGeben Sie diese Zahl als Antwort zurück!\n         \n\n\nLösung\nPakete starten:\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.6.0 (red = needs update)\n✖ bayestestR  0.13.0   ✔ correlation 0.8.4 \n✔ datawizard  0.7.1    ✔ effectsize  0.8.3 \n✔ insight     0.19.1   ✔ modelbased  0.8.6 \n✖ performance 0.10.2   ✖ parameters  0.20.3\n✔ report      0.5.7    ✔ see         0.7.5 \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`.\n\nlibrary(tidyverse)  # startet das Paket tidyverse\n\nDaten importieren:\n\nmariokart &lt;- data_read(\"https://vincentarelbundock.github.io/Rdatasets/csv/openintro/mariokart.csv\")\n\nZusammenfassen:\n\nmariokart_gruppiert &lt;- group_by(mariokart, stock_photo)  # Gruppieren\nmariokart_klein &lt;- summarise(mariokart_gruppiert, max_preis = max(total_pr))  # zusammenfassen\nmariokart_klein\n\n# A tibble: 2 × 2\n  stock_photo max_preis\n  &lt;chr&gt;           &lt;dbl&gt;\n1 no               327.\n2 yes               75 \n\n\n\nsummarise(mariokart_klein, max_preis_mw = mean(max_preis))\n\n# A tibble: 1 × 1\n  max_preis_mw\n         &lt;dbl&gt;\n1         201.\n\n\nmin analog.\nDie Lösung lautet: 201\n\nCategories:\n\ndatawrangling\neda\ntidyverse\ndplyr\nnum"
  },
  {
    "objectID": "posts/penguins-stan-01/penguins-stan-01.html",
    "href": "posts/penguins-stan-01/penguins-stan-01.html",
    "title": "penguins-stan-01",
    "section": "",
    "text": "Exercise\nWir untersuchen Einflussfaktoren bzw. Prädiktoren auf das Körpergewicht von Pinguinen. In dieser Aufgabe untersuchen wir in dem Zusammenhang den Zusammenhang von Schnabellänge (als UV) und Körpergewicht (als AV).\nWie groß ist der statistische Einfluss der UV auf die AV?\n\nBerechnen Sie den Punktschätzer des Effekts!\nWie viele Parameter hat das Modell?\nGeben Sie die Breite eines 90%-HDI an (zum Effekt)!\nWie groß ist die Wahrscheinlichkeit, dass der Effekt vorhanden ist (also größer als Null ist), die “Effektwahrscheinlichkeit”?\nWie groß ist das 95%-HDI, wenn Sie nur die Spezies Adelie untersuchen?\nGeben Sie die Prioris an für m1 für die Regressionskoeffizienten!\n\nHinweise:\n\nNutzen Sie den Datensatz zu den Palmer Penguins.\nVerwenden Sie Methoden der Bayes-Statistik und die Software Stan.\nFixieren Sie die Zufallszahlen auf den Startwert 42!\nSie können den Datensatz z.B. hier beziehen oder über das R-Paket palmerpenguins.\n\n         \n\n\nSolution\nZentrieren ist eigentlich immer nützlich, aber hier streng genommen nicht unbedingt nötig. Der Hauptgrund ist, dass Stan für uns den Prior für den Intercept festlegt, und zwar auf Basis der Daten, wir uns also nicht um die komische Frage zu kümmern brauchen, welchen Prior wir für den unzentrierten Achsenabschnitt vergeben wollten: Wie schwer sind Pinguins der Schnabellänge Null? Mit zentrierten Prädiktoren ist die Frage nach dem Prior viel einfacher zu beantworten: Wie schwer ist ein Pinguin mit mittelgroßem Schnabel?\nSetup:\n\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(rstanarm)\n\ndata(\"penguins\", package = \"palmerpenguins\")\n\nEs wird in dieser Aufgabe vorausgesetzt, dass Sie den Datensatz selbständig importieren können. Tipp: Kurzes Googeln hilft ggf., den Datensatz zu finden.\nAlternativ könnten Sie den Datensatz als CSV-Datei importieren:\n\nd_path &lt;- \"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\"\npenguins &lt;- data_read(d_path)\n\nEin Blick in die Daten zur Kontrolle, ob das Importieren richtig funktioniert hat:\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nVertrauen ist gut, aber - was Golems betrifft - ist Kontrolle eindeutig besser ;-)\n\nPunktschätzer\n\n\nm1 &lt;- stan_glm(body_mass_g ~  bill_length_mm,  # Regressionsgleichung\n               data = penguins, #  Daten\n               seed = 42,  # Repro.\n               refresh = 0)  # nicht so viel Output\n\n\nparameters(m1, ci_method = \"hdi\", ci = .9)\n\nParameter      | Median |            90% CI |     pd |  Rhat |     ESS |                       Prior\n----------------------------------------------------------------------------------------------------\n(Intercept)    | 356.57 | [-108.76, 827.61] | 89.92% | 1.000 | 3249.00 | Normal (4201.75 +- 2004.89)\nbill_length_mm |  87.45 | [  76.24,  97.70] |   100% | 1.000 | 3216.00 |     Normal (0.00 +- 367.22)\n\n\n\nUncertainty intervals (highest-density) and p-values (two-tailed)\n  computed using a MCMC distribution approximation.\n\n\n\nAnzahl Parameter\n\nDas Modell hat 3 Paramter:\n\n\\(\\beta_0\\) (oder \\(\\alpha\\))\n\\(\\beta_01\\)\n\\(\\sigma\\)\n\n\nBreite des Intervalls\n\n\n98.11  - 77.15\n\n[1] 20.96\n\n\nEinheit: mm\n\nEffektwahrscheinlichkeit\n\n\nm1_post &lt;-\n  m1 %&gt;% \n  as_tibble()\n\nm1_post %&gt;% \n  count(bill_length_mm &gt; 0)\n\n# A tibble: 1 × 2\n  `bill_length_mm &gt; 0`     n\n  &lt;lgl&gt;                &lt;int&gt;\n1 TRUE                  4000\n\n\nAlso: 100% oder 1.\nMan kann diesen Wert aus der Tabelle oben (Ausgabe von parameters()) einfach in der Spalte pd ablesen. pd steht für probability of direction, s. Details hier.\nOder so, ist auch einfach:\n\np_direction(m1) # aus Paket easystats\n\nProbability of Direction\n\nParameter      |     pd\n-----------------------\n(Intercept)    | 89.92%\nbill_length_mm |   100%\n\n\nUnd plotten ist meist hilfreich:\n\nplot(p_direction(m1))\n\n\n\n\n\nNur Adelie:\n\nWelche Spezies gibt es im Datensatz?\n\npenguins %&gt;% \n  count(species)\n\n# A tibble: 3 × 2\n  species       n\n  &lt;fct&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\nFiltern:\n\npenguins_adelie &lt;-\n  penguins %&gt;% \n  filter(species == \"Adelie\")\n\nModell berechnen:\n\nm2 &lt;- stan_glm(body_mass_g ~  bill_length_mm,  # Regressionsgleichung\n               data = penguins, #  Daten\n               seed = 42,  # Repro.\n               refresh = 0)  # nicht so viel Output\n\nDas Modell ist - bis auf die Daten - identisch zu m1.\n\nparameters(m2)\n\nParameter      | Median |            95% CI |     pd |  Rhat |     ESS |                       Prior\n----------------------------------------------------------------------------------------------------\n(Intercept)    | 356.57 | [-192.35, 924.99] | 89.92% | 1.000 | 3249.00 | Normal (4201.75 +- 2004.89)\nbill_length_mm |  87.45 | [  74.76,  99.91] |   100% | 1.000 | 3216.00 |     Normal (0.00 +- 367.22)\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a MCMC distribution approximation.\n\n\n\nhdi(m2, parameters = \"bill_length_mm\")\n\nHighest Density Interval\n\nParameter      |         95% HDI\n--------------------------------\nbill_length_mm | [75.22, 100.30]\n\n\nS. auch Tabelle oben.\n\n99.15 - 74.46\n\n[1] 24.69\n\n\n\nPrioris\n\n\ndescribe_prior(m1, component = \"auxiliary\")\n\n       Parameter Prior_Distribution Prior_Location Prior_Scale\n1    (Intercept)             normal       4201.754   2004.8863\n2 bill_length_mm             normal          0.000    367.2233\n\n\nSteht auch in der Tabelle, die von parameters ausgegeben wird.\n\nCategories:\n\nbayes\nregression"
  },
  {
    "objectID": "posts/purrr-map04/purrr-map04.html",
    "href": "posts/purrr-map04/purrr-map04.html",
    "title": "purrr-map04",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\nExercise\nImportieren Sie das Grundatzprogramm der Partei AfD (in der aktuellsten Version). Tokenisieren Sie nach Seiten. Dann verschachteln Sie die Spalte, in denen der Text der Seite steht, zu einer Listenspalte. Schließlich zählen Sie die Anzahl der Wörter pro Seite und berichten gängige deskriptive Statistiken dazu.\n         \n\n\nSolution\nText aus PDF-Dateien kann man mit dem Paket pdftools einlesen:\n\nlibrary(pdftools)\n\nUsing poppler version 22.02.0\n\nd_path &lt;- \"/Users/sebastiansaueruser/Google Drive/Literatur/_Div/Politik/afd-grundsatzprogramm-2022.pdf\"\n\nd &lt;- tibble(text = pdf_text(d_path),\n            page = 1:length(text))\n\nZu Seiten tokenisieren brauchen wir nicht; das Datenmaterial ist bereits nach Seiten organisiert.\nJetzt “verschachteln” (to nest) wir die Spalte mit dem Text:\n\nd2 &lt;-\n  d %&gt;% \n  nest(data = text)\n\nhead(d2)\n\n# A tibble: 6 × 2\n   page data            \n  &lt;int&gt; &lt;list&gt;          \n1     1 &lt;tibble [1 × 1]&gt;\n2     2 &lt;tibble [1 × 1]&gt;\n3     3 &lt;tibble [1 × 1]&gt;\n4     4 &lt;tibble [1 × 1]&gt;\n5     5 &lt;tibble [1 × 1]&gt;\n6     6 &lt;tibble [1 × 1]&gt;\n\n\nDann zählen wir die Wörter pro Seite:\n\nd3 &lt;-\n  d2 %&gt;% \n  mutate(word_count_per_page = map(data, ~ str_count(.x$text, \"\\\\w+\")))\n\nhead(d3)\n\n# A tibble: 6 × 3\n   page data             word_count_per_page\n  &lt;int&gt; &lt;list&gt;           &lt;list&gt;             \n1     1 &lt;tibble [1 × 1]&gt; &lt;int [1]&gt;          \n2     2 &lt;tibble [1 × 1]&gt; &lt;int [1]&gt;          \n3     3 &lt;tibble [1 × 1]&gt; &lt;int [1]&gt;          \n4     4 &lt;tibble [1 × 1]&gt; &lt;int [1]&gt;          \n5     5 &lt;tibble [1 × 1]&gt; &lt;int [1]&gt;          \n6     6 &lt;tibble [1 × 1]&gt; &lt;int [1]&gt;          \n\n\nWie sieht eine Zelle aus data aus?\n\nd3$data[[1]]\n\n# A tibble: 1 × 1\n  text                                                                          \n  &lt;chr&gt;                                                                         \n1 \"PROGRAMM FÜR\\nDEUTSCHLAND.\\nDas Grundsatzprogramm der Alternative für Deutsc…\n\n\nWie sieht eine Zelle aus word_count_per_page aus?\n\nd3$data[[1]]\n\n# A tibble: 1 × 1\n  text                                                                          \n  &lt;chr&gt;                                                                         \n1 \"PROGRAMM FÜR\\nDEUTSCHLAND.\\nDas Grundsatzprogramm der Alternative für Deutsc…\n\n\nAh! Darin steckt nur eine einzelne Zahl!\n\nd3$data[[1]] %&gt;% str()\n\ntibble [1 × 1] (S3: tbl_df/tbl/data.frame)\n $ text: chr \"PROGRAMM FÜR\\nDEUTSCHLAND.\\nDas Grundsatzprogramm der Alternative für Deutschland.\\n\"\n\n\nDas heißt, wir können vereinfachen, entschacheln:\n\nd4 &lt;-\n  d3 %&gt;% \n  unnest(word_count_per_page)\n\nhead(d4)\n\n# A tibble: 6 × 3\n   page data             word_count_per_page\n  &lt;int&gt; &lt;list&gt;                         &lt;int&gt;\n1     1 &lt;tibble [1 × 1]&gt;                   9\n2     2 &lt;tibble [1 × 1]&gt;                 410\n3     3 &lt;tibble [1 × 1]&gt;                 516\n4     4 &lt;tibble [1 × 1]&gt;                 297\n5     5 &lt;tibble [1 × 1]&gt;                   1\n6     6 &lt;tibble [1 × 1]&gt;                 414\n\n\nVisualisierung:\n\nd4 %&gt;% \n  ggplot(aes(x = word_count_per_page)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.5.2 (red = needs update)\n✖ insight     0.18.5   ✖ datawizard  0.6.2 \n✔ bayestestR  0.13.0   ✔ performance 0.10.0\n✔ parameters  0.19.0   ✖ effectsize  0.8.1 \n✔ modelbased  0.8.5    ✔ correlation 0.8.3 \n✔ see         0.7.3    ✔ report      0.5.5 \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`.\n\ndescribe_distribution(d4$word_count_per_page)\n\n  Mean |     SD |    IQR |          Range | Skewness | Kurtosis |  n | n_Missing\n--------------------------------------------------------------------------------\n285.84 | 172.27 | 322.75 | [1.00, 516.00] |    -0.64 |    -1.24 | 96 |         0\n\n\n\nCategories:\n\nr\nmap\ntidyverse"
  },
  {
    "objectID": "posts/Stichprobenziehen1/Stichprobenziehen1.html",
    "href": "posts/Stichprobenziehen1/Stichprobenziehen1.html",
    "title": "Stichprobenziehen1",
    "section": "",
    "text": "Exercise\nIn dieser Übung untersuchen wir den Effekt der Stichprobengröße auf die Genauigkeit der Schätzung. Und zwar auf praktische Art und Weise.\nAls praktisches Beispiel soll uns dabei die Körpergröße dienen. Wir erfragen die Körpergröße der Studis und betrachten den Mittelwert einer Stichrpobe in Abhängigkeit der Größe der Stichprobe.\n\nGeben Sie anonym Ihre Körpergröße hier ein.\nSie können die Daten hier beziehen.\nBerechnen Sie den Mittelwert der Körpergröße für eine zufällige Stichprobe der Größen \\(n=5\\) und \\(n=50\\)\nDann berechnen Sie die den “echten” Mittelwert der Studis; damit ist der Mittelwert aller Werte der Tabelle gemeint.\nDiskutieren Sie die Ergebnisse!\nWird die Schätzung genauer bei größerer Stichprobe?\nWird die Schätzung “robuster” (weniger schwankend) bei größerer Stichprobe?\n\n         \n\n\nSolution\nIndividuell\n\nCategories:\n\nlm\ninference\nqm2"
  },
  {
    "objectID": "posts/tidymodels-poly02/tidymodels-poly02.html",
    "href": "posts/tidymodels-poly02/tidymodels-poly02.html",
    "title": "tidymodels-poly02",
    "section": "",
    "text": "Aufgabe\nFitten Sie ein Polynomial-Modell für folgende Modellgleichung:\nbody_mass_g ~ bill_length_mm.\nGesucht ist der RMSE im Test-Set (optimal hinsichtlich minimalem Prognosefehler).\nHinweise:\n\nDatensatz penguins (palmerpenguins)\nVerwenden Sie Tidymodels\nFitten Sie Polynome des Grades 1 bis 10.\nDefinieren Sie die Polynomegrade als Tuningparameter.\nEntfernen Sie fehlende Werte in den Prädiktoren.\nWie immer gilt: Verwenden Sie die Standardeinstellungen der Funktionen, soweit nicht anders angegeben.\n\n         \n\n\nLösung\nSetup:\n\nlibrary(tidymodels)\ndata(penguins, package = \"palmerpenguins\")\n\nDatenaufteilung:\n\nd_split &lt;- initial_split(penguins)\nd_train &lt;- training(d_split)\nd_test &lt;- testing(d_split)\n\nRezept:\n\nrec1 &lt;- \n  recipe(body_mass_g ~ bill_length_mm, data = penguins) %&gt;% \n  step_naomit(all_predictors()) %&gt;% \n  step_poly(all_predictors(), degree = tune()) %&gt;% \n  update_role(contains(\"_poly_\"), new_role = \"predictor\")\n\nWarning: No columns were selected in `update_role()`.\n\n\nCheck:\n\nd_baked &lt;- bake(prep(rec1), new_data = NULL)\n\nRezepte mit Tuningparametern kann man nicht preppen/backen.\nWorkflow:\n\nwf1 &lt;-\n  workflow() %&gt;% \n  add_model(linear_reg()) %&gt;% \n  add_recipe(rec1)\n\nTuning:\n\nset.seed(42)\ntune1 &lt;-\n  tune_grid(\n    wf1,\n    resamples = vfold_cv(data = penguins),\n    metrics = metric_set(rmse),\n    grid = grid_regular(degree(range = c(1, 10)),\n                               levels = 10),\n    control = control_grid(save_workflow = TRUE)\n  )\n\n\nautoplot(tune1)\n\n\n\n\n\n\n\n\n\nshow_best(tune1)\n\n# A tibble: 5 × 7\n  degree .metric .estimator  mean     n std_err .config              \n   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1      2 rmse    standard    638.    10    22.7 Preprocessor02_Model1\n2      4 rmse    standard    641.    10    23.7 Preprocessor04_Model1\n3      1 rmse    standard    643.    10    21.8 Preprocessor01_Model1\n4      5 rmse    standard    643.    10    23.5 Preprocessor05_Model1\n5      3 rmse    standard    643.    10    24.2 Preprocessor03_Model1\n\n\nFinalisieren:\n\nbest1 &lt;- fit_best(tune1)\nbest1\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_naomit()\n• step_poly()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n          (Intercept)  bill_length_mm_poly_1  bill_length_mm_poly_2  \n                 4202                   8813                  -1708  \n\n\nPredicten:\n\nfinal1 &lt;- last_fit(best1, d_split)\ncollect_metrics(final1)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard     605.    Preprocessor1_Model1\n2 rsq     standard       0.323 Preprocessor1_Model1\n\n\nOder so:\n\nsol &lt;- \npredict(best1, new_data = d_test) %&gt;% \n  bind_cols(d_test) %&gt;% \n  rmse(truth = body_mass_g, estimate = .pred) %&gt;% \n  pull(.estimate) %&gt;% \n  pluck(1)\n\nsol\n\n[1] 598.9329\n\n\nDie Antwort lautet: 598.9328583.\n\nCategories:\n\nR\nstat-learning\ntidymodels\nnum"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "datenwerk",
    "section": "",
    "text": "Kennwert-robust\n\n\n\n\n\n\n\neda\n\n\nlagemaß\n\n\nschoice\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nmw-berechnen\n\n\n\n\n\n\n\neda\n\n\ndata-wrangling\n\n\ndyn\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nmariokart-max2\n\n\n\n\n\n\n\ndatawrangling\n\n\ndplyr\n\n\neda\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nnasa01\n\n\n\n\n\n\n\ndata\n\n\neda\n\n\nlagemaße\n\n\nstring\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nmariokart-mean1\n\n\n\n\n\n\n\ndatawrangling\n\n\ndplyr\n\n\neda\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nwrangle10\n\n\n\n\n\n\n\neda\n\n\nlagemaße\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nsummarise01\n\n\n\n\n\n\n\ndatawrangling\n\n\neda\n\n\ntidyverse\n\n\ndplyr\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nmariokart-max1\n\n\n\n\n\n\n\ndatawrangling\n\n\ndplyr\n\n\neda\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nSchiefe1\n\n\n\n\n\n\n\nschoice\n\n\neda\n\n\ndistributions\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nSchiefe-erkennen\n\n\n\n\n\n\n\neda\n\n\ndistributions\n\n\nschoice\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nnasa02\n\n\n\n\n\n\n\ndata\n\n\neda\n\n\nlagemaße\n\n\nstring\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nsummarise03\n\n\n\n\n\n\n\ndatawrangling\n\n\neda\n\n\ntidyverse\n\n\ndplyr\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nmariokart-mean2\n\n\n\n\n\n\n\ndatawrangling\n\n\ndplyr\n\n\neda\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nmariokart-mean4\n\n\n\n\n\n\n\ndatawrangling\n\n\ndplyr\n\n\neda\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nmariokart-mean3\n\n\n\n\n\n\n\ndatawrangling\n\n\ndplyr\n\n\neda\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nsummarise02\n\n\n\n\n\n\n\ndatawrangling\n\n\neda\n\n\ntidyverse\n\n\ndplyr\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\ntidymodels-penguins02\n\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nprediction\n\n\nyacsda\n\n\nstat-learning\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2023\n\n\n\n\n\n\n  \n\n\n\n\ntidymodels-penguins05\n\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nprediction\n\n\nyacsda\n\n\nstat-learning\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2023\n\n\n\n\n\n\n  \n\n\n\n\ntidymodels-poly01\n\n\n\n\n\n\n\nR\n\n\nstat-learning\n\n\ntidymodels\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2023\n\n\n\n\n\n\n  \n\n\n\n\ntidymodels-penguins04\n\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nprediction\n\n\nyacsda\n\n\nstat-learning\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2023\n\n\n\n\n\n\n  \n\n\n\n\ntidymodels-penguins03\n\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nprediction\n\n\nyacsda\n\n\nstat-learning\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2023\n\n\n\n\n\n\n  \n\n\n\n\ntidymodels-penguins01\n\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nprediction\n\n\nyacsda\n\n\nstat-learning\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2023\n\n\n\n\n\n\n  \n\n\n\n\nknn-ames01\n\n\n\n\n\n\n\nstat-learning\n\n\ntidymodels\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2023\n\n\n\n\n\n\n  \n\n\n\n\ntidymodels-poly02\n\n\n\n\n\n\n\nR\n\n\nstat-learning\n\n\ntidymodels\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2023\n\n\n\n\n\n\n  \n\n\n\n\ntidydata1\n\n\n\n\n\n\n\ndatawrangling\n\n\ntidy\n\n\nschoice\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n  \n\n\n\n\nwrangle9\n\n\n\n\n\n\n\neda\n\n\n2023\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n  \n\n\n\n\ntidymodels-ames-03\n\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nprediction\n\n\nyacsda\n\n\nstat-learning\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n  \n\n\n\n\nwrangle7\n\n\n\n\n\n\n\neda\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n  \n\n\n\n\ntidymodels-ames-04\n\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nprediction\n\n\nyacsda\n\n\nstat-learning\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n  \n\n\n\n\nfilter01\n\n\n\n\n\n\n\ndatawrangling\n\n\neda\n\n\ntidyverse\n\n\ndplyr\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n  \n\n\n\n\ntidymodels-ames-02\n\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nprediction\n\n\nyacsda\n\n\nstat-learning\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n  \n\n\n\n\nwrangle1\n\n\n\n\n\n\n\neda\n\n\ndatawrangling\n\n\ntidyverse\n\n\ndplyr\n\n\nschoice\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n  \n\n\n\n\naffairs-dplyr\n\n\n\n\n\n\n\ndatawrangling\n\n\neda\n\n\nstring\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n  \n\n\n\n\nbike01\n\n\n\n\n\n\n\nstat-learning\n\n\ntidymodels\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n  \n\n\n\n\nMWberechnen\n\n\n\n\n\n\n\neda\n\n\ndatawrangling\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n  \n\n\n\n\nmutate01\n\n\n\n\n\n\n\ndatawrangling\n\n\neda\n\n\ntidyverse\n\n\ndplyr\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n  \n\n\n\n\ndplyr-uebersetzen\n\n\n\n\n\n\n\ndatawrangling\n\n\ntidyverse\n\n\nstring\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n  \n\n\n\n\nwrangle4\n\n\n\n\n\n\n\neda\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n  \n\n\n\n\nwrangle3\n\n\n\n\n\n\n\ndatawrangling\n\n\neda\n\n\nschoice\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n  \n\n\n\n\ntidymodels-ames-01\n\n\n\n\n\n\n\nds1\n\n\ntidymodels\n\n\nprediction\n\n\nyacsda\n\n\nstat-learning\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n  \n\n\n\n\nwrangle5\n\n\n\n\n\n\n\neda\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n  \n\n\n\n\nhaeufigkeit01\n\n\n\n\n\n\n\ndatawrangling\n\n\neda\n\n\ncount\n\n\nstring\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2023\n\n\n\n\n\n\n  \n\n\n\n\nLogikpruefung2\n\n\n\n\n\n\n\nR\n\n\n2023\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n  \n\n\n\n\nTyp-Fehler-R-01\n\n\n\n\n\n\n\nR\n\n\n2023\n\n\nstring\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n  \n\n\n\n\nthere-is-no-package\n\n\n\n\n\n\n\nR\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n  \n\n\n\n\nWertberechnen2\n\n\n\n\n\n\n\nR\n\n\ndyn\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n  \n\n\n\n\nTyp-Fehler-R-07\n\n\n\n\n\n\n\nR\n\n\n2023\n\n\nmchoice\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n  \n\n\n\n\nWertzuweisen_mc\n\n\n\n\n\n\n\nR\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n  \n\n\n\n\nTyp-Fehler-R-06a\n\n\n\n\n\n\n\nR\n\n\n2023\n\n\nstring\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n  \n\n\n\n\nargumente\n\n\n\n\n\n\n\nR\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n  \n\n\n\n\nLogikpruefung1\n\n\n\n\n\n\n\nR\n\n\n2023\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n  \n\n\n\n\nimport-mtcars\n\n\n\n\n\n\n\nR\n\n\ndata\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n  \n\n\n\n\nWertzuweisen\n\n\n\n\n\n\n\nR\n\n\n2023\n\n\nstring\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n  \n\n\n\n\nWertpruefen\n\n\n\n\n\n\n\nR\n\n\n2023\n\n\nstring\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n  \n\n\n\n\nTyp-Fehler-R-02\n\n\n\n\n\n\n\nR\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n  \n\n\n\n\nTyp-Fehler-R-04\n\n\n\n\n\n\n\nR\n\n\n2023\n\n\nstring\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n  \n\n\n\n\nTyp-Fehler-R-03\n\n\n\n\n\n\n\nstring\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n  \n\n\n\n\nPfad\n\n\n\n\n\n\n\nR\n\n\npath\n\n\ndatawrangling\n\n\nqm1\n\n\nqm2\n\n\nstring\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n  \n\n\n\n\nWertberechnen\n\n\n\n\n\n\n\nR\n\n\ndyn\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n  \n\n\n\n\nboxhist\n\n\n\n\n\n\n\nvis\n\n\neda\n\n\nen\n\n\ncloze\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n  \n\n\n\n\nmax-corr1\n\n\n\n\n\n\n\nvis\n\n\n2023\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n  \n\n\n\n\nDiamonds-Histogramm-Vergleich2\n\n\n\n\n\n\n\nvis\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n  \n\n\n\n\nboxplots-de1a\n\n\n\n\n\n\n\nvis\n\n\neda\n\n\nschoice\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n  \n\n\n\n\nmovies-vis2\n\n\n\n\n\n\n\nvis\n\n\neda\n\n\nstring\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n  \n\n\n\n\nRidges-vergleichen\n\n\n\n\n\n\n\nvis\n\n\ndyn\n\n\nschoice\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n  \n\n\n\n\nDiamonds-Histogramm-Vergleich\n\n\n\n\n\n\n\nvis\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n  \n\n\n\n\nmax-corr2\n\n\n\n\n\n\n\nvis\n\n\n2023\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n  \n\n\n\n\nHistogramm-in-Boxplot\n\n\n\n\n\n\n\nvis\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n  \n\n\n\n\nmin-corr1\n\n\n\n\n\n\n\nvis\n\n\n2023\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n  \n\n\n\n\nwozu-balkendiagramm\n\n\n\n\n\n\n\nvis\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n  \n\n\n\n\nmovies-vis1\n\n\n\n\n\n\n\nvis\n\n\neda\n\n\nstring\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n  \n\n\n\n\nBoxplot-Aussagen\n\n\n\n\n\n\n\nvis\n\n\neda\n\n\ndyn\n\n\nschoice\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n  \n\n\n\n\nwozu-streudiagramm\n\n\n\n\n\n\n\nvis\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n  \n\n\n\n\nStreudiagramm\n\n\n\n\n\n\n\nvis\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n  \n\n\n\n\ndiamonds-histogram\n\n\n\n\n\n\n\nvis\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n  \n\n\n\n\nn-vars-diagram\n\n\n\n\n\n\n\nvis\n\n\n2023\n\n\nnum\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\n\n\n\n\n  \n\n\n\n\nvariation01\n\n\n\n\n\n\n\nvariation\n\n\nbasics\n\n\nschoice\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nDef-Statistik01\n\n\n\n\n\n\n\nschoice\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nKausale-Verben\n\n\n\n\n\n\n\ncausal\n\n\nresearch-question\n\n\nmchoice\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\ntidy1\n\n\n\n\n\n\n\ntidy\n\n\ndata-wrangling\n\n\nschoice\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nSkalenniveau1a\n\n\n\n\n\n\n\ndyn\n\n\nscale-level\n\n\nvariable-niveau\n\n\nschoice\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nZiele-Statistik\n\n\n\n\n\n\n\nbasics\n\n\n2023\n\n\nschoice\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nvariation02\n\n\n\n\n\n\n\nvariation\n\n\nbasics\n\n\nschoice\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nSkalenniveau1b\n\n\n\n\n\n\n\ndyn\n\n\nscale-level\n\n\nvariable-niveau\n\n\nmchoice\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nkausal02\n\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\nexam-22\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n  \n\n\n\n\nkausal03\n\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\nexam-22\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n  \n\n\n\n\nwuerfel01\n\n\n\n\n\n\n\nprobability\n\n\ndice\n\n\nexam-22\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n  \n\n\n\n\nmtcars-post2\n\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\npost\n\n\nexam-22\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n  \n\n\n\n\nmtcars-abhaengig_var2\n\n\n\n\n\n\n\ndyn\n\n\nprobability\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n  \n\n\n\n\nmtcars-post3\n\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\npost\n\n\nexam-22\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n  \n\n\n\n\npenguins-stan-04\n\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\nexam-22\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n  \n\n\n\n\npenguins-stan-03\n\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\nexam-22\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n  \n\n\n\n\npenguins-stan-02\n\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\nexam-22\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n  \n\n\n\n\npenguins-stan-05\n\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\nexam-22\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n  \n\n\n\n\nkausal01\n\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\nexam-22\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n  \n\n\n\n\niq01\n\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\nexam-22\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n  \n\n\n\n\npigs2\n\n\n\n\n\n\n\nbayes\n\n\n\n\n\nregression\n\n\nexam-22\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n  \n\n\n\n\nRegression6\n\n\n\n\n\n\n\ndyn\n\n\nregression\n\n\nexam-22\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n  \n\n\n\n\nrope2\n\n\n\n\n\n\n\nrope\n\n\nbayes\n\n\nregression\n\n\nexam-22\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n  \n\n\n\n\nPPV1a-mtcars\n\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\nexam-22\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n  \n\n\n\n\nmtcars-post\n\n\n\n\n\n\n\nbayes\n\n\npost\n\n\nestimation\n\n\nexam-22\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2023\n\n\n\n\n\n\n  \n\n\n\n\nkausal05\n\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nkausal04\n\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nkausal-bedrooms1\n\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nkausal-einfach\n\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nkausal21\n\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nkausal26\n\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nkausal10\n\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nkausal27\n\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nkausal20\n\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nkausal06\n\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nkausal08\n\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nkausal09\n\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nkausal07\n\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nrandomdag1\n\n\n\n\n\n\n\ncausal\n\n\ndag\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nkausal_corona_glatze\n\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nkausal25\n\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nkausal22\n\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nMediterran-Alk\n\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nkausal23\n\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nkausal24\n\n\n\n\n\n\n\ndag\n\n\ncausal\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nttest-skalenniveau\n\n\n\n\n\n\n\nttest\n\n\nregression\n\n\nvariable-levels\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n  \n\n\n\n\nRegression2\n\n\n\n\n\n\n\nregression\n\n\ndyn\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n  \n\n\n\n\nRegression3\n\n\n\n\n\n\n\ndyn\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n  \n\n\n\n\nRegr-Bayes-interpret\n\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n  \n\n\n\n\nNullhyp-Beispiel\n\n\n\n\n\n\n\nnullhypothesis\n\n\nsignificance\n\n\ninference\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n  \n\n\n\n\nInteraktionseffekt1\n\n\n\n\n\n\n\ninteraction\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n  \n\n\n\n\nRegr-Bayes-interpret03\n\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n  \n\n\n\n\nrope-regr\n\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\nrope\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n  \n\n\n\n\nRegr-Bayes-interpret02\n\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n  \n\n\n\n\nposterior_interval\n\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\npost\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n  \n\n\n\n\nmtcars-rope1\n\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n  \n\n\n\n\nstan_glm_prioriwerte\n\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n  \n\n\n\n\nrope4\n\n\n\n\n\n\n\nrope\n\n\nbayes\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n  \n\n\n\n\nrope3\n\n\n\n\n\n\n\nrope\n\n\nbayes\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n  \n\n\n\n\ndiamonds-nullhyp-mws\n\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\nnullhypothesis\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n  \n\n\n\n\nzwert-berechnen\n\n\n\n\n\n\n\nz-value\n\n\nR\n\n\nmath\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n  \n\n\n\n\nanova-skalenniveau\n\n\n\n\n\n\n\nvariable-levels\n\n\nanova\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n  \n\n\n\n\nGriech-Buchstaben-Inferenz\n\n\n\n\n\n\n\nqm2\n\n\ninference\n\n\nparameters\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2022\n\n\n\n\n\n\n  \n\n\n\n\nrope1\n\n\n\n\n\n\n\nrope\n\n\nbayes\n\n\n\n\n\n\n\n\n\n\n\nDec 13, 2022\n\n\n\n\n\n\n  \n\n\n\n\nstan_glm_parameterzahl\n\n\n\n\n\n\n\n\n\n\n\n\nDec 13, 2022\n\n\n\n\n\n\n  \n\n\n\n\nBayes-Ziel1\n\n\n\n\n\n\n\nregression\n\n\nbayes\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\n\n\n\n\n  \n\n\n\n\nLikelihood2\n\n\n\n\n\n\n\nregression\n\n\nbayes\n\n\nlikelihood\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\n\n\n\n\n  \n\n\n\n\nBayesmod-bestimmen01\n\n\n\n\n\n\n\nregression\n\n\nbayes\n\n\nprior\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\n\n\n\n\n  \n\n\n\n\nPost-befragen1\n\n\n\n\n\n\n\nregression\n\n\nbayes\n\n\nposterior\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\n\n\n\n\n  \n\n\n\n\nPostvert-Regr-01\n\n\n\n\n\n\n\nregression\n\n\nbayes\n\n\nposterior\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\n\n\n\n\n  \n\n\n\n\nregression1a\n\n\n\n\n\n\n\nregression\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\n\n\n\n\n  \n\n\n\n\nBed-Post-Wskt1\n\n\n\n\n\n\n\nregression\n\n\nbayes\n\n\nposterior\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\n\n\n\n\n  \n\n\n\n\nPriorwahl1\n\n\n\n\n\n\n\nfat-tails\n\n\ndistributions\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\n\n\n\n\n  \n\n\n\n\nBayesmod-bestimmen02\n\n\n\n\n\n\n\nregression\n\n\nbayes\n\n\nprior\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\n\n\n\n\n  \n\n\n\n\nAussagen-einfache-Regr\n\n\n\n\n\n\n\nregression\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\n\n\n\n\n  \n\n\n\n\nLikelihood-identifizieren\n\n\n\n\n\n\n\nregression\n\n\nbayes\n\n\nlikelihood\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\n\n\n\n\n  \n\n\n\n\nPriorwahl2\n\n\n\n\n\n\n\nregression\n\n\nbayes\n\n\ndistribution\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\n\n\n\n\n  \n\n\n\n\npenguins-stan-01\n\n\n\n\n\n\n\nbayes\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\n\n\n\n\n  \n\n\n\n\nfattails02\n\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n  \n\n\n\n\niq03\n\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n  \n\n\n\n\niq04\n\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n  \n\n\n\n\niq05\n\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n  \n\n\n\n\niq02\n\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n  \n\n\n\n\niq10\n\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\nbayes\n\n\nbayes-box\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n  \n\n\n\n\niq07\n\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n  \n\n\n\n\nfattails01\n\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nfat-tails\n\n\nnormal-distribution\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n  \n\n\n\n\niq06\n\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n  \n\n\n\n\niq08\n\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n  \n\n\n\n\nReThink3e1-7\n\n\n\n\n\n\n\nbayes\n\n\nprobability\n\n\npost\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n  \n\n\n\n\nReThink4e1\n\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n  \n\n\n\n\nVerteilungen-Quiz-05\n\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n  \n\n\n\n\nVerteilungen-Quiz-02\n\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n  \n\n\n\n\nVerteilungen-Quiz-03\n\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n  \n\n\n\n\nVerteilungen-Quiz-04\n\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n  \n\n\n\n\nVerteilungen-Quiz-17\n\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n  \n\n\n\n\nVerteilungen-Quiz-10\n\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n  \n\n\n\n\nVerteilungen-Quiz-11\n\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n  \n\n\n\n\nVerteilungen-Quiz-16\n\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n  \n\n\n\n\nVerteilungen-Quiz-18\n\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n  \n\n\n\n\nVerteilungen-Quiz-01\n\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n  \n\n\n\n\nVerteilungen-Quiz-06\n\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n  \n\n\n\n\nVerteilungen-Quiz-08\n\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n  \n\n\n\n\nVerteilungen-Quiz-09\n\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n  \n\n\n\n\nVerteilungen-Quiz-07\n\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n  \n\n\n\n\nVerteilungen-Quiz-13\n\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n  \n\n\n\n\nVerteilungen-Quiz-14\n\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n  \n\n\n\n\nVerteilungen-Quiz-15\n\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n  \n\n\n\n\nVerteilungen-Quiz-12\n\n\n\n\n\n\n\ndistributions\n\n\nVerteilungen-Quiz\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n  \n\n\n\n\ntwitter07\n\n\n\n\n\n\n\ntextmining\n\n\ntwitter\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nAnteil-Apple\n\n\n\n\n\n\n\nbayes\n\n\nbayes-box\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n  \n\n\n\n\nIQ-Studentis\n\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n  \n\n\n\n\nWarum-Bayes\n\n\n\n\n\n\n\nqm2\n\n\nbayes\n\n\nprobability\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n  \n\n\n\n\nSim-Prior\n\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n  \n\n\n\n\nReThink3m1\n\n\n\n\n\n\n\nbayes\n\n\nppv\n\n\nprobability\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n  \n\n\n\n\nPriori-Streuung\n\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\ndistribution\n\n\nbayes\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n  \n\n\n\n\nReThink4e3\n\n\n\n\n\n\n\nbayes\n\n\nprobability\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n  \n\n\n\n\nReThink4e2\n\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n  \n\n\n\n\nfat-tails-Artikel\n\n\n\n\n\n\n\nprobability\n\n\ndistribution\n\n\nfat-tails\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n  \n\n\n\n\nPupil-size\n\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n  \n\n\n\n\nstan_glm01\n\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n  \n\n\n\n\nReThink3m2\n\n\n\n\n\n\n\nbayes\n\n\npost\n\n\nprobability\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n  \n\n\n\n\nReThink3m5\n\n\n\n\n\n\n\nbayes\n\n\nppv\n\n\nprobability\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n  \n\n\n\n\nKung-height\n\n\n\n\n\n\n\nbayes\n\n\nppv\n\n\nprobability\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n  \n\n\n\n\nReThink3m4\n\n\n\n\n\n\n\nbayes\n\n\nppv\n\n\nprobability\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n  \n\n\n\n\nReThink3m3\n\n\n\n\n\n\n\nbayes\n\n\nppv\n\n\nprobability\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n  \n\n\n\n\nZwielichter-Dozent-Bayes\n\n\n\n\n\n\n\nbayes\n\n\nprobability\n\n\nppv\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n  \n\n\n\n\nsubjektiv-Bayes\n\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n  \n\n\n\n\nWeinhaendler\n\n\n\n\n\n\n\nprobability\n\n\nbayes-grid\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2022\n\n\n\n\n\n\n  \n\n\n\n\ntwitter01\n\n\n\n\n\n\n\ntextmining\n\n\ntwitter\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n  \n\n\n\n\ntwitter06\n\n\n\n\n\n\n\ntextmining\n\n\ntwitter\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n  \n\n\n\n\nLose-Nieten-Binomial-Grid\n\n\n\n\n\n\n\nprobability\n\n\nbinomial\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n  \n\n\n\n\nwuerfel04\n\n\n\n\n\n\n\nprobability\n\n\ndice\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n  \n\n\n\n\nwuerfel03\n\n\n\n\n\n\n\nprobability\n\n\ndice\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n  \n\n\n\n\nwuerfel02\n\n\n\n\n\n\n\nprobability\n\n\ndice\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n  \n\n\n\n\nBsp-Binomial\n\n\n\n\n\n\n\nprobability\n\n\nbinomial\n\n\nexample\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n  \n\n\n\n\ntwitter03\n\n\n\n\n\n\n\ntextmining\n\n\ntwitter\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n  \n\n\n\n\ntwitter04\n\n\n\n\n\n\n\ntextmining\n\n\ntwitter\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n  \n\n\n\n\ntwitter05\n\n\n\n\n\n\n\ntextmining\n\n\ntwitter\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n  \n\n\n\n\ntwitter02\n\n\n\n\n\n\n\ntextmining\n\n\ntwitter\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n  \n\n\n\n\nkekse02\n\n\n\n\n\n\n\nprobability\n\n\nbayes-grid\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n\n\n\n\n  \n\n\n\n\nRethink_2m3\n\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n\n\n\n\n  \n\n\n\n\nRethink_2m4\n\n\n\n\n\n\n\nprobability\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n\n\n\n\n  \n\n\n\n\nRethink_2m5\n\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n\n\n\n\n  \n\n\n\n\nRethink_2m2\n\n\n\n\n\n\n\nprobability\n\n\nbayes-grid\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n\n\n\n\n  \n\n\n\n\neuro-bayes\n\n\n\n\n\n\n\nprobability\n\n\nbayes-grid\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n\n\n\n\n  \n\n\n\n\nkekse01\n\n\n\n\n\n\n\nprobability\n\n\nbayes-grid\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n\n\n\n\n  \n\n\n\n\nRethink_2E4\n\n\n\n\n\n\n\nprobability\n\n\nphilosophy\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n\n\n\n\n  \n\n\n\n\nRethink_2m7\n\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n\n\n\n\n  \n\n\n\n\nRethink_2m1\n\n\n\n\n\n\n\nprobability\n\n\nbayes-grid\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n\n\n\n\n  \n\n\n\n\nRethink_2m6\n\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n\n\n\n\n  \n\n\n\n\nBed-Wskt2\n\n\n\n\n\n\n\nprobability\n\n\nconditional\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n  \n\n\n\n\npurrr-map01\n\n\n\n\n\n\n\nr\n\n\nmap\n\n\ntidyverse\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n  \n\n\n\n\npurrr-map06\n\n\n\n\n\n\n\nprogramming\n\n\nloop\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n  \n\n\n\n\ncorona-blutgruppe\n\n\n\n\n\n\n\nprobabillity\n\n\ndependent\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n  \n\n\n\n\nmtcars-abhaengig\n\n\n\n\n\n\n\nprobability\n\n\ndependent\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n  \n\n\n\n\nvoll-normal\n\n\n\n\n\n\n\nprobability\n\n\nmeta\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n  \n\n\n\n\nGem-Wskt1\n\n\n\n\n\n\n\nprobability\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n  \n\n\n\n\npurrr-map05\n\n\n\n\n\n\n\nprogramming\n\n\nloop\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n  \n\n\n\n\npurrr-map02\n\n\n\n\n\n\n\nr\n\n\nmap\n\n\ntidyverse\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n  \n\n\n\n\npurrr-map03\n\n\n\n\n\n\n\nr\n\n\nmap\n\n\ntidyverse\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n  \n\n\n\n\npurrr-map04\n\n\n\n\n\n\n\nr\n\n\nmap\n\n\ntidyverse\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n  \n\n\n\n\nungewiss-arten-regr\n\n\n\n\n\n\n\nqm2\n\n\ninference\n\n\nlm\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2022\n\n\n\n\n\n\n  \n\n\n\n\nnasa03\n\n\n\n\n\n\n\ndata\n\n\neda\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2022\n\n\n\n\n\n\n  \n\n\n\n\nkorr-als-regr\n\n\n\n\n\n\n\ncorrelation\n\n\nlm\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n  \n\n\n\n\nInferenz-fuer-alle\n\n\n\n\n\n\n\nqm2\n\n\ninference\n\n\nuncertainty\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n  \n\n\n\n\nlm1\n\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nqm2\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n  \n\n\n\n\nmtcars-simple1\n\n\n\n\n\n\n\nregresssion\n\n\nen\n\n\nbayes\n\n\nfrequentist\n\n\nqm1\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n  \n\n\n\n\nlog-y-regr3\n\n\n\n\n\n\n\nstats-nutshell\n\n\nqm2\n\n\nregression\n\n\nlog\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n  \n\n\n\n\nlog-y-regr2\n\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nqm2\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n  \n\n\n\n\nadjustieren1\n\n\n\n\n\n\n\nqm2\n\n\nlm\n\n\nbayes\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n  \n\n\n\n\ninterpret-koeff\n\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nbayes\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n  \n\n\n\n\nvorhersageintervall1\n\n\n\n\n\n\n\nlm\n\n\ninference\n\n\nqm2\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n  \n\n\n\n\nlm-Standardfehler\n\n\n\n\n\n\n\ninference\n\n\nlm\n\n\nqm2\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n  \n\n\n\n\npunktschaetzer-reicht-nicht\n\n\n\n\n\n\n\nregresssion\n\n\nen\n\n\nbayes\n\n\nfrequentist\n\n\nqm1\n\n\nstats-nutshell\n\n\nqm2\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n  \n\n\n\n\nmtcars-simple2\n\n\n\n\n\n\n\nregresssion\n\n\nen\n\n\nbayes\n\n\nfrequentist\n\n\nqm1\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n  \n\n\n\n\nmtcars-simple3\n\n\n\n\n\n\n\nregresssion\n\n\nen\n\n\nbayes\n\n\nfrequentist\n\n\nqm1\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n  \n\n\n\n\nttest-als-regr\n\n\n\n\n\n\n\nregr\n\n\nttest\n\n\nvariable-levels\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n  \n\n\n\n\nlog-y-regr1\n\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nqm2\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n  \n\n\n\n\nadjustieren2\n\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nqm2\n\n\nbayes\n\n\nadjust\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n  \n\n\n\n\nStichprobenziehen1\n\n\n\n\n\n\n\nlm\n\n\ninference\n\n\nqm2\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n  \n\n\n\n\nGriech-Buchstaben-Inferenz\n\n\n\n\n\n\n\nqm2\n\n\nqm2-thema01\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Ans Werk, Daten!",
    "section": "",
    "text": "Datenwerk ist eine Sammlung von Aufgaben mit Bezug zur Datenanalyse.\nAutor: Sebastian Sauer (soweit nicht anders ausgewiesen)\nDer Quellcode dieser Webseite findet sich hier.\nDie Lizenz ist permissiv, s. Hinweise hier."
  },
  {
    "objectID": "Hinweise.html",
    "href": "Hinweise.html",
    "title": "Hinweise",
    "section": "",
    "text": "Verwenden Sie Standardwerte (defaults) der R-Funktionen, soweit nicht anders in der jeweiligen Aufgabe verlangt.\nFindet sich in einer Auswahlliste möglicher Antworten nicht die exakte Lösung, wählen Sie die am besten passende.\nTreffen Sie Annahmen, (nur) wo nötig.\nDie Prüfung besteht auch aus Single. bzw. Multiple-Choice- (MC)-Aufgaben mit mehreren Antwortoptionen.\nBei Multiple-Choice-Aufgaben (MC-Aufgaben) ist zumeist genau eine Antwortoption auszuwählen aus vier oder fünf Antwortoptionen.\nIm Zweifel ist eine Aussage auf den Stoff, so wie im Unterricht behandelt, zu beziehen.\nBei Fragen zu R-Syntax spielen Aspekte wie Enter-Taste o.Ä. bei der Beantwortung der Frage keine Rolle; diese Aspekte dürfen zu ignorieren.\nJede Aussage einer MC-Aufgabe ist entweder richtig oder falsch (aber nicht beides oder keines).\nDie MC-Aufgaben sind nur mit Kreuzen zu beantworten; Text wird bei der Korrektur nicht berücksichtigt.\nJede Aussage gilt ceteris paribus (unter sonst gleichen Umständen). Aussagen der Art „A ist B“ (z.B. “Menschen sind sterblich”) sind nur dann als richtig auszuwählen, wenn die Aussage immer richtig ist.\nFalls Sie bei einer Aufgabe mehrere Antworten finden, aber nur nach einer gefragt ist, geben Sie nur eine an.\nFalls mehrere (widersprüchliche) Antworten gegeben wurden, wird im Zweifel die erst genannte gewertet.\nDie Aufgabenstellung in einer Moodle-Prüfung wird u.U. erst sichtbar, wenn Sie den Prüfungsbedingungen zugestimmt haben und die Prüfungszeit begonnen hat.\n\n\n\n\n\nJe nach Spracheinstellung in Moodle kann es sein, dass Sie als Dezimaltrennzeichen ein Komma oder einen Punkt verwenden müssen. Moodle weist Sie, wenn Sie die Aufgabe verlassen, darauf hin, falls eine Zahl nicht als Zahl erkannt wurde.\nRunden Sie bei Fragen, die auf Anteile abzielen auf zwei Dezimalstellen, ansonsten auf eine.\nGeben Sie keine Prozentzahlen an, sondern Anteile (also nicht “50%”, sondern “0.5” bzw. “0,5”).\nBei Aufgaben, die eine Zahl als Antwort verlangen, ist nur eine Zahl anzugeben (nicht etwa Buchstaben).\nAlle Berechnungen, die Zufallszahlen beinhalten, sollen mit fixierten Startwert der Zufallszahlen durchgeführt werden. Es ist die Zahl 42 zu verwenden.\nWie auch bei den übrigen Hinweisen gelten diese Maßgaben nur soweit nicht explizit andere Hinweise gegeben wurden.\nWenn Stichproben simuliert werden sollen, ziehen Sie \\(10^3\\) Zufallsstichproben.\nIn einigen Aufgaben kann verlangt sein, dass Sie einen bestimmten Datensatz in R importieren sollen. In diesem Fall wird vorausgesetzt, dass Ihnen diese Bezugsquelle von Datensätzen bekannt ist und dass Sie wissen, wie man einen Datensatz in R importiert.\nAchten Sie darauf, R und R-Pakete sowie R-Studio in aktueller Version zu verwenden. Das Verwenden älterer Versionen kann (in seltenen Fällen) zu abweichenden Lösungen führen. Im Zweifel beziehen sich alle Aufgaben auf die jeweils aktuellste Version der verwendeten Software.\nWenn Sie Text eingeben sollen: Geben Sie nur Kleinbuchstaben ein. Geben Sie nur ein einziges Wort ein. Geben Sie keine Leerzeichen ein.\n\n\n\n\n\nVerwenden Sie Methoden der Bayes-Statistik für inferenzstatistische Analysen (soweit nicht anders vorgegeben).\nBei Aufgaben zur “Bayes-Box” (Erstellung einer Gitterwert-Tabelle) gelten folgende Maßgaben:\n\n\nHandelt es sich um Parameter mit einem begrenzten Wertebereich (wie etwa Anteile), so ist der ganze Wertebereich zu modellieren. Es sind 100 verschiedene Parameterwerte zu berechnen.\nHandelt es sich um Parameter \\(X\\) mit einem unbegrenzten Wertebereich (wie normalverteilte Variablen), so ist der Wertebereich \\(X-2\\sigma \\le X \\le X+2\\sigma\\) zu simulieren.\n\n\nNutzen Sie die Software Stan in Form des R-Pakets rstanarm für Regressionsmodelle auf Basis der Bayes-Methode.\nVerwenden Sie immer folgenden Seed-Wert bei stan_glm(): 42. Dazu setzen Sie folgenden Parameter seed = 42 (innerhalb von stan_glm()).\nDer Tolerenzbereich einer Lösung liegt bei 5% des Wertes der Musterlösung. Beispiel: Die Musterlösung liegt bei 100; dann erstreckt sich der Toleranzbereich von 95 bis 105. Tolerenzbereiche werden verwendet für Aufgaben mit Zufallszahlen: Lösungen solcher Aufgaben können schwanken. Wenn Sie allerdings den angegebenen Seed-Wert gesetzt haben, so sollte Ihre (korrekte) Lösung exakt der Musterlösung entsprechen."
  },
  {
    "objectID": "Hinweise.html#bearbeitungshinweise",
    "href": "Hinweise.html#bearbeitungshinweise",
    "title": "Hinweise",
    "section": "",
    "text": "Verwenden Sie Standardwerte (defaults) der R-Funktionen, soweit nicht anders in der jeweiligen Aufgabe verlangt.\nFindet sich in einer Auswahlliste möglicher Antworten nicht die exakte Lösung, wählen Sie die am besten passende.\nTreffen Sie Annahmen, (nur) wo nötig.\nDie Prüfung besteht auch aus Single. bzw. Multiple-Choice- (MC)-Aufgaben mit mehreren Antwortoptionen.\nBei Multiple-Choice-Aufgaben (MC-Aufgaben) ist zumeist genau eine Antwortoption auszuwählen aus vier oder fünf Antwortoptionen.\nIm Zweifel ist eine Aussage auf den Stoff, so wie im Unterricht behandelt, zu beziehen.\nBei Fragen zu R-Syntax spielen Aspekte wie Enter-Taste o.Ä. bei der Beantwortung der Frage keine Rolle; diese Aspekte dürfen zu ignorieren.\nJede Aussage einer MC-Aufgabe ist entweder richtig oder falsch (aber nicht beides oder keines).\nDie MC-Aufgaben sind nur mit Kreuzen zu beantworten; Text wird bei der Korrektur nicht berücksichtigt.\nJede Aussage gilt ceteris paribus (unter sonst gleichen Umständen). Aussagen der Art „A ist B“ (z.B. “Menschen sind sterblich”) sind nur dann als richtig auszuwählen, wenn die Aussage immer richtig ist.\nFalls Sie bei einer Aufgabe mehrere Antworten finden, aber nur nach einer gefragt ist, geben Sie nur eine an.\nFalls mehrere (widersprüchliche) Antworten gegeben wurden, wird im Zweifel die erst genannte gewertet.\nDie Aufgabenstellung in einer Moodle-Prüfung wird u.U. erst sichtbar, wenn Sie den Prüfungsbedingungen zugestimmt haben und die Prüfungszeit begonnen hat.\n\n\n\n\n\nJe nach Spracheinstellung in Moodle kann es sein, dass Sie als Dezimaltrennzeichen ein Komma oder einen Punkt verwenden müssen. Moodle weist Sie, wenn Sie die Aufgabe verlassen, darauf hin, falls eine Zahl nicht als Zahl erkannt wurde.\nRunden Sie bei Fragen, die auf Anteile abzielen auf zwei Dezimalstellen, ansonsten auf eine.\nGeben Sie keine Prozentzahlen an, sondern Anteile (also nicht “50%”, sondern “0.5” bzw. “0,5”).\nBei Aufgaben, die eine Zahl als Antwort verlangen, ist nur eine Zahl anzugeben (nicht etwa Buchstaben).\nAlle Berechnungen, die Zufallszahlen beinhalten, sollen mit fixierten Startwert der Zufallszahlen durchgeführt werden. Es ist die Zahl 42 zu verwenden.\nWie auch bei den übrigen Hinweisen gelten diese Maßgaben nur soweit nicht explizit andere Hinweise gegeben wurden.\nWenn Stichproben simuliert werden sollen, ziehen Sie \\(10^3\\) Zufallsstichproben.\nIn einigen Aufgaben kann verlangt sein, dass Sie einen bestimmten Datensatz in R importieren sollen. In diesem Fall wird vorausgesetzt, dass Ihnen diese Bezugsquelle von Datensätzen bekannt ist und dass Sie wissen, wie man einen Datensatz in R importiert.\nAchten Sie darauf, R und R-Pakete sowie R-Studio in aktueller Version zu verwenden. Das Verwenden älterer Versionen kann (in seltenen Fällen) zu abweichenden Lösungen führen. Im Zweifel beziehen sich alle Aufgaben auf die jeweils aktuellste Version der verwendeten Software.\nWenn Sie Text eingeben sollen: Geben Sie nur Kleinbuchstaben ein. Geben Sie nur ein einziges Wort ein. Geben Sie keine Leerzeichen ein.\n\n\n\n\n\nVerwenden Sie Methoden der Bayes-Statistik für inferenzstatistische Analysen (soweit nicht anders vorgegeben).\nBei Aufgaben zur “Bayes-Box” (Erstellung einer Gitterwert-Tabelle) gelten folgende Maßgaben:\n\n\nHandelt es sich um Parameter mit einem begrenzten Wertebereich (wie etwa Anteile), so ist der ganze Wertebereich zu modellieren. Es sind 100 verschiedene Parameterwerte zu berechnen.\nHandelt es sich um Parameter \\(X\\) mit einem unbegrenzten Wertebereich (wie normalverteilte Variablen), so ist der Wertebereich \\(X-2\\sigma \\le X \\le X+2\\sigma\\) zu simulieren.\n\n\nNutzen Sie die Software Stan in Form des R-Pakets rstanarm für Regressionsmodelle auf Basis der Bayes-Methode.\nVerwenden Sie immer folgenden Seed-Wert bei stan_glm(): 42. Dazu setzen Sie folgenden Parameter seed = 42 (innerhalb von stan_glm()).\nDer Tolerenzbereich einer Lösung liegt bei 5% des Wertes der Musterlösung. Beispiel: Die Musterlösung liegt bei 100; dann erstreckt sich der Toleranzbereich von 95 bis 105. Tolerenzbereiche werden verwendet für Aufgaben mit Zufallszahlen: Lösungen solcher Aufgaben können schwanken. Wenn Sie allerdings den angegebenen Seed-Wert gesetzt haben, so sollte Ihre (korrekte) Lösung exakt der Musterlösung entsprechen."
  }
]