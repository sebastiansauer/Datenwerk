[
  {
    "objectID": "posts/korr-als-regr/korr-als-regr.html",
    "href": "posts/korr-als-regr/korr-als-regr.html",
    "title": "korr-als-regr",
    "section": "",
    "text": "Exercise\nDie Korrelation prüft, ob zwei Merkmale linear zusammenhängen.\nWie viele andere Verfahren kann die Korrelation als ein Spezialfall der Regression bzw. des linearen Modells \\(y = \\beta_0 + \\beta_1 + \\ldots \\beta_n + \\epsilon\\) betrachtet werden.\nAls ein spezielles Beispiel betrachten wir die Frage, ob das Gewicht eines Diamanten (carat) mit dem Preis (price) zusammenhängt (Datensatz diamonds).\nDen Datensatz können Sie so laden:\n\nlibrary(tidyverse)\ndata(diamonds)\n\n\nGeben Sie das Skalenniveau beider Variablen an!\nBetrachten Sie die Ausgabe von R:\n\n\nlm1 <- lm(price ~ carat, data = diamonds)\nsummary(lm1)\n\n\nCall:\nlm(formula = price ~ carat, data = diamonds)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-18585   -805    -19    537  12732 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -2256.4       13.1    -173   <2e-16 ***\ncarat         7756.4       14.1     551   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1550 on 53938 degrees of freedom\nMultiple R-squared:  0.849, Adjusted R-squared:  0.849 \nF-statistic: 3.04e+05 on 1 and 53938 DF,  p-value: <2e-16\n\n\nWie (bzw. wo) ist aus dieser Ausgabe die Korrelation herauszulesen?\n\nMacht es einen Unterschied, ob man Preis mit Karat bzw. Karat mit Preis korreliert?\nIn der klassischen Inferenzstatistik ist der \\(p\\)-Wert eine zentrale Größe; ist er klein (\\(p<.05\\)) so nennt man die zugehörige Statistik signifikant und verwirft die getestete Hypothese.\nIm Folgenden sehen Sie einen Korrelationstest auf statistische Signifikanz, mit R durchgeführt. Zeigt der Test ein (statistisch) signifikantes Ergebnis? Wie groß ist der “Unsicherheitskorridor”, um den Korrelationswert (zugleich Punktschätzer für den Populationswert)?\n\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.5.2\n✔ insight     0.18.2     ✔ datawizard  0.5.1   \n✔ bayestestR  0.12.1.1   ✔ performance 0.9.2   \n✔ parameters  0.18.2     ✔ effectsize  0.7.0.5 \n✔ modelbased  0.8.5      ✔ correlation 0.8.2   \n✔ see         0.7.2      ✔ report      0.5.5   \n\ndiamonds %>% \n  sample_n(30) %>% \n  select(price, carat) %>% \n  correlation()\n\n# Correlation Matrix (pearson-method)\n\nParameter1 | Parameter2 |    r |       95% CI | t(28) |         p\n-----------------------------------------------------------------\nprice      |      carat | 0.94 | [0.88, 0.97] | 14.87 | < .001***\n\np-value adjustment method: Holm (1979)\nObservations: 30\n\n\n         \n\n\nSolution\n\ncarat ist metrisch (verhältnisskaliert) und price ist metrisch (verhältnisskaliert)\n\\(R^2\\) kann bei einer einfachen (univariaten) Regression als das Quadrat von \\(r\\) berechnet werden. Daher \\(r = \\sqrt{R^2}\\).\n\n\nsqrt(0.8493)\n\n[1] 0.92\n\n\nZum Vergleich\n\ndiamonds %>% \n  summarise(r = cor(price, carat))\n\n# A tibble: 1 × 1\n      r\n  <dbl>\n1 0.922\n\n\nMan kann den Wert der Korrelation auch noch anderweitig berechnen (\\(\\beta\\) umrechnen in \\(\\rho\\)).\n\nNein. Die Korrelation ist eine symmetrische Relation.\nJa; die Zahl “3.81e-14” bezeichnet eine positive Zahl kleiner eins mit 13 Nullern vor der ersten Ziffer, die nicht Null ist (3.81 in diesem Fall). Der “Unsicherheitskorridor” reicht von etwa 0.87 bis 0.97.\n\n\nCategories:\n\ncorrelation\nlm"
  },
  {
    "objectID": "posts/Inferenz-fuer-alle/Inferenz-fuer-alle.html",
    "href": "posts/Inferenz-fuer-alle/Inferenz-fuer-alle.html",
    "title": "Inferenz-fuer-alle",
    "section": "",
    "text": "Solution\n\nFür (grundsätzlich) alle: Für jede Statistik kann man prinzipiell von der jeweiligen Stichprobe (auf Basis derer die Statistik berechnet wurde) auf eine zugehörige Grundgesamtheit schließen.\nFür (grundsätzlich) alle: Die Methoden der Inferenzstatistik sind prinzipiell unabhängig von den Spezifika bestimmter Forschungsfragen oder -bereiche. In den meisten Forschungsfragen ist man daran interessiert allgemeingültige Aussagen zu treffen. Da Statistiken sich nur auf eine Stichprobe - also einen zumeist nur kleinen Teil einer Grundgesamtheit beziehen - wird man sich kaum mit einer Statistik zufrieden geben, sondern nach Inferenzstatistik verlangen.\nIn einigen Ausnahmefällen wird man auf eine Inferenzstatistik verzichten. Etwa wenn man bereits eine Vollerhebung durchgeführt hat, z.B. alle Mitarbeitis eines Unternehmens befragt hat, dann kennt man ja bereits den wahren Populationswert. Ein anderer Fall ist, wenn man nicht an Verallgemeinerungen interessiert ist: Kennt man etwa die Überlebenschance \\(p\\) des Titanic-Unglücks, so ist es fraglich auf welche Grundgesamtheit man die Statistik \\(p\\) bzw. zu welchem Paramter \\(\\pi\\) (kleines Pi) man generalisieren möchte.\n\n\nCategories:\n\nqm2\nqm2-thema01"
  },
  {
    "objectID": "posts/lm1/lm1.html#answerlist",
    "href": "posts/lm1/lm1.html#answerlist",
    "title": "lm1",
    "section": "Answerlist",
    "text": "Answerlist\n\nmpg und hp sind positiv korreliert laut dem Modell.\nDer Achsenabschnitt ist nahe Null.\nDie Analyse beinhaltet einen nominal skalierten Prädiktor.\nDas geschätzte Betagewicht für hp liegt bei 30.099.\nDas geschätzte Betagewicht für hp liegt bei -0.068."
  },
  {
    "objectID": "posts/lm1/lm1.html#answerlist-1",
    "href": "posts/lm1/lm1.html#answerlist-1",
    "title": "lm1",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nFalsch\nRichtig\n\n\nCategories:\n\nregression\nlm\nqm2\nstats-nutshell"
  },
  {
    "objectID": "posts/ttest-skalenniveau/ttest-skalenniveau.html",
    "href": "posts/ttest-skalenniveau/ttest-skalenniveau.html",
    "title": "ttest-skalenniveau",
    "section": "",
    "text": "Der t-Test ist ein inferenzstatistisches Verfahren des Frequentismus. Welches Skalenniveau passt zu diesem Verfahren?\nHinweisse:\n\nDie folgende Abbildung gibt Tipps.\nInformationen, die zur Lösung einer Aufgabe nicht nötig sind, sollte man ignorieren.\n\n\n\n\n\n\n\n\n\n\n\n\n\nUV: nominal (mehrstufig), AV: metrisch\nUV: metrisch, AV: nominal (zweistufig)\nUV: nominal (mehrstufig), AV: nominal (mehrstufig)\nUV: metrisch, AV: nominal (zweistufig)\nUV: nominal (zweistufig), AV: metrisch"
  },
  {
    "objectID": "posts/ttest-skalenniveau/ttest-skalenniveau.html#answerlist-1",
    "href": "posts/ttest-skalenniveau/ttest-skalenniveau.html#answerlist-1",
    "title": "ttest-skalenniveau",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nFalsch\nWahr\n\n\nCategories:\n\nttest\nregr\nvariable-levels"
  },
  {
    "objectID": "posts/nasa01/nasa01.html",
    "href": "posts/nasa01/nasa01.html",
    "title": "nasa01",
    "section": "",
    "text": "Solution\nDekade berechnen:\n\nd <-\n  d %>% \n  mutate(decade = round(Year/10))\n\nStatistiken pro Dekade:\n\nd_summarized <- \n  d %>% \n  group_by(decade) %>% \n  summarise(temp_mean = mean(Jan),\n            temp_sd = sd(Jan))\n\nd_summarized\n\n\n\n\n\n\n\n  \n  \n    \n      decade\n      temp_mean\n      temp_sd\n    \n  \n  \n    188\n−0.20\n0.24\n    189\n−0.44\n0.22\n    190\n−0.27\n0.17\n    191\n−0.40\n0.22\n    192\n−0.28\n0.16\n    193\n−0.13\n0.22\n    194\n0.03\n0.21\n    195\n−0.05\n0.18\n    196\n0.03\n0.15\n    197\n−0.07\n0.17\n    198\n0.21\n0.19\n    199\n0.36\n0.13\n    200\n0.52\n0.19\n    201\n0.64\n0.20\n    202\n0.96\n0.14\n  \n  \n  \n\n\n\n\nZur Veranschaulichung visualisieren wir die Ergebnisse:\n\n\n\n\n\n\n\n\n\n\nCategories:\n~"
  },
  {
    "objectID": "posts/purrr-map01/purrr-map01.html",
    "href": "posts/purrr-map01/purrr-map01.html",
    "title": "purrr-map01",
    "section": "",
    "text": "Exercise\nErstellen Sie einen Tibble mit folgenden Spalten:\n\nBuchstaben A-Z, so dass in der 1. Zeile “A” steht, in der 2. Zeile “B” etc.\nBuchstaben a-z, so dass in der 1. Zeile “a” steht, in der 2. Zeile “b” etc.\nBuchstabenkombination der ersten beiden Spalten, so dass in der 1. Zeile “A-a” steht, in der 2. Zeile “B-b” etc.\n\n         \n\n\nSolution\nGeht es vielleicht so?\n\nd <-\n  tibble(\n    letter1 = LETTERS,\n    letter2 = letters,\n    letters = paste(letter1, letter2, collapse = \"-\")\n  )\n\nhead(d)\n\n# A tibble: 6 × 3\n  letter1 letter2 letters                                                       \n  <chr>   <chr>   <chr>                                                         \n1 A       a       A a-B b-C c-D d-E e-F f-G g-H h-I i-J j-K k-L l-M m-N n-O o-P…\n2 B       b       A a-B b-C c-D d-E e-F f-G g-H h-I i-J j-K k-L l-M m-N n-O o-P…\n3 C       c       A a-B b-C c-D d-E e-F f-G g-H h-I i-J j-K k-L l-M m-N n-O o-P…\n4 D       d       A a-B b-C c-D d-E e-F f-G g-H h-I i-J j-K k-L l-M m-N n-O o-P…\n5 E       e       A a-B b-C c-D d-E e-F f-G g-H h-I i-J j-K k-L l-M m-N n-O o-P…\n6 F       f       A a-B b-C c-D d-E e-F f-G g-H h-I i-J j-K k-L l-M m-N n-O o-P…\n\n\nNein, leider nicht.\nOK, neuer Versuch:\n\nd <-\n  tibble(\n    letter1 = LETTERS,\n    letter2 = letters) %>% \n  unite(\"letters\", c(letter1, letter2), remove = FALSE)\n\n\nhead(d)\n\n# A tibble: 6 × 3\n  letters letter1 letter2\n  <chr>   <chr>   <chr>  \n1 A_a     A       a      \n2 B_b     B       b      \n3 C_c     C       c      \n4 D_d     D       d      \n5 E_e     E       e      \n6 F_f     F       f      \n\n\nProbieren wir es mit purrr::map():\n\nd <-\n  tibble(\n    letter1 = LETTERS,\n    letter2 = letters,\n    letters = map2_chr(letter1, letter2, ~ paste(c(.x, .y), collapse =\"-\"))\n  )\n\nhead(d)\n\n# A tibble: 6 × 3\n  letter1 letter2 letters\n  <chr>   <chr>   <chr>  \n1 A       a       A-a    \n2 B       b       B-b    \n3 C       c       C-c    \n4 D       d       D-d    \n5 E       e       E-e    \n6 F       f       F-f    \n\n\nInfos zur Funktion paste() findet sich z.B. hier.\n\nCategories:\n\nr\nmap\ntidyverse"
  },
  {
    "objectID": "posts/mtcars-simple1/mtcars-simple1.html",
    "href": "posts/mtcars-simple1/mtcars-simple1.html",
    "title": "mtcars-simple1",
    "section": "",
    "text": "Solution\nCompute Model:\n\nlm1_freq <- lm(mpg ~ hp + cyl + disp, data = mtcars)\n\nlibrary(rstanarm)\nlm1_bayes <- stan_glm(mpg ~ hp + cyl + disp, data = mtcars, refresh = 0)\n\nGet parameters:\n\nlibrary(easystats)\n\n\nparameters(lm1_freq)\n\nParameter   | Coefficient |   SE |         95% CI | t(28) |      p\n------------------------------------------------------------------\n(Intercept) |       34.18 | 2.59 | [28.88, 39.49] | 13.19 | < .001\nhp          |       -0.01 | 0.01 | [-0.04,  0.02] | -1.00 | 0.325 \ncyl         |       -1.23 | 0.80 | [-2.86,  0.41] | -1.54 | 0.135 \ndisp        |       -0.02 | 0.01 | [-0.04,  0.00] | -1.81 | 0.081 \n\n\n\nparameters(lm1_bayes)\n\nParameter   | Median |         95% CI |     pd | % in ROPE |  Rhat |     ESS |                   Prior\n------------------------------------------------------------------------------------------------------\n(Intercept) |  34.28 | [28.87, 39.60] |   100% |        0% | 1.000 | 2600.00 | Normal (20.09 +- 15.07)\nhp          |  -0.01 | [-0.05,  0.02] | 82.93% |      100% | 1.000 | 2448.00 |   Normal (0.00 +- 0.22)\ncyl         |  -1.25 | [-2.84,  0.34] | 93.75% |     2.55% | 1.000 | 2108.00 |   Normal (0.00 +- 8.44)\ndisp        |  -0.02 | [-0.04,  0.00] | 96.05% |      100% | 1.001 | 2405.00 |   Normal (0.00 +- 0.12)\n\n\nThe coefficient is estimated as about -0.01\n\nCategories:\n\nregresssion\nen\nbayes\nfrequentist\nqm1\nstats-nutshell"
  },
  {
    "objectID": "posts/log-y-regr3/log-y-regr3.html",
    "href": "posts/log-y-regr3/log-y-regr3.html",
    "title": "log-y-regr3",
    "section": "",
    "text": "library(tidyverse)\nlibrary(easystats)\n\nIn dieser Aufgabe modellieren wir den (kausalen) Effekt von Schulbildung auf das Einkommen.\nImportieren Sie zunächst den Datensatz und verschaffen Sie sich einen Überblick.\n\nd_path <- \"https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Treatment.csv\"\n\nd <- data_read(d_path)\n\nDokumentation und Quellenangaben zum Datensatz finden sich hier.\n\nglimpse(d)\n\nRows: 2,675\nColumns: 11\n$ V1      <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…\n$ treat   <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n$ age     <int> 37, 30, 27, 33, 22, 23, 32, 22, 19, 21, 18, 27, 17, 19, 27, 23…\n$ educ    <int> 11, 12, 11, 8, 9, 12, 11, 16, 9, 13, 8, 10, 7, 10, 13, 10, 12,…\n$ ethn    <chr> \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\",…\n$ married <lgl> TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ re74    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ re75    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ re78    <dbl> 9930.05, 24909.50, 7506.15, 289.79, 4056.49, 0.00, 8472.16, 21…\n$ u74     <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n$ u75     <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR…\n\n\nWelcher der Prädiktoren hat den stärkesten Einfluss auf das Einkommen?\nHinweise:\n\nVerwenden Sie lm zur Modellierung.\nOperationalisieren Sie das Einkommen mit der Variable re74.\nGehen Sie von einem kausalen Effekt der Prädiktoren aus.\nGehen Sie von einem multiplikativen Modell aus (log-y).\nLassen Sie die Variablen zur Arbeitslosigkeit außen vor.\n\n\n\n\ntreat\nage\neduc\nethn\nmarried"
  },
  {
    "objectID": "posts/log-y-regr3/log-y-regr3.html#answerlist-1",
    "href": "posts/log-y-regr3/log-y-regr3.html#answerlist-1",
    "title": "log-y-regr3",
    "section": "Answerlist",
    "text": "Answerlist\n\nTRUE\nFALSE\nFALSE\nFALSE\nFALSE\n\n\nCategories:\n\nstats-nutshell\nqm2\nregression\nlog"
  },
  {
    "objectID": "posts/log-y-regr2/log-y-regr2.html",
    "href": "posts/log-y-regr2/log-y-regr2.html",
    "title": "log-y-regr2",
    "section": "",
    "text": "Solution\n\nd2 <-\n  d %>% \n  filter(re74 > 0) %>% \n  mutate(re74_log = log(re74))\n\n\nm <- lm(re74_log ~ educ, data = d2)\n\n\nggplot(d2) +\n  aes(x = re74) +\n  geom_density() +\n  labs(title = \"Income raw\")\n\n\nggplot(d2) +\n  aes(x = re74_log) +\n  geom_density() +\n  labs(title = \"Income log transformed\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBetrachten wir die deskriptiven Statistiken:\n\nd2 %>% \n  select(re74, re74_log) %>% \n  describe_distribution()\n\nVariable |     Mean |       SD |      IQR |             Range | Skewness | Kurtosis |    n | n_Missing\n------------------------------------------------------------------------------------------------------\nre74     | 20938.28 | 12631.52 | 15086.30 | [17.63, 1.37e+05] |     1.62 |     6.81 | 2329 |         0\nre74_log |     9.73 |     0.76 |     0.80 |     [2.87, 11.83] |    -1.67 |     6.01 | 2329 |         0\n\n\nDie Log-Transformation hat in diesem Fall nicht wirklich zu einer Normalisierung der Variablen beigetragen. Aber das war auch nicht unser Ziel.\n\nCategories:\n\nregression\nlm\nqm2\nstats-nutshell"
  },
  {
    "objectID": "posts/adjustieren1/adjustieren1.html",
    "href": "posts/adjustieren1/adjustieren1.html",
    "title": "adjustieren1",
    "section": "",
    "text": "Solution\n\nlibrary(rstanarm)\nlm2 <- stan_glm(mpg ~ hp_z + am, data = mtcars,\n                refresh = 0)\nsummary(lm2)\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 26.6    1.5 24.7  26.6  28.5 \nhp          -0.1    0.0 -0.1  -0.1   0.0 \nam           5.3    1.1  3.8   5.3   6.6 \nsigma        3.0    0.4  2.5   3.0   3.5 \nDie Spalte mean gibt den mittleren geschätzten Wert für den jeweiligen Koeffizienten an, also den Schätzwert zum Koeffizienten.\nDie Koeffizienten zeigen, dass der Achsenabschnitt für Autos mit Automatikgetriebe um etwa 5 Meilen geringer ist als für Autos mit manueller Schaltung: Ein durchschnittliches Auto mit manueller Schaltung kommt also etwa 5 Meilen weiter als ein Auto mit Automatikschaltung, glaubt unser Modell.\n\nmtcars %>% \n  mutate(am = factor(am)) %>% \n  ggplot() +\n  aes(x = hp_z, y = mpg, color = am) +\n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\n\n\nMan könnte hier noch einen Interaktionseffekt ergänzen.\n\nCategories:\n\nqm2\nqm2-thema01\nws22"
  },
  {
    "objectID": "posts/interpret-koeff/interpret-koeff.html",
    "href": "posts/interpret-koeff/interpret-koeff.html",
    "title": "interpret-koeff",
    "section": "",
    "text": "Solution\n\nIntercept (\\(\\beta_0\\)): Der Achsenabschnitt gibt den geschätzten mittleren Y-Wert (Spritverbrauch) an, wenn \\(x=0\\), also für ein Auto mit 0 PS (was nicht wirklich Sinn macht). hp (\\(\\beta_1\\)) ist der Regressionskoeffizient oder Regressionsgewicht und damit die Steigung der Regressionsgeraden.\nhp (\\(\\beta_1\\)) ist der Regressionskoeffizient oder Regressionsgewicht und gibt den statistischen “Effekt” der PS-Zahl auf den Spritverbrauch an. Vorsicht: Dieser “Effekt” darf nicht vorschnell als kausaler Effekt verstanden werden. Daher muss man vorsichtig sein, wenn man von einem “Effekt” spricht. Vorsichtiger wäre zu sagen: “Ein Auto mit einem PS mehr, kommt im Mittel 0,1 Meilen weniger weit mit einer Gallone Sprit, laut diesem Modell”.\n\n\nCategories:\n\nregression\nlm\nbayes\nstats-nutshell"
  },
  {
    "objectID": "posts/ungewiss-arten-regr/ungewiss-arten-regr.html",
    "href": "posts/ungewiss-arten-regr/ungewiss-arten-regr.html",
    "title": "ungewiss-arten-regr",
    "section": "",
    "text": "Solution\n\n\n\n\n7.04\n0.04\n\n\nCategories:\n\nqm2\ninference\nlm"
  },
  {
    "objectID": "posts/vorhersageintervall1/vorhersageintervall1.html",
    "href": "posts/vorhersageintervall1/vorhersageintervall1.html",
    "title": "vorhersageintervall1",
    "section": "",
    "text": "Vorhersagen, etwa in einem Regressionsmodell, sind mit mehreren Arten von Unsicherheit konfrontiert.\nBerechnen Sie dazu ein Regressionsmodell, Datensatz mtcars, mit hp als Prädiktor (UV) und mpg als AV (Kriterium)!\nDann sagen Sie bitte den Wert der AV für eine Beobachtungseinheit mit mittlerer Ausprägung im Präktor vorher:\nEinmal nur unter Berücksichtigung der Unsicherheit innerhalb des Modells (“Konfidenzintervall”); einmal unter Berücksichtigung der Unsicherheit innerhalb des Modells sowie die Unsicherheit durch die Koffizienten (“Vohersageintervall”).\nHinweise:\n\npredict() ist eine Funktion, die Sie zur Vorhersage von Regressionsmodellen verwenden können.\nVerwenden Sie lm() zur Berechnung eines Regressionsmodells.\nDas Argument type von predict() erlaubt Ihnen die Wahl der Art der Vorhersage, betrachten Sie Hilfe der Funktion z.B. hier.\n\nBei welchem Intervall ist die Ungewissheit in der Vorhersage größer?\n\n\n\nKonfidenzintervall\nVohersageintervall\nGleich groß\nKommt auf weitere Faktoren an, keine pauschale Antwort möglich"
  },
  {
    "objectID": "posts/vorhersageintervall1/vorhersageintervall1.html#answerlist-1",
    "href": "posts/vorhersageintervall1/vorhersageintervall1.html#answerlist-1",
    "title": "vorhersageintervall1",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nWahr\nFalsch\nFalsch\n\n\nCategories:\n\nlm\ninference\nqm2"
  },
  {
    "objectID": "posts/lm-Standardfehler/lm-Standardfehler.html",
    "href": "posts/lm-Standardfehler/lm-Standardfehler.html",
    "title": "lm-Standardfehler",
    "section": "",
    "text": "Man kann angeben, wie genau eine Schätzung von Regressionskoeffizienten die Grundgesamtheit widerspiegelt. Zumeist wird dazu der Standardfehler (engl. standard error, SE) verwendet.\nIn dieser Übung untersuchen wir, wie sich der SE als Funktion der Stichprobengröße, \\(n\\), verhält.\nErstellen Sie dazu folgenden Datensatz:\n\nlibrary(tidyverse)\n\nn <- 2^4\n\nd <-\n  tibble(x = rnorm(n = n),  # im Default: mean = 0, sd = 1\n         y = x + rnorm(n, mean = 0, sd = .5))\n\nHier ist das Ergebnis. Uns interessiert v.a. Std. Error für den Prädiktor x:\n\nlm(y ~ x, data = d) %>% \nsummary()\n\n\nCall:\nlm(formula = y ~ x, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.60191 -0.42922  0.09198  0.32313  0.59878 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.1226     0.1061  -1.156    0.267    \nx             1.0385     0.0888  11.694  1.3e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4223 on 14 degrees of freedom\nMultiple R-squared:  0.9071,    Adjusted R-squared:  0.9005 \nF-statistic: 136.8 on 1 and 14 DF,  p-value: 1.302e-08\n\n\nHier haben wir eine Tabelle mit zwei Variablen, x und y, definiert mit n=16.\nVerdoppeln Sie die Stichprobengröße 5 Mal und betrachten Sie, wie sich die Schätzgenauigkeit, gemessen über den SE, verändert. Berechnen Sie dazu für jedes n eine Regression mit x als Prädiktor und y als AV!\nBei welcher Stichprobengröße ist SE am kleinsten?\n\n\n\n\\(2^5\\)\n\\(2^6\\)\n\\(2^7\\)\n\\(2^8\\)\n\\(2^9\\)"
  },
  {
    "objectID": "posts/lm-Standardfehler/lm-Standardfehler.html#answerlist-1",
    "href": "posts/lm-Standardfehler/lm-Standardfehler.html#answerlist-1",
    "title": "lm-Standardfehler",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nFalsch\nWahr. Die größte Stichprobe impliziert den kleinsten SE, ceteris paribus.\n\n\nCategories:\n\ninference\nlm\nqm2"
  },
  {
    "objectID": "posts/punktschaetzer-reicht-nicht/punktschaetzer-reicht-nicht.html",
    "href": "posts/punktschaetzer-reicht-nicht/punktschaetzer-reicht-nicht.html",
    "title": "punktschaetzer-reicht-nicht",
    "section": "",
    "text": "Solution\nModell m1 hat eine kleinere Ungewissheit im Hinblick auf die Modellkoeffizienten \\(\\beta_0, \\beta_1\\) und ist daher gegenüber m2 zu bevorzugen.\n\nCategories:\n\nqm2\nqm2-thema01\nws22"
  },
  {
    "objectID": "posts/nasa03/nasa03.html",
    "href": "posts/nasa03/nasa03.html",
    "title": "nasa03",
    "section": "",
    "text": "Solution\ntemp_is_above erstellen:\n\nd <-\n  d %>% \n  mutate(temp_is_above = case_when(\n    Jan > 0 ~ \"yes\",\n    Jan <= 0 ~ \"no\"\n  ))\n\nJahrhundert berechnen:\n\nd <-\n  d %>% \n  mutate(century = case_when(\n    Year < 1900 ~ \"19th\",\n    Year >= 1900 ~ \"20th\"\n  ))\n\nErhöhte Werte der Januar-Temperatur pro Jahrhundert berechnen:\n\nd_summarized <- \nd %>% \n  group_by(century) %>% \n  count(temp_is_above)\n\nd_summarized\n\n# A tibble: 4 × 3\n# Groups:   century [2]\n  century temp_is_above     n\n  <chr>   <chr>         <int>\n1 19th    no               19\n2 19th    yes               1\n3 20th    no               56\n4 20th    yes              67\n\n\nDer Befehl count() zählt aus, wie häufig die Ausprägungen der angegebenen Variablen X sind, m.a.W. er gibt die Verteilung von X wieder.\nEs macht vermutlich Sinn, noch die Anteile (relative Häufigkeiten) zu den absoluten Häufigkeiten zu ergänzen:\n\nd_summarized %>% \n  mutate(prop = n / sum(n))\n\n# A tibble: 4 × 4\n# Groups:   century [2]\n  century temp_is_above     n  prop\n  <chr>   <chr>         <int> <dbl>\n1 19th    no               19 0.95 \n2 19th    yes               1 0.05 \n3 20th    no               56 0.455\n4 20th    yes              67 0.545\n\n\nOdds Ratio berechnen:\nWir bezeichnen mit c19 (für “Chance 1”) das Verhältnis von erhöhter Temperatur zu nicht erhöhter Temperatur im 19. Jahrhundert.\n\nc19 <- 1 / 19\n\nMit c20 bezeichnen wir die analoge Chance für das 20. Jahrhundert:\n\nc20 <- 56 / 67\n\nDas Verhältnis der beiden Chancen gibt das Chancenverhältnis (Odds Ratio, OR):\n\nc19 / c20\n\n[1] 0.06296992\n\n\nGenauso gut kann man das OR von c20 zu c19 ausrechnen, der Effekt bleibt identisch:\n\nc20 / c19\n\n[1] 15.8806\n\n\nIn beiden Fällen ist es ein Faktor von knapp 16.\n\nCategories:\n~"
  },
  {
    "objectID": "posts/randomdag1/randomdag1.html",
    "href": "posts/randomdag1/randomdag1.html",
    "title": "randomdag1",
    "section": "",
    "text": "Gegeben sei der DAG g (s. u.). Der DAG verfügt über \\(n = 6\\) Variablen, die als Knoten im Graph dargestellt sind und mit \\(x_1, x_2, \\ldots x_n\\) bezeichnet sind.\nWelche minimale Variablenmenge muss kontrolliert werden, um den kausalen Effekt von der UV zur AV zu identifizieren?\nUV: x2.\nAV: x6.\nHinweise:\n\nMengen sind mittels geschweifter Klammern gekennzeichnet, z.B. {x1, x2} meint die Menge mit den zwei Elementen x1 und x2.\nDie leere Menge { } bedeutet, dass keine Variable kontrolliert werden muss, um den kausalen Effekt zu identifizieren.\nAlle Variablen werden als gemessen vorausgesetzt.\nEs ist möglich, dass es keine Lösung gibt, dass es also keine Adjustierungsmenge gibt, um den kausalen Effekt zu identifizieren. Wenn dies der Fall sein sollte, wählen Sie “/”.\nEs ist möglich, dass einzelne Variablen keine Kanten besitzen, also keine Verbindung zu anderen Variablen (Knoten) haben. Diese Variablen sind dann kausal unabhängig von den übrigen Variablen.\n\n\n\n\n\n\n\n\n\n\n\n\n\n{ x1, x2 }\n{ x3, x6 }\n{ x1, x6 }\n{ x2, x3 }\n{ }"
  },
  {
    "objectID": "posts/randomdag1/randomdag1.html#answerlist-1",
    "href": "posts/randomdag1/randomdag1.html#answerlist-1",
    "title": "randomdag1",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nFalsch\nFalsch\nFalsch\nRichtig\n\n\nCategories:\n\ncausal\ndag"
  },
  {
    "objectID": "posts/nasa02/nasa02.html",
    "href": "posts/nasa02/nasa02.html",
    "title": "nasa02",
    "section": "",
    "text": "Solution\nDekade berechnen:\n\nd <-\n  d %>% \n  mutate(decade = round(Year/10))\n\nKorrelation:\n\nd %>% \n  summarise(temp_cor = cor(Jan, Feb))\n\n# A tibble: 1 × 1\n  temp_cor\n     <dbl>\n1    0.940\n\n\nKorrelation pro Dekade:\n\nd_summarized <- \n  d %>% \n  group_by(decade) %>% \n  summarise(temp_cor = cor(Jan, Feb))\n\n\n\n\n\n\n\n  \n  \n    \n      decade\n      temp_cor\n    \n  \n  \n    188\n0.87\n    189\n0.79\n    190\n0.53\n    191\n0.83\n    192\n0.82\n    193\n0.73\n    194\n0.50\n    195\n0.78\n    196\n0.56\n    197\n0.79\n    198\n0.80\n    199\n0.53\n    200\n0.56\n    201\n0.66\n    202\n0.96\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nDie Korrelation der Temperaturen und damit die Ähnlichkeit der Muster hat im Laufe der Dekaden immer mal wieder geschwankt.\n\nCategories:\n~"
  },
  {
    "objectID": "posts/griech-buchstaben/griech-buchstaben.html",
    "href": "posts/griech-buchstaben/griech-buchstaben.html",
    "title": "Griech-Buchstaben-Inferenz",
    "section": "",
    "text": "Solution\n\n\n\n\n\nKennwert\nStatistik\nParameter\n\n\n\n\nMittelwert\n\\[\\bar{X}\\]\n\\[\\mu\\]\n\n\nMittelwertsdifferenz\n\\[d=\\bar{X}_1-\\bar{X}_2\\]\n\\[\\mu_1\\]- \\[\\mu_2\\]\n\n\nStreuung\nsd\n\\[\\sigma\\]\n\n\nAnteil\np\n\\[\\pi\\]\n\n\nKorrelation\nr\n\\[\\rho\\]\n\n\nRegressionsgewicht\nb\n\\[\\beta\\]\n\n\n\n\n\n\nCategories:\n\nqm2\nqm2-thema01"
  },
  {
    "objectID": "posts/mtcars-simple2/mtcars-simple2.html",
    "href": "posts/mtcars-simple2/mtcars-simple2.html",
    "title": "mtcars-simple2",
    "section": "",
    "text": "Solution\nCompute Model:\n\nlm1_freq <- lm(mpg ~ hp + cyl + disp, data = mtcars)\n\nlibrary(rstanarm)\nlm1_bayes <- stan_glm(mpg ~ hp + cyl + disp, data = mtcars, refresh = 0)\n\nGet R2:\n\nlibrary(easystats)\n\n\nr2(lm1_freq)\n\n# R2 for Linear Regression\n       R2: 0.768\n  adj. R2: 0.743\n\n\n\nr2(lm1_bayes)\n\n# Bayesian R2 with Compatibility Interval\n\n  Conditional R2: 0.746 (95% CI [0.601, 0.859])\n\n\nThe coefficient is estimated as about 0.77.\n\nCategories:\n\nregresssion\nen\nbayes\nfrequentist\nqm1\nstats-nutshell"
  },
  {
    "objectID": "posts/purrr-map02/purrr-map02.html",
    "href": "posts/purrr-map02/purrr-map02.html",
    "title": "purrr-map02",
    "section": "",
    "text": "Exercise\nBestimmen Sie die häufigsten Worte im Grundatzprogramm der Partei AfD (in der aktuellsten Version).\n         \n\n\nSolution\nText aus PDF-Dateien kann man mit dem Paket pdftools einlesen:\n\nlibrary(pdftools)\n\nUsing poppler version 22.02.0\n\nd_path <- \"/Users/sebastiansaueruser/Google Drive/Literatur/_Div/Politik/afd-grundsatzprogramm-2022.pdf\"\n\nd <- tibble(text = pdf_text(d_path))\n\nDann erstellen wir eine Tidy-Version und tokenisieren nach Wörtern:\n\nlibrary(tidytext)\nd2 <-\n  d %>% \n  unnest_tokens(output = word, input = text)\n\nhead(d2)\n\n# A tibble: 6 × 1\n  word             \n  <chr>            \n1 programm         \n2 für              \n3 deutschland      \n4 das              \n5 grundsatzprogramm\n6 der              \n\n\nDann zählen wir die Wörter:\n\nd2 %>% \n  count(word, sort = TRUE) %>% \n  head(20)\n\n# A tibble: 20 × 2\n   word            n\n   <chr>       <int>\n 1 die          1151\n 2 und          1147\n 3 der           870\n 4 zu            435\n 5 für           392\n 6 in            392\n 7 den           271\n 8 von           257\n 9 ist           251\n10 das           225\n11 werden        214\n12 eine          211\n13 nicht         196\n14 ein           191\n15 deutschland   190\n16 sind          187\n17 wir           176\n18 afd           171\n19 des           169\n20 sich          158\n\n\n\nCategories:\n\nr\nmap\ntidyverse"
  },
  {
    "objectID": "posts/mtcars-simple3/mtcars-simple3.html",
    "href": "posts/mtcars-simple3/mtcars-simple3.html",
    "title": "mtcars-simple3",
    "section": "",
    "text": "We will use the dataset mtcars in this exercise.\nAssume your causal model of your research dictates that fuel economy is a linear function of horse power, cylinder count and displacement of the engine.\nWhich of the predictors in the above model has the weakest causal impact on the output variable?\nNotes:\n\nUse can either use frequentist or bayesian modeling.\nUse R for all computations.\nThere are multiple ways to find a solution.\n\n\n\n\ncyl\nhp\ndisp\nAll are equally strong\nnone of the above"
  },
  {
    "objectID": "posts/mtcars-simple3/mtcars-simple3.html#answerlist-1",
    "href": "posts/mtcars-simple3/mtcars-simple3.html#answerlist-1",
    "title": "mtcars-simple3",
    "section": "Answerlist",
    "text": "Answerlist\n\nwrong\ncorrect\nwrong\nwrong\nwrong\n\n\nCategories:\n\nregresssion\nen\nbayes\nfrequentist\nqm1\nstats-nutshell"
  },
  {
    "objectID": "posts/ttest-als-regr/ttest-als-regr.html",
    "href": "posts/ttest-als-regr/ttest-als-regr.html",
    "title": "ttest-als-regr",
    "section": "",
    "text": "Der t-Test kann als Spezialfall der Regressionsanalyse gedeutet werden.\nHierbei ist es wichtig, sich das Skalenniveau der Variablen, die ein t-Test verarbeitet, vor Augen zu führen.\nHinweisse:\n\nDie folgende Abbildung gibt Tipps.\nInformationen, die zur Lösung einer Aufgabe nicht nötig sind, sollte man ignorieren.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBenennen Sie die Skalenniveaus der UV eines t-Tests! Geben Sie nur ein Wort ein. Verwenden Sie nur Kleinbuchstaben (z.B. regression).\nBenennen Sie die Skalenniveaus der AV eines t-Tests! Geben Sie nur ein Wort ein. Verwenden Sie nur Kleinbuchstaben (z.B. regression).\nNennen Sie eine beispielhafte Forschungsfrage für einen t-Test.\nSkizzieren Sie ein Diagramm einer Regression, die analytisch identisch (oder sehr ähnlich) zu einem t-Test ist!"
  },
  {
    "objectID": "posts/ttest-als-regr/ttest-als-regr.html#answerlist-1",
    "href": "posts/ttest-als-regr/ttest-als-regr.html#answerlist-1",
    "title": "ttest-als-regr",
    "section": "Answerlist",
    "text": "Answerlist\n\nUV: binär\nAV: metrisch\nUnterscheiden sich die mittleren Einparkzeiten von Frauen und Männern?\nAus dem Datensatz mtcars:\n\n\ndata(mtcars)\nmtcars %>% \n  ggplot() +\n  aes(x = am, y = mpg) +\n  geom_point(alpha = .5) +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nCategories:\n\nregr\nttest\nvariable-levels"
  },
  {
    "objectID": "posts/log-y-regr1/log-y-regr1.html",
    "href": "posts/log-y-regr1/log-y-regr1.html",
    "title": "log-y-regr1",
    "section": "",
    "text": "Solution\n\nd2 <-\n  d %>% \n  filter(re74 > 0) %>% \n  mutate(re74_log = log(re74))\n\n\nm <- lm(re74_log ~ educ, data = d2)\n\nHier sind die parameters des Modells.\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(2327)\np\n\n\n\n\n(Intercept)\n8.83\n0.06\n(8.70, 8.95)\n142.91\n< .001\n\n\neduc\n0.07\n4.94e-03\n(0.07, 0.08)\n15.16\n< .001\n\n\n\n\nFür jedes Jahr Bildung steigt das Einkommen also ca. um den Faktor 1.07.\nEtwas genauer:\n\\(\\hat{\\beta_1} = 0.07\\) bedeutet, dass ein Jahr Bildung zu einen erwarteten Unterschied im Einkommen in Höhe von 0.07 in Log-Einkommen führt. Anders gesagt wird das Einkommen um exp(0.07) erhöht. Dabei gilt \\(e^{0.07} \\approx 1.07\\):\n\nexp(0.07)\n\n[1] 1.072508\n\n\nDie Lösung lautet also: “Pro Jahr Bildung steigt das Einkommen - laut Modell um den Faktor ca. 1.07”.\nMan darf dabei nicht vergessen, dass wir wir uns hier auf die Schnelle ein Modell ausgedacht haben. Ob es in Wirklichkeit so ist, wie unser Modell meint, ist eine andere Sache!\n\nCategories:\n\nregression\nlm\nqm2\nstats-nutshell"
  },
  {
    "objectID": "posts/adjustieren2/adjustieren2.html",
    "href": "posts/adjustieren2/adjustieren2.html",
    "title": "adjustieren2",
    "section": "",
    "text": "Solution\n\n\n\n\nUnser Modell lm1 schätzt den Preis eines Diamanten mittlerer Größe auf etwa 3932.5 (was immer auch die Einheiten sind, Dollar vermutlich).\nprice ~ carat_z + cut\n\nDieses zweite Modell könnten wir so berechnen:\n\nlm2 <- stan_glm(price ~ carat_z + cut, data = diamonds,\n                chains = 1,\n                refresh = 0)\nparameters(lm2)\n\nParameter    |  Median |             95% CI |   pd | % in ROPE |  Rhat |     ESS |                       Prior\n--------------------------------------------------------------------------------------------------------------\n(Intercept)  | 2406.07 | [2334.35, 2488.89] | 100% |        0% | 1.000 |  321.00 | Normal (3932.80 +- 9973.60)\ncarat_z      | 7870.55 | [7843.93, 7897.58] | 100% |        0% | 1.005 | 1047.00 |   Normal (0.00 +- 21040.85)\ncutGood      | 1118.79 | [1027.17, 1203.09] | 100% |        0% | 0.999 |  434.00 |   Normal (0.00 +- 34685.38)\ncutIdeal     | 1799.71 | [1714.31, 1869.75] | 100% |        0% | 0.999 |  338.00 |   Normal (0.00 +- 20362.28)\ncutPremium   | 1437.68 | [1353.65, 1512.30] | 100% |        0% | 0.999 |  351.00 |   Normal (0.00 +- 22862.49)\ncutVery Good | 1508.84 | [1422.69, 1582.24] | 100% |        0% | 1.000 |  344.00 |   Normal (0.00 +- 23922.15)\n\n\nEin “normales” (frequentistisches) lm käme zu ähnlichen Ergebnissen:\n\nlm(price ~ carat_z + cut, data = diamonds)\n\n\nCall:\nlm(formula = price ~ carat_z + cut, data = diamonds)\n\nCoefficients:\n (Intercept)       carat_z       cutGood      cutIdeal    cutPremium  \n        2405          7871          1120          1801          1439  \ncutVery Good  \n        1510  \n\n\nMan könnte hier noch einen Interaktionseffekt ergänzen, wenn man Grund zur Annahme hat, dass es einen gibt.\n\nCategories:\n\nqm2\n‘2022’"
  },
  {
    "objectID": "posts/Griech-Buchstaben-Inferenz/Griech-Buchstaben-Inferenz.html",
    "href": "posts/Griech-Buchstaben-Inferenz/Griech-Buchstaben-Inferenz.html",
    "title": "Griech-Buchstaben-Inferenz",
    "section": "",
    "text": "Solution\n\n\n\n\n\nKennwert\nStatistik\nParameter\n\n\n\n\nMittelwert\n\\[\\bar{X}\\]\n\\[\\mu\\]\n\n\nMittelwertsdifferenz\n\\[d=\\bar{X}_1-\\bar{X}_2\\]\n\\[\\mu_1\\]- \\[\\mu_2\\]\n\n\nStreuung\nsd\n\\[\\sigma\\]\n\n\nAnteil\np\n\\[\\pi\\]\n\n\nKorrelation\nr\n\\[\\rho\\]\n\n\nRegressionsgewicht\nb\n\\[\\beta\\]\n\n\n\n\n\n\nCategories:\n\nqm2\nqm2-thema01"
  },
  {
    "objectID": "posts/purrr-map03/purrr-map03.html",
    "href": "posts/purrr-map03/purrr-map03.html",
    "title": "purrr-map03",
    "section": "",
    "text": "Exercise\nImportieren Sie das Grundatzprogramm der Partei AfD (in der aktuellsten Version). Tokenisieren Sie nach Sätzen. Dann entfernen Sie alle Zahlen. Dann zählen Sie die Anzahl der Wörter pro Satz und berichten gängige deskriptive Statistiken dazu.\n         \n\n\nSolution\nText aus PDF-Dateien kann man mit dem Paket pdftools einlesen:\n\nlibrary(pdftools)\n\nUsing poppler version 22.02.0\n\nd_path <- \"/Users/sebastiansaueruser/Google Drive/Literatur/_Div/Politik/afd-grundsatzprogramm-2022.pdf\"\n\nd <- tibble(text = pdf_text(d_path))\n\nDann erstellen wir eine Tidy-Version und tokenisieren nach Sätzen:\n\nlibrary(tidytext)\nd2 <-\n  d %>% \n  unnest_sentences(output = word, input = text)\n\nhead(d2)\n\n# A tibble: 6 × 1\n  word                                                                          \n  <chr>                                                                         \n1 programm für deutschland.                                                     \n2 das grundsatzprogramm der alternative für deutschland.                        \n3 2   programm für deutschland | inhalt         präambel                       …\n4 familien stärken        43             und parteiferne rechnungshöfe         …\n5 3   programm für deutschland | inhalt         7 | kultur, sprache und identit…\n6 förder- und                         10.10.3 deutsche literatur im inland digi…\n\n\nDann entfernen wir die Zahlen:\n\nd3 <- \n  d2 %>% \n  mutate(word = str_remove_all(word, pattern = \"[:digit:]+\"))\n\nPrüfen wir, ob es geklappt hat:\n\nd2$word[10]\n\n[1] \"weniger subventionen    88      13.7 fischerei, forst und jagd: im einklang mit der natur     88      13.8 flächenkonkurrenz:           nicht zu lasten der land- und forstwirtschaft            88\"\n\nd3$word[10]\n\n[1] \"weniger subventionen          . fischerei, forst und jagd: im einklang mit der natur           . flächenkonkurrenz:           nicht zu lasten der land- und forstwirtschaft            \"\n\n\nOk.\nDann zählen wir die Wörter pro Satz:\n\nd4 <- \n  d3 %>% \n  summarise(word_count_per_sentence = str_count(word, \"\\\\w+\"))\n\nhead(d4)\n\n# A tibble: 6 × 1\n  word_count_per_sentence\n                    <int>\n1                       3\n2                       6\n3                     196\n4                      40\n5                     254\n6                      15\n\n\nVisualisierung:\n\nd4 %>% \n  ggplot(aes(x = word_count_per_sentence)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.5.2 (red = needs update)\n✔ insight     0.18.4    ✖ datawizard  0.6.1  \n✔ bayestestR  0.13.0    ✖ performance 0.9.2  \n✖ parameters  0.18.2    ✖ effectsize  0.7.0.5\n✔ modelbased  0.8.5     ✖ correlation 0.8.2  \n✔ see         0.7.3     ✔ report      0.5.5  \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`.\n\ndescribe_distribution(d4)\n\nVariable                |  Mean |    SD | IQR |          Range | Skewness | Kurtosis |    n | n_Missing\n-------------------------------------------------------------------------------------------------------\nword_count_per_sentence | 21.86 | 17.24 |  19 | [0.00, 254.00] |     3.84 |    37.52 | 1208 |         0\n\n\n\nCategories:\n\nr\nmap\ntidyverse"
  },
  {
    "objectID": "posts/purrr-map04/purrr-map04.html",
    "href": "posts/purrr-map04/purrr-map04.html",
    "title": "purrr-map04",
    "section": "",
    "text": "Exercise\nImportieren Sie das Grundatzprogramm der Partei AfD (in der aktuellsten Version). Tokenisieren Sie nach Seiten. Dann verschachteln Sie die Spalte, in denen der Text der Seite steht, zu einer Listenspalte. Schließlich zählen Sie die Anzahl der Wörter pro Seite und berichten gängige deskriptive Statistiken dazu.\n         \n\n\nSolution\nText aus PDF-Dateien kann man mit dem Paket pdftools einlesen:\n\nlibrary(pdftools)\n\nUsing poppler version 22.02.0\n\nd_path <- \"/Users/sebastiansaueruser/Google Drive/Literatur/_Div/Politik/afd-grundsatzprogramm-2022.pdf\"\n\nd <- tibble(text = pdf_text(d_path),\n            page = 1:length(text))\n\nZu Seiten tokenisieren brauchen wir nicht; das Datenmaterial ist bereits nach Seiten organisiert.\nJetzt “verschachteln” (to nest) wir die Spalte mit dem Text:\n\nd2 <-\n  d %>% \n  nest(data = text)\n\nhead(d2)\n\n# A tibble: 6 × 2\n   page data            \n  <int> <list>          \n1     1 <tibble [1 × 1]>\n2     2 <tibble [1 × 1]>\n3     3 <tibble [1 × 1]>\n4     4 <tibble [1 × 1]>\n5     5 <tibble [1 × 1]>\n6     6 <tibble [1 × 1]>\n\n\nDann zählen wir die Wörter pro Seite:\n\nd3 <-\n  d2 %>% \n  mutate(word_count_per_page = map(data, ~ str_count(.x$text, \"\\\\w+\")))\n\nhead(d3)\n\n# A tibble: 6 × 3\n   page data             word_count_per_page\n  <int> <list>           <list>             \n1     1 <tibble [1 × 1]> <int [1]>          \n2     2 <tibble [1 × 1]> <int [1]>          \n3     3 <tibble [1 × 1]> <int [1]>          \n4     4 <tibble [1 × 1]> <int [1]>          \n5     5 <tibble [1 × 1]> <int [1]>          \n6     6 <tibble [1 × 1]> <int [1]>          \n\n\nWie sieht eine Zelle aus data aus?\n\nd3$data[[1]]\n\n# A tibble: 1 × 1\n  text                                                                          \n  <chr>                                                                         \n1 \"PROGRAMM FÜR\\nDEUTSCHLAND.\\nDas Grundsatzprogramm der Alternative für Deutsc…\n\n\nWie sieht eine Zelle aus word_count_per_page aus?\n\nd3$data[[1]]\n\n# A tibble: 1 × 1\n  text                                                                          \n  <chr>                                                                         \n1 \"PROGRAMM FÜR\\nDEUTSCHLAND.\\nDas Grundsatzprogramm der Alternative für Deutsc…\n\n\nAh! Darin steckt nur eine einzelne Zahl!\n\nd3$data[[1]] %>% str()\n\ntibble [1 × 1] (S3: tbl_df/tbl/data.frame)\n $ text: chr \"PROGRAMM FÜR\\nDEUTSCHLAND.\\nDas Grundsatzprogramm der Alternative für Deutschland.\\n\"\n\n\nDas heißt, wir können vereinfachen, entschacheln:\n\nd4 <-\n  d3 %>% \n  unnest(word_count_per_page)\n\nhead(d4)\n\n# A tibble: 6 × 3\n   page data             word_count_per_page\n  <int> <list>                         <int>\n1     1 <tibble [1 × 1]>                   9\n2     2 <tibble [1 × 1]>                 410\n3     3 <tibble [1 × 1]>                 516\n4     4 <tibble [1 × 1]>                 297\n5     5 <tibble [1 × 1]>                   1\n6     6 <tibble [1 × 1]>                 414\n\n\nVisualisierung:\n\nd4 %>% \n  ggplot(aes(x = word_count_per_page)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.5.2 (red = needs update)\n✔ insight     0.18.4    ✖ datawizard  0.6.1  \n✔ bayestestR  0.13.0    ✖ performance 0.9.2  \n✖ parameters  0.18.2    ✖ effectsize  0.7.0.5\n✔ modelbased  0.8.5     ✖ correlation 0.8.2  \n✔ see         0.7.3     ✔ report      0.5.5  \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`.\n\ndescribe_distribution(d4$word_count_per_page)\n\n  Mean |     SD |    IQR |          Range | Skewness | Kurtosis |  n | n_Missing\n--------------------------------------------------------------------------------\n285.84 | 172.27 | 322.75 | [1.00, 516.00] |    -0.64 |    -1.24 | 96 |         0\n\n\n\nCategories:\n\nr\nmap\ntidyverse"
  },
  {
    "objectID": "posts/Stichprobenziehen1/Stichprobenziehen1.html",
    "href": "posts/Stichprobenziehen1/Stichprobenziehen1.html",
    "title": "Stichprobenziehen1",
    "section": "",
    "text": "Solution\nIndividuell\n\nCategories:\n\nlm\ninference\nqm2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "datenwerk",
    "section": "",
    "text": "r\n\n\nmap\n\n\ntidyverse\n\n\n\n\n\n\n\n\n\n\n\nOct 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nmap\n\n\ntidyverse\n\n\n\n\n\n\n\n\n\n\n\nOct 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nmap\n\n\ntidyverse\n\n\n\n\n\n\n\n\n\n\n\nOct 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nmap\n\n\ntidyverse\n\n\n\n\n\n\n\n\n\n\n\nOct 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nqm2\n\n\ninference\n\n\nlm\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\neda\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\neda\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\neda\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncausal\n\n\ndag\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncorrelation\n\n\nlm\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nqm2\n\n\ninference\n\n\nuncertainty\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nqm2\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nttest\n\n\nregr\n\n\nvariable-levels\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregresssion\n\n\nen\n\n\nbayes\n\n\nfrequentist\n\n\nqm1\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstats-nutshell\n\n\nqm2\n\n\nregression\n\n\nlog\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nqm2\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nqm2\n\n\nlm\n\n\nbayes\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nbayes\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlm\n\n\ninference\n\n\nqm2\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninference\n\n\nlm\n\n\nqm2\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregresssion\n\n\nen\n\n\nbayes\n\n\nfrequentist\n\n\nqm1\n\n\nstats-nutshell\n\n\nqm2\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregresssion\n\n\nen\n\n\nbayes\n\n\nfrequentist\n\n\nqm1\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregresssion\n\n\nen\n\n\nbayes\n\n\nfrequentist\n\n\nqm1\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregr\n\n\nttest\n\n\nvariable-levels\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nqm2\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nqm2\n\n\nbayes\n\n\nadjust\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nqm2\n\n\ninference\n\n\nparameters\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlm\n\n\ninference\n\n\nqm2\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nqm2\n\n\nqm2-thema01\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Hallo, Datenwerk",
    "section": "",
    "text": "Autor: Sebastian Sauer\nDer Quellcode findet sich hier.\nDie Lizenz ist permissiv, s. Hinweise hier."
  }
]