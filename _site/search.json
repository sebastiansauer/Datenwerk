[
  {
    "objectID": "posts/ReThink3m2/ReThink3m2.html",
    "href": "posts/ReThink3m2/ReThink3m2.html",
    "title": "ReThink3m2",
    "section": "",
    "text": "Solution\n\nlibrary(tidyverse)\n\n\n\n\nPost-Verteilung berechnen:\n\np_grid <- seq(from = 0, to = 1, length.out = 1000)\nprior <- rep(1, 1000)\nlikelihood <- dbinom(8, size = 15, prob = p_grid)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\nStichproben-Postverteilung erstellen:\n\nsamples <- \n  tibble(anteil_wasser = sample(p_grid, prob = posterior, size = 1e4, replace = TRUE))\n\nhead(samples)\n\n# A tibble: 6 × 1\n  anteil_wasser\n          <dbl>\n1         0.357\n2         0.413\n3         0.559\n4         0.667\n5         0.419\n6         0.268\n\n\n\n\n\n\nsamples %>% \n  ggplot() +\n  aes(x = anteil_wasser) +\n  geom_histogram() + \n  labs(title = \"Stichproben aus der Posteriori-Verteilung\")\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(easystats)\nhdi(samples, prob = 0.9)\n\nHighest Density Interval\n\nParameter     |      95% HDI\n----------------------------\nanteil_wasser | [0.31, 0.76]\n\n\n\nCategories:\n\nbayes\npost\nprobability"
  },
  {
    "objectID": "posts/ReThink3e1-7/ReThink3e1-7.html",
    "href": "posts/ReThink3e1-7/ReThink3e1-7.html",
    "title": "ReThink3e1-7",
    "section": "",
    "text": "Solution\nEs finden sich auch Lösungsvorschläge online, z.B. hier\n\nWie viel Wahrscheinlichkeitsmasse liegt unter \\(p=0.2\\)?\n\n\nsamples %>% \n  count(p < 0.2)\n\n# A tibble: 2 × 2\n  `p < 0.2`     n\n  <lgl>     <int>\n1 FALSE      9993\n2 TRUE          7\n\n\nFast nix!\n\nWie viel Wahrscheinlichkeitsmasse liegt über \\(p=0.8\\)?\n\n\nsamples %>% \n  count(p > 0.8)\n\n# A tibble: 2 × 2\n  `p > 0.8`     n\n  <lgl>     <int>\n1 FALSE      8842\n2 TRUE       1158\n\n\nNaja, so gut 10%!\n\nWelcher Anteil der Posteriori-Verteilung liegt zwischen \\(p=0.2\\) und \\(p=0.8\\)?\n\n\nsamples %>% \n  count(p > 0.2 & p < 0.8) \n\n# A tibble: 2 × 2\n  `p > 0.2 & p < 0.8`     n\n  <lgl>               <int>\n1 FALSE                1165\n2 TRUE                 8835\n\n\nKnapp 90%!\n\nUnter welchem Wasseranteil \\(p\\) liegen 20% der Posteriori-Verteilung?\n\nEine Möglichkeit: Wir sortieren \\(p\\) der Größe nach (aufsteigend), filtern dann so, dass wir nur die ersten 20% der Zeilen behalten und schauen dann, was der größte Wert ist.\n\nsamples %>% \n  arrange(p) %>% \n  slice_head(prop = 0.2) %>% \n  summarise(quantil_20 = max(p))\n\n# A tibble: 1 × 1\n  quantil_20\n       <dbl>\n1      0.517\n\n\nAndererseits: Das, was wir gerade gemacht haben, nennt man auch ein Quantil berechnen, s. auch hier. Dafür gibt’s fertige Funktionen in R, wie quantile():\n\nsamples %>% \n  summarise(q_20 = quantile(p, 0.2))\n\n# A tibble: 1 × 1\n   q_20\n  <dbl>\n1 0.517\n\n\n\nÜber welchem Wasseranteil \\(p\\) liegen 10% der Posteriori-Verteilung?\n\n\nsamples %>% \n  summarise(quantile(p, 0.9))\n\n# A tibble: 1 × 1\n  `quantile(p, 0.9)`\n               <dbl>\n1              0.810\n\n\nMit 90% Wahrscheinlichkeit ist der Wasseranteil höchstns bei 81%.\n\nWelches schmälstes Intervall von \\(p\\) enthält 66% der Posteriori-Wahrscheinlichkeit?\n\n\nlibrary(easystats)\nhdi(samples, ci = 0.66)\n\nHighest Density Interval\n\nParameter |      66% HDI\n------------------------\np         | [0.52, 0.79]\n\n\n\nWelcher Wertebereich von \\(p\\) enthält 66% der Posteriori-Wahrscheinlichkeit (hier wird Posteriori-Wahrscheinlichkeit syonyom gebraucht zu Posteriori-Verteilung)?\n\nWir nutzen hier die Equal-Tail-Intervall (oder Perzentilintervall genannt), da die Aufgabe keine genauen Angaben macht.\n\neti(samples, ci = 0.66)\n\nEqual-Tailed Interval\n\nParameter |      66% ETI\n------------------------\np         | [0.50, 0.77]\n\n\nEin “mittleres” 2/3-Intervall lässt 1/3 der Wahrscheinlichkeitsmasse außen vor, und zwar gleichmäßig in zwei Hälften links und rechts, also jeweils 1/6 (17%). So ein Intervall heißt Perzentilintervall. Daher synonym:\n\nsamples %>% \n  summarise(PI_66 = quantile(p, prob = c(0.17, .84)))\n\n# A tibble: 2 × 1\n  PI_66\n  <dbl>\n1 0.501\n2 0.779\n\n\n\nCategories:\n\nbayes\nprobability\npost"
  },
  {
    "objectID": "posts/nasa02/nasa02.html",
    "href": "posts/nasa02/nasa02.html",
    "title": "nasa02",
    "section": "",
    "text": "Solution\nDekade berechnen:\n\nd <-\n  d %>% \n  mutate(decade = round(Year/10))\n\nKorrelation:\n\nd %>% \n  summarise(temp_cor = cor(Jan, Feb))\n\n# A tibble: 1 × 1\n  temp_cor\n     <dbl>\n1    0.940\n\n\nKorrelation pro Dekade:\n\nd_summarized <- \n  d %>% \n  group_by(decade) %>% \n  summarise(temp_cor = cor(Jan, Feb))\n\n\n\n\n\n\n\n  \n  \n    \n      decade\n      temp_cor\n    \n  \n  \n    188\n0.87\n    189\n0.79\n    190\n0.53\n    191\n0.83\n    192\n0.82\n    193\n0.73\n    194\n0.50\n    195\n0.78\n    196\n0.56\n    197\n0.79\n    198\n0.80\n    199\n0.53\n    200\n0.56\n    201\n0.66\n    202\n0.96\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nDie Korrelation der Temperaturen und damit die Ähnlichkeit der Muster hat im Laufe der Dekaden immer mal wieder geschwankt.\n\nCategories:\n~"
  },
  {
    "objectID": "posts/ReThink3m4/ReThink3m4.html",
    "href": "posts/ReThink3m4/ReThink3m4.html",
    "title": "ReThink3m4",
    "section": "",
    "text": "Solution\nErstellen wir zuerst wieder die Posteriori-Verteilung für den Globusversuch.\n\np_grid <- seq( from=0 , to=1 , length.out=1000 )  # Gitterwerte\n\nprior <- rep( 1 , 1000 )  # Priori-Gewichte\n\nset.seed(42)\nlikelihood <- dbinom( 6 , size=9 , prob=p_grid ) \n\nunstandardisierte_posterior <- likelihood * prior \n\nposterior <- unstandardisierte_posterior / sum(unstandardisierte_posterior)\n\nDann ziehen wir unsere Stichproben daraus:\n\n# um die Zufallszahlen festzulegen, damit alle die gleichen Zufallswerte bekommen: \nset.seed(100) \n\n# Stichproben ziehen aus der Posteriori-Verteilung\nsamples <- \n  tibble(\n    p = sample( p_grid , prob=posterior, size=1e4, replace=TRUE)) \n\nJetzt erstellen wir die PPV für einen anderen Versuch, nämlich mit 9 Zügen:\n\nPPV <-\n  samples %>% \n  mutate(anzahl_wasser2 = rbinom(1e4, size = 9, prob = p))\n\nSchließlich zählen wir, wie oft 6 Treffer beobachtet werden:\n\nPPV %>% \n  count(anzahl_wasser2 == 6) \n\n# A tibble: 2 × 2\n  `anzahl_wasser2 == 6`     n\n  <lgl>                 <int>\n1 FALSE                  8059\n2 TRUE                   1941\n\n\nQuelle\n\nCategories:\n\nbayes\nppv\nprobability"
  },
  {
    "objectID": "posts/Rethink_2m1/Rethink_2m1.html",
    "href": "posts/Rethink_2m1/Rethink_2m1.html",
    "title": "Rethink_2m1",
    "section": "",
    "text": "Solution\nThe solution is taken from this source.\n\nlibrary(tidyverse)\n\ndist <- \n  tibble(\n    # Gridwerte bestimmen:\n    p_grid = seq(from = 0, to = 1, length.out = 20),\n    # Priori-Wskt bestimmen:\n    prior = rep(1, times = 20)) %>%\n  mutate(\n    # Likelihood berechnen:\n    likelihood_1 = dbinom(3, size = 3, prob = p_grid),  # WWW\n    likelihood_2 = dbinom(3, size = 4, prob = p_grid),  # WWWL\n    likelihood_3 = dbinom(5, size = 7, prob = p_grid),  # LWWLWWW\n    # unstand. Posterior-Wskt:\n    unstand_post_1 = likelihood_1 * prior,\n    unstand_post_2 = likelihood_2 * prior,\n    unstand_post_3 = likelihood_3 * prior,\n    # stand. Post-Wskt:\n    std_post_1 = unstand_post_1 / sum(unstand_post_1),\n    std_post_2 = unstand_post_2 / sum(unstand_post_2),\n    std_post_3 = unstand_post_3 / sum(unstand_post_3)\n    ) \n\nJetzt können wir das Diagramm zeichnen:\n\nggplot(dist) +\n  aes(x = p_grid, y= std_post_1) +\n  geom_line()+\n  geom_point() +\n  labs(x = \"p(W)\",\n       y = \"Posteriori-Wahrscheinlichkeit\",\n       title = \"Daten: WWW\")\n\n\n\n\n\n\n\nggplot(dist) +\n  aes(x = p_grid, y= std_post_2) +\n  geom_line()+\n  geom_point() +\n  labs(x = \"p(W)\",\n       y = \"Posteriori-Wahrscheinlichkeit\",\n       title = \"Daten: WWWL\")\n\n\n\n\n\n\n\nggplot(dist) +\n  aes(x = p_grid, y= std_post_3) +\n  geom_line()+\n  geom_point() +\n  labs(x = \"p(W)\",\n       y = \"Posteriori-Wahrscheinlichkeit\",\n       title = \"Daten: LWWLWWW\")\n\n\n\n\n\n\n\n\nEtwas eleganter (und komplizierter) kann man es auch so in R schreiben (Quelle):\n\nlibrary(tidyverse)\n\ndist <- tibble(p_grid = seq(from = 0, to = 1, length.out = 20),\n               prior = rep(1, times = 20)) %>%\n  mutate(likelihood_1 = dbinom(3, size = 3, prob = p_grid),\n         likelihood_2 = dbinom(3, size = 4, prob = p_grid),\n         likelihood_3 = dbinom(5, size = 7, prob = p_grid),\n         across(starts_with(\"likelihood\"), ~ .x * prior),\n         across(starts_with(\"likelihood\"), ~ .x / sum(.x))) %>%\n  pivot_longer(cols = starts_with(\"likelihood\"), names_to = \"pattern\",\n               values_to = \"posterior\") %>%\n  separate(pattern, c(NA, \"pattern\"), sep = \"_\", convert = TRUE) %>%\n  mutate(obs = case_when(pattern == 1L ~ \"W, W, W\",\n                         pattern == 2L ~ \"W, W, W, L\",\n                         pattern == 3L ~ \"L, W, W, L, W, W, W\"))\n\nggplot(dist, aes(x = p_grid, y = posterior)) +\n  facet_wrap(vars(fct_inorder(obs)), nrow = 1) +\n  geom_line() +\n  geom_point() +\n  labs(x = \"Proportion Water (p)\", y = \"Posterior Density\")\n\n\n\n\n\n\n\n\n\nCategories:\n\nprobability\nbayes-grid"
  },
  {
    "objectID": "posts/Rethink_2m6/Rethink_2m6.html",
    "href": "posts/Rethink_2m6/Rethink_2m6.html",
    "title": "Rethink_2m6",
    "section": "",
    "text": "Solution\nLet’s label the cards bb (black on both sides), bw (black on one, white on the other), and ww (both sides are white), respectively.\nWanted is the probability that the second side of the card is black (2b), given one side is black (1b): \\(Pr(2b|1b)\\).\n\nd <-\n  tibble::tribble(\n  ~Hyp, ~Prior,\n  \"bb\",     1L, \n  \"bw\",     2L,   \n  \"ww\",     3L, \n  ) %>% \n  mutate(Likelihood = c(2,1,0),\n         unstand_post = Prior*Likelihood,\n         std_post = unstand_post / sum(unstand_post))\n\nd %>% \n  gt() %>% \n  fmt_number(columns = 5)\n\n\n\n\n\n  \n  \n    \n      Hyp\n      Prior\n      Likelihood\n      unstand_post\n      std_post\n    \n  \n  \n    bb\n1\n2\n2\n0.50\n    bw\n2\n1\n2\n0.50\n    ww\n3\n0\n0\n0.00\n  \n  \n  \n\n\n\n\n\nCategories:\n\nprobability\nbayes"
  },
  {
    "objectID": "posts/ReThink3m3/ReThink3m3.html",
    "href": "posts/ReThink3m3/ReThink3m3.html",
    "title": "ReThink3m3",
    "section": "",
    "text": "Solution\n\nlibrary(tidyverse)\n\n\n\n\nErstellen wir zuerst wieder die Posteriori-Verteilung für den Globusversuch.\n\np_grid <- seq( from=0 , to=1 , length.out=1000 )  # Gitterwerte\n\nprior <- rep(1, 1000 )  # Priori-Gewichte\n\nlikelihood <- dbinom(8 , size= 15, prob=p_grid ) \n\nunstandardisierte_posterior <- likelihood * prior \n\nposterior <- unstandardisierte_posterior / sum(unstandardisierte_posterior)\n\nDann ziehen wir unsere Stichproben daraus:\n\n# um die Zufallszahlen festzulegen, damit alle die gleichen Zufallswerte bekommen: \nset.seed(42) \n\n# Stichproben ziehen aus der Posteriori-Verteilung\nsamples <- \n  tibble(\n    p = sample(p_grid , prob=posterior, size=1e4, replace=TRUE))\n\n\nPPV <- \n  samples %>% \n  mutate( anzahl_wasser = rbinom(1e4, size = 15, prob = p))\n\nDurch prob = p gewichten wir die Wahrscheinlichkeit an den Werten der Posteriori-Verteilung.\nSo sehen die ersten paar Zeilen von PPV aus:\n\n\n\n\n\n\n  \n  \n    \n      p\n      anzahl_wasser\n    \n  \n  \n    0.4304304\n4\n    0.5575576\n11\n    0.6516517\n4\n    0.6156156\n9\n    0.6716717\n6\n  \n  \n  \n\n\n\n\n\n\n\n\nPPV %>% \n  ggplot() +\n  aes(x = anzahl_wasser) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n\nPPV %>% \n  count(anzahl_wasser == 8)\n\n# A tibble: 2 × 2\n  `anzahl_wasser == 8`     n\n  <lgl>                <int>\n1 FALSE                 8536\n2 TRUE                  1464\n\n\nAlternativer R-Code:\n\nw <- rbinom(1e4, size = 15, prob = samples$p)\nmean(w == 8)\n\n[1] 0.1504\n\n\nQuelle\n\nCategories:\n\nbayes\nppv\nprobability"
  },
  {
    "objectID": "posts/griech-buchstaben/griech-buchstaben.html",
    "href": "posts/griech-buchstaben/griech-buchstaben.html",
    "title": "Griech-Buchstaben-Inferenz",
    "section": "",
    "text": "Solution\n\n\n\n\n\nKennwert\nStatistik\nParameter\n\n\n\n\nMittelwert\n\\[\\bar{X}\\]\n\\[\\mu\\]\n\n\nMittelwertsdifferenz\n\\[d=\\bar{X}_1-\\bar{X}_2\\]\n\\[\\mu_1\\]- \\[\\mu_2\\]\n\n\nStreuung\nsd\n\\[\\sigma\\]\n\n\nAnteil\np\n\\[\\pi\\]\n\n\nKorrelation\nr\n\\[\\rho\\]\n\n\nRegressionsgewicht\nb\n\\[\\beta\\]\n\n\n\n\n\n\nCategories:\n\nqm2\nqm2-thema01"
  },
  {
    "objectID": "posts/mtcars-simple2/mtcars-simple2.html",
    "href": "posts/mtcars-simple2/mtcars-simple2.html",
    "title": "mtcars-simple2",
    "section": "",
    "text": "Solution\nCompute Model:\n\nlm1_freq <- lm(mpg ~ hp + cyl + disp, data = mtcars)\n\nlibrary(rstanarm)\nlm1_bayes <- stan_glm(mpg ~ hp + cyl + disp, data = mtcars, refresh = 0)\n\nGet R2:\n\nlibrary(easystats)\n\n\nr2(lm1_freq)\n\n# R2 for Linear Regression\n       R2: 0.768\n  adj. R2: 0.743\n\n\n\nr2(lm1_bayes)\n\n# Bayesian R2 with Compatibility Interval\n\n  Conditional R2: 0.746 (95% CI [0.601, 0.859])\n\n\nThe coefficient is estimated as about 0.77.\n\nCategories:\n\nregresssion\nen\nbayes\nfrequentist\nqm1\nstats-nutshell"
  },
  {
    "objectID": "posts/Zwielichter-Dozent-Bayes/Zwielichter-Dozent-Bayes.html",
    "href": "posts/Zwielichter-Dozent-Bayes/Zwielichter-Dozent-Bayes.html",
    "title": "Zwielichter-Dozent-Bayes",
    "section": "",
    "text": "Exercise\nNach einem langen Unitag machen Sie sich auf den Weg nach Hause; ihr Weg führt Sie durch eine dunkle Ecke. Just dort regt sich auf einmal eine Gestalt in den Schatten. Die Person spricht Sie an: „Na, Lust auf ein Spielchen?“. Sie willigen sofort ein. Die Person stellt sich als ein Statistiker vor, dessen Namen nichts zur Sache tue; das Gesicht kommt Ihnen vage bekannt vor. „Pass auf“, erklärt der Statistiker, „wir werfen eine Münze, ich setze auf Zahl“. Dass er auf Zahl setzt, überrascht Sie nicht. „Wenn ich gewinne“, fährt der Statistiker fort, „bekomme ich 10 Euro von Dir, wenn Du gewinnst, bekommst Du 11 Euro von mir. Gutes Spiel, oder?“. Sie einigen sich auf 10 Durchgänge, in denen der Statistiker jedes Mal eine Münze wirft, fängt und dann die oben liegende Seite prüft. Erster Wurf: Zahl! Der Statistiker gewinnt. Pech für Sie. Zweiter Wurf: Zahl! Schon wieder 10 Euro für den Statistiker. Hm. Dritter Wurf: . . . Zahl! Schon wieder. Aber kann ja passieren, bei einer fairen Münze, oder? Vierter Wurf: Zahl! Langsam regen sich Zweifel bei Ihnen. Kann das noch mit rechten Dingen zugehen? Ist die Münze fair? Insgesamt gewinnt der zwielichte Statistiker 8 von 10 Durchgängen.\nUnter leisem Gelächter des Statistikers (und mit leeren Taschen) machen Sie sich von dannen. Hat er falsch gespielt? Wie plausibel ist es, bei 10 Würfen 8 Treffer zu erhalten, wenn die Münze fair ist? Ist das ein häufiges, ein typisches Ereignis oder ein seltenes, untypisches Ereignis bei einer fairen Münze? Wenn es ein einigermaßen häufiges Ereignis sein sollte, dann spricht das für die Fairness der Münze. Zumindest spricht ein Ereignis, welches von einer Hypothese als häufig vorausgesagt wird und schließlich eintritt, nicht gegen eine Hypothese. Zuhause angekommen, denken Sie sich, jetzt müssen Sie erstmal in Ruhe die Posteriori-Verteilung und die PPV ausrechnen!\n\nBerechnen Sie die Posteriori-Verteilung mit der Gittermethode! Gehen Sie von einer gleichverteilten Priori-Wahrscheinlichkeit aus. Visualisieren Sie sie. Alle folgenden Teil-Fragen bauen auf der Post-Verteilung auf.\nWie groß ist die Wahrscheinlichkeit, auf Basis der Post-Verteilung, dass die Münze zugunsten des Dozenten gezinkt ist?\nGeben Sie das 50%-PI und 50%-HDPI zum Parameterwert (\\(p\\) der Münze) an!\nMit welcher Wahrscheinlichkeit liegt die Trefferchance der Münze zwischen \\(p=.45\\) und \\(p=.55\\), ist also nicht “nennenswert” gezinkt?\nWas ist der wahrscheinlichste Parameterwert (Trefferchance der Münze)?\nGeben Sie das 90%-PI und 90%-HDI zu Parameterwert (\\(p\\) der Münze) an!\nBerechnen Sie die PPV! Visualisieren Sie sie. Interpretieren Sie die PPV.\nDiskutieren Sie die Annahme einer Gleichverteilung des Priori-Wertes von \\(p\\)!\n\n         \n\n\nSolution\n\nBerechnen Sie die Posteriori-Verteilung mit der Gittermethode! Visualisieren Sie sie. Alle folgenden Teil-Fragen bauen auf der Post-Verteilung auf.\n\n\np_grid <- seq( from=0 , to=1 , length.out=1000 )  # Gitterwerte\n\nprior <- rep( 1 , 1000 )  # Priori-Gewichte\n\nlikelihood <- dbinom(8, size = 10, prob=p_grid) \n\nunstandardisierte_posterior <- likelihood * prior \n\nposterior <- unstandardisierte_posterior / sum(unstandardisierte_posterior)\n\n# Stichproben ziehen aus der Posteriori-Verteilung:\nsamples <- \n  tibble(\n    gewinnchance_muenze = sample(p_grid , prob=posterior, size=1e4, replace=TRUE))\n\nVisualisierung:\n\nsamples %>% \n  ggplot() +\n  aes(x = gewinnchance_muenze) +\n  geom_histogram() +\n  labs(title = \"Posterior-Verteilung\",\n       x = \"Gewinnchance der Münze (50%: faire Münze)\")\n\n\n\n\n\n\n\n\n\nWie groß ist die Wahrscheinlichkeit, auf Basis der Post-Verteilung, dass die Münze zugunsten des Dozenten gezinkt ist?\n\n\nsamples %>% \n  count(gewinnchance_muenze > .5) %>% \n  mutate(prop = n / sum(n))\n\n# A tibble: 2 × 3\n  `gewinnchance_muenze > 0.5`     n   prop\n  <lgl>                       <int>  <dbl>\n1 FALSE                         322 0.0322\n2 TRUE                         9678 0.968 \n\n\n\nGeben Sie das 50%-PI (Perzentilintervall) und 50%-HDI zum Parameterwert (\\(p\\) der Münze) an!\n\n\nlibrary(easystats)\neti(samples, ci = .5)\n\nEqual-Tailed Interval\n\nParameter           |      50% ETI\n----------------------------------\ngewinnchance_muenze | [0.67, 0.84]\n\nhdi(samples, ci = .5)\n\nHighest Density Interval\n\nParameter           |      50% HDI\n----------------------------------\ngewinnchance_muenze | [0.72, 0.88]\n\n\nEin PI wird auch equal tail interval genannt, weil die beiden “abgeschnitten Randbereiche” links und rechts die gleichen Flächenanteil (Wahrscheinlichkeitsmasse) aufweisen.\nInteresant ist, dass das PI und das HDI zu unterschiedlichen Ergebnissen kommen. Das lässt auf eine schiefe Verteilung schließen. Außerdem eröffnet es den Raum zur Diskussion, welches Intervall man berichtet. Um diese Frage besser zu verstehen, können wir die Intervalle visualisieren.\nBonus: Visualisieren wir die Intervalle:\nPI:\n\neti(samples, ci = .5) %>% plot()\n\n\n\n\n\n\n\n\nHDI:\n\nhdi(samples, ci = .5) %>% plot()\n\n\n\n\n\n\n\n\nDas HDI ist schmäler und liegt näher am Modus. Vermutlich ist das HDI zu bevorzugen.\n\nMit welcher Wahrscheinlichkeit liegt die Trefferchance der Münze zwischen \\(p=.45\\) und \\(p=.55\\), ist also nicht “nennenswert” gezinkt (auf Basis unserer Modellannahmen)?\n\n\nsamples %>% \n  count(gewinnchance_muenze >= 0.45 & gewinnchance_muenze <= .55) %>% \n  mutate(prop = n/sum(n))\n\n# A tibble: 2 × 3\n  `gewinnchance_muenze >= 0.45 & gewinnchance_muenze <= 0.55`     n   prop\n  <lgl>                                                       <int>  <dbl>\n1 FALSE                                                        9534 0.953 \n2 TRUE                                                          466 0.0466\n\n\nDie Wahrscheinlichkeit, dass die Münze nicht nennenswert gezinkt ist (nach unserer Definition), ist gering. Man sollte vielleicht erwähnen, dass unsere Definition von “nicht nennenswert gezinkt” plausibel ist, und andere (vernünftige) Definitionen zu einem sehr ähnlichen Ergebnis kämen.\n\nWas ist der wahrscheinlichste Parameterwert (Trefferchance der Münze)?\n\n\nsamples %>% \n   map_estimate()\n\nMAP Estimate\n\nParameter           | MAP_Estimate\n----------------------------------\ngewinnchance_muenze |         0.78\n\n\nmap_estimate steht für …\n\nFind the Highest Maximum A Posteriori probability estimate (MAP) of a posterior, i.e., the value associated with the highest probability density (the “peak” of the posterior distribution). In other words, it is an estimation of the mode for continuous parameters.\n\n(aus der Hilfeseite der Funktion)\n\nGeben Sie das 90%-PI und 90%-HDI zum Parameterwert (\\(p\\) der Münze) an!\n\n\nlibrary(easystats)\neti(samples, ci = .9)\n\nEqual-Tailed Interval\n\nParameter           |      90% ETI\n----------------------------------\ngewinnchance_muenze | [0.53, 0.92]\n\nhdi(samples, ci = .9)\n\nHighest Density Interval\n\nParameter           |      90% HDI\n----------------------------------\ngewinnchance_muenze | [0.56, 0.94]\n\n\n\nBerechnen Sie die PPV! Visualisieren Sie sie. Interpretieren Sie die PPV.\n\n\nPPV <-\n  samples %>% \n  mutate(anzahl_kopf = rbinom(n = 1e4, size = 10, prob = gewinnchance_muenze))\n\nVisualisierung:\n\nPPV %>% \n  ggplot() +\n  aes(x = anzahl_kopf) +\n  labs(title = \"PPV\") +\n  geom_bar()  # geom_bar() ginge auch, sieht aber bei wenig Balken nicht so gut aus.\n\n\n\n\n\n\n\n\nLaut der PPV sind 8 von 10 Treffern der Wert, der mit der höchsten Wahrscheinlichkeit zu beobachten sein wird. Allerdings sind 7 oder 9 Treffer fast genauso wahrscheinlich. Etwas genauer:\n\nPPV %>% \n  count(between(anzahl_kopf, 7,9))   # \"zähle mir, wie oft ein Wert ZWISCHEN (between) 7 und 9 vorkommt\"\n\n# A tibble: 2 × 2\n  `between(anzahl_kopf, 7, 9)`     n\n  <lgl>                        <int>\n1 FALSE                         3815\n2 TRUE                          6185\n\n\nMit dieser Wahrscheinlichkeit ist ein Wert zwischen 7 und 9 zu beobachten, wenn man den Versuch wiederholt, laut dem Modell.\n\nPPV %>% \n  eti(anzahl_kopf, ci = .9)\n\nEqual-Tailed Interval\n\nParameter           |       90% ETI\n-----------------------------------\ngewinnchance_muenze | [0.53,  0.92]\nanzahl_kopf         | [4.00, 10.00]\n\n\nUnser Modell sieht einen “Passungsbereich” (ein Perzentilintervall) von 4 bis 10 Treffern als mit 90% Wahrscheinlichkeit passend an.\n\nDiskutieren Sie die Annahme einer Gleichverteilung des Priori-Wertes von \\(p\\)!\n\nZwar hat eine Gleichverteilung der Priori-Werte den Vorteil, dass sie “objektiv” ist in dem Sinne, dass kein Wert “bevorteilt” wird; alle gelten als gleich wahrscheinlich. Aber das ist hochgradig unplausibel: So ist z.B. der Wert \\(p=1\\) logisch unmöglich, da wir nicht nur Treffer beobachtet haben. Ein Wert von z.B. \\(p=0.999\\) erscheint uns ebenfalls sehr unwahrscheinlich. Nützlicher erscheint daher vielleicht doch eine Priori-Verteilung, die extreme Werte von \\(p\\) als unwahrscheinlich bemisst.\n\nCategories:\n\nbayes\nprobability\nppv"
  },
  {
    "objectID": "posts/purrr-map05/purrr-map05.html",
    "href": "posts/purrr-map05/purrr-map05.html",
    "title": "purrr-map05",
    "section": "",
    "text": "Exercise\nErstellen Sie eine Tabelle mit mit folgenden Spalten:\n\nID-Spalte: \\(1,2,..., 10\\)\nEine Spalte, in der jede Zelle eine Tabelle mit einem Vektor \\(x\\), einer standardnormalverteilten Zufallszahlen (n=1000), enthält\n\nBerechnen Sie den Mittelwert von jedem \\(x\\)! Diese Ergebnisse sollen als weitere Spalte der Tabelle hinzugefügt werden.\n         \n\n\nSolution\n\nd <- tibble(\n  id = 1:10) %>% \n  mutate(x = map(id, ~ rnorm(n = 1e3))\n) \n\nstr(d)\n\ntibble [10 × 2] (S3: tbl_df/tbl/data.frame)\n $ id: int [1:10] 1 2 3 4 5 6 7 8 9 10\n $ x :List of 10\n  ..$ : num [1:1000] 0.559 -0.588 -0.452 -1.966 1.329 ...\n  ..$ : num [1:1000] -0.54581 0.89293 -0.66738 1.26568 -0.00726 ...\n  ..$ : num [1:1000] -1.527 0.934 -0.289 0.508 1.108 ...\n  ..$ : num [1:1000] 0.0425 0.7778 -0.7187 -1.3656 -2.5356 ...\n  ..$ : num [1:1000] 1.323 -0.453 0.628 1.592 0.187 ...\n  ..$ : num [1:1000] -0.751 -0.482 0.439 1.804 0.971 ...\n  ..$ : num [1:1000] -0.268 0.182 1.62 0.372 -0.261 ...\n  ..$ : num [1:1000] 0.0568 0.8057 0.1397 0.907 0.4752 ...\n  ..$ : num [1:1000] 0.831 -0.743 -0.402 1.098 -0.905 ...\n  ..$ : num [1:1000] 0.987 -0.225 1.544 -1.503 -0.502 ...\n\n\nSo kann man sich die Mittelwerte ausgeben lassen:\n\nd$x %>% \n  map(mean)\n\n[[1]]\n[1] -0.03513212\n\n[[2]]\n[1] 0.01395107\n\n[[3]]\n[1] 0.01891006\n\n[[4]]\n[1] -0.02203389\n\n[[5]]\n[1] 0.05744261\n\n[[6]]\n[1] 0.004502295\n\n[[7]]\n[1] -0.00387942\n\n[[8]]\n[1] -0.01771766\n\n[[9]]\n[1] -0.06057087\n\n[[10]]\n[1] 0.01811559\n\n\nJetzt fügen wir den letzten Schritt als Spalte hinzu:\n\nd2 <-\n  d %>% \n  mutate(x_mean = map_dbl(x, ~ mean(.x))) \n\nhead(d2)\n\n# A tibble: 6 × 3\n     id x               x_mean\n  <int> <list>           <dbl>\n1     1 <dbl [1,000]> -0.0351 \n2     2 <dbl [1,000]>  0.0140 \n3     3 <dbl [1,000]>  0.0189 \n4     4 <dbl [1,000]> -0.0220 \n5     5 <dbl [1,000]>  0.0574 \n6     6 <dbl [1,000]>  0.00450\n\n\nHier hätten wir auch schreiben können:\n\nd %>% \n  mutate(x_mean = map(x, mean)) %>% \n  unnest(x_mean) %>% \n  head()\n\n# A tibble: 6 × 3\n     id x               x_mean\n  <int> <list>           <dbl>\n1     1 <dbl [1,000]> -0.0351 \n2     2 <dbl [1,000]>  0.0140 \n3     3 <dbl [1,000]>  0.0189 \n4     4 <dbl [1,000]> -0.0220 \n5     5 <dbl [1,000]>  0.0574 \n6     6 <dbl [1,000]>  0.00450\n\n\n\nCategories:\n~"
  },
  {
    "objectID": "posts/purrr-map02/purrr-map02.html",
    "href": "posts/purrr-map02/purrr-map02.html",
    "title": "purrr-map02",
    "section": "",
    "text": "Exercise\nBestimmen Sie die häufigsten Worte im Grundatzprogramm der Partei AfD (in der aktuellsten Version).\n         \n\n\nSolution\nText aus PDF-Dateien kann man mit dem Paket pdftools einlesen:\n\nlibrary(pdftools)\n\nUsing poppler version 22.02.0\n\nd_path <- \"/Users/sebastiansaueruser/Google Drive/Literatur/_Div/Politik/afd-grundsatzprogramm-2022.pdf\"\n\nd <- tibble(text = pdf_text(d_path))\n\nDann erstellen wir eine Tidy-Version und tokenisieren nach Wörtern:\n\nlibrary(tidytext)\nd2 <-\n  d %>% \n  unnest_tokens(output = word, input = text)\n\nhead(d2)\n\n# A tibble: 6 × 1\n  word             \n  <chr>            \n1 programm         \n2 für              \n3 deutschland      \n4 das              \n5 grundsatzprogramm\n6 der              \n\n\nDann zählen wir die Wörter:\n\nd2 %>% \n  count(word, sort = TRUE) %>% \n  head(20)\n\n# A tibble: 20 × 2\n   word            n\n   <chr>       <int>\n 1 die          1151\n 2 und          1147\n 3 der           870\n 4 zu            435\n 5 für           392\n 6 in            392\n 7 den           271\n 8 von           257\n 9 ist           251\n10 das           225\n11 werden        214\n12 eine          211\n13 nicht         196\n14 ein           191\n15 deutschland   190\n16 sind          187\n17 wir           176\n18 afd           171\n19 des           169\n20 sich          158\n\n\n\nCategories:\n\nr\nmap\ntidyverse"
  },
  {
    "objectID": "posts/twitter03/twitter03.html",
    "href": "posts/twitter03/twitter03.html",
    "title": "twitter03",
    "section": "",
    "text": "Solution\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(rtweet)\n\n\nAttaching package: 'rtweet'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\n\nEinloggen bei Twitter; zuerst die Credentials bereithalten:\n\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\n\nDann anmelden:\n\nauth <- rtweet_app(bearer_token = Bearer_Token)\n\nTweets an Karl Lauterbach suchen:\n\nkarl1 <- search_tweets(\"@karl_lauterbach min_faves:100 OR min_retweets:100\", n = 10)\n\n\nkarl1 %>% \n  select(retweet_count, favorite_count)\n\n# A tibble: 10 × 2\n   retweet_count favorite_count\n           <int>          <int>\n 1            56            210\n 2            56            229\n 3            44           1626\n 4            60            225\n 5            30            494\n 6             5            148\n 7            27            435\n 8            12            178\n 9            13            162\n10            46            375\n\nCategories:\n\ntextmining\ntwitter"
  },
  {
    "objectID": "posts/twitter04/twitter04.html",
    "href": "posts/twitter04/twitter04.html",
    "title": "twitter04",
    "section": "",
    "text": "Laden Sie \\(n=10^k\\) Tweets von Twitter herunter (mit \\(k=2\\)) via der Twitter API; Suchterm soll sein “@karl_lauterbach”. Bereiten Sie die Textdaten mit grundlegenden Methoden des Textminings auf (Tokenisieren, Stopwörter entfernen, Zahlen entfernen, …). Berichten Sie dann die 10 häufigsten Wörter als Schätzer für die Dinge, die an Karl Lauterbach getweetet werden."
  },
  {
    "objectID": "posts/twitter04/twitter04.html#lösung",
    "href": "posts/twitter04/twitter04.html#lösung",
    "title": "twitter04",
    "section": "Lösung",
    "text": "Lösung\n\nlibrary(rtweet)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()  masks stats::filter()\n✖ purrr::flatten() masks rtweet::flatten()\n✖ dplyr::lag()     masks stats::lag()\n\nlibrary(tidytext)\nlibrary(lsa)  # Stopwörter\n\nLoading required package: SnowballC\n\nlibrary(SnowballC)  # Stemming\n\n\nsource(\"/home/sebastian/Documents/credentials/hate-speech2-twitter.R\")\n\n\nauth <- rtweet_bot(api_key = api_key,\n                   api_secret = api_secret,\n                   access_token = access_token,\n                   access_secret = access_secret)\n\n\nkarl1 <- search_tweets(\"@karl_lauterbach\", n = 1e2, include_rts = FALSE)\n#write_rds(karl1, file = \"karl1.rds\", compress = \"gz\")\n\n\n\n\n\nkarl2 <- \n  karl1 %>% \n  select(full_text)\n\n\nkarl3 <- \n  karl2 %>% \n  unnest_tokens(output = word, input = full_text)\n\n\nkarl4 <- \nkarl3 %>% \n  anti_join(tibble(word = lsa::stopwords_de)) \n\nJoining, by = \"word\"\n\n\n\nkarl5 <- \n  karl4 %>% \n  mutate(word = str_replace_na(word, \"^[:digit:]+$\")) %>% \n  mutate(word = str_replace_na(word, \"hptts?://\\\\w+\")) %>% \n  mutate(word = str_replace_na(word, \" +\")) %>% \n  drop_na()\n\n\nkarl6 <-\n  karl5 %>% \n  mutate(word = wordStem(word))\n\n\nkarl6 %>% \n  count(word, sort = TRUE) %>% \n  slice_head(n=10)\n\n# A tibble: 10 × 2\n   word                       n\n   <chr>                  <int>\n 1 karl_lauterbach          100\n 2 rt                        60\n 3 ultrakaerl                19\n 4 corona                    16\n 5 wirwollenmaskenpflicht    16\n 6 länder                    12\n 7 gesundheitsminist         11\n 8 polarstern64              11\n 9 schon                     11\n10 shomburg                  11\n\n\n\nCategories:\n~"
  },
  {
    "objectID": "posts/mtcars-simple3/mtcars-simple3.html",
    "href": "posts/mtcars-simple3/mtcars-simple3.html",
    "title": "mtcars-simple3",
    "section": "",
    "text": "We will use the dataset mtcars in this exercise.\nAssume your causal model of your research dictates that fuel economy is a linear function of horse power, cylinder count and displacement of the engine.\nWhich of the predictors in the above model has the weakest causal impact on the output variable?\nNotes:\n\nUse can either use frequentist or bayesian modeling.\nUse R for all computations.\nThere are multiple ways to find a solution.\n\n\n\n\ncyl\nhp\ndisp\nAll are equally strong\nnone of the above"
  },
  {
    "objectID": "posts/mtcars-simple3/mtcars-simple3.html#answerlist-1",
    "href": "posts/mtcars-simple3/mtcars-simple3.html#answerlist-1",
    "title": "mtcars-simple3",
    "section": "Answerlist",
    "text": "Answerlist\n\nwrong\ncorrect\nwrong\nwrong\nwrong\n\n\nCategories:\n\nregresssion\nen\nbayes\nfrequentist\nqm1\nstats-nutshell"
  },
  {
    "objectID": "posts/ttest-als-regr/ttest-als-regr.html",
    "href": "posts/ttest-als-regr/ttest-als-regr.html",
    "title": "ttest-als-regr",
    "section": "",
    "text": "Der t-Test kann als Spezialfall der Regressionsanalyse gedeutet werden.\nHierbei ist es wichtig, sich das Skalenniveau der Variablen, die ein t-Test verarbeitet, vor Augen zu führen.\nHinweisse:\n\nDie folgende Abbildung gibt Tipps.\nInformationen, die zur Lösung einer Aufgabe nicht nötig sind, sollte man ignorieren.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBenennen Sie die Skalenniveaus der UV eines t-Tests! Geben Sie nur ein Wort ein. Verwenden Sie nur Kleinbuchstaben (z.B. regression).\nBenennen Sie die Skalenniveaus der AV eines t-Tests! Geben Sie nur ein Wort ein. Verwenden Sie nur Kleinbuchstaben (z.B. regression).\nNennen Sie eine beispielhafte Forschungsfrage für einen t-Test.\nSkizzieren Sie ein Diagramm einer Regression, die analytisch identisch (oder sehr ähnlich) zu einem t-Test ist!"
  },
  {
    "objectID": "posts/ttest-als-regr/ttest-als-regr.html#answerlist-1",
    "href": "posts/ttest-als-regr/ttest-als-regr.html#answerlist-1",
    "title": "ttest-als-regr",
    "section": "Answerlist",
    "text": "Answerlist\n\nUV: binär\nAV: metrisch\nUnterscheiden sich die mittleren Einparkzeiten von Frauen und Männern?\nAus dem Datensatz mtcars:\n\n\ndata(mtcars)\nmtcars %>% \n  ggplot() +\n  aes(x = am, y = mpg) +\n  geom_point(alpha = .5) +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nCategories:\n\nregr\nttest\nvariable-levels"
  },
  {
    "objectID": "posts/log-y-regr1/log-y-regr1.html",
    "href": "posts/log-y-regr1/log-y-regr1.html",
    "title": "log-y-regr1",
    "section": "",
    "text": "Solution\n\nd2 <-\n  d %>% \n  filter(re74 > 0) %>% \n  mutate(re74_log = log(re74))\n\n\nm <- lm(re74_log ~ educ, data = d2)\n\nHier sind die parameters des Modells.\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(2327)\np\n\n\n\n\n(Intercept)\n8.83\n0.06\n(8.70, 8.95)\n142.91\n< .001\n\n\neduc\n0.07\n4.94e-03\n(0.07, 0.08)\n15.16\n< .001\n\n\n\n\nFür jedes Jahr Bildung steigt das Einkommen also ca. um den Faktor 1.07.\nEtwas genauer:\n\\(\\hat{\\beta_1} = 0.07\\) bedeutet, dass ein Jahr Bildung zu einen erwarteten Unterschied im Einkommen in Höhe von 0.07 in Log-Einkommen führt. Anders gesagt wird das Einkommen um exp(0.07) erhöht. Dabei gilt \\(e^{0.07} \\approx 1.07\\):\n\nexp(0.07)\n\n[1] 1.072508\n\n\nDie Lösung lautet also: “Pro Jahr Bildung steigt das Einkommen - laut Modell um den Faktor ca. 1.07”.\nMan darf dabei nicht vergessen, dass wir wir uns hier auf die Schnelle ein Modell ausgedacht haben. Ob es in Wirklichkeit so ist, wie unser Modell meint, ist eine andere Sache!\n\nCategories:\n\nregression\nlm\nqm2\nstats-nutshell"
  },
  {
    "objectID": "posts/subjektiv-Bayes/subjektiv-Bayes.html",
    "href": "posts/subjektiv-Bayes/subjektiv-Bayes.html",
    "title": "subjektiv-Bayes",
    "section": "",
    "text": "Solution\n\nLinearitätsannahme in (linearen) Modellen\nWahl des Likelihoods\nWahl der Daten\nMethoden der Modellprüfung\nGeneralisierung des Modells auf andere Situationen\nWahl der Prädiktoren\n\n\nCategories:\n~"
  },
  {
    "objectID": "posts/adjustieren2/adjustieren2.html",
    "href": "posts/adjustieren2/adjustieren2.html",
    "title": "adjustieren2",
    "section": "",
    "text": "Solution\n\n\n\n\nUnser Modell lm1 schätzt den Preis eines Diamanten mittlerer Größe auf etwa 3932.5 (was immer auch die Einheiten sind, Dollar vermutlich).\nprice ~ carat_z + cut\n\nDieses zweite Modell könnten wir so berechnen:\n\nlm2 <- stan_glm(price ~ carat_z + cut, data = diamonds,\n                chains = 1,\n                refresh = 0)\nparameters(lm2)\n\nParameter    |  Median |             95% CI |   pd | % in ROPE |  Rhat |     ESS |                       Prior\n--------------------------------------------------------------------------------------------------------------\n(Intercept)  | 2406.07 | [2334.35, 2488.89] | 100% |        0% | 1.000 |  321.00 | Normal (3932.80 +- 9973.60)\ncarat_z      | 7870.55 | [7843.93, 7897.58] | 100% |        0% | 1.005 | 1047.00 |   Normal (0.00 +- 21040.85)\ncutGood      | 1118.79 | [1027.17, 1203.09] | 100% |        0% | 0.999 |  434.00 |   Normal (0.00 +- 34685.38)\ncutIdeal     | 1799.71 | [1714.31, 1869.75] | 100% |        0% | 0.999 |  338.00 |   Normal (0.00 +- 20362.28)\ncutPremium   | 1437.68 | [1353.65, 1512.30] | 100% |        0% | 0.999 |  351.00 |   Normal (0.00 +- 22862.49)\ncutVery Good | 1508.84 | [1422.69, 1582.24] | 100% |        0% | 1.000 |  344.00 |   Normal (0.00 +- 23922.15)\n\n\nEin “normales” (frequentistisches) lm käme zu ähnlichen Ergebnissen:\n\nlm(price ~ carat_z + cut, data = diamonds)\n\n\nCall:\nlm(formula = price ~ carat_z + cut, data = diamonds)\n\nCoefficients:\n (Intercept)       carat_z       cutGood      cutIdeal    cutPremium  \n        2405          7871          1120          1801          1439  \ncutVery Good  \n        1510  \n\n\nMan könnte hier noch einen Interaktionseffekt ergänzen, wenn man Grund zur Annahme hat, dass es einen gibt.\n\nCategories:\n\nqm2\n‘2022’"
  },
  {
    "objectID": "posts/Griech-Buchstaben-Inferenz/Griech-Buchstaben-Inferenz.html",
    "href": "posts/Griech-Buchstaben-Inferenz/Griech-Buchstaben-Inferenz.html",
    "title": "Griech-Buchstaben-Inferenz",
    "section": "",
    "text": "Solution\n\n\n\n\n\nKennwert\nStatistik\nParameter\n\n\n\n\nMittelwert\n\\[\\bar{X}\\]\n\\[\\mu\\]\n\n\nMittelwertsdifferenz\n\\[d=\\bar{X}_1-\\bar{X}_2\\]\n\\[\\mu_1\\]- \\[\\mu_2\\]\n\n\nStreuung\nsd\n\\[\\sigma\\]\n\n\nAnteil\np\n\\[\\pi\\]\n\n\nKorrelation\nr\n\\[\\rho\\]\n\n\nRegressionsgewicht\nb\n\\[\\beta\\]\n\n\n\n\n\n\nCategories:\n\nqm2\nqm2-thema01"
  },
  {
    "objectID": "posts/twitter05/twitter05.html",
    "href": "posts/twitter05/twitter05.html",
    "title": "twitter05",
    "section": "",
    "text": "Laden Sie \\(n=10^k\\) Tweets von Twitter herunter (mit \\(k=2\\)) via der Twitter API; Suchterm soll sein “@karl_lauterbach”. Bereiten Sie die Textdaten mit grundlegenden Methoden des Textminings auf (Tokenisieren, Stopwörter entfernen, Zahlen entfernen, …).\nNutzen Sie die Daten, um eine Sentimentanalyse zu erstellen."
  },
  {
    "objectID": "posts/twitter05/twitter05.html#lösung",
    "href": "posts/twitter05/twitter05.html#lösung",
    "title": "twitter05",
    "section": "Lösung",
    "text": "Lösung\nZuerst muss man sich anmelden und die Tweets herunterladen; dieser Teil ist hier nicht aufgeführt (s. andere Aufgaben).\n\nlibrary(rtweet)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()  masks stats::filter()\n✖ purrr::flatten() masks rtweet::flatten()\n✖ dplyr::lag()     masks stats::lag()\n\nlibrary(tidytext)\nlibrary(lsa)  # Stopwörter\n\nLoading required package: SnowballC\n\nlibrary(SnowballC)  # Stemming\n\n\n\n\n\nkarl2 <- \n  karl1 %>% \n  select(full_text)\n\n\nkarl3 <- \n  karl2 %>% \n  unnest_tokens(output = word, input = full_text)\n\n\nkarl4 <- \nkarl3 %>% \n  anti_join(tibble(word = lsa::stopwords_de)) \n\nJoining, by = \"word\"\n\n\n\nkarl5 <- \n  karl4 %>% \n  mutate(word = str_replace_na(word, \"^[:digit:]+$\")) %>% \n  mutate(word = str_replace_na(word, \"hptts?://\\\\w+\")) %>% \n  mutate(word = str_replace_na(word, \" +\")) %>% \n  drop_na()\n\n\ndata(sentiws, package = \"pradadata\")\n\n\nkarl7 <-\n  karl5 %>% \n  inner_join(sentiws)\n\nJoining, by = \"word\"\n\n\n\nkarl7 %>% \n  group_by(neg_pos) %>% \n  summarise(senti_avg = mean(value, na.rm = TRUE),\n            senti_sd = sd(value, na.rm = TRUE),\n            senti_n = n())\n\n# A tibble: 2 × 4\n  neg_pos senti_avg senti_sd senti_n\n  <chr>       <dbl>    <dbl>   <int>\n1 neg        -0.300    0.198      11\n2 pos         0.140    0.203      28\n\n\nAchtung, Sentimentanalyse sollte vor dem Stemming kommen.\n\nCategories:\n~"
  },
  {
    "objectID": "posts/twitter02/twitter02.html",
    "href": "posts/twitter02/twitter02.html",
    "title": "twitter02",
    "section": "",
    "text": "Solution\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(rtweet)\n\n\nAttaching package: 'rtweet'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\n\nEinloggen bei Twitter; zuerst die Credentials bereithalten:\n\nsource(\"/Users/sebastiansaueruser/credentials/hate-speech-analysis-v01-twitter.R\")\n\n\nauth <- rtweet_app(bearer_token = Bearer_Token)\n\nAus der Hilfe zu search_tweets:\nDescription\nReturns Twitter statuses matching a user provided search query. ONLY RETURNS DATA FROM THE PAST 6-9 DAYS.\nTweets an Karl Lauterbach suchen:\n\nkarl_tweets <- search_tweets(q = \"@karl_lauterbach\", n = 150000, retryonratelimit = TRUE)\n\nWir könnten n auch auf Inf setzen, aber, da wir auf das Refreshen des Rate Limits warten müssen, könnte sehr lange dauern. Daher nehmen wir hier nur einen kürzeren Wert.\n\ndim(karl_tweets)\n\n[1] 18000    43\n\nhead(karl_tweets)\n\n# A tibble: 6 × 43\n  created_at               id id_str      full_…¹ trunc…² displ…³ entities     metad…⁴\n  <dttm>                <dbl> <chr>       <chr>   <lgl>     <dbl> <list>       <list> \n1 2022-10-23 13:30:18 1.58e18 1584145185… \"Bei ⁦@… FALSE       122 <named list> <df>   \n2 2022-10-22 18:34:37 1.58e18 1583859379… \"Es is… FALSE       263 <named list> <df>   \n3 2022-10-22 17:56:39 1.58e18 1583849826… \"Die S… FALSE       215 <named list> <df>   \n4 2022-10-24 08:10:35 1.58e18 1584427113… \"Zu we… FALSE       219 <named list> <df>   \n5 2022-10-24 08:10:35 1.58e18 1584427113… \"RT @K… FALSE       140 <named list> <df>   \n6 2022-10-24 08:10:25 1.58e18 1584427072… \"RT @U… FALSE       139 <named list> <df>   \n# … with 35 more variables: source <chr>, in_reply_to_status_id <dbl>,\n#   in_reply_to_status_id_str <chr>, in_reply_to_user_id <dbl>,\n#   in_reply_to_user_id_str <chr>, in_reply_to_screen_name <chr>, geo <list>,\n#   coordinates <list>, place <list>, contributors <lgl>, is_quote_status <lgl>,\n#   retweet_count <int>, favorite_count <int>, favorited <lgl>, retweeted <lgl>,\n#   possibly_sensitive <lgl>, lang <chr>, quoted_status_id <dbl>,\n#   quoted_status_id_str <chr>, quoted_status <list>, retweeted_status <list>, …\n# ℹ Use `colnames()` to see all variable names\n\nCategories:\n\ntextmining\ntwitter"
  },
  {
    "objectID": "posts/ReThink4e1/ReThink4e1.html",
    "href": "posts/ReThink4e1/ReThink4e1.html",
    "title": "ReThink4e1",
    "section": "",
    "text": "Welche der folgenden Zeilen zeigt den Likelihood?\n\n\n\n\\(\\mu \\sim \\mathcal{N}(0, 10)\\)\n\\(\\sigma \\sim \\mathcal{U}(0, 1)\\)\n\\(y_i = \\beta_0 + \\beta_1\\cdot x\\)\n\\(y_i \\sim \\mathcal{N}(\\mu, \\sigma)\\)"
  },
  {
    "objectID": "posts/ReThink4e1/ReThink4e1.html#answerlist-1",
    "href": "posts/ReThink4e1/ReThink4e1.html#answerlist-1",
    "title": "ReThink4e1",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch. Priori-Verteilung.\nFalsch. Priori-Verteilung.\nFalsch. Regressionsformel.\nWahr. Likelihood.\n\nMan könnte den Likelihood auch so schreiben:\n$y_i| , (, ) $,\nwas noch deutlicher macht, dass die Likelihood die Wahrscheinlichkeit der Daten (y) ausdrückt, gegeben der Modellparameter (\\(\\mu, \\sigma)\\).\n\nCategories:\n\nprobability\nbayes"
  },
  {
    "objectID": "posts/purrr-map03/purrr-map03.html",
    "href": "posts/purrr-map03/purrr-map03.html",
    "title": "purrr-map03",
    "section": "",
    "text": "Exercise\nImportieren Sie das Grundatzprogramm der Partei AfD (in der aktuellsten Version). Tokenisieren Sie nach Sätzen. Dann entfernen Sie alle Zahlen. Dann zählen Sie die Anzahl der Wörter pro Satz und berichten gängige deskriptive Statistiken dazu.\n         \n\n\nSolution\nText aus PDF-Dateien kann man mit dem Paket pdftools einlesen:\n\nlibrary(pdftools)\n\nUsing poppler version 22.02.0\n\nd_path <- \"/Users/sebastiansaueruser/Google Drive/Literatur/_Div/Politik/afd-grundsatzprogramm-2022.pdf\"\n\nd <- tibble(text = pdf_text(d_path))\n\nDann erstellen wir eine Tidy-Version und tokenisieren nach Sätzen:\n\nlibrary(tidytext)\nd2 <-\n  d %>% \n  unnest_sentences(output = word, input = text)\n\nhead(d2)\n\n# A tibble: 6 × 1\n  word                                                                          \n  <chr>                                                                         \n1 programm für deutschland.                                                     \n2 das grundsatzprogramm der alternative für deutschland.                        \n3 2   programm für deutschland | inhalt         präambel                       …\n4 familien stärken        43             und parteiferne rechnungshöfe         …\n5 3   programm für deutschland | inhalt         7 | kultur, sprache und identit…\n6 förder- und                         10.10.3 deutsche literatur im inland digi…\n\n\nDann entfernen wir die Zahlen:\n\nd3 <- \n  d2 %>% \n  mutate(word = str_remove_all(word, pattern = \"[:digit:]+\"))\n\nPrüfen wir, ob es geklappt hat:\n\nd2$word[10]\n\n[1] \"weniger subventionen    88      13.7 fischerei, forst und jagd: im einklang mit der natur     88      13.8 flächenkonkurrenz:           nicht zu lasten der land- und forstwirtschaft            88\"\n\nd3$word[10]\n\n[1] \"weniger subventionen          . fischerei, forst und jagd: im einklang mit der natur           . flächenkonkurrenz:           nicht zu lasten der land- und forstwirtschaft            \"\n\n\nOk.\nDann zählen wir die Wörter pro Satz:\n\nd4 <- \n  d3 %>% \n  summarise(word_count_per_sentence = str_count(word, \"\\\\w+\"))\n\nhead(d4)\n\n# A tibble: 6 × 1\n  word_count_per_sentence\n                    <int>\n1                       3\n2                       6\n3                     196\n4                      40\n5                     254\n6                      15\n\n\nVisualisierung:\n\nd4 %>% \n  ggplot(aes(x = word_count_per_sentence)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.5.2 (red = needs update)\n✔ insight     0.18.4    ✖ datawizard  0.6.1  \n✔ bayestestR  0.13.0    ✖ performance 0.9.2  \n✖ parameters  0.18.2    ✖ effectsize  0.7.0.5\n✔ modelbased  0.8.5     ✖ correlation 0.8.2  \n✔ see         0.7.3     ✔ report      0.5.5  \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`.\n\ndescribe_distribution(d4)\n\nVariable                |  Mean |    SD | IQR |          Range | Skewness | Kurtosis |    n | n_Missing\n-------------------------------------------------------------------------------------------------------\nword_count_per_sentence | 21.86 | 17.24 |  19 | [0.00, 254.00] |     3.84 |    37.52 | 1208 |         0\n\n\n\nCategories:\n\nr\nmap\ntidyverse"
  },
  {
    "objectID": "posts/purrr-map04/purrr-map04.html",
    "href": "posts/purrr-map04/purrr-map04.html",
    "title": "purrr-map04",
    "section": "",
    "text": "Exercise\nImportieren Sie das Grundatzprogramm der Partei AfD (in der aktuellsten Version). Tokenisieren Sie nach Seiten. Dann verschachteln Sie die Spalte, in denen der Text der Seite steht, zu einer Listenspalte. Schließlich zählen Sie die Anzahl der Wörter pro Seite und berichten gängige deskriptive Statistiken dazu.\n         \n\n\nSolution\nText aus PDF-Dateien kann man mit dem Paket pdftools einlesen:\n\nlibrary(pdftools)\n\nUsing poppler version 22.02.0\n\nd_path <- \"/Users/sebastiansaueruser/Google Drive/Literatur/_Div/Politik/afd-grundsatzprogramm-2022.pdf\"\n\nd <- tibble(text = pdf_text(d_path),\n            page = 1:length(text))\n\nZu Seiten tokenisieren brauchen wir nicht; das Datenmaterial ist bereits nach Seiten organisiert.\nJetzt “verschachteln” (to nest) wir die Spalte mit dem Text:\n\nd2 <-\n  d %>% \n  nest(data = text)\n\nhead(d2)\n\n# A tibble: 6 × 2\n   page data            \n  <int> <list>          \n1     1 <tibble [1 × 1]>\n2     2 <tibble [1 × 1]>\n3     3 <tibble [1 × 1]>\n4     4 <tibble [1 × 1]>\n5     5 <tibble [1 × 1]>\n6     6 <tibble [1 × 1]>\n\n\nDann zählen wir die Wörter pro Seite:\n\nd3 <-\n  d2 %>% \n  mutate(word_count_per_page = map(data, ~ str_count(.x$text, \"\\\\w+\")))\n\nhead(d3)\n\n# A tibble: 6 × 3\n   page data             word_count_per_page\n  <int> <list>           <list>             \n1     1 <tibble [1 × 1]> <int [1]>          \n2     2 <tibble [1 × 1]> <int [1]>          \n3     3 <tibble [1 × 1]> <int [1]>          \n4     4 <tibble [1 × 1]> <int [1]>          \n5     5 <tibble [1 × 1]> <int [1]>          \n6     6 <tibble [1 × 1]> <int [1]>          \n\n\nWie sieht eine Zelle aus data aus?\n\nd3$data[[1]]\n\n# A tibble: 1 × 1\n  text                                                                          \n  <chr>                                                                         \n1 \"PROGRAMM FÜR\\nDEUTSCHLAND.\\nDas Grundsatzprogramm der Alternative für Deutsc…\n\n\nWie sieht eine Zelle aus word_count_per_page aus?\n\nd3$data[[1]]\n\n# A tibble: 1 × 1\n  text                                                                          \n  <chr>                                                                         \n1 \"PROGRAMM FÜR\\nDEUTSCHLAND.\\nDas Grundsatzprogramm der Alternative für Deutsc…\n\n\nAh! Darin steckt nur eine einzelne Zahl!\n\nd3$data[[1]] %>% str()\n\ntibble [1 × 1] (S3: tbl_df/tbl/data.frame)\n $ text: chr \"PROGRAMM FÜR\\nDEUTSCHLAND.\\nDas Grundsatzprogramm der Alternative für Deutschland.\\n\"\n\n\nDas heißt, wir können vereinfachen, entschacheln:\n\nd4 <-\n  d3 %>% \n  unnest(word_count_per_page)\n\nhead(d4)\n\n# A tibble: 6 × 3\n   page data             word_count_per_page\n  <int> <list>                         <int>\n1     1 <tibble [1 × 1]>                   9\n2     2 <tibble [1 × 1]>                 410\n3     3 <tibble [1 × 1]>                 516\n4     4 <tibble [1 × 1]>                 297\n5     5 <tibble [1 × 1]>                   1\n6     6 <tibble [1 × 1]>                 414\n\n\nVisualisierung:\n\nd4 %>% \n  ggplot(aes(x = word_count_per_page)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.5.2 (red = needs update)\n✔ insight     0.18.4    ✖ datawizard  0.6.1  \n✔ bayestestR  0.13.0    ✖ performance 0.9.2  \n✖ parameters  0.18.2    ✖ effectsize  0.7.0.5\n✔ modelbased  0.8.5     ✖ correlation 0.8.2  \n✔ see         0.7.3     ✔ report      0.5.5  \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`.\n\ndescribe_distribution(d4$word_count_per_page)\n\n  Mean |     SD |    IQR |          Range | Skewness | Kurtosis |  n | n_Missing\n--------------------------------------------------------------------------------\n285.84 | 172.27 | 322.75 | [1.00, 516.00] |    -0.64 |    -1.24 | 96 |         0\n\n\n\nCategories:\n\nr\nmap\ntidyverse"
  },
  {
    "objectID": "posts/Stichprobenziehen1/Stichprobenziehen1.html",
    "href": "posts/Stichprobenziehen1/Stichprobenziehen1.html",
    "title": "Stichprobenziehen1",
    "section": "",
    "text": "Solution\nIndividuell\n\nCategories:\n\nlm\ninference\nqm2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Ans Werk, Daten!",
    "section": "",
    "text": "Autor: Sebastian Sauer\nDer Quellcode findet sich hier.\nDie Lizenz ist permissiv, s. Hinweise hier."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "datenwerk",
    "section": "",
    "text": "Nov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbayes\n\n\nbayes-box\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nqm2\n\n\nbayes\n\n\nprobability\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nsimulation\n\n\nnormal-distribution\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbayes\n\n\npost\n\n\nprobability\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbayes\n\n\nppv\n\n\nprobability\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbayes\n\n\nprobability\n\n\npost\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbayes\n\n\nppv\n\n\nprobability\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbayes\n\n\nppv\n\n\nprobability\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbayes\n\n\nprobability\n\n\nppv\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nbayes-grid\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\ndice\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntextmining\n\n\ntwitter\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntextmining\n\n\ntwitter\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nbinomial\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\ndice\n\n\nsimulation\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\ndice\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\ndice\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nbinomial\n\n\nexample\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntextmining\n\n\ntwitter\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntextmining\n\n\ntwitter\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntextmining\n\n\ntwitter\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntextmining\n\n\ntwitter\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nbayes-grid\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nbayes-grid\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nbayes-grid\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nbayes-grid\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nphilosophy\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nbayes-grid\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nconditional\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nmap\n\n\ntidyverse\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\nloop\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobabillity\n\n\ndependent\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\ndependent\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\nmeta\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprobability\n\n\n2022\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\nloop\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nmap\n\n\ntidyverse\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nmap\n\n\ntidyverse\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr\n\n\nmap\n\n\ntidyverse\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nqm2\n\n\ninference\n\n\nlm\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\neda\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\neda\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\neda\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncausal\n\n\ndag\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncorrelation\n\n\nlm\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nqm2\n\n\ninference\n\n\nuncertainty\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nqm2\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nttest\n\n\nregr\n\n\nvariable-levels\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregresssion\n\n\nen\n\n\nbayes\n\n\nfrequentist\n\n\nqm1\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstats-nutshell\n\n\nqm2\n\n\nregression\n\n\nlog\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nqm2\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nqm2\n\n\nlm\n\n\nbayes\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nbayes\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlm\n\n\ninference\n\n\nqm2\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninference\n\n\nlm\n\n\nqm2\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregresssion\n\n\nen\n\n\nbayes\n\n\nfrequentist\n\n\nqm1\n\n\nstats-nutshell\n\n\nqm2\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregresssion\n\n\nen\n\n\nbayes\n\n\nfrequentist\n\n\nqm1\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregresssion\n\n\nen\n\n\nbayes\n\n\nfrequentist\n\n\nqm1\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregr\n\n\nttest\n\n\nvariable-levels\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nqm2\n\n\nstats-nutshell\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregression\n\n\nlm\n\n\nqm2\n\n\nbayes\n\n\nadjust\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nqm2\n\n\ninference\n\n\nparameters\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlm\n\n\ninference\n\n\nqm2\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nqm2\n\n\nqm2-thema01\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Likelihood2/Likelihood2.html",
    "href": "posts/Likelihood2/Likelihood2.html",
    "title": "Likelihood2",
    "section": "",
    "text": "Der Likelihood eines Datensatzes ist definiert als das Produkt der Likelihoods aller Beobachtungen:\n\\[\\mathcal{L} = \\prod_{i=1}^n \\mathcal{L_i}\\]\nwobei die Beobachtungen bzw. ihre Likelihood als unabhängig angenommen werden: \\(\\mathcal{L_i} \\perp \\mathcal{L_j}, \\quad i \\ne j\\).\nJe größer \\(n\\), desto …….. \\(\\mathcal{L}\\)!\nFüllen Sie die Lücke!\n\n\n\ngrößer\nkleiner\nunabhängig voneinander\nkeine Aussage möglich\nkommt auf weitere, hier nicht benannte Bedingungen an"
  },
  {
    "objectID": "posts/Likelihood2/Likelihood2.html#answerlist-1",
    "href": "posts/Likelihood2/Likelihood2.html#answerlist-1",
    "title": "Likelihood2",
    "section": "Answerlist",
    "text": "Answerlist\n\nFalsch\nRichtig\nFalsch\nFalsch\nFalsch\n\n\nCategories:\n~"
  }
]